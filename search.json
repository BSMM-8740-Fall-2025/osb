[
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you’re having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you’ll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course forum to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. They’ll be able to help diagnose the issue."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Fri, Feb 18\nProposal due Fri, Mar 18\nDraft report due Fri, Apr 8\nPeer review due Fri, Apr 15\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30"
  },
  {
    "objectID": "project-description.html#timeline",
    "href": "project-description.html#timeline",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Fri, Feb 18\nProposal due Fri, Mar 18\nDraft report due Fri, Apr 8\nPeer review due Fri, Apr 15\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30"
  },
  {
    "objectID": "project-description.html#introduction",
    "href": "project-description.html#introduction",
    "title": "Project description",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group’s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio, and all components of the project must be reproducible (with the exception of the presentation).\n\nLogistics\nYou will work on the project with your lab groups.\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis\nA GitHub repository corresponding to your report\nSlides + a video presentation\nFormal peer review on another team’s project"
  },
  {
    "objectID": "project-description.html#topic-ideas",
    "href": "project-description.html#topic-ideas",
    "title": "Project description",
    "section": "Topic ideas",
    "text": "Topic ideas\nIdentify 2-3 data sets you’re interested in potentially using for the final project. If you’re unsure where to find data, you can use the list of potential data sources in the Tips + Resources section as a starting point. It may also help to think of topics you’re interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help you be successful in the project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by the teaching team.\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as “name”, “social security number”, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\nPlease ask a member of the teaching team if you’re unsure whether your data set meets the criteria.\nFor each data set, include the following:\n\nIntroduction and data\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data)\nDescribe the observations and the general characteristics being measured in the data\n\n\n\nResearch question\n\nDescribe a research question you’re interested in answering using this data.\n\n\n\nGlimpse of data\n\nUse the glimpse function to provide an overview of each data set\n\nSubmit the PDF of the topic ideas to Gradescope. Mark all pages associated with each data set."
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Project description",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is to help you think about your analysis strategy early.\nInclude the following in the proposal:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating\nthe motivation for your research question (citing any relevant literature)\nthe general research question you wish to explore\nyour hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data set you wish to explore. This includes\n\ndescription of the observations in the data set,\ndescription of how the data was originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the response variable.\nVisualization and summary statistics for the response variable.\nList of variables that will be considered as predictors\nRegression model technique (multiple linear regression and logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of your project repo, in the data folder. Link to this file from your proposal writeup.\n\n\nSubmission\nPush all of your final changes to the GitHub repo, and submit the PDF of your proposal to Gradescope.\n\n\nProposal grading\n\n\n\nTotal\n10 pts\n\n\n\n\nIntroduction\n3 pts\n\n\nData description\n2 pts\n\n\nAnalysis plan\n4 pts\n\n\nData dictionary\n1 pts\n\n\n\nEach component will be graded as follows:\n\nMeets expectations (full credit): All required elements are completed and are accurate. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nClose to expectations (half credit): There are some elements missing and/or inaccurate. There are some issues with formatting.\nDoes not meet expectations (no credit): Major elements missing. Work is not neatly formatted and would not be presentable in a professional setting."
  },
  {
    "objectID": "project-description.html#draft-report",
    "href": "project-description.html#draft-report",
    "title": "Project description",
    "section": "Draft report",
    "text": "Draft report\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model."
  },
  {
    "objectID": "project-description.html#peer-review",
    "href": "project-description.html#peer-review",
    "title": "Project description",
    "section": "Peer review",
    "text": "Peer review\nCritically reviewing others’ work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teams’s projects to review. Each team should push their draft to their GitHub repo by the due date. One lab in the following week will be dedicated to the peer review, and all reviews will be due by the end of that lab session.\nDuring the peer review process, you will be provided read-only access to your partner teams’ GitHub repos. Provide your review in the form of GitHub issues to your partner team’s GitHub repo using the issue template provided. The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team’s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\n\nPairings\n\nSection 1 - M 1:45PM - 3:00PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\nchaa_chaa_chaa\nyay_stats\nstat_over_flow\n\n\ndekk\nchaa_chaa_chaa\nyay_stats\n\n\neight\ndekk\nchaa_chaa_chaa\n\n\nhousecats\neight\ndekk\n\n\nkrafthouse\nhousecats\neight\n\n\nrrawr\nkrafthouse\nhousecats\n\n\nstat_over_flow\nrrawr\nkrafthouse\n\n\nyay_stats\nstat_over_flow\nrrawr\n\n\n\n\n\nSection 2 - M 3:30PM - 4:45PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\na_plus_plus_plus\nwe_r\ntina\n\n\npredictors\na_plus_plus_plus\nwe_r\n\n\nsixers\npredictors\na_plus_plus_plus\n\n\nsoy_nuggets\nsixers\npredictors\n\n\ntina\nsoy_nuggets\nsixers\n\n\nwe_r\ntina\nsoy_nuggets\n\n\n\n\n\nSection 3 - M 5:15PM - 6:30PM\n\n\n\n\n\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\ndown_to_earth_goats\nthe_three_musketeers\nteam_five\n\n\nginger_and_stats\ndown_to_earth_goats\nthe_three_musketeers\n\n\npineapple_wedge_and_diced_papaya\nginger_and_stats\ndown_to_earth_goats\n\n\nstatchelorettes\npineapple_wedge_and_diced_papaya\nginger_and_stats\n\n\nstatisix\nstatchelorettes\npineapple_wedge_and_diced_papaya\n\n\nstats_squad\nstatisix\nstatchelorettes\n\n\nteam_five\nstats_squad\nstatisix\n\n\nthe_three_musketeers\nteam_five\nstats_squad\n\n\n\n\n\n\nProcess and questions\nSpend ~30 mins to review each team’s project.\n\nFind your team name on the Reviewer 1 and Reviewer 2 columns.\nFor each of the columns, find the name of the team to review in the Team being reviewed column. You should already have access to this team’s repo.\nOpen the repo of the team you’re reviewing, read their project draft, and browser around the rest of their repo.\nThen, go to the Issues tab in that repo, click on New issue, and click on Get started for the Peer review issue. Fill out this issue, answering the following questions:\n\nPeer review by: [NAME OF TEAM DOING THE REVIEW]\nNames of team members that participated in this review: [FULL NAMES OF TEAM MEMBERS DOING THE REVIEW]\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nIs there anything that is unclear from the proposal?\nProvide constructive feedback on how the team might be able to improve their project. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?"
  },
  {
    "objectID": "project-description.html#written-report",
    "href": "project-description.html#written-report",
    "title": "Project description",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\nBefore you finalize your write up, make sure the printing of code chunks is off with the option echo = FALSE.\nYou will submit the PDF of your final report on Gradescope.\nThe PDF you submit must match the files in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\nClick here for a PDF of the written report rubric.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model’s predictive power is thoroughly assessed.\n\n\n\nDiscussion + Conclusion\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages."
  },
  {
    "objectID": "project-description.html#video-presentation-slides",
    "href": "project-description.html#video-presentation-slides",
    "title": "Project description",
    "section": "Video presentation + slides",
    "text": "Video presentation + slides\n\nSlides\nIn addition to the written report, your team will also create presentation slides and record a video presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nFor submission, convert these slides to a .pdf document, and submit the PDF of the slides on Gradescope.\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\nVideo presentation\nFor the video presentation, you can speak over your slide deck, similar to the lecture content videos. The video presentation must be no longer than 8 minutes. It is fine if the video is shorter than 8 minutes, but it cannot exceed 8 minutes. You may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire, then embed the video in an new discussion post on Conversations.\n\nTo upload your video to Warpwire:\n\nClick the Warpwire tab in the course Sakai site.\nClick the “+” and select “Upload files”.\nLocate the video on your computer and click to upload.\nOnce you’ve uploaded the video to Warpwire, click to share the video and copy the video’s URL. You will need this when you post the video in the discussion forum.\n\n\n\nTo post the video to the discussion forum\n\nClick the Presentations tab in the course Sakai site.\nClick the Presentations topic.\nClick “Start a new conversation”.\nMake the title “Your Team Name: Project Title”. For example, “Teaching Team: Our Awesome Presentation”.\nClick the Warpwire icon (between the table and shopping cart icons).\nSelect your video, then click “Insert 1 item.” This will embed your video in the conversation.\nUnder the video, paste the URL to your video.\nYou’re done!"
  },
  {
    "objectID": "project-description.html#presentation-comments",
    "href": "project-description.html#presentation-comments",
    "title": "Project description",
    "section": "Presentation comments",
    "text": "Presentation comments\nEach student will be assigned 2 presentations to watch. Your viewing assignments will be posted later in the semester.\nWatch the group’s video, then click “Reply” to post a question for the group. You may not post a question that’s already been asked on the discussion thread. Additionally, the question should be (i) substantive (i.e. it shouldn’t be “Why did you use a bar plot instead of a pie chart”?), (ii) demonstrate your understanding of the content from the course, and (iii) relevant to that group’s specific presentation, i.e demonstrating that you’ve watched the presentation.\nThis portion of the project will be assessed individually.\n\nPairings\nFind your team name in the first column, watch videos from teams in the second column and leave comments.\n\n\n\n\n\n\n\n\nReviewer\nFirst video to review\nSecond video to review\n\n\n\n\nGinger and Stats\nEight\nWe R\n\n\nKrafthouse\nGinger and Stats\nEight\n\n\nSoy Nuggets\nKrafthouse\nGinger and Stats\n\n\nDown To Earth Goats\nSoy Nuggets\nKrafthouse\n\n\nA+++\nDown To Earth Goats\nSoy Nuggets\n\n\nTeam Five\nA+++\nDown To Earth Goats\n\n\nRrawr\nTeam Five\nA+++\n\n\nHousecats\nRrawr\nTeam Five\n\n\nDekk\nHousecats\nRrawr\n\n\nStat OverFlow\nDekk\nHousecats\n\n\nThe Three Musketeers\nStat OverFlow\nDekk\n\n\nPredictors\nThe Three Musketeers\nStat OverFlow\n\n\nStats Squad\nPredictors\nThe Three Musketeers\n\n\nStatisix\nStats Squad\nPredictors\n\n\nSixers\nStatisix\nStats Squad\n\n\nYay Stats\nSixers\nStatisix\n\n\nTINA\nYay Stats\nSixers\n\n\nStatchelorettes\nTINA\nYay Stats\n\n\nPineapple Wedge and Diced Papaya\nStatchelorettes\nTINA\n\n\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\nStatchelorettes\n\n\nWe R\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\n\n\nEight\nWe R\nChaa Chaa Chaa"
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project description",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/previous-work: Folder that contains the topic-ideas and project-proposal files.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-description.html#peer-teamwork-evaluation",
    "href": "project-description.html#peer-teamwork-evaluation",
    "title": "Project description",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.\nIf you have concerns with the teamwork and/or contribution from any team members, please email me by the project video deadline. You only need to email me if you have concerns. Otherwise, I will assume everyone on the team equally contributed and will receive full credit for the teamwork portion of the grade."
  },
  {
    "objectID": "project-description.html#overall-grading",
    "href": "project-description.html#overall-grading",
    "title": "Project description",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n5 pts\n\n\nProject proposal\n10 pts\n\n\nPeer review\n10 pts\n\n\nWritten report\n40 pts\n\n\nSlides + video presentation\n20 pts\n\n\nReproducibility + organization\n5 pts\n\n\nVideo comments\n5 pts\n\n\nPeer teamwork evaluation\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\nLate work policy\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html",
    "href": "exams/BSMM_8740_midterm_solutions.html",
    "title": "2024-midterm",
    "section": "",
    "text": "# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, broom, rsample, ggdag, causaldata, halfmoon, ggokabeito, malcolmbarrett/causalworkshop\n  , magrittr, ggplot2, estimatr, Formula, r-causal/propensity, gt, gtExtras, timetk, modeltime)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#packages",
    "href": "exams/BSMM_8740_midterm_solutions.html#packages",
    "title": "2024-midterm",
    "section": "",
    "text": "# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, broom, rsample, ggdag, causaldata, halfmoon, ggokabeito, malcolmbarrett/causalworkshop\n  , magrittr, ggplot2, estimatr, Formula, r-causal/propensity, gt, gtExtras, timetk, modeltime)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q-1",
    "href": "exams/BSMM_8740_midterm_solutions.html#q-1",
    "title": "2024-midterm",
    "section": "Q-1",
    "text": "Q-1\nIn the context of time series, partial autocorrelation measures are:\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\n\nThe correlation between two variables, removing the effect of intervening variables"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q-2",
    "href": "exams/BSMM_8740_midterm_solutions.html#q-2",
    "title": "2024-midterm",
    "section": "Q-2",
    "text": "Q-2\nIn a causal DAG, a confounder is:\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\n\nA variable that influences both the cause and effect"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q-3",
    "href": "exams/BSMM_8740_midterm_solutions.html#q-3",
    "title": "2024-midterm",
    "section": "Q-3",
    "text": "Q-3\nStationarity in time series analysis means that:\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\n\nThe series has a constant mean and variance over time"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q-4",
    "href": "exams/BSMM_8740_midterm_solutions.html#q-4",
    "title": "2024-midterm",
    "section": "Q-4",
    "text": "Q-4\nFor the binary classifier with the confusion matrix below:\n\n\n\n\n\nThe precision of this binary classifier is approximately:\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\n\n0.74\n\nPrecision is calculated as:\nPrecision=True Positives (TP)True Positives (TP)+False Positives (FP)\n\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n where:\n\nTrue Positives (TP): The number of correctly predicted positive instances.\nFalse Positives (FP): The number of instances incorrectly predicted as positive.\n\nAlternatively, precision is the number of true positives out of all positives predicted. Here the number of true positives is 45, and the mnumber of positives predicted is 60, so the precision is 45/60 = 0.75"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q-5",
    "href": "exams/BSMM_8740_midterm_solutions.html#q-5",
    "title": "2024-midterm",
    "section": "Q-5",
    "text": "Q-5\nWhich distance metric is not commonly used in kNN classifiers?\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\n\nHamming distance\n\nThe Hamming distance measures the number of positions at which the corresponding elements of two vectors differ. It is primarily used for comparing categorical, binary, or string data, where it calculates the count of mismatches between two vectors.\nHamming distance is specialized for exact matches in binary or categorical data, which is rare in most kNN applications that rely on numerical distances in continuous space."
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q6",
    "href": "exams/BSMM_8740_midterm_solutions.html#q6",
    "title": "2024-midterm",
    "section": "Q6",
    "text": "Q6\nIn causal DAGs, what does a directed edge represent?\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\n\nCausation"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q7",
    "href": "exams/BSMM_8740_midterm_solutions.html#q7",
    "title": "2024-midterm",
    "section": "Q7",
    "text": "Q7\nHow does the kNN algorithm typically perform on very large datasets?:\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\n\nIt becomes slower due to the increased computation of distances"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q8",
    "href": "exams/BSMM_8740_midterm_solutions.html#q8",
    "title": "2024-midterm",
    "section": "Q8",
    "text": "Q8\n\n\n\n\n\nHow many open paths are in the DAG above?\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\n\n4\nA path is open if it has no non-collider nodes that have been conditioned on.\nIf a collider exists along the path, conditioning on the collider (or its descendants) can open the path.\n\n\n# define the DAG coordinates\n  coord_dag &lt;- list(\n    x = c(z2 = 0, z1 = 0, z3 = -1, x = 1, y = 2),\n    y = c(z2 = -1, z1 = 1, z3 = 0, x = 0, y = 0)\n  )\n# specify the DAG \n  dag &lt;- ggdag::dagify(\n    y ~ x + z1 + z2 + z3,\n    x ~ z3 + z1 + z2,\n    coords = coord_dag,\n    labels = labels,\n    exposure = \"x\",\n    outcome = \"y\"\n  )\n# find the paths and indicate which are open \n  dag |&gt; dagitty::paths()\n\n$paths\n[1] \"x -&gt; y\"       \"x &lt;- z1 -&gt; y\" \"x &lt;- z2 -&gt; y\" \"x &lt;- z3 -&gt; y\"\n\n$open\n[1] TRUE TRUE TRUE TRUE"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q9",
    "href": "exams/BSMM_8740_midterm_solutions.html#q9",
    "title": "2024-midterm",
    "section": "Q9",
    "text": "Q9\nWhat is the purpose of introducing a soft margin in a SVM?\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\n\nTo allow for a certain degree of misclassification in the training data\n\nIn Support Vector Machines (SVM), a soft margin is introduced to allow for some misclassification of data points in order to handle cases where the data is not perfectly linearly separable. The soft margin concept relaxes the strict requirement of a hard margin (where all points must lie on the correct side of the margin), allowing the SVM to create a more flexible and generalizable decision boundary."
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q10",
    "href": "exams/BSMM_8740_midterm_solutions.html#q10",
    "title": "2024-midterm",
    "section": "Q10",
    "text": "Q10\nWhich of the following is not a typical component of time series data?\n\n\n\n\n\n\nSOLUTION (1 point) :\n\n\n\nDelete the wrong answer(s) below\n\nTrend\nSeasonality\nCyclical"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q11",
    "href": "exams/BSMM_8740_midterm_solutions.html#q11",
    "title": "2024-midterm",
    "section": "Q11",
    "text": "Q11\nThis question uses data for the closing prices of the five major Canadian banks from 2005-08-10 to 2023-09-29. The data was obtained using the following code (the difference in the time range is due to elimination of rows with NA values:\ntidyquant::tq_get(\n  c(\"TD\",\"BMO\",\"BNS\",\"RBC\",\"CM\")\n  , get = \"stock.prices\"\n  , from = \"2000-01-01\"\n  , to = \"2023-10-01\"\n)\nThe data can be found in your data directory\n\narima_data &lt;- readr::read_csv('data/stock_data.csv', show_col_types = FALSE)\n\n(1) Plot the data using functions in the timetk package (0.5 point)\n\n\n\n\n\n\nSOLUTION :\n\n\n\n\n# A PLOT OF THE CLOSING PRICES FOR THE FIVE MAJOR CANADIAN BANKS\narima_data %&gt;%\n  tidyr::pivot_longer(-date) %&gt;%\n  timetk::plot_time_series(\n    .date_var = date\n    , .value = value\n    , .color_var = name\n    , .smooth = F\n  )\n\n\n\n\n\n\n\nThe goal is to build and evaluate an arima model to predict the stock price of CIBC (symbol ‘CM’), using the workflow we developed in class.\n(2) Create test/trains splits of the data, where the initial period is 10 years and the assessment period is 1 year. Plot the test/train series for CIBC (symbol ‘CM’). (0.5 point)\n\n\n\n\n\n\nSOLUTION :\n\n\n\n\n# DEFINITION AND A PLOT (CM) OF TEST & TRAINING SPLITS OF THE DATA\nsplits &lt;-\n  timetk::time_series_split(\n     data = arima_data\n     , initial = \"10 year\"\n     , assess = \"1 year\"\n   )\n\nsplits %&gt;%\n  timetk::tk_time_series_cv_plan() %&gt;%\n  timetk::plot_time_series_cv_plan(.date_var = date, .value = CM)\n\n\n\n\n\n\n\n(3) Define a data preprocessing recipe and a model definition. The recipe is based on the formula CM ~ ., and make sure the data argument uses the training data. The model engine should be auto_arima.\nFinally, create a workflow object containing the recipe and the model spec, and then fit the model using the training data. (1 point)\n\n\n\n\n\n\nSOLUTION :\n\n\n\n\n# A RECIPE\ntime_rec &lt;- arima_data %&gt;%\n  recipes::recipe( CM ~ ., data = rsample::training(splits))\n  \n# A MODEL SPECIFICATION\nmodel_spec_arima &lt;- modeltime::arima_reg() %&gt;%\n  parsnip::set_engine(\"auto_arima\")\n  \n# A FITTED WORKFLOW\nworkflow_fit_arima &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(time_rec) %&gt;%\n  workflows::add_model(model_spec_arima) %&gt;%\n  parsnip::fit(rsample::training(splits))\n\nfrequency = 5 observations per 1 week\n\n\n\n\n(4) Create a models table with your fitted model and a calibration table that uses the testing data. Generate a forecast with the testing data and the original arima_data. Plot the forecast. (1 point)\n\n\n\n\n\n\nSOLUTION :\n\n\n\n\n# A MODELS TABLE\nmodels_tbl &lt;- \n  modeltime::modeltime_table(workflow_fit_arima)\n\n# A CALIBRATION TABLE\ncalibration_tbl &lt;- models_tbl %&gt;%\n  modeltime::modeltime_calibrate(new_data = rsample::testing(splits))\n\n# PLOT OF THE FITTED MODEL FORECAST OF THE TRAINING DATA  \ncalibration_tbl %&gt;%\n  modeltime::modeltime_forecast(\n    new_data    = rsample::testing(splits),\n    actual_data = arima_data\n ) %&gt;%\n  modeltime::plot_modeltime_forecast(\n    .legend_max_width = 25, # For mobile screens\n    .interactive      = TRUE\n  )\n\n\n\n\n\n\n\n(5) Compute the accuracy metrics for the forecast. What is the R2R^2 (rsq) metric. (1 point)\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ncalibration_tbl %&gt;%\n  modeltime::modeltime_accuracy() %&gt;%\n  modeltime::table_modeltime_accuracy(\n    .interactive = FALSE\n  )\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nREGRESSION WITH ARIMA(0,1,0) ERRORS\nTest\n2.11\n5\n4.47\n4.83\n2.49\n0.65\n\n\n\n\n\n\n\nThe rsq metric for the fit of the arima model to the testing data is: 0.65, as can be read off the table."
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q12",
    "href": "exams/BSMM_8740_midterm_solutions.html#q12",
    "title": "2024-midterm",
    "section": "Q12",
    "text": "Q12\nExecute the following code to create simulated observational data, where D is the treatment variable and Y is the response variable.\n\nset.seed(8740)\n\nn &lt;- 800\nV &lt;- rbinom(n, 1, 0.2)\nW &lt;- 3*V + rnorm(n)\nD &lt;- V + rnorm(n)\nY &lt;- D + W^2 + 1 + rnorm(n)\nZ &lt;- D + Y + rnorm(n)\ndata_obs &lt;- tibble::tibble(V=V, W=W, D=D, Y=Y, Z=Z)\n\nIn the code below we fit several different outcome models. Compare the resulting coefficients for D. Which regressions appear to lead to unbiased estimates of the causal effect? (1.5 points)\n\n# linear model of Y on X\nlin_YX &lt;- lm(Y ~ D, data=data_obs)\n# linear model of Y on X and V\nlin_YV &lt;- lm(Y ~ D + V, data=data_obs)\n# linear model Y on X and W\nlin_YW &lt;- lm(Y ~ D + W, data=data_obs)\n\nList all valid adjustment sets for the causal structure in this data (a good first step is to sketch the causal relations between variables - you don’t need ggdag::dagify). (1.5 points)\n\n\n\n\n\n\nSOLUTION :\n\n\n\n\n# extract the coefficients for each regression\ntibble::tibble(\n  model = c('lin_YX','lin_YV','lin_YW')\n  , m = list(lin_YX,lin_YV,lin_YW )\n) %&gt;%\n  dplyr::mutate(\n    D_coefficient =\n      purrr::map_dbl(\n        m\n        , function(s){\n          s %&gt;% broom::tidy() %&gt;%\n            dplyr::filter(term == \"D\") %&gt;%\n            dplyr::pull(estimate)\n        }\n      )\n  )\n\n# A tibble: 3 × 3\n  model  m      D_coefficient\n  &lt;chr&gt;  &lt;list&gt;         &lt;dbl&gt;\n1 lin_YX &lt;lm&gt;           2.27 \n2 lin_YV &lt;lm&gt;           0.921\n3 lin_YW &lt;lm&gt;           1.30 \n\n\n\nRegressions that appear to lead to unbiased estimates of the causal effect: (lin_YV and lin_YW are less biased than linYX)\nFrom the code used to create the data, the coefficient of D is 1. lin_YV (controlling for V) gives a more accurate estimate, while lin_YW also finds an estimate close to the correct value.\nValid adjustment sets for the data used in this question:\n\n\n# specify the DAG for this question, based on the data generation mechanism\nQ10dag &lt;- ggdag::dagify(\n  W ~ V\n  , D ~ V\n  , Y ~ D + W\n  , Z ~ D + Y\n  , exposure = \"D\"\n  , outcome = \"Y\"\n)\n\n\n# Plot the DAG\nQ10dag %&gt;% ggdag::ggdag(use_text = TRUE, layout = \"time_ordered\") +\n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\nQ10dag %&gt;% ggdag::ggdag_adjustment_set(use_text = TRUE) +\n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nHere both variables V and W will block the open backdoor path from D -&gt; Y. Note that the Dag doesn’t capture the quadratic relationship of W to Y, just that Y depends on W. The quadratic relationship is why lin_YW doesn’t product a great estimate.\nIn practice you would note the difference between lin_YV and lin_YW and experiment with different regression specifications, e.g.:\n\nlm(Y ~ D + I(W^2), data=data_obs)\n\n\nCall:\nlm(formula = Y ~ D + I(W^2), data = data_obs)\n\nCoefficients:\n(Intercept)            D       I(W^2)  \n     0.9833       0.9800       1.0107"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q13",
    "href": "exams/BSMM_8740_midterm_solutions.html#q13",
    "title": "2024-midterm",
    "section": "Q13",
    "text": "Q13\nFor this question we’ll use the Spam Classification Dataset available from the UCI Machine Learning Repository. It features a collection of spam and non-spam emails represented as feature vectors, making it suitable for a logistic regression model. The data is in your data/ directory and the metadata is in the data/spambase/ directory.\nWe’ll use this data to create a model for detecting email spam using logistic regression.\n\nspam_data &lt;- readr::read_csv('data/spam.csv', show_col_types = FALSE) %&gt;% \n  tibble::as_tibble() %&gt;% \n  dplyr::mutate(type = forcats::as_factor(type))\n\n(1) Split the data into test and training sets, and create a default recipe and a default model specification. Use the glmnet engine for the model, with penalty = 0.05 & mixture = 0.5. (1 point)\n\n\n\n\n\n\nSOLUTION :\n\n\n\n\nset.seed(8740)\n\n# create test/train splits\nsplits &lt;- rsample::initial_split(spam_data)\ntrain &lt;- rsample::training(splits)\ntest &lt;- rsample::testing(splits)\n\ndefault_recipe &lt;- train %&gt;%\n  recipes::recipe(formula = type ~ .)\n  \ndefault_model &lt;- parsnip::logistic_reg(penalty = 0.05, mixture = 0.5) %&gt;%\n  parsnip::set_engine(\"glmnet\") %&gt;%\n  parsnip::set_mode(\"classification\")\n\n\n\n(2) create a default workflow object with the recipe and the model specification, fit the workflow using parnsip::fit and the training data, and then generate the testing results by applying the fit to the testing data using broom::augment . (1 point)\n\n\n\n\n\n\nSOLUTION :\n\n\n\n\ndefault_workflow &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(default_recipe) %&gt;%\n  workflows::add_model(default_model)\n  \nlm_fit &lt;- default_workflow %&gt;%\n  parsnip::fit(train)\n\ntesting_results &lt;- broom::augment(lm_fit , test)\n\n\n\n(3) Evaluate the testing results by plotting the roc_auc curve, and calculating the accuracy. (1 point)\n\n\n\n\n\n\nSOLUTION :\n\n\n\n\n# ROC_AUC PLOT\ntesting_results %&gt;% \n    yardstick::roc_curve(\n    truth = type\n    , .pred_spam\n    ) %&gt;%\n  ggplot2::autoplot() +\n  ggplot2::theme_bw(base_size = 18)\n\n\n\n\n\n\n\n\n\n# CALCULATION OF ACCURACY\ntesting_results %&gt;%\n    yardstick::roc_auc(\n    truth = type\n    , .pred_spam\n    )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.949\n\n\n\n\n(4) Is there a way you could improve the accuracy of this model? (1 point)\n\n\n\n\n\n\nSOLUTION :\n\n\n\n\nThis model could be made more accurate by tuning the meta parameters of the logistic model."
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q14",
    "href": "exams/BSMM_8740_midterm_solutions.html#q14",
    "title": "2024-midterm",
    "section": "Q14",
    "text": "Q14\n\nWhen preprocessing data for time series models, what is the function timetk::step_fourier() used for? (1 point)\nGive an example of its use in a recipe that is engineered by use with weekly data records. (1 point)\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nThe timetk::step_fourier() function is used for adding seasonality to variable(s) via sinusoidal features.\nAn example of its use in a recipe that is engineered for use with weekly data records is:\n\n\n# Sample weekly data\nset.seed(123)\nweekly_data &lt;- tibble(\n  date = seq(as.Date(\"2023-01-01\"), by = \"week\", length.out = 52),\n  sales = 100 + 10 * sin(2 * pi * seq(1, 52) / 52) + rnorm(52, sd = 5)\n)\n\n# Create a recipe\nrec &lt;- recipes::recipe(sales ~ date, data = weekly_data) %&gt;%\n  # Add Fourier terms for seasonality with a weekly frequency\n  # Use 1 or 2 harmonics for weekly seasonality depending on desired complexity\n  timetk::step_fourier(date, period = 12, K = 2)  # period = 12 weeks (quarterly seasonality for weekly data)\n\n# Prepare and bake the recipe to see the engineered features\nrec_prep &lt;- recipes::prep(rec)\nweekly_data_with_fourier &lt;- recipes::bake(rec_prep, new_data = NULL)\n\nweekly_data_with_fourier\n\n# A tibble: 52 × 6\n   date       sales date_sin12_K1 date_cos12_K1 date_sin12_K2 date_cos12_K2\n   &lt;date&gt;     &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 2023-01-01  98.4         0.295        -0.956        -0.563        0.826 \n 2 2023-01-08 101.         -0.223        -0.975         0.434        0.901 \n 3 2023-01-15 111.         -0.680        -0.733         0.997        0.0747\n 4 2023-01-22 105.         -0.956        -0.295         0.563       -0.826 \n 5 2023-01-29 106.         -0.975         0.223        -0.434       -0.901 \n 6 2023-02-05 115.         -0.733         0.680        -0.997       -0.0747\n 7 2023-02-12 110.         -0.295         0.956        -0.563        0.826 \n 8 2023-02-19 102.          0.223         0.975         0.434        0.901 \n 9 2023-02-26 105.          0.680         0.733         0.997        0.0747\n10 2023-03-05 107.          0.956         0.295         0.563       -0.826 \n# ℹ 42 more rows"
  },
  {
    "objectID": "exams/BSMM_8740_midterm_solutions.html#q15",
    "href": "exams/BSMM_8740_midterm_solutions.html#q15",
    "title": "2024-midterm",
    "section": "Q15",
    "text": "Q15\nIn a paper in the prestigious Proceedings of the National Academy of Science (PNAS) earlier this year:\n\n\n\n\n\n\nNote\n\n\n\nS. A. Rains, A. S. Richards, US state vaccine mandates did not influence COVID-19 vaccination rates but reduced uptake of COVID-19 boosters and flu vaccines compared to bans on vaccine restrictions. Proc. Natl. Acad. Sci. U.S.A. 121(8), e2313610121 (2024).\n\n\nRains & Richards performed a causal analysis and found that compared to states that banned COVID-19 vaccination requirements, states that imposed COVID-19 vaccination mandates exhibit lower adult and child uptake of flu vaccines and lower uptake of COVID-19 boosters. They included their data and their code (in R), as is best practice.\nIn their analysis, the treatment was binary (vaccine mandate (1) or ban (0)). The proportion of people in a state that had been vaccinated was included to account for the general inclination toward COVID-19 vaccination in a state (mean centered). The outcome variable reflected the proportion of eligible people in a state who had received a booster or flu shot.\nHowever in a letter to the PNAS on September 30, 2024 , the author of the letter, Jack Fitzgerald, argued that Rains & Richards had included a bad control in their analysis, a variable that biased their results.\n\n\n\n\n\n\nNote\n\n\n\nFitzgerald, J. US states that mandated COVID-19 vaccination see higher, not lower, take-up of COVID-19 boosters and flu vaccines. Proc. Natl. Acad. Sci. U.S.A. 121(41), e2403758121 (2024).\n\n\nHere is Fitzgerald’s DAG from his letter:\n\n\n\n\n\n\n\n\n\n\n\nSOLUTION (2 points):\n\n\n\nFitzgerald thought that the vaccination rate variable was the bad control, since it is a collider.\nControlling for a collider opens up the backdoor path from the treatment to the outcome through unobserved factors, for example vaccine hesitancy, and that this backdoor would bias the results. He re-ran the analyses without controlling for vaccination rate and found that the effect changed sign, from reducing booster uptake to increasing booster uptake."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Brightspace Discussions as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Brightspace Discussions), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Lou Odette at lodette@uwindsor.ca.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “BSMM-8740” in the subject line. Barring extenuating circumstances, I will respond to BSMM-8740 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at UWindsor, and the university offers resources to support students in managing daily stress and self-care. Uwindsor offers resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nMySSP: 1-844-451-9700, MySSP\noff-campus resources\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Generative Artificial Intelligence (AI) models, such as ChatGPT, Google Gemini, Claude, Jenni, Github Co-pilot, DaLL- E, and Midjourney, may be used for any lab assignment in this course with appropriate acknowledgement and citation. Examples of citing AI language models are available here. You are responsible for fact-checking statements and code composed by AI language models. Failure to acknowledge or cite GAI use will constitute academic misconduct and may be subject to discipline under Bylaw 31: Academic Integrity.\nUse of Generative Artificial Intelligence (AI) models will not be permitted for quizzes, or exams.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#can-i-use-generative-ai-for-class-assignments",
    "href": "course-faq.html#can-i-use-generative-ai-for-class-assignments",
    "title": "FAQ",
    "section": "",
    "text": "Generative Artificial Intelligence (AI) models, such as ChatGPT, Google Gemini, Claude, Jenni, Github Co-pilot, DaLL- E, and Midjourney, may be used for any lab assignment in this course with appropriate acknowledgement and citation. Examples of citing AI language models are available here. You are responsible for fact-checking statements and code composed by AI language models. Failure to acknowledge or cite GAI use will constitute academic misconduct and may be subject to discipline under Bylaw 31: Academic Integrity.\nUse of Generative Artificial Intelligence (AI) models will not be permitted for quizzes, or exams.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-lab",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-lab",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the Lab?",
    "text": "Can I use a local install of R and RStudio instead of using the Lab?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TA are always happy to provide help with any computational questions when you’re working in the lab. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.3: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment set up for you in the lab.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "supplemental/regularization-derivations.html",
    "href": "supplemental/regularization-derivations.html",
    "title": "Regularized Regression: Ridge, Lasso and Elastic Net",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes are based on this page. They are provided for students who want to dive deeper into the mathematics behind regularized regression. Additional supplemental notes will be added throughout the semester.\nIn the simple linear regression model, you have nn observations of the response variable YY with a linear combination of mm predictor variables 𝐱\\mathbf{x} where\nπ(Y=y|𝐱,𝛉)=𝒩(y|β0+𝛃′𝐱,σ2)(1)\n\\pi\\left(Y=y|\\mathbf{x,\\theta}\\right)=\\mathcal{N}\\left(\\left.y\\right|\\beta_{0}+\\mathbf{\\mathbf{\\mathbf{\\beta}}}'\\mathbf{x},\\sigma^{2}\\right)\n \\qquad(1)\nwhere θ=(β0,𝛃,σ2)\\theta=\\left(\\beta_{0},\\mathbf{\\mathbf{\\mathbf{\\beta}}},\\sigma^{2}\\right) are all the parameters of the model. The vector of parameters β1:D\\beta_{1:D} are the weights or regression coefficients. Each coefficient βi\\beta_i specifies the change in the output that we expect if the corresponding input feature xix_i changes by one unit. The term β0\\beta_0 is the offset or bias term, and specifies the output if all the inputs are zero. This captures the unconditional response, and acts as a baseline. We sometimes write the input as (1,x1,…,xD)\\left(1,x_{1},\\ldots,x_{D}\\right) so the offset can be absorbed into the weight vector.\nWe can always apply a transformation ϕ\\phi (linear or non-linear) to the input vector, replacing β\\beta with ϕ(β)\\phi(\\beta). As long as the parameters of the feature extractor are fixed, the model remain linear in the parameters even if not linear in the inputs.\nLeast squares estimation\nTo fit the linear regression model to data, we minimize the negative log-likelihood on the training set.\nNLL(β,σ2)=∑n=1Nlog[(12πσ2)12exp(−12σ2(yn−β′xn)2)]=−12σ2∑n=1N(yn−ŷn)2−Nlog(2πσ2)\n\\begin{align*}\n\\text{NLL}\\left(\\beta,\\sigma^{2}\\right) & =\\sum_{n=1}^{N}\\log\\left[\\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{\\frac{1}{2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(y_{n}-\\beta'x_{n}\\right)^{2}\\right)\\right]\\\\\n & =-\\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{N}\\left(y_{n}-\\hat{y}_{n}\\right)^{2}-N\\log\\left(2\\pi\\sigma^{2}\\right)\n\\end{align*}\nwhere the predicted response is ŷ≡β′xn\\hat{y}\\equiv\\beta'x_{n}. Focusing on just the weights, the NLL is (up to a constant):\nRSS(β)=12∑n=1N(yn−β′xn)2=12‖yn−β′xn‖2=12(yn−β′xn)′(yn−β′xn) \n\\begin{align*}\n\\text{RSS}\\left(\\beta\\right) & =\\frac{1}{2}\\sum_{n=1}^{N}\\left(y_{n}-\\beta'x_{n}\\right)^{2}=\\frac{1}{2}\\left\\Vert y_{n}-\\beta'x_{n}\\right\\Vert ^{2}=\\frac{1}{2}\\left(y_{n}-\\beta'x_{n}\\right)'\\left(y_{n}-\\beta'x_{n}\\right)\\\\\n\\end{align*}\nWe must estimate the parameter values 𝛃̂\\mathbf{\\hat{\\beta}} from the data, and using the OLS method, the loss function is\nLOLS(β̂)=∑i=1n(yi−xi′β̂)2=‖y−Xβ̂‖2(2)\n\\text{L}_{OLS}\\left(\\hat{\\beta}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{'}\\hat{\\beta}\\right)^{2}=\\left\\Vert y-X\\hat{\\beta}\\right\\Vert ^{2}\n \\qquad(2)\nwhich is minimized with the estimate\n𝛃̂OLS=(X′X)−1(X′Y)(3)\n\\hat{\\mathbf{\\beta}}_{OLS}=\\left(X'X\\right)^{-1}\\left(X'Y\\right)\n \\qquad(3)"
  },
  {
    "objectID": "supplemental/regularization-derivations.html#ridge-regression",
    "href": "supplemental/regularization-derivations.html#ridge-regression",
    "title": "Regularized Regression: Ridge, Lasso and Elastic Net",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nFrom the discussion so far we have concluded that we would like to decrease the model complexity, that is the number of predictors. We could use the forward or backward selection for this, but that way we would not be able to tell anything about the removed variables’ effect on the response. Removing predictors from the model can be seen as settings their coefficients to zero. Instead of forcing them to be exactly zero, let’s penalize them if they are too far from zero, thus enforcing them to be small in a continuous way. This way, we decrease model complexity while keeping all variables in the model. This, basically, is what Ridge Regression does.\n\nModel Specification\nIn Ridge Regression, the OLS loss function is augmented in such a way that we not only minimize the sum of squared residuals but also penalize the size of parameter estimates, in order to shrink them towards zero:\nLridge(β̂)=∑i=1n(yi−xi′β̂)2+λ∑j=1mβ̂j2=‖y−Xβ̂‖2+λ‖β̂‖2\n\\text{L}_{ridge}\\left(\\hat{\\beta}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{'}\\hat{\\beta}\\right)^{2} + \\lambda\\sum_{j=1}^{m}\\hat{\\beta}^2_j=\\left\\Vert y-X\\hat{\\beta}\\right\\Vert ^{2} + \\lambda\\left\\Vert \\hat{\\beta}\\right\\Vert ^{2}\n\nSolving this for β̂\\hat\\beta gives the the ridge regression estimates β̂ridge=(X′X+λI)−1(X′Y)\\hat\\beta_{ridge} = (X'X+\\lambda I)^{-1}(X'Y), where I denotes the identity matrix.\nThe λ\\lambda parameter is the regularization penalty. We will talk about how to choose it in the next sections of this tutorial, but for now notice that:\n\nAs λ→0,β̂ridge→β̂OLS\\lambda \\rightarrow 0, \\quad \\hat\\beta_{ridge} \\rightarrow \\hat\\beta_{OLS};\nAs λ→∞,β̂ridge→0\\lambda \\rightarrow \\infty, \\quad \\hat\\beta_{ridge} \\rightarrow 0.\n\nSo, setting λ\\lambda to 0 is the same as using the OLS, while the larger its value, the stronger is the coefficients’ size penalized.\n\n\nBias-Variance Trade-Off in Ridge Regression\nBias(𝛃̂ridge)=λ(X′X+λI)−1βVar(𝛃̂ridge)=σ2(X′X+λI)−1X′X(X′X+λI)−1\n\\begin{align*}\n\\text{Bias}\\left(\\hat{\\mathbf{\\beta}}_{ridge}\\right) & =\\lambda(X'X+\\lambda I)^{-1}\\beta\\\\\n\\text{Var}\\left(\\hat{\\mathbf{\\beta}}_{ridge}\\right) & =\\sigma^{2}(X'X+\\lambda I)^{-1}X'X(X'X+\\lambda I)^{-1}\n\\end{align*}\n\nFrom there you can see that as λ\\lambda becomes larger, the variance decreases, and the bias increases. This poses the question: how much bias are we willing to accept in order to decrease the variance? Or: what is the optimal value for λ\\lambda?\nThere are two ways we could tackle this issue. A more traditional approach would be to choose λ such that some information criterion, e.g., AIC or BIC, is the smallest. A more machine learning-like approach is to perform cross-validation and select the value of λ that minimizes the cross-validated sum of squared residuals (or some other measure). The former approach emphasizes the model’s fit to the data, while the latter is more focused on its predictive performance. Let’s discuss both.\n\n\nMinimizing Information Criteria\nThis approach boils down to estimating the model with many different values for λ\\lambda and choosing the one that minimizes the Akaike or Bayesian Information Criterion:\nAICridge=log(e′e)+2dfridgeBICridge=log(e′e)+2dfridgelogn\n\\begin{align*}\n\\text{AIC}_{\\text{ridge}} & =\\log(e'e)+2\\text{df}_{\\text{ridge}}\\\\\n\\text{BIC}_{\\text{ridge}} & =\\log(e'e)+2\\text{df}_{\\text{ridge}}\\log n\n\\end{align*}\n\nwhere dfridge\\text{df}_{\\text{ridge}} is the number of degrees of freedom. Watch out here! The number of degrees of freedom in ridge regression is different than in the regular OLS! This is often overlooked which leads to incorrect inference. In both OLS and ridge regression, degrees of freedom are equal to the trace of the so-called hat matrix, which is a matrix that maps the vector of response values to the vector of fitted values as follows: ŷ=Hy\\hat y = H y.\nIn OLS, we find that HOLS=X(X′X)−1X\\text{H}_{\\text{OLS}}=X(X′X)^{−1}X, which gives dfOLS=trHOLS=m\\text{df}_{\\text{OLS}}=\\text{tr}\\text{H}_{\\text{OLS}}=m, where mm is the number of predictor variables. In ridge regression, however, the formula for the hat matrix should include the regularization penalty: Hridge=X(X′X+λI)−1X\\text{H}_{\\text{ridge}}=X(X′X+\\lambda I)^{−1}X, which gives dfridge=trHridge\\text{df}_{\\text{ridge}}=\\text{tr}\\text{H}_{\\text{ridge}}, which is no longer equal to mm. Some ridge regression software produce information criteria based on the OLS formula. To be sure you are doing things right, it is safer to compute them manually, which is what we will do later in this tutorial.\n\n\nRidge Regression: R example\nIn R, the glmnet package contains all you need to implement ridge regression. We will use the infamous mtcars dataset as an illustration, where the task is to predict miles per gallon based on car’s other characteristics. One more thing: ridge regression assumes the predictors are standardized and the response is centered! You will see why this assumption is needed in a moment. For now, we will just standardize before modeling.\n\n# Load libraries, get data & set seed for reproducibility ---------------------\nset.seed(123)    # seef for reproducibility\nlibrary(glmnet)  # for ridge regression\nlibrary(dplyr)   # for data cleaning\nlibrary(psych)   # for function tr() to compute trace of a matrix\n\ndata(\"mtcars\")\n# Center y, X will be standardized in the modelling function\ny &lt;- mtcars %&gt;% select(mpg) %&gt;% scale(center = TRUE, scale = FALSE) %&gt;% as.matrix()\nX &lt;- mtcars %&gt;% select(-mpg) %&gt;% as.matrix()\n\n\n# Perform 10-fold cross-validation to select lambda ---------------------------\nlambdas_to_try &lt;- 10^seq(-3, 5, length.out = 100)\n# Setting alpha = 0 implements ridge regression\nridge_cv &lt;- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,\n                      standardize = TRUE, nfolds = 10)\n# Plot cross-validation results\nplot(ridge_cv)\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (β1\\beta_1) and intercept (β0\\beta_0). We obtain the estimates, β̂1\\hat{\\beta}_1 and β̂0\\hat{\\beta}_0 by finding the values that minimize the sum of squared residuals, as shown in Equation 8.\nSSR=∑i=1n[yi−ŷi]2=[yi−(β̂0+β̂1xi)]2=[yi−β̂0−β̂1xi]2(8)\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n \\qquad(8)\nRecall that we can find the values of β̂1\\hat{\\beta}_1 and β̂0\\hat{\\beta}_0 that minimize /eq-ssr by taking the partial derivatives of Equation 8 and setting them to 0. Thus, the values of β̂1\\hat{\\beta}_1 and β̂0\\hat{\\beta}_0 that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 9.\n∂SSR∂β̂1=−2∑i=1nxi(yi−β̂0−β̂1xi)∂SSR∂β̂0=−2∑i=1n(yi−β̂0−β̂1xi)(9)\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n \\qquad(9)\nThe derivation of deriving β̂0\\hat{\\beta}_0 is shown in Equation 10.\n∂SSR∂β̂0=−2∑i=1n(yi−β̂0−β̂1xi)=0⇒−∑i=1n(yi+β̂0+β̂1xi)=0⇒−∑i=1nyi+nβ̂0+β̂1∑i=1nxi=0⇒nβ̂0=∑i=1nyi−β̂1∑i=1nxi⇒β̂0=1n(∑i=1nyi−β̂1∑i=1nxi)⇒β̂0=y‾−β̂1x‾(10)\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n \\qquad(10)\nThe derivation of β̂1\\hat{\\beta}_1 using the β̂0\\hat{\\beta}_0 we just derived is shown in Equation 11.\n∂SSR∂β̂1=−2∑i=1nxi(yi−β̂0−β̂1xi)=0⇒−∑i=1nxiyi+β̂0∑i=1nxi+β̂1∑i=1nxi2=0(Fill in β̂0)⇒−∑i=1nxiyi+(y‾−β̂1x‾)∑i=1nxi+β̂1∑i=1nxi2=0⇒(y‾−β̂1x‾)∑i=1nxi+β̂1∑i=1nxi2=∑i=1nxiyi⇒y‾∑i=1nxi−β̂1x‾∑i=1nxi+β̂1∑i=1nxi2=∑i=1nxiyi⇒ny‾x‾−β̂1nx‾2+β̂1∑i=1nxi2=∑i=1nxiyi⇒β̂1∑i=1nxi2−β̂1nx‾2=∑i=1nxiyi−ny‾x‾⇒β̂1(∑i=1nxi2−nx‾2)=∑i=1nxiyi−ny‾x‾β̂1=∑i=1nxiyi−ny‾x‾∑i=1nxi2−nx‾2(11)\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n \\qquad(11)\nTo write β̂1\\hat{\\beta}_1 in a form that’s more recognizable, we will use the following:\n∑xiyi−ny‾x‾=∑(x−x‾)(y−y‾)=(n−1)Cov(x,y)(12)\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n \\qquad(12)\n∑xi2−nx‾2−∑(x−x‾)2=(n−1)sx2(13)\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n \\qquad(13)\nwhere Cov(x,y)\\text{Cov}(x,y) is the covariance of xx and yy, and sx2s_x^2 is the sample variance of xx (sxs_x is the sample standard deviation).\nThus, applying Equation 12 and Equation 13, we have\nβ̂1=∑i=1nxiyi−ny‾x‾∑i=1nxi2−nx‾2=∑i=1n(x−x‾)(y−y‾)∑i=1n(x−x‾)2=(n−1)Cov(x,y)(n−1)sx2=Cov(x,y)sx2(14)\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n \\qquad(14)\nThe correlation between xx and yy is r=Cov(x,y)sxsyr = \\frac{\\text{Cov}(x,y)}{s_x s_y}. Thus, Cov(x,y)=rsxsy\\text{Cov}(x,y) = r s_xs_y. Plugging this into Equation 14, we have\nβ̂1=Cov(x,y)sx2=rsysxsx2=rsysx(15)\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n \\qquad(15)"
  },
  {
    "objectID": "supplemental/PCA_and_component_selection.html",
    "href": "supplemental/PCA_and_component_selection.html",
    "title": "Principal Component Analysis (PCA) and Component Selection",
    "section": "",
    "text": "Note\n\n\n\nThe PCA material here is taken from Probabilistic View of Principal Component Analysis\nWe’ll assume all columns in our data have been normalized - with zero mean and unit standard deviation.\n\n\n\n\nFor any square matrix AA, xx is an eigenvector of AA and λ\\lambda the corresponding eigenvalue of AA if Ax=λxAx=\\lambda x. Suppose AA has a full set of NN independent eigenvectors (most matrices do, but not all).\nIf we put the eigenvectors into a matrix QQ (the eigenvectors are the column vectors of the matrix), then AQ=QΛAQ=Q\\Lambda, where Λ\\Lambda is a diagonal matrix, with the eigenvalues on the diagonal. Thus\nA=QΛQ−1=QΛQ⊤(1)\nA = Q\\Lambda Q^{-1} =  Q\\Lambda Q^\\top\n \\qquad(1)\nThis is the Eigenvalue Decomposition: a square N×NN\\times N matrix (AA) which is diagonalizable can be factorized as:\nA=QΛQ⊤\nA = Q\\Lambda Q^\\top \n\nwhere QQ is the square N×NN\\times N matrix whose iith column is the eigenvector qiq_i of BB, and Λ\\Lambda is the diagonal matrix whose diagonal elements are the corresponding eigenvalues. QQ is square and orthogonal (Q=Q−1=Q⊤Q=Q^{-1}=Q^\\top), because the eigenvectors are orthogonal.\n\n\n\nIf AA is not square we need a different decomposition. Suppose AA is an N×DN\\times D matrix (NN rows and DD columns) - then we need a square, orthogonal N×NN\\times N matrix VV to multiply on the right, and a square, orthogonal D×DD\\times D matrix UU to multiply on the left. In which case our matrix (say A) can be factorized as:\nA=UΣV⊤(2)\nA = U\\Sigma V^\\top \n \\qquad(2)\nΣ\\Sigma will then be an N×DN\\times D matrix where the D×DD\\times D subset will be diagonal with rr singular values σii∈{1,2,…,r}\\sigma_i\\;i\\in\\{1,2,\\ldots,r\\} and the remaining entries will be zero.\nWe have\nA=UΣV⊤=σ1u1v1⊤+σ2u2v2⊤+…+σrurvr⊤\nA = U\\Sigma V^\\top =\\sigma_1u_1v_1^\\top + \\sigma_2u_2v_2^\\top + \\ldots+\\sigma_ru_rv_r^\\top\n Note that this is a sum of matrices. The first is the best rank 1 approximation to AA; the first kk is the best rank kk approximation to AA\n\n\n\nThe PCA decomposition requires that one compute the eigenvalues and eigenvectors of the covariance matrix of AA (again an N×DN\\times D matrix), which is the product 1n−1AA⊤\\frac{1}{n-1}AA^\\top. Since the covariance matrix is symmetric, the matrix is diagonalizable, and the eigenvectors can be normalized such that they are orthonormal.\nThe square corvariance matrix (1n−1AA⊤\\frac{1}{n-1}AA^\\top) is symmetric and thus diagonizable, and so from equation (Equation 1), it can be factorized as:\n1n−1AA⊤=QΛQ⊤\n\\frac{1}{n-1}AA^\\top = Q\\Lambda Q^\\top \n where Λ\\Lambda is the diagonal matrix with the eigenvalues of the covariance matrix and QQ has column vectors that are the eigenvectors of the covariance matrix.\nHowever, using the SVD (Equation 2) we can also write\n1n−1AA⊤=1n−1(UΣV⊤)(VΣU⊤)=UΣ2U⊤\n\\frac{1}{n-1}AA^\\top = \\frac{1}{n-1} \\left(U\\Sigma V^\\top\\right)\\left(V\\Sigma U^\\top\\right) = U\\Sigma^2U^\\top\n\nUsing the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the formation of AA⊤AA^\\top can cause loss of precision.\nHere is a simple PCA example:\n\nDataCorrelationPCA\n\n\n\n\nCode\nset.seed(1) # For data reproducibility\n# create some data\ndat1 &lt;- tibble::tibble(\n  x = rnorm(50, 50, sd = 3)\n  , y = .5*x + rnorm(50, sd = sqrt(3))\n)\n# plot it\ndat1 |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point(,color = \"blue\", size = 2) +\n  xlab(\"Variable 1\") +\n  ylab(\"Variable 2\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n# center and\ndat1 &lt;- dat1 |&gt;  \n  dplyr::mutate(x = x - mean(x), y = y - mean(y))\n# convert to matrix\ndat2 &lt;- dat1 |&gt; dplyr::select(x,y) |&gt; as.matrix()\n\n\n\n\n\n# Calculate the covariance matrix\ncov_m &lt;- (t(dat2) %*% dat2) / (nrow(dat2) - 1) \ncov_m\n\n         x        y\nx 6.220943 2.946877\ny 2.946877 4.207523\n\n\n\n# we can also use the cov function in base R\ncov(dat2)\n\n         x        y\nx 6.220943 2.946877\ny 2.946877 4.207523\n\n\n\n\n\n\nCode\n# Use eigen() to obtain eigenvectors and eigenvalues\ncov_e &lt;- eigen(cov_m)\n\n# Eigenvectors\ne_vec &lt;- cov_e$vectors\n\n# Eigenvalues\ne_val &lt;- cov_e$values\n\n# First eigenvector \nev_1 &lt;- e_vec[,1]\n\n# Second eigenvector \nev_2 &lt;- e_vec[,2]\n\n# eigenvectors are orthogonal and eigenvalues capture total variance\nc( ev_1 %*% ev_2, e_val, e_val |&gt; sum(), cov_m |&gt; diag() |&gt; sum())\n\n\n[1] 2.749766e-17 8.328321e+00 2.100145e+00 1.042847e+01 1.042847e+01\n\n\n\n\n\nFor a positive semi-definite matrix SVD and eigendecomposition are equivalent. PCA boils down to the eigendecomposition of the covariance matrix. Finding the maximum eigenvalue(s) and corresponding eigenvector(s) can be thought of as finding the direction of maximum variance.\nIf we have a lot of data (many rows or many columns or both), we’ll have a large covariance matrix and large number of eigenvalues and their corresponding eigenvectors (though there can be duplicates).\nDo we need them all? How many are just due to noise or measurement error? First look at random matrices, then at covariance matrices formed from random matrices.\n\n\n\nLet’s perform an experiment, generating a large random N×NN\\times N data set using N(0,1)N(0,1) measurements.\n\n\neigenvalues from random symmetric matrix (Normally distributed measurements)\nrequire(ggplot2, quietly=TRUE)\n\n# 5000 rows and columns\nn &lt;- 5000\n# generate n^2 samples from N(0,1)\nm &lt;- array( rnorm(n^2) ,c(n,n))\n# make it symmetric\nm2 &lt;- (m + t(m))/sqrt(2*n) # t(m) %*% m\n# compute eigenvalues and vectors\nlambda &lt;- eigen(m2, symmetric=T, only.values = T)\n\n# plot the eignevalues\ntibble::tibble(lambda = lambda$values) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  labs(x = 'eigenvalues', title = 'Normal random symmetric matrix') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLet’s do the same, but with uniform U(0,1)U(0,1) distributed measurements.\n\n\neigenvalues from random symmetric matrix (Uniformly distributed measurements)\n# 5000 rows and columns\nn &lt;- 5000\n# generate n^2 samples from U(0,1)\nm &lt;- array( runif(n^2) ,c(n,n))\n# make it symmetric\nm2 &lt;- sqrt(12)*(m + t(m) -1)/sqrt(2*n) # t(m) %*% m\n# compute eigenvalues and vectors\nlambda &lt;- eigen(m2, symmetric=T, only.values = T)\n\n# plot the eignevalues\ntibble::tibble(lambda = lambda$values) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  labs(x = 'eigenvalues', title = 'Uniform random symmetric matrix') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNote the striking pattern: the density of eigenvalues is a semicircle.\n\n\n\nLet Ã\\tilde{A} be an N×NN\\times N matrix with entries Ãi,j∼𝒩(0,σ2)\\tilde{A}_{i,j}\\sim\\mathcal{N}\\left(0,\\sigma^2\\right). Define\nAN=1N(A+A⊤2)\nA_N=\\frac{1}{\\sqrt{N}}\\left(\\frac{A+A^\\top}{2}\\right)\n then ANA_N is symmetric with variance\nVar[ai,j]={σ2/Nifi≠jσ2/Nifi=j\n\\mathrm{Var}\\left[a_{i,j}\\right]=\\left\\{ \\begin{array}{cc}\n\\sigma^{2}/N & \\mathrm{if}\\,i\\ne j\\\\\n\\sigma^{2}/N & \\mathrm{if}\\,i=j\n\\end{array}\\right.\n and the density of the eigenvalues of ANA_N is given by\nρN(λ)≡1N∑i=1Nδ(λ−λj)\n\\rho_N\\left(\\lambda\\right)\\equiv\\frac{1}{N}\\sum_{i=1}^N\\delta\\left(\\lambda-\\lambda_j\\right)\n which, as shown by Wigner, as\nn→∞→12πσ24σ2−α2if|λ|≤2σ0otherwise\nn\\rightarrow\\infty\\rightarrow\\begin{array}{cc}\n\\frac{1}{2\\pi\\sigma^{2}}\\sqrt{4\\sigma^{2}-\\alpha^{2}} & \\mathrm{if}\\,\\left|\\lambda\\right|\\le2\\sigma\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\n\n\n\nWe have MM variables with TT rows. The elements of the M×MM\\times M empirical correlation matrix EE are given by:\nEi,j=1T∑t=1Txi,jxj,i\nE_{i,j}=\\frac{1}{T}\\sum_{t=1}^Tx_{i,j}x_{j,i}\n where xi,jx_{i,j} denotes the jj-th (normalized) value of variable ii. This can be written as E=H⊤HE=H^\\top H where HH is the T×MT\\times M dataset.\nAssuming the values of HH are random with variance σ2\\sigma^2 then in the limit T,M→∞T,M\\rightarrow\\infty, while keeping the ratio Q≡TM≥1Q\\equiv\\frac{T}{M}\\ge1 constant, the density of eigenvalues of EE is given by\nρ(λ)=Q2πσ2(λ+−λ)(λ−λ−)λ\n\\rho\\left(\\lambda\\right) = \\frac{Q}{2\\pi\\sigma^2}\\frac{\\sqrt{\\left(\\lambda_+-\\lambda\\right)\\left(\\lambda-\\lambda_-\\right)}}{\\lambda}\n where the minimum and maximum eigenvalues are given by\nλ±=σ2(1±1Q)2\n\\lambda_\\pm=\\sigma^2\\left(1\\pm\\sqrt{\\frac{1}{Q}}\\right)^2\n\nis also known as the Marchenko-Pastur distribution that describes the asymptotic behavior of eigenvalues of large random correlation matrices.\n\n\ncode for Marchenko-Pastur distribution\nmpd &lt;- function(lambda,T,M,sigma=1){\n  Q &lt;- T/M\n  lambda_plus  &lt;- (1+sqrt(1/Q))^2 * sigma^2\n  lambda_minus &lt;- (1-sqrt(1/Q))^2 * sigma^2\n  if(lambda &lt; lambda_minus | lambda &gt; lambda_plus){\n    0\n  }else{\n    (Q/(2*pi*sigma^2)) * sqrt((lambda_plus-lambda)*(lambda-lambda_minus)) / lambda\n  }\n}\n\n\n\nM = 1000, T = 5000M = 100, T = 500M = 10, T = 50\n\n\n\n\nCode\nt &lt;- 5000;\nm &lt;- 1000;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=TRUE, only.values = TRUE);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nt &lt;- 500;\nm &lt;- 100;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=T, only.values = T);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=30) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nt &lt;- 50;\nm &lt;- 10;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=T, only.values = T);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=15) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()",
    "crumbs": [
      "Supplemental notes",
      "PCA and component selection"
    ]
  },
  {
    "objectID": "supplemental/PCA_and_component_selection.html#pca-short-version",
    "href": "supplemental/PCA_and_component_selection.html#pca-short-version",
    "title": "Principal Component Analysis (PCA) and Component Selection",
    "section": "",
    "text": "Note\n\n\n\nThe PCA material here is taken from Probabilistic View of Principal Component Analysis\nWe’ll assume all columns in our data have been normalized - with zero mean and unit standard deviation.\n\n\n\n\nFor any square matrix AA, xx is an eigenvector of AA and λ\\lambda the corresponding eigenvalue of AA if Ax=λxAx=\\lambda x. Suppose AA has a full set of NN independent eigenvectors (most matrices do, but not all).\nIf we put the eigenvectors into a matrix QQ (the eigenvectors are the column vectors of the matrix), then AQ=QΛAQ=Q\\Lambda, where Λ\\Lambda is a diagonal matrix, with the eigenvalues on the diagonal. Thus\nA=QΛQ−1=QΛQ⊤(1)\nA = Q\\Lambda Q^{-1} =  Q\\Lambda Q^\\top\n \\qquad(1)\nThis is the Eigenvalue Decomposition: a square N×NN\\times N matrix (AA) which is diagonalizable can be factorized as:\nA=QΛQ⊤\nA = Q\\Lambda Q^\\top \n\nwhere QQ is the square N×NN\\times N matrix whose iith column is the eigenvector qiq_i of BB, and Λ\\Lambda is the diagonal matrix whose diagonal elements are the corresponding eigenvalues. QQ is square and orthogonal (Q=Q−1=Q⊤Q=Q^{-1}=Q^\\top), because the eigenvectors are orthogonal.\n\n\n\nIf AA is not square we need a different decomposition. Suppose AA is an N×DN\\times D matrix (NN rows and DD columns) - then we need a square, orthogonal N×NN\\times N matrix VV to multiply on the right, and a square, orthogonal D×DD\\times D matrix UU to multiply on the left. In which case our matrix (say A) can be factorized as:\nA=UΣV⊤(2)\nA = U\\Sigma V^\\top \n \\qquad(2)\nΣ\\Sigma will then be an N×DN\\times D matrix where the D×DD\\times D subset will be diagonal with rr singular values σii∈{1,2,…,r}\\sigma_i\\;i\\in\\{1,2,\\ldots,r\\} and the remaining entries will be zero.\nWe have\nA=UΣV⊤=σ1u1v1⊤+σ2u2v2⊤+…+σrurvr⊤\nA = U\\Sigma V^\\top =\\sigma_1u_1v_1^\\top + \\sigma_2u_2v_2^\\top + \\ldots+\\sigma_ru_rv_r^\\top\n Note that this is a sum of matrices. The first is the best rank 1 approximation to AA; the first kk is the best rank kk approximation to AA\n\n\n\nThe PCA decomposition requires that one compute the eigenvalues and eigenvectors of the covariance matrix of AA (again an N×DN\\times D matrix), which is the product 1n−1AA⊤\\frac{1}{n-1}AA^\\top. Since the covariance matrix is symmetric, the matrix is diagonalizable, and the eigenvectors can be normalized such that they are orthonormal.\nThe square corvariance matrix (1n−1AA⊤\\frac{1}{n-1}AA^\\top) is symmetric and thus diagonizable, and so from equation (Equation 1), it can be factorized as:\n1n−1AA⊤=QΛQ⊤\n\\frac{1}{n-1}AA^\\top = Q\\Lambda Q^\\top \n where Λ\\Lambda is the diagonal matrix with the eigenvalues of the covariance matrix and QQ has column vectors that are the eigenvectors of the covariance matrix.\nHowever, using the SVD (Equation 2) we can also write\n1n−1AA⊤=1n−1(UΣV⊤)(VΣU⊤)=UΣ2U⊤\n\\frac{1}{n-1}AA^\\top = \\frac{1}{n-1} \\left(U\\Sigma V^\\top\\right)\\left(V\\Sigma U^\\top\\right) = U\\Sigma^2U^\\top\n\nUsing the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the formation of AA⊤AA^\\top can cause loss of precision.\nHere is a simple PCA example:\n\nDataCorrelationPCA\n\n\n\n\nCode\nset.seed(1) # For data reproducibility\n# create some data\ndat1 &lt;- tibble::tibble(\n  x = rnorm(50, 50, sd = 3)\n  , y = .5*x + rnorm(50, sd = sqrt(3))\n)\n# plot it\ndat1 |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point(,color = \"blue\", size = 2) +\n  xlab(\"Variable 1\") +\n  ylab(\"Variable 2\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n# center and\ndat1 &lt;- dat1 |&gt;  \n  dplyr::mutate(x = x - mean(x), y = y - mean(y))\n# convert to matrix\ndat2 &lt;- dat1 |&gt; dplyr::select(x,y) |&gt; as.matrix()\n\n\n\n\n\n# Calculate the covariance matrix\ncov_m &lt;- (t(dat2) %*% dat2) / (nrow(dat2) - 1) \ncov_m\n\n         x        y\nx 6.220943 2.946877\ny 2.946877 4.207523\n\n\n\n# we can also use the cov function in base R\ncov(dat2)\n\n         x        y\nx 6.220943 2.946877\ny 2.946877 4.207523\n\n\n\n\n\n\nCode\n# Use eigen() to obtain eigenvectors and eigenvalues\ncov_e &lt;- eigen(cov_m)\n\n# Eigenvectors\ne_vec &lt;- cov_e$vectors\n\n# Eigenvalues\ne_val &lt;- cov_e$values\n\n# First eigenvector \nev_1 &lt;- e_vec[,1]\n\n# Second eigenvector \nev_2 &lt;- e_vec[,2]\n\n# eigenvectors are orthogonal and eigenvalues capture total variance\nc( ev_1 %*% ev_2, e_val, e_val |&gt; sum(), cov_m |&gt; diag() |&gt; sum())\n\n\n[1] 2.749766e-17 8.328321e+00 2.100145e+00 1.042847e+01 1.042847e+01\n\n\n\n\n\nFor a positive semi-definite matrix SVD and eigendecomposition are equivalent. PCA boils down to the eigendecomposition of the covariance matrix. Finding the maximum eigenvalue(s) and corresponding eigenvector(s) can be thought of as finding the direction of maximum variance.\nIf we have a lot of data (many rows or many columns or both), we’ll have a large covariance matrix and large number of eigenvalues and their corresponding eigenvectors (though there can be duplicates).\nDo we need them all? How many are just due to noise or measurement error? First look at random matrices, then at covariance matrices formed from random matrices.\n\n\n\nLet’s perform an experiment, generating a large random N×NN\\times N data set using N(0,1)N(0,1) measurements.\n\n\neigenvalues from random symmetric matrix (Normally distributed measurements)\nrequire(ggplot2, quietly=TRUE)\n\n# 5000 rows and columns\nn &lt;- 5000\n# generate n^2 samples from N(0,1)\nm &lt;- array( rnorm(n^2) ,c(n,n))\n# make it symmetric\nm2 &lt;- (m + t(m))/sqrt(2*n) # t(m) %*% m\n# compute eigenvalues and vectors\nlambda &lt;- eigen(m2, symmetric=T, only.values = T)\n\n# plot the eignevalues\ntibble::tibble(lambda = lambda$values) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  labs(x = 'eigenvalues', title = 'Normal random symmetric matrix') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLet’s do the same, but with uniform U(0,1)U(0,1) distributed measurements.\n\n\neigenvalues from random symmetric matrix (Uniformly distributed measurements)\n# 5000 rows and columns\nn &lt;- 5000\n# generate n^2 samples from U(0,1)\nm &lt;- array( runif(n^2) ,c(n,n))\n# make it symmetric\nm2 &lt;- sqrt(12)*(m + t(m) -1)/sqrt(2*n) # t(m) %*% m\n# compute eigenvalues and vectors\nlambda &lt;- eigen(m2, symmetric=T, only.values = T)\n\n# plot the eignevalues\ntibble::tibble(lambda = lambda$values) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  labs(x = 'eigenvalues', title = 'Uniform random symmetric matrix') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNote the striking pattern: the density of eigenvalues is a semicircle.\n\n\n\nLet Ã\\tilde{A} be an N×NN\\times N matrix with entries Ãi,j∼𝒩(0,σ2)\\tilde{A}_{i,j}\\sim\\mathcal{N}\\left(0,\\sigma^2\\right). Define\nAN=1N(A+A⊤2)\nA_N=\\frac{1}{\\sqrt{N}}\\left(\\frac{A+A^\\top}{2}\\right)\n then ANA_N is symmetric with variance\nVar[ai,j]={σ2/Nifi≠jσ2/Nifi=j\n\\mathrm{Var}\\left[a_{i,j}\\right]=\\left\\{ \\begin{array}{cc}\n\\sigma^{2}/N & \\mathrm{if}\\,i\\ne j\\\\\n\\sigma^{2}/N & \\mathrm{if}\\,i=j\n\\end{array}\\right.\n and the density of the eigenvalues of ANA_N is given by\nρN(λ)≡1N∑i=1Nδ(λ−λj)\n\\rho_N\\left(\\lambda\\right)\\equiv\\frac{1}{N}\\sum_{i=1}^N\\delta\\left(\\lambda-\\lambda_j\\right)\n which, as shown by Wigner, as\nn→∞→12πσ24σ2−α2if|λ|≤2σ0otherwise\nn\\rightarrow\\infty\\rightarrow\\begin{array}{cc}\n\\frac{1}{2\\pi\\sigma^{2}}\\sqrt{4\\sigma^{2}-\\alpha^{2}} & \\mathrm{if}\\,\\left|\\lambda\\right|\\le2\\sigma\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\n\n\n\nWe have MM variables with TT rows. The elements of the M×MM\\times M empirical correlation matrix EE are given by:\nEi,j=1T∑t=1Txi,jxj,i\nE_{i,j}=\\frac{1}{T}\\sum_{t=1}^Tx_{i,j}x_{j,i}\n where xi,jx_{i,j} denotes the jj-th (normalized) value of variable ii. This can be written as E=H⊤HE=H^\\top H where HH is the T×MT\\times M dataset.\nAssuming the values of HH are random with variance σ2\\sigma^2 then in the limit T,M→∞T,M\\rightarrow\\infty, while keeping the ratio Q≡TM≥1Q\\equiv\\frac{T}{M}\\ge1 constant, the density of eigenvalues of EE is given by\nρ(λ)=Q2πσ2(λ+−λ)(λ−λ−)λ\n\\rho\\left(\\lambda\\right) = \\frac{Q}{2\\pi\\sigma^2}\\frac{\\sqrt{\\left(\\lambda_+-\\lambda\\right)\\left(\\lambda-\\lambda_-\\right)}}{\\lambda}\n where the minimum and maximum eigenvalues are given by\nλ±=σ2(1±1Q)2\n\\lambda_\\pm=\\sigma^2\\left(1\\pm\\sqrt{\\frac{1}{Q}}\\right)^2\n\nis also known as the Marchenko-Pastur distribution that describes the asymptotic behavior of eigenvalues of large random correlation matrices.\n\n\ncode for Marchenko-Pastur distribution\nmpd &lt;- function(lambda,T,M,sigma=1){\n  Q &lt;- T/M\n  lambda_plus  &lt;- (1+sqrt(1/Q))^2 * sigma^2\n  lambda_minus &lt;- (1-sqrt(1/Q))^2 * sigma^2\n  if(lambda &lt; lambda_minus | lambda &gt; lambda_plus){\n    0\n  }else{\n    (Q/(2*pi*sigma^2)) * sqrt((lambda_plus-lambda)*(lambda-lambda_minus)) / lambda\n  }\n}\n\n\n\nM = 1000, T = 5000M = 100, T = 500M = 10, T = 50\n\n\n\n\nCode\nt &lt;- 5000;\nm &lt;- 1000;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=TRUE, only.values = TRUE);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nt &lt;- 500;\nm &lt;- 100;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=T, only.values = T);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=30) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nt &lt;- 50;\nm &lt;- 10;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=T, only.values = T);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=15) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()",
    "crumbs": [
      "Supplemental notes",
      "PCA and component selection"
    ]
  },
  {
    "objectID": "supplemental/PCA_and_component_selection.html#application-to-correlation-matrices",
    "href": "supplemental/PCA_and_component_selection.html#application-to-correlation-matrices",
    "title": "Principal Component Analysis (PCA) and Component Selection",
    "section": "Application to correlation matrices",
    "text": "Application to correlation matrices\nFor the special case of correlation matrices (e.g. PCA), we know that σ2=1\\sigma^2=1 and Q=T/MQ = T/M. This bounds the probability mass over the interval defined by (1±1Q)2\\left(1\\pm\\sqrt{\\frac{1}{Q}}\\right)^2.\nSince this distribution describes the spectrum of random matrices with mean 0, the eigenvalues of correlation matrices (read PCA component weights) that fall inside of the aforementioned interval could be considered spurious or noise. For instance, obtaining a correlation matrix of 10 variables with 252 observations would render\nλ+=(1±1Q)2≈1.43\n\\lambda_+=\\left(1\\pm\\sqrt{\\frac{1}{Q}}\\right)^2\\approx1.43\n\nThus, out of 10 eigenvalues/components of said correlation matrix, only the values higher than 1.43 would be considered significantly different from random.",
    "crumbs": [
      "Supplemental notes",
      "PCA and component selection"
    ]
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\ny=β0+β1x(1)\ny = \\beta_0 + \\beta_1 x\n \\qquad(1)\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm.",
    "crumbs": [
      "Supplemental notes",
      "Log transformations"
    ]
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with log(y)\\log(y), the log-transformed yy, as the response variable. Under this model, we assume a linear relationship exists between xx and log(y)\\log(y), such that log(y)∼N(β0+β1x,σ2)\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2) for some β0\\beta_0, β1\\beta_1 and σ2\\sigma^2. In other words, we can model the relationship between xx and log(y)\\log(y) using the model in Equation 2.\nlog(y)=β0+β1x(2)\n\\log(y) = \\beta_0 + \\beta_1 x\n \\qquad(2)\nIf we interpret the model in terms of log(y)\\log(y), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable yy, since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), exp{x}=ex\\exp\\{x\\} = e^x. Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of yy.\nexp{log(y)}=exp{β0+β1x}⇒y=exp{β0+β1x}⇒y=exp{β0}exp{β1x}(3)\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n \\qquad(3)\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\nMean[log(y)]≠log[Mean(y)]\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)] - the mean and log are not commutable\nMedian[log(y)]=log[Median(y)]\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)] - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean log(y)\\log(y) for any given value of xx is β0+β1x\\beta_0 + \\beta_1 x; however, this does not indicate that the mean of y=exp{β0+β1x}y = \\exp\\{\\beta_0 + \\beta_1 x\\} (see previous section). From the assumptions of linear regression, we assume that for any given value of xx, the distribution of log(y)\\log(y) is Normal, and therefore symmetric. Thus the median of log(y)\\log(y) is equal to the mean of log(y)\\log(y), i.e Median(log(y))=β0+β1x\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x.\nSince the log and the median are commutable, Median(log(y))=β0+β1x⇒Median(y)=exp{β0+β1x}\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}. Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of yy.\nIntercept: The intercept is expected median of yy when the predictor variable equals 0. Therefore, when x=0x=0,\nlog(y)=β0+β1×0=β0⇒y=exp{β0}\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\nInterpretation: When x=0x=0, the median of yy is expected to be exp{β0}\\exp\\{\\beta_0\\}.\nSlope: The slope is the expected change in the median of yy when xx increases by 1 unit. The change in the median of yy is\nexp{[β0+β1(x+1)]−[β0+β1x]}=exp{β0+β1(x+1)}exp{β0+β1x}=exp{β0}exp{β1x}exp{β1}exp{β0}exp{β1x}=exp{β1}\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\nThus, the median of yy for x+1x+1 is exp{β1}\\exp\\{\\beta_1\\} times the median of yy for xx.\nInterpretation: When xx increases by one unit, the median of yy is expected to multiply by a factor of exp{β1}\\exp\\{\\beta_1\\}.",
    "crumbs": [
      "Supplemental notes",
      "Log transformations"
    ]
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with log(x)\\log(x), the log-transformed xx, as the predictor variable. Under this model, we assume a linear relationship exists between log(x)\\log(x) and yy, such that y∼N(β0+β1log(x),σ2)y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2) for some β0\\beta_0, β1\\beta_1 and σ2\\sigma^2. In other words, we can model the relationship between log(x)\\log(x) and yy using the model in #eq-log-x.\ny=β0+β1log(x)(4)\ny = \\beta_0 + \\beta_1 \\log(x)\n \\qquad(4)\nIntercept: The intercept is the mean of yy when log(x)=0\\log(x) = 0, i.e. x=1x = 1.\nInterpretation: When x=1x = 1 (log(x)=0)(\\log(x) = 0), the mean of yy is expected to be β0\\beta_0.\nSlope: The slope is interpreted in terms of the change in the mean of yy when xx is multiplied by a factor of CC, since log(Cx)=log(x)+log(C)\\log(Cx) = \\log(x) + \\log(C). Thus, when xx is multiplied by a factor of CC, the change in the mean of yy is\n[β0+β1log(Cx)]−[β0+β1log(x)]=β1[log(Cx)−log(x)]=β1[log(C)+log(x)−log(x)]=β1log(C)\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt] \n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt] \n& = \\beta_1 \\log(C)\n\\end{aligned}\n\nThus the mean of yy changes by β1log(C)\\beta_1 \\log(C) units.\nInterpretation: When xx is multiplied by a factor of CC, the mean of yy is expected to change by β1log(C)\\beta_1 \\log(C) units. For example, if xx is doubled, then the mean of yy is expected to change by β1log(2)\\beta_1 \\log(2) units.",
    "crumbs": [
      "Supplemental notes",
      "Log transformations"
    ]
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with log(x)\\log(x), the log-transformed xx, as the predictor variable and log(y)\\log(y), the log-transformed yy, as the response variable. Under this model, we assume a linear relationship exists between log(x)\\log(x) and log(y)\\log(y), such that log(y)∼N(β0+β1log(x),σ2)\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2) for some β0\\beta_0, β1\\beta_1 and σ2\\sigma^2. In other words, we can model the relationship between log(x)\\log(x) and log(y)\\log(y) using the model in Equation 5.\nlog(y)=β0+β1log(x)(5)\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n \\qquad(5)\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of yy (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of yy when log(x)=0\\log(x) = 0, i.e. x=1x = 1. Therefore, when log(x)=0\\log(x) = 0,\nlog(y)=β0+β1×0=β0⇒y=exp{β0}\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\nInterpretation: When x=1x = 1 (log(x)=0)(\\log(x) = 0), the median of yy is expected to be exp{β0}\\exp\\{\\beta_0\\}.\nSlope: The slope is interpreted in terms of the change in the median yy when xx is multiplied by a factor of CC, since log(Cx)=log(x)+log(C)\\log(Cx) = \\log(x) + \\log(C). Thus, when xx is multiplied by a factor of CC, the change in the median of yy is\nexp{[β0+β1log(Cx)]−[β0+β1log(x)]}=exp{β1[log(Cx)−log(x)]}=exp{β1[log(C)+log(x)−log(x)]}=exp{β1log(C)}=Cβ1\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &= \n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt] \n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt] \n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\nThus, the median of yy for CxCx is Cβ1C^{\\beta_1} times the median of yy for xx.\nInterpretation: When xx is multiplied by a factor of CC, the median of yy is expected to multiple by a factor of Cβ1C^{\\beta_1}. For example, if xx is doubled, then the median of yy is expected to multiply by 2β12^{\\beta_1}.",
    "crumbs": [
      "Supplemental notes",
      "Log transformations"
    ]
  },
  {
    "objectID": "supplemental/Bias_Variance_Trade_offs.html",
    "href": "supplemental/Bias_Variance_Trade_offs.html",
    "title": "Bias - Variance Trade-offs",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes are based on this page. They are provided for students who want to dive deeper into the mathematics behind regularized regression. Additional supplemental notes will be added throughout the semester.\nIn the simple linear regression model, you have nn observations of the response variable YY with a linear combination of mm predictor variables 𝐱\\mathbf{x} where\nπ(Y=y|𝐱,𝛉)=𝒩(y|β0+𝛃′𝐱,σ2)(1)\n\\pi\\left(Y=y|\\mathbf{x,\\theta}\\right)=\\mathcal{N}\\left(\\left.y\\right|\\beta_{0}+\\mathbf{\\mathbf{\\mathbf{\\beta}}}'\\mathbf{x},\\sigma^{2}\\right)\n \\qquad(1)\nwhere θ=(β0,𝛃,σ2)\\theta=\\left(\\beta_{0},\\mathbf{\\mathbf{\\mathbf{\\beta}}},\\sigma^{2}\\right) are all the parameters of the model. The vector of parameters β1:D\\beta_{1:D} are the weights or regression coefficients. Each coefficient βi\\beta_i specifies the change in the output that we expect if the corresponding input feature xix_i changes by one unit. The term β0\\beta_0 is the offset or bias term, and specifies the output if all the inputs are zero. This captures the unconditional response, and acts as a baseline. We sometimes write the input as (1,x1,…,xD)\\left(1,x_{1},\\ldots,x_{D}\\right) so the offset can be absorbed into the weight vector.\nWe can always apply a transformation ϕ\\phi (linear or non-linear) to the input vector, replacing β\\beta with ϕ(β)\\phi(\\beta). As long as the parameters of the feature extractor are fixed, the model remain linear in the parameters even if not linear in the inputs.\nLeast squares estimation\nTo fit the linear regression model to data, we minimize the negative log-likelihood on the training set.\nNLL(β,σ2)=∑n=1Nlog[(12πσ2)12exp(−12σ2(yn−β′xn)2)]=−12σ2∑n=1N(yn−ŷn)2−Nlog(2πσ2)\n\\begin{align*}\n\\text{NLL}\\left(\\beta,\\sigma^{2}\\right) & =\\sum_{n=1}^{N}\\log\\left[\\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{\\frac{1}{2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(y_{n}-\\beta'x_{n}\\right)^{2}\\right)\\right]\\\\\n & =-\\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{N}\\left(y_{n}-\\hat{y}_{n}\\right)^{2}-N\\log\\left(2\\pi\\sigma^{2}\\right)\n\\end{align*}\nwhere the predicted response is ŷ≡β′xn\\hat{y}\\equiv\\beta'x_{n}. Focusing on just the weights, the NLL is (up to a constant):\nRSS(β)=12∑n=1N(yn−β′xn)2=12‖yn−β′xn‖2=12(yn−β′xn)′(yn−β′xn) \n\\begin{align*}\n\\text{RSS}\\left(\\beta\\right) & =\\frac{1}{2}\\sum_{n=1}^{N}\\left(y_{n}-\\beta'x_{n}\\right)^{2}=\\frac{1}{2}\\left\\Vert y_{n}-\\beta'x_{n}\\right\\Vert ^{2}=\\frac{1}{2}\\left(y_{n}-\\beta'x_{n}\\right)'\\left(y_{n}-\\beta'x_{n}\\right)\\\\\n\\end{align*}\nWe must estimate the parameter values 𝛃̂\\mathbf{\\hat{\\beta}} from the data, and using the OLS method, the loss function is\nLOLS(β̂)=∑i=1n(yi−xi′β̂)2=‖y−Xβ̂‖2(2)\n\\text{L}_{OLS}\\left(\\hat{\\beta}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{'}\\hat{\\beta}\\right)^{2}=\\left\\Vert y-X\\hat{\\beta}\\right\\Vert ^{2}\n \\qquad(2)\nwhich is minimized with the estimate\n𝛃̂OLS=(X′X)−1(X′Y)(3)\n\\hat{\\mathbf{\\beta}}_{OLS}=\\left(X'X\\right)^{-1}\\left(X'Y\\right)\n \\qquad(3)",
    "crumbs": [
      "Supplemental notes",
      "Bias-Variance Trade offs"
    ]
  },
  {
    "objectID": "supplemental/Bias_Variance_Trade_offs.html#ridge-regression",
    "href": "supplemental/Bias_Variance_Trade_offs.html#ridge-regression",
    "title": "Bias - Variance Trade-offs",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nFrom the discussion so far we have concluded that we would like to decrease the model complexity, that is the number of predictors. We could use the forward or backward selection for this, but that way we would not be able to tell anything about the removed variables’ effect on the response. Removing predictors from the model can be seen as settings their coefficients to zero. Instead of forcing them to be exactly zero, let’s penalize them if they are too far from zero, thus enforcing them to be small in a continuous way. This way, we decrease model complexity while keeping all variables in the model. This, basically, is what Ridge Regression does.\n\nModel Specification\nIn Ridge Regression, the OLS loss function is augmented in such a way that we not only minimize the sum of squared residuals but also penalize the size of parameter estimates, in order to shrink them towards zero:\nLridge(β̂)=∑i=1n(yi−xi′β̂)2+λ∑j=1mβ̂j2=‖y−Xβ̂‖2+λ‖β̂‖2\n\\text{L}_{ridge}\\left(\\hat{\\beta}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{'}\\hat{\\beta}\\right)^{2} + \\lambda\\sum_{j=1}^{m}\\hat{\\beta}^2_j=\\left\\Vert y-X\\hat{\\beta}\\right\\Vert ^{2} + \\lambda\\left\\Vert \\hat{\\beta}\\right\\Vert ^{2}\n\nSolving this for β̂\\hat\\beta gives the the ridge regression estimates β̂ridge=(X′X+λI)−1(X′Y)\\hat\\beta_{ridge} = (X'X+\\lambda I)^{-1}(X'Y), where I denotes the identity matrix.\nThe λ\\lambda parameter is the regularization penalty. We will talk about how to choose it in the next sections of this tutorial, but for now notice that:\n\nAs λ→0,β̂ridge→β̂OLS\\lambda \\rightarrow 0, \\quad \\hat\\beta_{ridge} \\rightarrow \\hat\\beta_{OLS};\nAs λ→∞,β̂ridge→0\\lambda \\rightarrow \\infty, \\quad \\hat\\beta_{ridge} \\rightarrow 0.\n\nSo, setting λ\\lambda to 0 is the same as using the OLS, while the larger its value, the stronger is the coefficients’ size penalized.\n\n\nBias-Variance Trade-Off in Ridge Regression\nBias(𝛃̂ridge)=λ(X′X+λI)−1βVar(𝛃̂ridge)=σ2(X′X+λI)−1X′X(X′X+λI)−1\n\\begin{align*}\n\\text{Bias}\\left(\\hat{\\mathbf{\\beta}}_{ridge}\\right) & =\\lambda(X'X+\\lambda I)^{-1}\\beta\\\\\n\\text{Var}\\left(\\hat{\\mathbf{\\beta}}_{ridge}\\right) & =\\sigma^{2}(X'X+\\lambda I)^{-1}X'X(X'X+\\lambda I)^{-1}\n\\end{align*}\n\nFrom there you can see that as λ\\lambda becomes larger, the variance decreases, and the bias increases. This poses the question: how much bias are we willing to accept in order to decrease the variance? Or: what is the optimal value for λ\\lambda?\nThere are two ways we could tackle this issue. A more traditional approach would be to choose λ such that some information criterion, e.g., AIC or BIC, is the smallest. A more machine learning-like approach is to perform cross-validation and select the value of λ that minimizes the cross-validated sum of squared residuals (or some other measure). The former approach emphasizes the model’s fit to the data, while the latter is more focused on its predictive performance. Let’s discuss both.\n\n\nMinimizing Information Criteria\nThis approach boils down to estimating the model with many different values for λ\\lambda and choosing the one that minimizes the Akaike or Bayesian Information Criterion:\nAICridge=log(e′e)+2dfridgeBICridge=log(e′e)+2dfridgelogn\n\\begin{align*}\n\\text{AIC}_{\\text{ridge}} & =\\log(e'e)+2\\text{df}_{\\text{ridge}}\\\\\n\\text{BIC}_{\\text{ridge}} & =\\log(e'e)+2\\text{df}_{\\text{ridge}}\\log n\n\\end{align*}\n\nwhere dfridge\\text{df}_{\\text{ridge}} is the number of degrees of freedom. Watch out here! The number of degrees of freedom in ridge regression is different than in the regular OLS! This is often overlooked which leads to incorrect inference. In both OLS and ridge regression, degrees of freedom are equal to the trace of the so-called hat matrix, which is a matrix that maps the vector of response values to the vector of fitted values as follows: ŷ=Hy\\hat y = H y.\nIn OLS, we find that HOLS=X(X′X)−1X\\text{H}_{\\text{OLS}}=X(X′X)^{−1}X, which gives dfOLS=trHOLS=m\\text{df}_{\\text{OLS}}=\\text{tr}\\text{H}_{\\text{OLS}}=m, where mm is the number of predictor variables. In ridge regression, however, the formula for the hat matrix should include the regularization penalty: Hridge=X(X′X+λI)−1X\\text{H}_{\\text{ridge}}=X(X′X+\\lambda I)^{−1}X, which gives dfridge=trHridge\\text{df}_{\\text{ridge}}=\\text{tr}\\text{H}_{\\text{ridge}}, which is no longer equal to mm. Some ridge regression software produce information criteria based on the OLS formula. To be sure you are doing things right, it is safer to compute them manually, which is what we will do later in this tutorial.\n\n\nRidge Regression: R example\nIn R, the glmnet package contains all you need to implement ridge regression. We will use the infamous mtcars dataset as an illustration, where the task is to predict miles per gallon based on car’s other characteristics. One more thing: ridge regression assumes the predictors are standardized and the response is centered! You will see why this assumption is needed in a moment. For now, we will just standardize before modeling.\n\n# Load libraries, get data & set seed for reproducibility ---------------------\nset.seed(123)    # seef for reproducibility\nlibrary(glmnet)  # for ridge regression\nlibrary(dplyr)   # for data cleaning\nlibrary(psych)   # for function tr() to compute trace of a matrix\n\ndata(\"mtcars\")\n# Center y, X will be standardized in the modelling function\ny &lt;- mtcars %&gt;% select(mpg) %&gt;% scale(center = TRUE, scale = FALSE) %&gt;% as.matrix()\nX &lt;- mtcars %&gt;% select(-mpg) %&gt;% as.matrix()\n\n\n# Perform 10-fold cross-validation to select lambda ---------------------------\nlambdas_to_try &lt;- 10^seq(-3, 5, length.out = 100)\n# Setting alpha = 0 implements ridge regression\nridge_cv &lt;- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,\n                      standardize = TRUE, nfolds = 10)\n# Plot cross-validation results\nplot(ridge_cv)",
    "crumbs": [
      "Supplemental notes",
      "Bias-Variance Trade offs"
    ]
  },
  {
    "objectID": "supplemental/smd.html",
    "href": "supplemental/smd.html",
    "title": "Standardized Mean Differences (SMD)",
    "section": "",
    "text": "Standardized Mean Differences (SMD) are used in propensity score analysis to assess the balance of covariates between treatment and control groups. They provide a way to measure how similar the groups are in terms of observed characteristics, which is crucial for ensuring valid causal inference in observational studies.",
    "crumbs": [
      "Supplemental notes",
      "Standardized Mean Differences (SMD)"
    ]
  },
  {
    "objectID": "supplemental/smd.html#definition-and-purpose",
    "href": "supplemental/smd.html#definition-and-purpose",
    "title": "Standardized Mean Differences (SMD)",
    "section": "Definition and Purpose:",
    "text": "Definition and Purpose:\nStandardized Mean Difference:\n\nStandardized Mean Difference: a measure of effect size, used to compare the difference in means of a covariate between two groups (e.g., treatment and control groups) relative to the standard deviation of that covariate.\nPurpose: SMD is used to assess the balance of covariates in observational studies. Unlike p-values, SMD is not influenced by sample size, making it a more reliable measure of balance.",
    "crumbs": [
      "Supplemental notes",
      "Standardized Mean Differences (SMD)"
    ]
  },
  {
    "objectID": "supplemental/smd.html#calculation",
    "href": "supplemental/smd.html#calculation",
    "title": "Standardized Mean Differences (SMD)",
    "section": "Calculation",
    "text": "Calculation\nSMD=Xt‾−Xc‾SDpooled\n\\text{SMD}=\\frac{\\bar{X_t}-\\bar{X_c}}{\\text{SD}_\\text{pooled}}\n where:\n\nXt‾\\bar{X_t} is the mean of the covariate in the treatment group\nXc‾\\bar{X_c} is the mean of the covariate in the non-treatment (control) group\nSDpooled\\text{SD}_\\text{pooled} is the pooled standard deviation of the covariate across both groups.\n\nThe pooled standard deviation is calculated as\nSDpooled=(nt−1)SDt2+(nc−1)SDc2nt+nc−2\n\\text{SD}_\\text{pooled} = \\sqrt{\\frac{(n_t -1)\\text{SD}_t^2 + (n_c -1)\\text{SD}_c^2}{n_t + n_c -2}}\n where\n\nntn_t and ncn_c are the sample sizes of the treatment and control groups, respectively.\nSDt\\text{SD}_t and SDc\\text{SD}_c are the standard deviations of the covariate in the treatment and control groups, respectively.\n\nInterpretation:\n\nSMD = 0: Perfect balance. The covariate has the same mean in both groups.\nSMD &lt; 0.1: Generally considered a small and acceptable difference.\nSMD ≥ 0.1: Indicates a meaningful imbalance in the covariate between the groups. The threshold for what constitutes a “meaningful” imbalance can vary by context.\n\nUsage in Propensity Score Analysis:\n\nBefore Matching: Calculate SMD for each covariate to assess initial imbalances between treatment and control groups.\nAfter Matching: Recalculate SMDs to ensure that the propensity score matching has adequately balanced the covariates. The goal is to achieve SMDs below a certain threshold (commonly 0.1).\n\nAdvantages:\n\nNot Sample Size Dependent: Unlike statistical significance tests, SMD is not influenced by the size of the sample, making it particularly useful in large datasets.\nEasy Comparison: Provides a straightforward way to compare balance across multiple covariates.",
    "crumbs": [
      "Supplemental notes",
      "Standardized Mean Differences (SMD)"
    ]
  },
  {
    "objectID": "supplemental/collinearity.html",
    "href": "supplemental/collinearity.html",
    "title": "Collinearity and ridge regression",
    "section": "",
    "text": "In statistics, collinearity (also multicollinearity) is a phenomenon in which one feature/predictor variable in a regression model is highly correlated with another feature variable.\nIn mathematics, a set of vectors v1,v2,…,vnv_1,v_2,\\ldots,v_n (e.g. a set of column vectors of predictors) are linearly dependent if there are constants a1,a2,…,ana_1,a_2,\\ldots,a_n, not all zero, such that a1v1+a2v2+⋯+anvn=0\na_1v_1+a_2v_2+ \\cdots + a_nv_n = 0\nIn other words some combination of predictor columns (vectors), after scaling and adding them together, equal one or more other predictors.\nIf the predictors are linearly dependent, they are correlated, and this means the regression coefficients are not uniquely determined.",
    "crumbs": [
      "Supplemental notes",
      "Collinearity"
    ]
  },
  {
    "objectID": "supplemental/collinearity.html#example",
    "href": "supplemental/collinearity.html#example",
    "title": "Collinearity and ridge regression",
    "section": "Example:",
    "text": "Example:\nIn this example we will simulate what happens with linearly dependent predictors.\n\nBase example: ordinary regression\nFirst we’ll create a simple dataset with one outcome and one predictor and estimate the coefficients with repeated regressions. In the dataset described by 5+3x5 + 3x, i.e. intercept 5 and slope 3. There is no linear dependence as we have only one predictor.\n&gt; set.seed(8740)\n&gt; \n&gt; # N rows\n&gt; N &lt;- 100\n&gt; # predictor x runs from 0-5 in steps of 5/N plus a bit of noise\n&gt; x &lt;- seq( 0, 5, 5/(N-1) ) + rnorm(N, 0, 0.1)\n&gt; \n&gt; # when we regress y on x we should: \n&gt; #   - estimate an intercept close to 5 \n&gt; #   - estimate the coefficient of x as close to 3 \n&gt; dat0 &lt;- \n+   tibble::tibble( \n+     x = x \n+     # y is linearly related to x, plus some noise\n+     , y = 5 + 3*x + rnorm(N, 0, 1.5)\n+   )\n&gt; \n&gt; dat0 %&gt;% \n+   ggplot(aes(x = x, y = y)) +\n+   geom_point()\n&gt; lm(y~x, dat0)\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = y ~ x, data = dat0)\n\nCoefficients:\n(Intercept)            x  \n      5.100        2.895  \n\n\n\nWe see that an ordinary linear regression gives us results that are close to what we expect.\nNow we do the same regression on similar data (only the noise is different) and take the mean values of the coefficient estimates:\n\n&gt; # create a list with 100 elements,\n&gt; # just so we run the regression 100 times\n&gt; 1:100 %&gt;% as.list() %&gt;% \n+   # for each element of the list, run the function \n+   purrr::map(\n+     .f = function(...){ # we don't use any of arguments\n+       # run the regression on the same data\n+       tibble::tibble( \n+         x = x \n+         # y is linearly related to x, plus some noise\n+         , y = 5 + 3*x + rnorm(N, 0, 1.5)\n+       ) %&gt;% lm(y~., .) %&gt;% \n+         # extract the intercept and coefficient\n+         broom::tidy() %&gt;% \n+         dplyr::select(1:2) \n+     }\n+   ) %&gt;% \n+   # combine all the estimates\n+   dplyr::bind_rows() %&gt;% \n+   dplyr::group_by(term) %&gt;% \n+   # summarize the combined estimates\n+   dplyr::summarize(\n+     mean = mean(estimate)\n+     , variance = var(estimate) \n+   ) \n\n# A tibble: 2 × 3\n  term         mean variance\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.98  0.0705 \n2 x            3.00  0.00732\n\n\nWe see that we are means of the coefficient estimates are close to what we expect.\n\n\nBase example: ordinary regression with colinearity\nNow we create two colinear predictors a,ba,b as in the code below, where b=2×ab=2\\times a so a,ba,b are collinear, a+b=xa+b=x , and the regression estimates two coefficients βa,βb\\beta_a,\\beta_b such that\nβa×a+βb×b=βa3×x+2βb3×x→βa3+2βb3=3\n\\beta_a\\times a+ \\beta_b\\times b = \\frac{\\beta_a}{3}\\times x + \\frac{2\\beta_b}{3}\\times x \\rightarrow \\frac{\\beta_a}{3} + \\frac{2\\beta_b}{3} = 3\n\n&gt; dat1 &lt;- \n+   tibble::tibble( \n+     a = x/3 + rnorm(N,0, 0.01)\n+     , b = x*2/3 + rnorm(N,0, 0.01)\n+     , y = 5 + 3*(a+b) + rnorm(N, 0, 2)\n+   )\n&gt; \n&gt; dat1 %&gt;% \n+   ggplot(aes(x = x, y = y)) +\n+   geom_point()\n&gt; lm(y ~ a + b, dat1)\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = y ~ a + b, data = dat1)\n\nCoefficients:\n(Intercept)            a            b  \n      5.450       26.721       -9.104  \n\n\n\nSure enough, we have βa3+2βb3≈3\\frac{\\beta_a}{3} + \\frac{2\\beta_b}{3} \\approx 3. The code below performs the check:\n\n&gt; lm(y ~ a + b, dat1) %&gt;% broom::tidy() %&gt;% dplyr::select(1,2) %&gt;% \n+   tidyr::pivot_wider(names_from = term, values_from = estimate) %&gt;% \n+   dplyr::mutate(\n+     check = a/3 + 2 * b/3\n+   )\n\n# A tibble: 1 × 4\n  `(Intercept)`     a     b check\n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1          5.45  26.7 -9.10  2.84\n\n\nNote that we have two unknowns, but only one equation, so the problem does not have a unique solution, as seen by the wide variation of estimates in a repeated regression:\n\n&gt; # create a list with 100 elements,\n&gt; # just so we run the regression 100 times\n&gt; 1:100 %&gt;% as.list() %&gt;% \n+   # run the \n+   purrr::map(\n+     .f = function(...){ # we don't use any of arguments\n+       # run the regression on the same data\n+       tibble::tibble( \n+         a = x/3 + rnorm(N,0, 0.01)\n+         , b = x*2/3 + rnorm(N,0, 0.01)\n+         , y = 5 + 3*(a+b) + rnorm(N, 0, 2)\n+       ) %&gt;% lm(y~., .) %&gt;% \n+         # extract the intercept and coefficient\n+         broom::tidy() %&gt;% \n+         dplyr::select(1:2) \n+     }\n+   ) %&gt;% \n+   # combine all the estimates\n+   dplyr::bind_rows() %&gt;% \n+   dplyr::group_by(term) %&gt;% \n+   # summarize the combined estimates\n+   dplyr::summarize(\n+     mean = mean(estimate)\n+     , variance = var(estimate) \n+   ) \n\n# A tibble: 3 × 3\n  term         mean variance\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.99    0.176\n2 a            2.99  304.   \n3 b            3.01   76.2  \n\n\nGranted this is an extreme case of collinearity, but it illustrates the issue.\nHow can we mitigate the problem?\n\n\nRidge regression example:\nIndeterminancy of the predictor coefficient estimates is a symptom of collinearity, as indicated by the large variance of the estimates.\nRidge regression penalizes large predictor coefficients, and we can use it here to address the collinearity.\nFor the similar data (same relation between y and x, but different noise) under ridge regression (glmnet::glmnet with alpha=0):\n&gt; # create the dataset\n&gt; dat1 &lt;- \n+   tibble::tibble( \n+     a = x/3 + rnorm(N,0, 0.01)\n+     , b = x*2/3 + rnorm(N,0, 0.01)\n+     , y = 5 + 3*(a+b) + rnorm(N, 0, 2)\n+   )\n&gt; \n&gt; # fit with glmnet (no cross validation and a range of penalty parameters)\n&gt; fit1 = glmnet::glmnet(\n+   y = dat1$y\n+   , x = model.matrix(y ~ a + b, data = dat1)\n+   , alpha = 0\n+ )\n&gt; \n&gt; # plot the coefficient estimates as a function of the penalty parameter lambda\n&gt; plot(fit1, xvar='lambda')\n\n\n\n\n\n\n\n\n\n\n\nEven using the defaults we can see that the coefficients estimated under the l2l_2 (sum of squared coefficients) penalty are in the right ballpark. The only step remaining is to find the best penalty coefficient λ\\lambda.\nWe can do this with the built-in cross validation of cv.glmnet:: and alpha = 0, as follows:\n&gt; # fit with cv.glmnet (cross validation and a range of penalty parameters)\n&gt; fit_cv &lt;- glmnet::cv.glmnet(\n+   y = dat1$y\n+   , x = model.matrix(y ~ a + b, data = dat1)\n+   , alpha = 0\n+ )\n&gt; \n&gt; # get coefficients from fit1 with the penalty \n&gt; # generating the smallest mse\n&gt; coef(fit1, s = fit_cv$lambda.min)\n&gt; # do the check\n&gt; fit1 %&gt;% broom::tidy() %&gt;% \n+   dplyr::filter(lambda == fit_cv$lambda.min) %&gt;% \n+   dplyr::select(c(1,3)) %&gt;% \n+   tidyr::pivot_wider(names_from = term, values_from = estimate) %&gt;% \n+   dplyr::mutate(\n+     check = a/3 + 2 * b/3\n+   )\n\n\n\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                  s1\n(Intercept) 5.405189\n(Intercept) .       \na           4.528323\nb           2.198966\n\n\n# A tibble: 1 × 4\n  `(Intercept)`     a     b check\n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1          5.41  4.53  2.20  2.98\n\n\n\nSo we can see that penalized regression, and in particular ridge regression, is useful for mitigating collinearity.",
    "crumbs": [
      "Supplemental notes",
      "Collinearity"
    ]
  },
  {
    "objectID": "supplemental/did.html",
    "href": "supplemental/did.html",
    "title": "DiD",
    "section": "",
    "text": "In the canonical DiD model, we have:\n\nperiods: treatment occurs (for some units) in period 2\nIdentification of the ATT from parallel trends and no anticipation\nEstimation using sample analogs, equivalent to OLS with TWFE\nA large number of independent observations (or clusters)"
  },
  {
    "objectID": "supplemental/did.html#the-simplest-case",
    "href": "supplemental/did.html#the-simplest-case",
    "title": "DiD",
    "section": "",
    "text": "In the canonical DiD model, we have:\n\nperiods: treatment occurs (for some units) in period 2\nIdentification of the ATT from parallel trends and no anticipation\nEstimation using sample analogs, equivalent to OLS with TWFE\nA large number of independent observations (or clusters)"
  },
  {
    "objectID": "supplemental/did.html#canonical-did-with-math",
    "href": "supplemental/did.html#canonical-did-with-math",
    "title": "DiD",
    "section": "Canonical DiD – with math",
    "text": "Canonical DiD – with math\n\nPanel data on YitY_{it} for t=1,2t=1,2 and i=1,...,Ni = 1,...,N\nTreatment timing: Some units (Di=1D_i=1) are treated in period 22; everyone else is untreated (Di=0)(D_i=0)\nPotential outcomes (POs): Observe Yit(1)≡Yit(0,1)Y_{it}(1) \\equiv Y_{it}(0,1) for treated units; and Yit(0)≡Yit(0,0)Y_{it}(0) \\equiv Y_{it}(0,0) for comparison"
  },
  {
    "objectID": "supplemental/did.html#key-identifying-assumption---parallel-trends",
    "href": "supplemental/did.html#key-identifying-assumption---parallel-trends",
    "title": "DiD",
    "section": "Key Identifying Assumption - Parallel Trends",
    "text": "Key Identifying Assumption - Parallel Trends\n\nThe parallel trends assumption states that if the treatment hadn’t occurred, average outcomes for the treatment and control groups would have evolved in parallel E[Yi2(0)−Yi1(0)∣Di=1]⏟Counterfactual change for treated group=E[Yi2(0)−Yi1(0)∣Di=0]⏟Change for untreated group\\underbrace{ E[Y_{i2}(0) - Y_{i1}(0) \\mid D_i =1] }_{\\text{Counterfactual change for treated group}}= \\underbrace{ E[Y_{i2}(0) - Y_{i1}(0) \\mid D_i =0] }_{\\text{Change for untreated group}} \nThe parallel trends assumption can also be viewed as a selection bias stability assumption:\nE[Yi2(0)∣Di=1]−E[Yi2(0)∣Di=0]⏟Selection bias in period 2=E[Yi1(0)∣Di=1]−E[Yi1(0)∣Di=0]⏟Selection bias in period 1\\underbrace{ E[Y_{i2}(0)  \\mid D_i =1] - E[Y_{i2}(0) \\mid D_i=0] }_{\\text{Selection bias in period 2}}= \\underbrace{ E[Y_{i1}(0)  \\mid D_i =1] - E[Y_{i1}(0) \\mid D_i=0] }_{\\text{Selection bias in period 1}} \nPT allows for there to be selection bias! But it must be stable over time"
  },
  {
    "objectID": "supplemental/did.html#key-identifying-assumptions",
    "href": "supplemental/did.html#key-identifying-assumptions",
    "title": "DiD",
    "section": "Key identifying assumptions",
    "text": "Key identifying assumptions\n\nParallel trends\n\n𝔼[Yi2(0)−Yi1(0)|Di=1]=𝔼[Yi2(0)−Yi1(0)|Di=0](1)\n    \\begin{align}\n    \\mathbb{E}[Y_{i2}(0) - Y_{i1}(0) \\vert D_i = 1]  = \\mathbb{E}[Y_{i2}(0) - Y_{i1}(0) \\vert D_i = 0]\n    \\end{align}\n \\qquad(1)\n\nNo anticipation: Yi1(1)=Yi1(0)Y_{i1}(1) = Y_{i1}(0)\n\nIntuitively, outcome in period 1 isn’t affected by treatment status in period 2\nOften left implicit in notation, but important for interpreting DiD estimand as a causal effect in period 2"
  },
  {
    "objectID": "supplemental/did.html#identification",
    "href": "supplemental/did.html#identification",
    "title": "DiD",
    "section": "Identification",
    "text": "Identification\n\nTarget parameter: Average treatment effect on the treated (ATT) in period 2 τATT=E[Yi2(1)−Yi2(0)|Di=1]\\tau_{ATT} = E[Y_{i2}(1) - Y_{i2}(0) | D_i=1] \nUnder parallel trends and no anticipation, can show that\n\nτATT=(E[Yi2|Di=1]−E[Yi1|Di=1])⏟Change for treated−(E[Yi2|Di=0]−E[Yi1|Di=0])⏟Change for control\n\\tau_{ATT} = \\underbrace{(E[Y_{i2} | D_i = 1] - E[Y_{i1}| D_i =1])}_{\\text{Change for treated}} - \\underbrace{(E[Y_{i2} | D_i = 0] - E[Y_{i1}| D_i =0]) }_{\\text{Change for control}}\n a “difference-in-differences”” of population means"
  },
  {
    "objectID": "supplemental/did.html#proof-of-identification-argument",
    "href": "supplemental/did.html#proof-of-identification-argument",
    "title": "DiD",
    "section": "Proof of Identification Argument",
    "text": "Proof of Identification Argument\n\nStart with E[Yi2−Yi1|Di=1]−E[Yi2−Yi1|Di=0]E[Y_{i2}- Y_{i1}| D_i =1] - E[Y_{i2} - Y_{i1}| D_i =0]\nApply definition of POs to obtain: E[Yi2(1)−Yi1(1)|Di=1]−E[Yi2(0)−Yi1(0)|Di=0]E[Y_{i2}(1) - Y_{i1}(1)| D_i =1] - E[Y_{i2}(0) - Y_{i1}(0)| D_i =0]\nUse No Anticipation to substitute Yi1(0)Y_{i1}(0) for Yi1(1)Y_{i1}(1): E[Yi2(1)−Yi1(0)|Di=1]−E[Yi2(0)−Yi1(0)|Di=0]E[Y_{i2}(1) - Y_{i1}(0)| D_i =1] - E[Y_{i2}(0) - Y_{i1}(0)| D_i =0]\nAdd and subtract E[Yi2(0)|Di=1]E[ Y_{i2}(0) | D_i =1] to obtain:\n\nE[Yi2(1)−Yi2(0)|Di=1]+[(E[Yi2(0)|Di=1]−E[Yi1(0)|Di=1])−(E[Yi2(0)|Di=0]−E[Yi1(0)|Di=0])]\n        \\begin{align*}\n            & E[ Y_{i2}(1) - Y_{i2}(0) | D_i =1] + \\\\\n            & \\hspace{1cm} \\left[ (E[Y_{i2}(0) | D_i = 1] - E[Y_{i1}(0)| D_i =1]) - (E[Y_{i2}(0) | D_i = 0] - E[Y_{i1}(0)| D_i =0]) \\right]\n        \\end{align*}\n         - Cancel the last terms using PT to get E[Yi2(1)−Yi2(0)|Di=1]=τATTE[Y_{i2}(1) - Y_{i2}(0) | D_i = 1] = \\tau_{ATT}"
  },
  {
    "objectID": "supplemental/did.html#estimation-and-inference",
    "href": "supplemental/did.html#estimation-and-inference",
    "title": "DiD",
    "section": "Estimation and Inference",
    "text": "Estimation and Inference\n\nThe most conceptually simple estimator replaces population means with sample analogs: τ̂DiD=(Y‾12−Y‾11)−(Y‾02−Y‾01)\\hat{\\tau}_{DiD} = (\\bar{Y}_{12} - \\bar{Y}_{11}) - (\\bar{Y}_{02} - \\bar{Y}_{01})  where Y‾dt\\bar{Y}_{dt} is sample mean for group dd in period tt\nConveniently, τ̂DID\\hat\\tau_{DID} is algebraically equal to OLS coefficient β̂\\hat\\beta from Yit=αi+ϕt+Ditβ+ϵit,(2)\\begin{align*}\nY_{it} = \\alpha_i + \\phi_t + D_{it} \\beta  + \\epsilon_{it}, \n\\end{align*} \\qquad(2) where Dit=Di*1[t=2]D_{it} = D_i * 1[t=2]. Also equivalent to β\\beta from ΔYi=α+ΔDiβ+uit\\Delta Y_{i} = \\alpha +  \\Delta D_i \\beta + u_{it}.\nInference: And clustered standard errors are valid as number of clusters grows large"
  },
  {
    "objectID": "supplemental/did.html#characterizing-the-recent-literature",
    "href": "supplemental/did.html#characterizing-the-recent-literature",
    "title": "DiD",
    "section": "Characterizing the recent literature",
    "text": "Characterizing the recent literature\nWe can group the recent innovations in DiD lit by which elements of the canonical model they relax:\n\nMultiple periods and staggered treatment timing\nRelaxing or allowing PT to be violated\nInference with a small number of clusters"
  },
  {
    "objectID": "supplemental/did.html#staggered-timing",
    "href": "supplemental/did.html#staggered-timing",
    "title": "DiD",
    "section": "Staggered Timing",
    "text": "Staggered Timing\n\nRemember that in the canonical DiD model we had:\n\nTwo periods and a common treatment date\nIdentification from parallel trends and no anticipation\nA large number of clusters for inference\n\nA very active recent literature has focused on relaxing the first assumption: what if there are multiple periods and units adopt treatment at different times?\nThis literature typically maintains the remaining ingredients: parallel trends and many clusters"
  },
  {
    "objectID": "supplemental/did.html#overview-of-staggered-timing-literature",
    "href": "supplemental/did.html#overview-of-staggered-timing-literature",
    "title": "DiD",
    "section": "Overview of Staggered Timing Literature",
    "text": "Overview of Staggered Timing Literature\n\nNegative results: TWFE OLS doesn’t give us what we want with treatment effect heterogeneity\nNew estimators: perform better under treatment effect heterogeneity"
  },
  {
    "objectID": "supplemental/did.html#staggered-timing-set-up",
    "href": "supplemental/did.html#staggered-timing-set-up",
    "title": "DiD",
    "section": "Staggered timing set-up",
    "text": "Staggered timing set-up\n\nPanel of observations for periods t=1,...,Tt = 1,...,T\nSuppose units adopt a binary treatment at different dates Gi∈{1,...,T}∪∞G_i \\in \\{1,...,T \\} \\cup \\infty (where Gi=∞G_i = \\infty means “never-treated”)\n\nActive literature considering cases with continuous treatment & treatments that turn on/off (see Section 3.4 of review paper)\n\nPotential outcomes Yit(g)Y_{it}(g) – depend on time and time you were first-treated"
  },
  {
    "objectID": "supplemental/did.html#extending-the-identifying-assumptions",
    "href": "supplemental/did.html#extending-the-identifying-assumptions",
    "title": "DiD",
    "section": "Extending the Identifying Assumptions",
    "text": "Extending the Identifying Assumptions\n\nThe key identifying assumptions from the canonical model are extended in the natural way\nParallel trends: Intuitively, says that if treatment hadn’t happened, all “adoption cohorts’’ would have parallel average outcomes in all periods E[Yit(∞)−Yi,t−1(∞)|Gi=g]=E[Yit(∞)−Yi,t−1(∞)|Gi=g′] for all g,g′,t E[ Y_{it}(\\infty) - Y_{i,t-1}(\\infty) | G_i = g ] = E[ Y_{it}(\\infty) - Y_{i,t-1}(\\infty) | G_i = g' ] \\text{ for all } g,g',t Note: can impose slightly weaker versions (e.g. only require PT post-treatment)\nNo anticipation: Intuitively, says that treatment has no impact before it is implemented Yit(g)=Yit(∞) for all t&lt;gY_{it}(g) = Y_{it}(\\infty) \\text{ for all } t&lt;g"
  },
  {
    "objectID": "supplemental/did.html#negative-results",
    "href": "supplemental/did.html#negative-results",
    "title": "DiD",
    "section": "Negative results",
    "text": "Negative results\n\nSuppose we again run the regression Yit=αi+ϕt+Ditβ+ϵit,Y_{it} = \\alpha_i + \\phi_t + D_{it} \\beta  + \\epsilon_{it},  where Dit=1[t≥Gi]D_{it} = 1[t \\geq G_i] is a treatment indicator.\nSuppose we’re willing to assume no anticipation and parallel trends across all adoption cohorts as described above\nGood news: If each unit has a constant treatment effect over time, Yit(g)−Yit(∞)≡τiY_{it}(g) - Y_{it}(\\infty) \\equiv \\tau_i, get a weighted avg of τi\\tau_i\nBad news: if treatment effects are heterogeneous (within unit over time), then β\\beta may put negative weights on treatment effects for some units and time periods\n\nE.g., if treatment effect depends on time since treatment, Yit(t−r)−Yit(∞)=τr{Y_{it}(t-r) - Y_{it}(\\infty) = \\tau_{r}}, then some τr\\tau_r values may get negative weight"
  },
  {
    "objectID": "supplemental/did.html#where-do-these-negative-results-come-from",
    "href": "supplemental/did.html#where-do-these-negative-results-come-from",
    "title": "DiD",
    "section": "Where do these negative results come from?",
    "text": "Where do these negative results come from?\n\nThe intuition for these negative results is that the TWFE OLS specification combines two sources of comparisons:\n\nClean comparisons: DiD’s between treated and not-yet-treated units\nForbidden comparisons: DiD’s between newly-treated and already-treated units\n\nThese forbidden comparisons can lead to negative weights: the control group is already treated, so we run into problems if their treatment effects change over time"
  },
  {
    "objectID": "supplemental/did.html#some-intuition-for-forbidden-comparisons",
    "href": "supplemental/did.html#some-intuition-for-forbidden-comparisons",
    "title": "DiD",
    "section": "Some intuition for forbidden comparisons",
    "text": "Some intuition for forbidden comparisons\n\nConsider the two period model, except suppose now that our two groups are always-treated* units (treated in both periods) and switchers** (treated only in period 2)\nWith two periods, the coefficient β\\beta from Yit=αi+ϕt+Ditβ+ϵitY_{it} = \\alpha_i + \\phi_t + D_{it} \\beta  + \\epsilon_{it} is the same as from the first-differenced regression ΔYi=α+ΔDiβ+ui\\Delta Y_i = \\alpha + \\Delta D_i \\beta + u_i\nObserve that ΔDi\\Delta D_i is one for switchers and zero for stayers. That is, the stayers function as the “control group.” Thus, β̂=(Y‾Switchers,2−Y‾Switchers,1)⏟Change for switchers−(Y‾AT,2−Y‾AT,1)⏟Change for always treated\\hat\\beta =  \\underbrace{ \\left(\\bar{Y}_{Switchers, 2} - \\bar{Y}_{Switchers, 1} \\right) }_{\\text{Change for switchers}} - \\underbrace{ \\left(\\bar{Y}_{AT, 2} - \\bar{Y}_{AT, 1} \\right) }_{\\text{Change for always treated}}  \nProblem: if the effect for the always-treated grows over time, that will enter β̂\\hat\\beta negatively!\nWith staggered timing, units who are treated early are like ``always-treated’’ in later pairs of periods"
  },
  {
    "objectID": "supplemental/did.html#second-intuition-for-negative-weights",
    "href": "supplemental/did.html#second-intuition-for-negative-weights",
    "title": "DiD",
    "section": "Second Intuition for Negative Weights",
    "text": "Second Intuition for Negative Weights\nThe literature has placed a lot of emphasis on the fact that some treatment effects may get negative weights\n\nBut even if the weights are non-negative, they might not give us the most intuitive parameter\nFor example, suppose each unit ii has treatment effect τi\\tau_i in every period if they are treated (no dynamics). Then β\\beta gives a weighted average of the τi\\tau_i where the weights are largest for units treated closest to the middle of the panel\nIt is not obvious that these weights are relevant for policy, even if they are all non-negative!"
  },
  {
    "objectID": "supplemental/did.html#issues-with-dynamic-twfe",
    "href": "supplemental/did.html#issues-with-dynamic-twfe",
    "title": "DiD",
    "section": "Issues with dynamic TWFE",
    "text": "Issues with dynamic TWFE\n\n{sun_estimating_2020} show that similar issues arise with dynamic TWFE specifications: Yi,t=αi+λt+∑k≠0γkDi,tk+εi,t,\n    \\begin{equation*}\n        Y_{i,t} = \\alpha_i + \\lambda_t +  \\sum_{k \\neq 0} \\gamma_k D_{i,t}^{k} + \\varepsilon_{i,t},\n    \\end{equation*} where Di,tk=1{t−Gi=k}D_{i,t}^{k} = 1\\left\\{t-G_{i}=k\\right\\} are ``event-time’’ dummies.\nLike for the static spec, γk\\gamma_k may be a non-convex weighted average of the dynamic treatment effect kk periods after treatment\nSA also show that γk\\gamma_k may be ``contaminated’’ by treatment effects at lags k′≠kk'\\neq k"
  },
  {
    "objectID": "supplemental/did.html#dynamic-twfe---continued",
    "href": "supplemental/did.html#dynamic-twfe---continued",
    "title": "DiD",
    "section": "Dynamic TWFE - Continued",
    "text": "Dynamic TWFE - Continued\n\nThe results in SA suggest that interpreting the γ̂k\\hat\\gamma_k for k=1,2,...k=1,2,... as estimates of the dynamic effects of treatment may be misleading\nThese results also imply that pre-trends tests of the γk\\gamma_k for k&lt;0k&lt;0 may be misleading – could be non-zero even if parallel trends holds, since they may be ``contaminated’’ by post-treatment effects!\nThe issues discussed in SA arise if dynamic path of treatment effects is heterogeneous across adoption cohorts\n\nBiases may be less severe than for ``static’’ specs if dynamic patterns are similar across cohorts"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_09.html#recap-of-last-week",
    "title": "Monte Carlo Methods",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we looked at causal effect estimation methods in greater depth, including\n\ninverse probaiity weighting\nregression adjustment\ndoubly robust estimation\nmatching estimation\nDifference in Differences (DiD)\nPanel Data and fixed effects (FE)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#this-week",
    "href": "slides/BSMM_8740_lec_09.html#this-week",
    "title": "Monte Carlo Methods",
    "section": "This week",
    "text": "This week\n\nWe will explore Monte Carlo methods as a way to integrate difficult functions, and to sample from difficult probability distributions.\nAlong the way we will look at Markov Chains, which can both underlie sampling methods and provide a way to model the generation of data.\nFinally, we look at Markov Chain Monte Carlo (MCMC) methods to sample from probability distributions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#monte-carlo-mc-methods-1",
    "href": "slides/BSMM_8740_lec_09.html#monte-carlo-mc-methods-1",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo (MC) Methods",
    "text": "Monte Carlo (MC) Methods\nMonte Carlo methods are a class of simulation-based methods that seek to avoid complicated and/or intractable mathematical computations.\nEspecially those that arise from probability distributions.\nFirst we consider how ‘random numbers’ are computer generated."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nRandom numbers from a given distribution are drawn using random numbers from a (\\([0,1]\\)) uniform distribution, using the inverse transform sampling method as follows:\n\nUniform Distribution: Start by drawing a random number (\\(U\\)) from a (\\([0,1]\\)) uniform distribution. This gives us a random variable that is uniformly distributed between \\(0\\) and \\(1\\).\nCumulative Distribution Function (CDF): Every probability distribution has a CDF (\\(F_X(x)\\)), which gives the probability that a random variable (\\(X\\)) is less than or equal to (\\(x\\)).\nInverse CDF: The key idea is that if (\\(U\\)) is uniformly distributed in (\\([0,1]\\)), then (\\(F_X^{-1}(U)\\)) (the inverse of the CDF) will give us a random sample from the distribution of (\\(X\\))."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-1",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-1",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nProcess:\n\nDraw a random number (\\(U\\)) from the uniform (\\([0,1]\\)) distribution.\nUse the inverse CDF of the desired distribution: (\\(X = F_X^{-1}(U)\\)).\nThe result (X) is a random number that follows the target distribution.\n\nExample: For an exponential distribution with rate parameter (\\(\\lambda\\)), the CDF is\\(F_X(x) = 1 - e^{-\\lambda x}\\). To generate a random draw from this distribution: Draw \\(U \\sim \\mathcal{U}(0,1)\\), then use the inverse of the CDF: (\\(X = -\\frac{1}{\\lambda} \\ln(1 - U)\\)), to get an exponentially distributed random variable.\nThis method applies broadly to any distribution where you can compute or approximate the inverse CDF."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-2",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-2",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nPseudo-random generators found in computers use a deterministic (i.e. repeatable) algorithm to generate a sequence of (apparently) random numbers on the \\((0, 1)\\) interval.\nWhat defines a good random number generator (RNG) is a long period – how long it takes before the sequence repeats itself. A period of \\(2^{32}\\) is not enough (need at least \\(2^{40}\\)). Various statistical tests exist to measure “randomness” – well validated software will have gone through these checks."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-3",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-3",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nAlternatives to inverse transform sampling are special algorithms that generate random samples from particular distributions, e.g. one way to sample from a Gaussian random number generation uses the Box-Muller method:\n\ngenerate two independent uniform \\([0, 1]\\) random variables, and\nuse some trigonometry.\n\nVery important: never write your own generator, always use a well validated generator from a reputable source (e.g. R)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-4",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-4",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nRecall that \\(\\mathscr{N}(0, 1)\\) Normal random variables (mean 0, variance 1) have the probability density function:\n\\[\np(x)=\\frac{1}{2\\pi}e^{-\\frac{1}{2}x^2}\\equiv\\phi(x)\n\\] and if \\(X\\sim\\mathscr{N}(0, 1)\\) then its CDF is:\n\\[\n\\mathbb{P}[X\\le x] = \\int_{-\\infty}^x\\phi(x)dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-5",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-5",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nThe Box-Muller transformation method takes two independent uniform \\((0, 1)\\) random numbers \\(y_1\\), \\(y_2\\), and defines:\n\\[\n\\begin{align*}\nx_{1} & =\\sqrt{-2\\log y_{1}}\\cos(2\\pi y_{2})\\\\\nx_2& =\\sqrt{-2\\log y_{1}}\\sin(2\\pi y_{2})\n\\end{align*}\n\\]\nIt can be proved that \\(x_1\\) and \\(x_2\\) are independent \\(\\mathscr{N}(0, 1)\\) random variables"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-6",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-6",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\n\nCode\n# The Box-Muller transformation\nsamples &lt;- matrix(runif(10000), ncol=2) |&gt; data.frame() |&gt; \n  dplyr::mutate(\n    normals = \n      purrr::map2(\n        X1, X2\n        ,(\\(x1,x2){\n          data.frame(\n            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)\n            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) \n          )\n        })\n      )\n  ) |&gt; \n  tidyr::unnest(normals)  \n  \n\np0 &lt;- samples |&gt; \n  tidyr::pivot_longer(-c(X1,X2)) |&gt; \n  ggplot(aes(x=value, color=name, fill=name)) + \n  geom_histogram(\n    aes(y=..density..), bins = 60, position=\"identity\", alpha=0.3\n  ) + \n  labs(x=\"Value\", y=\"Density\") + theme_minimal()\n\np1 &lt;- samples |&gt; \nggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()\n\np0+p1\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in\nggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\nNormal y1 vs Normal y2; independent random RVs"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-7",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-7",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nYour computer is only capable of producing pseudorandom numbers. These are made by running a pseudorandom number generator algorithm which is deterministic, e.g.\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\nOnce the RNG seed is set, the “random” numbers that R generates aren’t random at all. But someone looking at these random numbers would have a very hard time distinguishing these numbers from truly random numbers."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nWe can use MC methods to estimate probabilities: for a random variable \\(Z\\) with outcomes in a set \\(\\Omega\\), given a subset \\(S\\subset\\Omega\\) and defining an event \\(E\\equiv Z\\in S\\), we can compute the probability of \\(E\\) (i.e. \\(\\mathbb{P}(E)\\)) with of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) as\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-1",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-1",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample\nIf \\(Z\\sim\\mathscr{N}(1,3)\\) and \\(S=Z:0\\le Z\\le 3\\) then\n\\[\n\\mathbb{P}(E) = \\mathbb{P}(0\\le Z\\le 3) = \\int_0^3\\frac{1}{\\sqrt{2\\pi3}}e^{-\\frac{(t-1)^2}{2*3}}dt\n\\] In which case it is easier to just use R and calculate:\n\n\nCode\npnorm(3, mean=1, sd=sqrt(3)) - pnorm(0, mean=1, sd=sqrt(3))\n\n\n[1] 0.594\n\n\nthan it is to compute the integral."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-2",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-2",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample continued\nIn case we don’t know that pnorm exists, we could do this:\n\n\nMC computation\n# define the event\nevent_E_happened &lt;- function( x ) {\n  if( 0 &lt;= x & x &lt;= 3 ) {\n    return( TRUE ) # The event happened\n  } else {\n    return( FALSE ) # The event DIDN'T happen\n  }\n}\nset.seed(8740)\n# generate lots of copies of Z...\nNMC &lt;- 10000; # 10000 seems like \"a lot\".\nrnorm( NMC, mean=1, sd=sqrt(3) ) |&gt; \n  purrr::map_lgl(event_E_happened) |&gt; \n  sum()/NMC\n\n\n[1] 0.5941"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-3",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-3",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nGiven event \\(E\\equiv Z\\in S\\),\n\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]\n\nis the MC estimate of \\(\\mathbb{E}[E]\\), which is unbiased because for each \\(i\\), \\(\\mathbb{E}[1_{z_i\\in S}]=\\mathbb{E}[E]\\), and the variance is\n\n\\[\n\\begin{align*}\n\\mathrm{Var}\\left({\\mathbb{P}(E)}\\right) & =\\frac{1}{M^{2}}\\mathrm{Var}\\left(\\sum_{i=1}^{M}1_{z_{i}\\in S}\\right)\\\\\n& =\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\mathrm{Var}\\left(1_{z_{i}\\in S}\\right)=\\frac{1}{M}\\mathrm{Var}\\left(1_{z_{i}\\in S}\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-4",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-4",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nThis is true for any function of the random variable \\(Z\\), and\n\\[\n\\mathbb{E}\\left[h(Z)\\right]\\approx \\hat{h}=\\frac{1}{M}\\sum_{i=1}^M h(z_i)\n\\]\nand the variance of \\(h(Z)\\) decreases as \\(1/M\\) by the same reasoning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\n\nThe \\(k\\)-th moment of a random variable \\(X\\) is the expected value of: \\(X\\) raised to the \\(k\\)-th power.\n\\[\n\\acute{\\mu}_k(X)=\\mathbb{E}[X^k]\n\\] which is known as the \\(k\\)-th raw moment.\nThe \\(k\\)-th moment of a random variable \\(X\\) around some value \\(c\\) is known as the \\(k\\)-th central moment of \\(X\\). It is the \\(k\\)-th raw moment of \\(X-c\\).\n\\[\n\\mu_k(X)=\\mathbb{E}[(X-c)^k]\n\\]\nThe \\(k\\)-th standardized moment of a random variable \\(X\\) is the \\(k\\)-th central moment of \\(X\\) divided by the \\(k\\)-th power of the standard deviation of \\(X\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-1",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-1",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nThe Taylor series expansion of \\(e^{tX}\\) is\n\\[\ne^{tX} = \\sum_{k=0}^\\infty\\frac{(tX)^k}{k!}=1+\\frac{t}{1!}X+\\frac{t^2}{2!}X^2+\\cdots\n\\]\nThe Moment Generating Function (MGF) is \\(\\mathbb{E}[e^{tX}]\\)\n\\[\n\\begin{align*}\nM_X(t)=\\mathbb{E}[e^{tX}] & =\\sum_{k=0}^{\\infty}\\mathbb{E}\\left[\\frac{(tX)^{k}}{k!}\\right]\\\\\n& =1+\\frac{t}{1!}\\mathbb{E}\\left[X\\right]+\\frac{t^{2}}{2!}\\mathbb{E}\\left[X^{2}\\right]+\\cdots\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-2",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-2",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nNote that the derivatives evaluated at \\(t=0\\) give the corresponding moments\n\\[\n\\begin{align*}\n\\left.\\frac{d^{2}}{dt^{2}}M_{X}(t)\\right|_{t=0} & =\\mathbb{E}\\left[X^{2}\\right]\\\\\n\\left.\\frac{d^{3}}{dt^{3}}M_{X}(t)\\right|_{t=0} & =\\mathbb{E}\\left[X^{3}\\right]\n\\end{align*}\n\\]\nand so \\(\\left.\\frac{d^{n}}{dt^{n}}M_{X}(t)\\right|_{t=0}=\\mathbb{E}\\left[X^{n}\\right]\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-3",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-3",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nKey facts needed for the CLT\n\nThe exponential function can be defined by the limit1:\n\\[\ne^x=\\lim_{n\\rightarrow\\infty}\\left(1+\\frac{x}{n}\\right)^n\n\\]\nA Taylor series around point \\(a\\) can be defined as2:\n\\[\nf(x)=\\sum_{n=0}^k\\frac{f^{(n)}(a)}{n!}(x-a)^n+h_k(x)(x-a)^k\n\\]\nwhere \\(\\lim_{n\\rightarrow\\infty..}\\frac{h_k(x)}{(x-a)^k}=0\\)\n\nreferencesee here"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-4",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-4",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nOther properties of the MGF\n\nif \\(Y=\\beta_0+\\beta_1X\\) then\n\n\\[\nM_Y(t)=\\mathbb{E}[e^{t(\\beta_0+\\beta_1X)}]=e^{\\beta_0t}M_X(\\beta_1t)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-5",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-5",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nOther properties of the MGF\n\nif \\(Y=X_1+X_2+\\cdots X_n\\) then \\(M_Y(t)=\\mathbb{E}[e^{t(X_1+X_2+\\cdots X_n)}]\\) and:\n\n\\[\nM_Y(t) = \\prod_{i=1}^n M_{X_i}(t)\n\\]\nif the \\(X\\) are independent, and if the \\(X\\) are IID:\n\\[\nM_Y(t) = \\left(M_{X}(t)\\right)^n\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-6",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-6",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nOther properties of the MGF1\nif \\(Y=\\mathscr{N}(0,1)\\) then starting with the Taylor series expansion of \\(e^{t^2/2}\\):\n\\[\n\\begin{align*}\ne^{t^{2}/2} & =1+\\frac{1}{2}t^{2}+\\frac{1}{2^{2}}\\frac{t^{4}}{2!}+\\frac{1}{2^{3}}\\frac{t^{6}}{3!}+\\cdots\\\\\n\\left.\\frac{d^{n}}{dt^{n}}M_{Y}(t)\\right|_{t=0} & =\\mathbb{E}\\left[Y^{n}\\right]=\\begin{cases}\n0 & n\\,\\mathrm{\\mathrm{odd}}\\\\\n2^{-n/2}\\frac{n!}{(n/2)!} & n\\,\\mathrm{even}\n\\end{cases}\\\\\nM_{Y}(t) & =e^{t^{2}/2}\n\\end{align*}\n\\]\nsee here for a derivation of the moments of the standard Normal pdf."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#central-limit-theorem",
    "href": "slides/BSMM_8740_lec_09.html#central-limit-theorem",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nLet \\(X_1+X_2+\\cdots X_n\\) be IID with mean \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\) and standardized mean \\(\\bar{Z}_n=(\\bar{X}_n-\\mu)/(\\sigma/\\sqrt{n})\\)\n\\[\n\\begin{align*}\n\\bar{Z}_{n} & =\\frac{\\bar{X}_{n}-\\mu}{\\sigma/\\sqrt{n}}=\\frac{\\sqrt{n}}{\\sigma}\\left[\\frac{X_{1}+X_{2}+\\cdots X_{n}}{n}-\\mu\\right]\\\\\n& =\\frac{\\sqrt{n}}{\\sigma}\\left[\\frac{X_{1}+X_{2}+\\cdots X_{n}-n\\mu}{n}\\right]\\\\\n& =\\sqrt{n}\\left[\\frac{\\frac{X_{1}-\\mu}{\\sigma}+\\frac{X_{2}-\\mu}{\\sigma}+\\cdots\\frac{X_{n}-\\mu}{\\sigma}}{n}\\right]=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}Z_{i}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#central-limit-theorem-1",
    "href": "slides/BSMM_8740_lec_09.html#central-limit-theorem-1",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\\[\n\\begin{align*}\nM_{\\bar{Z}_{n}}(t) & =\\left(M_{Z/\\sqrt{n}}(t)\\right)^{n}=\\left(M_{Z}(t/\\sqrt{n})\\right)^{n}\\\\\nM_{Z}(t/\\sqrt{n}) & =1+0+\\frac{\\left(t/\\sqrt{n}\\right)^{2}}{2!}+\\sum_{k=3}^{\\infty}M_{Z}^{(k)}(0)\\frac{\\left(t/\\sqrt{n}\\right)^{k}}{k!}\n\\end{align*}\n\\] Now \\(M_{Z}(t/\\sqrt{n}) = T_2(t/\\sqrt{n}) + R_2(t/\\sqrt{n})\\) where \\(T_2\\) is the Taylor series up to order 2, and \\(R_2\\) is the residual - the Taylor series terms greater than order 2."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#central-limit-theorem-2",
    "href": "slides/BSMM_8740_lec_09.html#central-limit-theorem-2",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\\[\n\\lim_{n\\rightarrow\\infty}\\frac{R_2(t/\\sqrt{n})}{(t/\\sqrt{n})^2}=\\lim_{n\\rightarrow\\infty}nR_2(t/\\sqrt{n})=0\n\\] and\n\\[\n\\begin{align*}\n\\lim_{n\\rightarrow\\infty}M_{\\bar{Z}_{n}}(t) & =\\lim_{n\\rightarrow\\infty}\\left(1+0+\\frac{\\left(t/\\sqrt{n}\\right)^{2}}{2!}+R_{2}(t/\\sqrt{n})\\right)^{n}\\\\\n& =\\lim_{n\\rightarrow\\infty}\\left(1+\\frac{1}{n}\\left(\\frac{t^{2}}{2}+nR_{2}(t/\\sqrt{n})\\right)\\right)^{n}=\\lim_{n\\rightarrow\\infty}\\left(1+\\frac{t^{2}/2}{n}\\right)^{n}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#central-limit-theorem-3",
    "href": "slides/BSMM_8740_lec_09.html#central-limit-theorem-3",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nand since\n\\[\ne^x=\\lim_{n\\rightarrow\\infty}\\left(1+\\frac{x}{n}\\right)^n\n\\]\nwe have\n\\[\n\\lim_{n\\rightarrow\\infty}M_{\\bar{Z}_{n}}(t)=\\lim_{n\\rightarrow\\infty}\\left(1+\\frac{t^{2}/2}{n}\\right)^{n}=e^{t^2/2}\n\\]\ni.e. the sum of \\(n\\) IID samples of a random variable converges to a Gaussian in the limit \\(n\\rightarrow\\infty\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#central-limit-theorem-4",
    "href": "slides/BSMM_8740_lec_09.html#central-limit-theorem-4",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT says that for \\(f=\\mathbb{P}(E)\\) if \\(\\sigma^2\\equiv\\mathrm{Var}\\left(f\\right)\\) is finite then the error of the MC estimate\n\\[\ne_N(f)=\\bar{f}-\\mathbb{E}[f]\n\\]\nis approximately Normal in distribution for large \\(M\\), i.e.\n\\[\ne_N(f)\\sim\\sigma M^{1/2}Z\n\\] where \\(Z\\sim\\mathscr{N}(0,1)\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-5",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-5",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nSuppose we need to compute an expectation \\(\\mathbb{E}[h(Z)]\\) for some random variable \\(Z\\) and some function \\(h:\\mathbb{R}\\to\\mathbb{R}\\).\nMonte Carlo methods avoid doing any integration and instead just generate many samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) and estimate \\(\\mathbb{E}[h(Z)]\\) as \\(\\frac{1}{M}\\sum_{i=1}^Mh(z_i)\\), relying on empirical probability. The law of large numbers states that this sample mean should be close to \\(\\mathbb{E}[h(Z)]\\).\nMonte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating many random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-6",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-6",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nsampling\n\nIf \\(X\\sim\\mathscr{N}(\\mu,\\sigma)\\) and we want to compute \\(\\mathbb{E}[\\log|X|]\\), we could set up and solve the integral\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[\\log|X|\\right] & =\\int_{-\\infty}^{\\infty}\\left(\\log|t|\\right)f(t;\\mu,\\sigma)dt\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{\\log|t|}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ \\frac{-(t-\\mu)^{2}}{2\\sigma^{2}}\\right\\} dt\n\\end{align*}\n\\]\nAlternatively, we could just draw lots of Monte Carlo replicates \\(X_1,X_2,\\cdots,X_M\\) from a normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and look at the sample mean \\(M^{-1}\\sum_{i=1}^M\\log|x_i|\\), once again appealing to the law of large numbers to ensure that this sample mean is close to its expectation."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-7",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-7",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nsampling\n\nSuppose that we want to compute an integral\n\\[\n\\int_Dg_0(x)dx\n\\] where \\(D\\) is some domain of integration and \\(g_0(.)\\) is a function.\nLet \\(f(x)\\) be the density of some random variable with \\(f(x)&gt;0, \\forall x\\in D\\). Then we can rewrite the integral as\n\\[\n\\int_Dg_0(x)dx = \\int_D\\frac{g_0(x)}{f(x)}f(x)dx = \\mathbb{E}_f[h(x)]\n\\]\nwhere \\(h(x)=g_0(x)/f(x)\\) and \\(X\\sim f\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-8",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-8",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nSimilarly, if we are given \\(h(x)\\) and we want to compute \\(\\mathbb{E}_f[h(X)]\\) where \\(X\\sim f,\\,x\\in D\\). What if we could not sample from \\(f\\) directly?\nIf there were some other distribution \\(g_1(x)\\) we could sample from, such that \\(g_1(x)&gt;0,\\,x\\in D\\), then\n\\[\n\\begin{align*}\n\\mathbb{E}_{f}\\left[h(x)\\right] & =\\int_{D}h(x)f(x)dx\\\\\n& =\\int_{S}h(x)\\frac{f(x)}{g_1(x)}g_1(x)dx=\\mathbb{E}_{g_1}\\left[h(x)\\frac{f(x)}{g_1(x)}\\right]\\\\\n& =\\frac{1}{n}\\sum_{i=1}^{n}h(x_{i})\\frac{f(x_{i})}{g_1(x_{i})}\\quad x_{i}\\sim g_1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-9",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-9",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis method is called importance sampling (IS).\n\ndraw iid \\(x_1,x_2,\\ldots,x_n\\) from \\(g_1\\) and calculate the importance weight\n\n\\[\nw(x_i)=\\frac{f(x_{i})}{g_1(x_{i})}\n\\] 2. estimate \\(\\mathbb{E}_f\\left[h(X)\\right]\\) by\n\\[\n\\hat{\\mu}_h=\\frac{1}{n}\\sum_{i=1}^nw(x_i)h(x_i)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-10",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-10",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nexample\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = \\sqrt{2/\\pi}e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution)\n\n\n\nintegrate the half-normal\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\n# calculate the importance weights\nW &lt;- exp(-0.5 * X^2 + 2*X) / sqrt(2 * pi)\n\nmu_h  &lt;- mean(W*X)\nvar_h &lt;- var(W*X)/n\nse_h  &lt;- sqrt(var_h)\n\ntibble::tibble(mean = mu_h,  variance = var_h, 'standard error' = se_h) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4) |&gt; \n  gt::tab_options( table.font.size = gt::px(30) ) |&gt;  \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.8208\n0.0003\n0.0186"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-11",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-11",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrepeating the prior example, but un-normalized\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution, un-normalized)\n\n\n\nintegrate the (un-normalized) half-normal\n# un-normalized weights\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\n# calculate the weights\nW &lt;- exp(-0.5 * X^2 + 2*X)\n\nmu_h2  &lt;- sum(W*X)/sum(W)\nvar_h2 &lt;- var(W/mean(W))\nse_h2  &lt;- sqrt(var_h2)\n\ntibble::tibble(mean = mu_h2,  variance = var_h2, 'standard error' = se_h2) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4) |&gt; \n  gt::tab_options( table.font.size = gt::px(30) ) |&gt;  \n  #gt::tab_header(title = )\n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.7994\n0.4183\n0.6468"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-12",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-12",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nAssume we have an un-normalized \\(g(x)\\), i.e. \\(\\pi(x)=cg(x)\\) but \\(c\\) is unknown.\nWe want to generate iid samples \\(x_1,x_2,\\ldots,x_M\\sim \\pi\\) to estimate \\(\\mathbb{E}_\\pi[h(X)]\\).\nNow assume we have an easily sampled density \\(f\\), and known \\(K&gt;0\\), such that \\(Kf(x)\\ge g(x),\\;\\forall x\\), i.e. \\(Kf(x)\\ge \\pi(x)/c\\) (or \\(cKf(x)\\ge \\pi(x)\\))."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-13",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-13",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nThen use the following procedure:\n\nsample \\(X\\sim f\\) and \\(U\\sim \\mathrm{uniform}[0,1]\\)\nif \\(U\\le\\frac{g(x)}{Kf(x)}\\), the accept \\(x\\) as a draw from \\(\\pi\\)\notherwise reject the sample and repeat"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-14",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-14",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nSince \\(0\\le\\frac{g(x)}{Kf(x)}\\le 1\\) we know that \\(\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\\)\nand so\n\n\\[\n\\begin{align*}\n\\mathbb{E}_{f}\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right] & =\\mathbb{E}_{f}\\left[\\frac{g(X)}{Kf(X)}\\right]\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{g(x)}{Kf(x)}f(x)dx\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{g(x)}{K}dx\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-15",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-15",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nSimilarly, for any \\(y\\in\\mathbb{R}\\), we can calculate the joint probability\n\\[\n\\begin{align*}\n\\mathbb{P}\\left(X\\le y,U\\le\\frac{g(X)}{Kf(X)}\\right) & =\\mathbb{E}_f\\left[1_{X\\le y}1_{U\\le\\frac{g(X)}{Kf(X)}}\\right]\\\\\n& =\\mathbb{E}_f\\left[1_{X\\le y}\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]\\\\\n& =\\mathbb{E}_f\\left[1_{X\\le y}\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^{y}\\frac{g(x)}{Kf(x)}f(x)dx\\\\\n& =\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx\n\\end{align*}\n\\]\n\nand so we have the joint probability (above - \\(\\mathbb{P}(A,B)\\)), and the probability of acceptance (previous slide - \\(\\mathbb{P}(B)\\)), so the probability, conditional on acceptance (\\(\\mathbb{P}(A|B)\\)) is \\(\\frac{\\mathbb{P}(A,B)}{\\mathbb{P}(B)}\\) by Bayes rule."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-16",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-16",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\nSubstituting for the numerator and denominator:\n\n\\[\n\\mathbb{P}\\left(X\\le y|U\\le\\frac{g(X)}{Kf(X)}\\right)=\\frac{\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx}{\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx}=\\int_{-\\infty}^{y}\\pi(x)dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-17",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-17",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nWe have \\(\\mathbb{P}(X=x) = \\sum_{i=1}^n\\mathbb{P}(\\mathrm{reject }\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw }\\,Y=x\\,\\mathrm{and\\, accept})\\)\nWe also have\n\\[\n\\begin{align*}\n& \\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n= & \\mathbb{P}(\\mathrm{draw}\\,Y=x)\\mathbb{P}(\\left.\\mathrm{accept}\\,Y\\right|Y=x)\\\\\n= & f(x)\\mathbb{P}\\left(\\left.U\\le\\frac{q(Y)}{Kf(Y)}\\right|Y=x\\right)\\\\\n= & \\frac{q(x)}{K} = \\frac{\\pi(x)}{cK}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-18",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-18",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nThe probability of rejection of a draw is\n\\[\n\\begin{align*}\n\\mathbb{P}(\\mathrm{{reject}}\\,Y) & =\\sum_{x\\in D}\\mathbb{P}(\\mathrm{{draw}}\\,Y=x\\,\\mathrm{and\\,reject\\,it})\\\\\n& =\\sum_{x\\in D}f(x)\\mathbb{P}\\left(\\left.U\\ge\\frac{q(Y)}{Kf(Y)}\\right|Y=x\\right)\\\\\n& =\\sum_{x\\in D}f(x)\\left(1-\\frac{q(x)}{Kf(x)}\\right)=1-\\frac{1}{cK}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-19",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-19",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nand so1\n\\[\n\\begin{align*}\n\\mathbb{P}(X=x) & =\\sum_{n=1}^{\\infty}\\mathbb{P}(\\mathrm{reject}\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n& =\\sum_{n=1}^{\\infty}\\left(1-\\frac{1}{cK}\\right)^{n-1}\\frac{\\pi(x)}{cK}=\\pi(x)\n\\end{align*}\n\\]\n\nThe geometric distribution is a discrete pmf that can be interpreted as the number of failures before the first success (\\(\\mathbb{P}(X=k)=(1-p)^{k-1}p\\), with mean \\(p\\))."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#stochastic-processes-1",
    "href": "slides/BSMM_8740_lec_09.html#stochastic-processes-1",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\n\nA stochastic process, which we will usually write as \\((X_n)\\), is an indexed sequence of random variables that are possibly dependent on each other.\nEach random variable \\(X_n\\) takes a value in a state space \\(\\mathcal S\\) which is the set of possible values for the process. As with usual random variables, the state space \\(\\mathcal S\\) can be discrete or continuous.\nA discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, \\(\\mathcal S = \\{\\text{Heads},\\text{Tails}\\}\\) is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers \\(\\mathcal S = \\mathbb Z_+ = \\{0,1,2,\\dots\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#stochastic-processes-2",
    "href": "slides/BSMM_8740_lec_09.html#stochastic-processes-2",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nThe stochastic process has an index set that orders the random variables that make up the process.\nThe index set is usually interpreted as a time variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at distinct points, often denoted by \\(n = 0,1,2,\\dots\\), while continuous time denotes a process monitored constantly over time, often denoted by \\(t \\in \\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-property",
    "href": "slides/BSMM_8740_lec_09.html#markov-property",
    "title": "Monte Carlo Methods",
    "section": "Markov property",
    "text": "Markov property\n\nConsider a simple board game where we roll a dice and move that many squares forward on the board. Suppose we are currently on the square \\(X_n\\). What can we say about which square \\(X_{n+1}\\) we move to on our next turn?\n\n\\(X_{n+1}\\) is random, since it depends on the roll of the dice.\n\\(X_{n+1}\\) depends on where we are now \\(X_n\\), since the score of dice will be added onto the number our current square,\nGiven the square \\(X_n\\) we are now, \\(X_{n+1}\\) doesn’t depend on which sequence of squares \\(X_0, X_1, \\dots, X_{n-1}\\) we used to get here."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-property-1",
    "href": "slides/BSMM_8740_lec_09.html#markov-property-1",
    "title": "Monte Carlo Methods",
    "section": "Markov property",
    "text": "Markov property\n\nThe third point is called the Markov property or memoryless property.\nThe key property of Markov Chains is memoryless: to get the next value we only need to know where we are, not which squares we visited to get here.\nThe past and the future are conditionally independent given the present."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider the following simple random walk on the positive integers \\(\\mathbb Z\\):\nWe start at \\(0\\), then at each time step, we go up by one with probability \\(p\\) and down by one with probability \\(q = 1-p\\). When \\(p = q = \\frac12\\), we’re equally as likely to go up as down, and we call this the simple symmetric random walk.\nThe simple random walk is a simple but very useful model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In many modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the “drunkard’s walk.”"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-1",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-1",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nrandom walks\nrequire(ggplot2, quietly = TRUE)\nset.seed(315)\n\nrrw &lt;- function(n, p = 1/2) {\n  q &lt;- 1 - p\n  Z &lt;- sample(c(1, -1), n, replace = TRUE, prob = c(p, q))\n  X &lt;- c(0, cumsum(Z))\n  c(0, cumsum(Z))\n}\n\nn &lt;- 2000\nrw_dat &lt;- tibble::tibble(x=0:n) |&gt; \n  dplyr::mutate(\n    \"p = 2/3\" = rrw(n, 2/3)\n    , \"p = 1/3\" = rrw(n, 1/3)\n    , \"p = 1/2\" = rrw(n, 1/2)\n  )\n\np0 &lt;- rw_dat |&gt; dplyr::slice_head(n=20) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np1 &lt;- rw_dat |&gt; dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np3 &lt;- rw_dat |&gt; #dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np0+p1+p3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-2",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-2",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWe can write this as a stochastic process \\((X_n)\\) with discrete time \\(n = \\{0,1,2,\\dots\\} = \\mathbb Z_+\\) and discrete state space \\(\\mathcal S = \\mathbb Z\\), where \\(X_0 = 0\\) and, for \\(n \\geq 0\\), we have\n\\[\nX_{n+1} =\n\\begin{cases}\nX_n + 1 & \\text{with probability $p$,} \\\\\nX_n - 1 & \\text{with probability $1-p$}\n\\end{cases}\n\\]\nIt’s clear from this definition that \\(X_{n+1}\\) (the future) depends on \\(X_n\\) (the present), but, given \\(X_n\\), does not depend on \\(X_{n-1}, \\dots, X_1, X_0\\) (the past). Thus the Markov property holds, and the simple random walk is a discrete time Markov process or Markov chain."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-3",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-3",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTo define a Markov chain, we first need to say where we start from, and then what the probabilities of transitions from one state to another are.\nIn our examples of the simple random walk and gambler’s ruin, we specified the start point \\(X_0 = i\\) exactly, but we could also pick the start point at random according to some distribution \\(\\pi_i = \\mathbb P(X_0 = i)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-4",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-4",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWe also need to know the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for \\(i,j \\in \\mathcal S\\). Because of the Markov property, the transition probability only needs to condition on the state we’re in now \\(X_n = i\\), and not on the whole history of the process.\nIn the case of the simple random walk, for example, we had initial distribution\n\\[\n\\lambda_i = \\mathbb P(X_0 = i) = \\begin{cases} 1 & \\text{if $i = 0$} \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\nand transition probabilities\n\\[\n\\mathbb P(X_{n+1} = j \\mid X_n = i) = \\begin{cases} p & \\text{if $j = i+1$} \\\\ q & \\text{if $j = i-1$} \\\\ 0 & \\text{otherwise.} \\end{cases}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-5",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-5",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nFor the random walk (and also the gambler’s ruin), the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) don’t depend on \\(n\\); in other words, the transition probabilities stay the same over time.\nA Markov process with this property is called time homogeneous. We will only consider time homogeneous processes in this course."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-6",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-6",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWrite \\(p_{ij} = \\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for the transition probabilities, which are independent of \\(n\\). We must have \\(p_{ij} \\geq 0\\), since it is a probability, and we must also have \\(\\sum_j p_{ij} = 1\\) for all states \\(i\\), as this is the sum of the probabilities of all the places you can move to from state i.\nWhen the state space is finite (and even sometimes when it’s not), it’s convenient to write the transition probabilities \\((p_{ij})\\) as a matrix \\(\\mathsf P\\), called the transition matrix, whose \\((i,j)\\)th entry is \\(p_{ij}\\).\nThen the condition that \\(\\sum_j p_{ij} = 1\\) is the condition that each of the rows of \\(\\mathsf P\\) add up to \\(1\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-7",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-7",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider a simple two-state Markov chain with state space \\(\\mathcal S = \\{0,1\\}\\) and transition matrix \\[ \\mathsf P = \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\beta & 1-\\beta \\end{pmatrix}  \\] for some \\(0 &lt; \\alpha&lt;1, 0&gt;\\beta &lt; 1\\). Note that the rows of \\(\\mathsf P\\) add up to \\(1\\), as they must.\nlike\nWe can illustrate \\(\\mathsf P\\) by a transition diagram/graph, where the nodes/vertices are the states and the arrows give the transition probabilities. (We don’t draw the arrow if \\(p_{ij} = 0\\).) In this case, our transition diagram looks the graph on the next slide:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-8",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-8",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTransition diagram for the two-state Markov chain"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-9",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-9",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWe can use this graph as a simple model of customer churn, for example. If the customer has closed their account (state 0) on one period, then with probability \\(\\alpha\\) we will be able to entice them to open their account again (state 1) by the next period; while if the customer has an account (state 1), then with probability \\(\\beta\\) they will have closed their account (state 0) by the next period."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-10",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-10",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\nIf the customer has an account in period \\(n\\), what’s the probability they also have an account in period \\(n+2\\)?\n\\[\np_{11}(2) = \\mathbb P (X_{n+2} = 1 \\mid X_n = 1)\n\\]\nThe key to calculating this is to condition on the first step again – that is, on whether the customer has an account in period \\(n\\). We have\n\\[\n\\begin{align*}\n  p_{11}(2) &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0, X_n = 1) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1, X_n = 1) \\\\\n  &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1) \\\\\n  &= p_{10}p_{01} + p_{11}p_{11} \\\\\n  &= \\beta\\alpha + (1-\\beta)^2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-11",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-11",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIn the second equality, we used the Markov property to mean conditional probabilities like \\(\\mathbb P(X_{n+2} = 1 \\mid X_{n+1} = k)\\) did not have to depend on \\(X_n\\).\nAnother way to think of this as we summing the probabilities of all length-2 paths from 1 to 1, which are \\(1\\to 0\\to 1\\) with probability \\(\\beta\\alpha\\) and \\(1 \\to 1 \\to 1\\) with probability \\((1-\\beta)^2\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-12",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-12",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIn the above example, we calculated a two-step transition probability:\n\\[\np_{ij}(2) = \\mathbb P (X_{n+2} = j \\mid X_n = i)\n\\]\nby conditioning on the first step. That is, by considering all the possible intermediate steps \\(k\\), we have\n\\[\n\\begin{align*}\np_{ij}(2) & =\\sum_{k\\in\\mathcal{S}}\\mathbb{P}(X_{n+1}=k\\mid X_{n}=i)\\mathbb{P}(X_{n+2}=j\\mid X_{n+1}=k)\\\\\n& =\\sum_{k\\in\\mathcal{S}}p_{ik}p_{kj}\n\\end{align*}\n\\]\nBut this is exactly the formula for multiplying the matrix \\(\\mathsf P\\) with itself! In other words, \\(p_{ij}(2) = \\sum_{k} p_{ik}p_{kj}\\) is the \\((i,j)\\)th entry of the matrix square \\(\\mathsf P^2 = \\mathsf{P}\\times\\mathsf{P}\\). If we write \\(\\mathsf P(2)  = (p_{ij}(2))\\) for the matrix of two-step transition probabilities, we have \\(\\mathsf P(2) = \\mathsf P^2\\).\nMore generally, we see that this rule holds over multiple steps, provided we sum over all the possible paths \\(i\\to k_1 \\to k_2 \\to \\cdots \\to k_{n-1} \\to j\\) of length \\(n\\) from \\(i\\) to \\(j\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-13",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-13",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\nTheorem 1 Let \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). For \\(i,j \\in \\mathcal S\\), write \\[ p_{ij}(n) = \\mathbb P(X_n = j \\mid X_0 = i) \\] for the \\(n\\)-step transition probability. Then\n\\[\np_{ij}(n) = \\sum_{k_1, k_2, \\dots, k_{n-1} \\in \\mathcal S} p_{ik_1} p_{k_1k_2} \\cdots p_{k_{n-2}k_{n-1}} p_{k_{n-1}j}\n\\]\nIn particular, \\(p_{ij}(n)\\) is the \\((i,j)\\)th element of the matrix power \\(\\mathsf P^n\\), and the matrix of \\(n\\)-step transition probabilities is given by \\(\\mathsf P(n) = \\mathsf P^n\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-14",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-14",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nThe so-called Chapman–Kolmogorov equations follow immediately from this.\n\nLet \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). Then, for non-negative integers \\(n,m\\), we have \\[ p_{ij}(n+m) = \\sum_{k \\in \\mathcal S} p_{ik}(n)p_{kj}(m) , \\] or, in matrix notation, \\(\\mathsf P(n+m) = \\mathsf P(n)\\mathsf P(m)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-15",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-15",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIn other words, a trip of length \\(n + m\\) from \\(i\\) to \\(j\\) is a trip of length \\(n\\) from \\(i\\) to some other state \\(k\\), then a trip of length \\(m\\) from \\(k\\) back to \\(j\\), and this intermediate stop \\(k\\) can be any state, so we have to sum the probabilities.\nOf course, once we know that \\(\\mathsf P(n) = \\mathsf P^n\\) is given by the matrix power, it’s clear to see that \\(\\mathsf P(n+m) = \\mathsf P^{n+m} = \\mathsf P^n \\mathsf P^m = \\mathsf P(n)\\mathsf P(m)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-16",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-16",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\ninitial distribution\n\nIf we start from a state given by a pmf \\(\\boldsymbol \\pi_0 = (\\pi_{0,i})_{i\\in S}\\), then after step 1 the probability we’re in state \\(j\\) is \\(\\sum_i \\pi_{0,i} p_{ij}\\).\nGiven transition matrix \\(\\mathsf P\\) and initial state pmf \\(\\boldsymbol \\pi_0\\), the pmf for the state after one step is \\(\\boldsymbol \\pi_1=\\boldsymbol \\pi_0 \\times\\mathsf P\\), i.e. \\(\\boldsymbol \\pi_1\\) and \\(\\boldsymbol \\pi_0\\) are row vectors, and for any given state pmf, the pmf for the states on the next step of the Markov chain is the current state pmf multiplied1 by the transition matrix.\n\nnote: on the left"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-17",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-17",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nStarting from a state given by a distribution \\(\\boldsymbol \\pi = (\\pi_i)_{i\\in S}\\), then after step 1 the probability we’re in state \\(j\\) is \\(\\sum_i \\pi_i p_{ij}\\). So if \\(\\pi_j = \\sum_i \\pi_i p_{ij}\\), we stay in this distribution forever. We call such a distribution a stationary distribution. We again recognise this formula as a matrix-vector multiplication, so this is \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\), where \\(\\boldsymbol \\pi\\) is a row vector.\n\nLet \\((X_n)\\) be a Markov chain on a state space \\(\\mathcal S\\) with transition matrix \\(\\mathsf P\\). Let \\(\\boldsymbol \\pi = (\\pi_i)\\) be a distribution on \\(\\mathcal S\\), in that \\(\\pi_i \\geq 0\\) for all \\(i \\in \\mathcal S\\) and \\(\\sum_{i \\in \\mathcal S} \\pi_i = 1\\). We call \\(\\boldsymbol \\pi\\) a stationary distribution if\n\\[\n\\pi_j = \\sum_{i\\in \\mathcal S} \\pi_i p_{ij} \\quad \\text{for all $j \\in \\mathcal S$}\n\\]\nor, equivalently, if \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-18",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-18",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nNote that we’re saying the distribution \\(\\mathbb P(X_n = i)\\) stays the same; the Markov chain \\((X_n)\\) itself will keep moving.\nOne way to think is that if we started off a thousand Markov chains, choosing each starting position to be \\(i\\) with probability \\(\\pi_i\\), then (roughly) \\(1000 \\pi_j\\) of them would be in state \\(j\\) at any time in the future – but not necessarily the same ones each time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-19",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-19",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nstationary distributions\n\nSo our definition of the stationary state pmf is: \\(\\pi\\) such that \\(\\pi=\\pi\\times \\mathsf P\\). We can calculate it as follows:\n\\[\n\\begin{align*}\n\\pi-\\pi\\mathsf{P} & =0\\\\\n\\pi\\left(I-\\mathsf{P}\\right) & =0\\\\\n\\left(I-\\mathsf{P}\\right)^{\\top}\\pi^{\\top} & =0\n\\end{align*}\n\\]\nalong with the additional constraint \\(\\sum_i\\pi_i=1\\).\nIf we add a row of ones at the bottom of our matrix \\(\\left(I-\\mathsf{P}\\right)^{\\top}\\) and a \\(1\\) at the bottom of the vector on the right we can solve for \\(\\pi\\), e.g. using qr.solve."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-20",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-20",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nstationary distribution example\n\nPt(I-P)solveconfirm\n\n\n\n\nCode\nP = matrix(\n  c(\n    0.2, 0.3, 0.5, 0,\n    0, 0.1, 0.1, 0.8,\n    0.5, 0.2, 0, 0.3,\n    0.3,0.1,0.3,0.3\n  )\n  , nrow = 4, byrow = TRUE\n)\nP\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  0.2  0.3  0.5  0.0\n[2,]  0.0  0.1  0.1  0.8\n[3,]  0.5  0.2  0.0  0.3\n[4,]  0.3  0.1  0.3  0.3\n\n\n\n\n\n\nCode\nA &lt;- t(diag(1,4) - P)\nA\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  0.8  0.0 -0.5 -0.3\n[2,] -0.3  0.9 -0.2 -0.1\n[3,] -0.5 -0.1  1.0 -0.3\n[4,]  0.0 -0.8 -0.3  0.7\n\n\n\n\n\n\nCode\npi &lt;- \n  qr.solve(\n    A |&gt; rbind(c(1,1,1,1))\n    , c(0,0,0,0,1)\n    , tol = 1e-10\n  )\npi\n\n\n[1] 0.2686 0.1782 0.2447 0.3085\n\n\n\n\n\n\nCode\npi %*% P\n\n\n       [,1]   [,2]   [,3]   [,4]\n[1,] 0.2686 0.1782 0.2447 0.3085"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-21",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-21",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nproperties\n\n\nA Markov Chain is irreducible if you have positive probability of eventually getting to anywhere in the state space from anywhere else in the state space\nA Markov Chain is aperiodic if there are no forced cycles, i.e. there do not exist disjoint non-empty subsets \\(X_1,X_2,...,X_d\\) for \\(d≥2\\), such that \\(P(x,X_{i+1})=1\\) for all \\(x\\in X_i \\;(1≤i≤d−1)\\), and \\(P(x,X_1)=1\\) for all \\(x\\in X_d\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-22",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-22",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nMC with cycle"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chain-monte-carlo-1",
    "href": "slides/BSMM_8740_lec_09.html#markov-chain-monte-carlo-1",
    "title": "Monte Carlo Methods",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\nSuppose have complicated, high-dimensional density \\(\\pi = cg\\) and we want samples \\(X_1, X_2,\\dots \\sim \\pi\\).\nDefine a Markov chain \\(X_0, X_1,X_2\\dots\\) in such a way that for large enough \\(m\\), we have \\(X_n\\sim \\pi,\\,\\forall n\\ge m\\).\nThen we can estimate \\(\\mathbb{E}_{\\pi}(h) ≡ \\int h(x) \\pi(x) dx\\) by:\n\\[\n\\mathbb{E}_{\\pi}[h] \\approx \\frac{1}{M-B}\\sum_{i=B+1}^{M}h(x_i)\n\\]\nwhere \\(B\\) (“burn-in”) is chosen large enough so \\(X_B\\sim\\pi\\), and \\(M\\) is chosen large enough to get good Monte Carlo estimates."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\n\nchoose some initial value \\(X_0\\), then\ngiven \\(X_{n-1}\\), choose a proposal state \\(Y_n\\sim \\textrm{MVN}(X_{n-1},\\sigma^2\\textrm{I})\\), for some fixed \\(\\sigma^2&gt;0\\)\nlet \\(A_n=\\pi(Y_n)/\\pi(X_{n-1})=g(Y_n)/g(X_{n-1})\\) and \\(U_n\\sim\\textrm{U}[0,1]\\), then\nif \\(U_n&lt;A_n\\) set \\(X_n=Y_n\\) (“accept”), otherwise set \\(X_n = X_{n-1}\\) (“reject”)\nrepeat for \\(n=1,2,3,\\ldots,M\\)\n\nThis version is called random-walk Metropolis - if we always accepted the proposals, they would form a traditional random walk process."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-1",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\nthe burn-in period is a matter of trial and error\nthe start values don’t matter much, but central. ones are ‘better’\nif \\(\\sigma\\) is too small then we usually accept and the chain doesn’t move much\nif \\(\\sigma\\) is too big then we usually reject and the chain doesn’t move much\ngenerally the acceptance rate should be far from both zero and 1"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-2",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-2",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nThe variance of the estimate is\n\\[\n\\frac{1}{M-B}\\textrm{Var}_{\\pi}(h)\\times \\textrm{varfact}\n\\]\nwhere\n\\[\n\\textrm{varfact} = 1+2\\sum_{k=1}^{\\infty}\\textrm{Corr}_{\\pi} \\left(h(X_0),h(X_k)\\right)=\\sum_{-\\infty}^{\\infty}\\rho_k\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-3",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-3",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\n\na simple Metropolis algorithm in one dimension (continuous RV)\ng = function(y) {\n    if ( (y&lt;0) || (y&gt;1) )\n    return(0)\n    else\n    return( y^3 * sin(y^4) * cos(y^5) )\n}\n\nh = function(y) { return(y^2) }\n\nM = 11000  # run length\nB = 1000  # amount of burn-in\nX = runif(1)  # overdispersed starting distribution\nsigma = 1  # proposal scaling\nxlist = rep(0,M)  # for keeping track of chain values\nhlist = rep(0,M)  # for keeping track of h function values\nnumaccept = 0;\n\nfor (i in 1:M) {\n  Y = X + sigma * rnorm(1)  # proposal value\n  U = runif(1)              # for accept/reject\n  alpha = g(Y) / g(X)       # for accept/reject\n  if (U &lt; alpha) {\n    X = Y                   # accept proposal\n    numaccept = numaccept + 1;\n  }\n    xlist[i] = X;\n    hlist[i] = h(X);\n}\n\nu = mean(hlist[(B+1):M])\nse1 =  sd(hlist[(B+1):M]) / sqrt(M-B)\nvarfact &lt;- function(xxx) { 2 * sum(acf(xxx, plot=FALSE)$acf) - 1 }\nthevarfact = varfact(hlist[(B+1):M])\nse = se1 * sqrt( thevarfact )\n\n\n\n\n\n\nCode\ntibble::tibble(\n  measure = c(\"iterations\",\"burn-in\",\"mean of h\",\"iid se\",\"varfact\",\"true se\",\"95% CI lb\",\"95% CI ub\")\n  , value = c(M,B,u,se1,thevarfact,se,u-1.96*se,u+1.96*se)\n) |&gt; \ngt::gt(\"measure\") |&gt; \ngt::fmt_number(rows = 3:8, decimals = 4) |&gt; \ngt::fmt_number(rows = 1:2, decimals = 0) |&gt; \ngtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\niterations\n11,000\n\n\nburn-in\n1,000\n\n\nmean of h\n0.7730\n\n\niid se\n0.0017\n\n\nvarfact\n21.0156\n\n\ntrue se\n0.0076\n\n\n95% CI lb\n0.7581\n\n\n95% CI ub\n0.7879\n\n\n\n\n\n\n\n\n\n\nCode\ntibble::tibble(x=1:length(xlist), y=xlist) |&gt; \n  ggplot(aes(x=x,y=y)) + geom_line() +\n  labs(title = \"Plot of accepted values\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-4",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-4",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\ntargetcodesampleshistogram\n\n\n\n\ntarget density shape\np &lt;- 0.4\nmu &lt;- c(-1, 2)\nsd &lt;- c(.5, 2)\nf &lt;- function(x)\n    p     * dnorm(x, mu[1], sd[1]) +\n    (1-p) * dnorm(x, mu[2], sd[2])\npar(mar=c(10.1, 2, .5, .5), oma=c(0, 1, 0, 0))\ncurve(f(x), col=\"red\", -4, 8, n=301, las=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis sampling algrithm\npropose &lt;- function(x) rnorm(1, x, 4)\nstep &lt;- function(x, f, q) {\n    ## Pick new point\n    xp &lt;- propose(x)\n    ## Acceptance probability:\n    alpha &lt;- min(1, f(xp) / f(x))\n    ## Accept new point with probability alpha:\n    if (runif(1) &lt; alpha) x &lt;- xp\n    ## Returning the point:\n    x\n}\nrun &lt;- function(x, f, q, nsteps) {\n    res &lt;- matrix(NA, nsteps, length(x))\n    for (i in seq_len(nsteps))\n        res[i,] &lt;- x &lt;- step(x, f, q)\n    drop(res)\n}\n\n\n\n\n\n\nMetropolis samples from target\nres &lt;- run(-10, f, q, 1000)\n\nlayout(matrix(c(1, 2), 1, 2), widths=c(4, 1))\npar(mar=c(10.1, 1.5, .5, .5), oma=c(0, 1, 0, 0))\nplot(res, type=\"s\", xpd=NA, ylab=\"Parameter\", xlab=\"Sample\", las=1)\nusr &lt;- par(\"usr\")\nxx &lt;- seq(usr[3], usr[4], length=301)\nplot(f(xx), xx, type=\"l\", yaxs=\"i\", axes=FALSE, xlab=\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis samples from target: histogram\npar(mar=c(10.1, 1.5, .5, .5), oma=c(0, 1, 0, 0))\nhist(res, 50, freq=FALSE, main=\"\", ylim=c(0, .4), las=1,\n     xlab=\"x\", ylab=\"Probability density\")\nz &lt;- integrate(f, -Inf, Inf)$value\ncurve(f(x) / z, add=TRUE, col=\"red\", n=200)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-5",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-5",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nDiscrete case\n\nstationary means that if \\(X_0\\sim\\pi\\), i.e. \\(\\mathbb{P}(X_0=i)=\\pi(i)\\), then \\(\\mathbb{P}(X_1=j)=\\pi(j),\\;\\forall j\\)\n\\(\\mathbb{P}(X_1=j)=\\sum_{i\\in S}\\mathbb{P}\\left(X_0=i,X_1=j\\right)\\) which is \\(\\sum_{i\\in S}\\mathbb{P}(X_0=i)\\mathsf{P}(i,j)\\)\nso \\(\\pi\\) is stationary if \\(\\sum_{i\\in S}\\pi(i)\\mathsf{P}(i,j)=\\pi(j),\\;\\forall j\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-6",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-6",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nDiscrete case\nlet \\(q(x,y)=\\mathbb{P}(Y_n=y|X_{n-1}=x)\\) be the proposal distribution\n\nassume \\(q\\) is symmetic, i.e. \\(q(x,y)=q(y,x),\\;\\forall x,y\\)\nthen if \\(\\alpha(x,y)\\) is the acceptance probability from \\(x\\) to \\(y\\):\n\n\\[\n\\begin{align*}\n\\alpha(x,y) & =\\mathbb{P}(U_{n}&lt;A_{n}|X_{n-1}=x,Y_{n}=y)\\\\\n& \\mathbb{P}\\left(U_{n}&lt;\\frac{\\pi(y)}{\\pi(x)}\\right)=\\min\\left[1,\\frac{\\pi(y)}{\\pi(x)}\\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-7",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-7",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nDiscrete case\nThen, for \\(i,j\\) in the state space, with \\(i\\ne j\\)\n\\[\n\\begin{align*}\n\\mathsf{P}(i,j)=q(i,j)\\alpha(i,j)=q(i,j)\\min\\left[1,\\frac{\\pi(j)}{\\pi(i)}\\right]\n\\end{align*}\n\\]\nand, by symmetry of \\(q\\)\n\\[\n\\begin{align*}\n\\pi(i)\\mathsf{P}(i,j) & =q(i,j)\\min\\left(\\pi(i),\\pi(j)\\right)\\\\\n& =q(j,i)\\min\\left(\\pi(j),\\pi(i)\\right)=\\pi(j)\\mathsf{P}(j,i)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-8",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-8",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nDiscrete case\nIf \\(X_0\\sim\\pi\\), then\n\\[\n\\begin{align*}\n\\mathbb{P}(X_{1}=j) & =\\sum_{i\\in\\chi}\\mathbb{P}(X_{0}=i)\\mathsf{P}(i,j)=\\sum_{i\\in\\chi}\\pi(i)\\mathsf{P}(i,j)\\\\\n& =\\sum_{i\\in\\chi}\\pi(j)\\mathsf{P}(j,i)=\\pi(j)\\sum_{i\\in\\chi}\\mathsf{P}(j,i)\\\\\n& =\\pi(j)\n\\end{align*}\n\\]\nso \\(X_1\\sim\\pi\\) too, and the Markov chain preserves \\(\\pi\\), i.e. \\(\\pi\\) is a stationary distribution."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-9",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-9",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nexample\n\\(\\chi=\\mathbb{Z}\\), \\(\\pi(x)=2^{-|x|}/3\\), and \\(q(x,y)=\\frac{1}{2}\\) if \\(|x-y|=1\\), otherwise \\(0\\).\n\nreversible? Yes, it’s a Metropolis algorithm\n\\(\\pi\\) stationary? Yes, follows from reversibility\naperiodic? Yes, since \\(\\mathsf{P}(x,\\{x\\})&gt;0\\)\nirreducible? Yes, since \\(\\pi(x)&gt;0,\\forall x\\in\\chi\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nMCMC algorithms require that the MC chain is reversible, \\(\\pi\\)-stationary, aperiodic, and reducible.\nThe Metropolis algorithm uses the symmetry of the proposal distribution to ensure reversibility and irreducibility.\nIf the proposal distribution was not symmetric we might not have those two properties, e.g. if \\(q(x,y) \\gg q(y,x)\\) the the Metropolis chain would spend too much time at \\(y\\) and not enough at \\(x\\), so it accepts fewer moves \\(x\\rightarrow y\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-1",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nIf we only require that \\(q(x,y)&gt;0\\;\\textrm{iff}\\; q(y,x)&gt;0\\), then replace\n\\[\nA_n = \\frac{\\pi(Y_n)}{\\pi(X_{n-1})}\\;\\mathrm{with}\\; A_n = \\frac{\\pi(Y_n)q(Y_n,X_{n-1})}{\\pi(X_{n-1})q(X_{n-1},Y_n)}\n\\]\nand the algorithm is still valid even if \\(q\\) is not symmetric."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-2",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-2",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nWhy is this valid?\nFor metropolis, the key was that the Markov chain was reversible, i.e. \\(\\pi(x)\\mathsf{P}(x,y)=\\pi(y)\\mathsf{P}(y,x)\\)\nIf instead \\(A_n = \\frac{\\pi(Y_n)q(Y_n,X_{n-1})}{\\pi(X_{n-1})q(X_{n-1},Y_n)}\\), with acceptance probability \\(\\alpha(x,y)=\\min\\left[1,\\frac{\\pi(Y_n)q(Y_n,X_{n-1})}{\\pi(X_{n-1})q(X_{n-1},Y_n)}\\right]\\), then\n\\[\n\\begin{align*}\nq(x,y)\\alpha(x,y)\\pi(x) & =q(x,y)\\min\\left[1,\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}\\right]\\pi(x)\\\\\n& =\\min\\left[\\pi(x)q(x,y),\\pi(y)q(y,x)\\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-3",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-3",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nThen\n\n\\(\\pi(x)\\mathsf{P}(x,y)\\) is still symmetric, even if \\(q\\) wasn’t.\nthe chain is still reversible, so there is a stationary distribution \\(\\pi\\).\nif the chain is irreducible and aperiodic, then it converges to \\(\\pi\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-4",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-4",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nexample: independence sampler\nHere \\(\\{Y_n\\}\\sim q(\\cdot)\\) i,e, the \\(\\{Y_n\\}\\) are IID from some fixed density \\(q\\), independent of \\(X_{n-1}\\) and we accept \\(Y_n\\) if \\(U_n&lt;A_n\\) where \\(U_n\\sim \\textrm{Uniform}[0,1]\\) and\n\\[\nA_n = \\frac{\\pi(Y_n)q(X_{n-1})}{\\pi(X_{n-1})q(Y_n)}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\nThe Bayesian model for updating parameter estimates is (to within a scaling constant)\n\\[\n\\begin{align*}\n\\pi_\\theta\\left(\\left.\\theta\\right|X\\right)\\sim\\pi_X\\left(\\left.X\\right|\\theta\\right)\\times\\pi_\\theta\\left(\\theta\\right)\n\\end{align*}\n\\tag{1}\\]\nwhere the parameter set \\(\\theta\\) depends on the assumed data generating process \\(\\pi_X\\).\n\nIn words: the joint probability of the parameters given the observed data is equal to (to within a scaling constant) the probability of the observed data given the parameters, times the prior probabilities of the parameters. In practice we refer to the probabilities as likelihoods, and use log-likelihoods in equation (Equation 1) to avoid numerical problems arising from the product of small probabilities."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-1",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\ninitialupdatingplotsplot code\n\n\n\n\nInitial state: Bayesian parameter estimation, Binomial process\nset.seed(8740)\np &lt;- 0.5\nn &lt;- 10\n# range of probability [0.01 - 0.99]\np_values &lt;- seq(0.01,0.99,0.001)\n# prior probability is beta(1,1)\npr &lt;- dbeta(p_values,7,2)\n# Have to normalize given discreteness\npr &lt;- pr / sum(pr) \n# create the data\ndat &lt;- tibble::tibble(parameters = p_values, prob = pr, x = 0, step= 0) \n\n\n\n\n\n\n7 Bayesian updating steps\n# Run for M samples\nfor (i in 1:8) {\n  # have the data generating process generate a data point\n  x &lt;- rbinom(1, n, p)\n  # multiply the likelihood of observing the data point at each p-value\n  # by the prior probability of each p-value:\n  # this gives the posterior probability for each p-value\n  ps &lt;- dbinom(x, n, p_values) * pr\n  # normalize\n  ps &lt;- ps / sum(ps) \n  # lines(ps~p_values, col=(i+1))\n  # same the posterior p-value probabilities at this step\n  dat &lt;- dat |&gt;\n    dplyr::bind_rows(\n      tibble::tibble(parameters = p_values, prob = ps, x = x) |&gt;\n        dplyr::mutate(step = i)\n    )\n  # update the prior probability of each p-value to be its posterior probability\n  pr = ps\n}\n\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in\nggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlots of Bayesian updating steps\ndat &lt;- dat |&gt; dplyr::group_by(step) |&gt;\n  dplyr::mutate(\n    title =\n      dplyr::case_when(\n        step == 0 ~ stringr::str_glue(\"Step {step}: prior mean is {(parameters*prob) |&gt; sum() |&gt; round(digits=3)}\")\n        , TRUE ~ stringr::str_glue(\"Step {step}: sample is {x} & posterior mean is {(parameters*prob) |&gt; sum() |&gt; round(digits=3)}\")\n      )\n  )\n\nlabels &lt;- dat |&gt; dplyr::distinct(step, title) |&gt; dplyr::mutate(step = factor(step))\nstep_labels &lt;- split(labels$title, labels$step)\nstep_labeller &lt;- function(value){return(step_labels[value])}\n\ndat |&gt; dplyr::mutate(step = factor(step)) |&gt; dplyr::group_by(step) |&gt;\n  ggplot(aes(x = parameters, y=prob, color = step)) +\n  geom_line() +\n  geom_vline(xintercept=0.5, color=\"grey\", size=1, linetype = \"dashed\") +\n  facet_wrap(vars(step), nrow = 2, labeller=labeller(step = step_labeller)) +\n  theme(plot.margin=unit(c(5,1,5,1), 'cm')) +\n  theme_minimal() + \n  labs(title = \"Bayesian updating\", subtitle = \"Binomial data; n known, true p = 0.5\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-2",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-2",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\nIt is more common to have multiple data points and multiple parameters. In this case we sample from the posterior using \\(\\pi_X\\left(\\left.X\\right|\\theta\\right)\\times\\pi_\\theta\\left(\\theta\\right)\\) and a MCMC method like Metropolis Hastings.\nIn the example to follow, we observe \\((y,x)\\) and we have parameters \\(\\theta=(a,b,\\sigma)\\) that we want to estimate\n\nthe data generation process is assumed to be \\(y = \\mathscr{N}(ax+b, \\sigma^2)\\)\nwe choose priors on the parameters as follows\n\n\\(a=U[0,10]\\)\n\\(b=\\mathscr{N}(0,5)\\)\n\\(\\sigma=U[0,30]\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-3",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-3",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\ndatapriorlikelihoodproposalMH Algo\n\n\n\nset.seed(8740)\na = 0; b = 5; sigma = 10; sampleSize = 31\n\ndata &lt;- \n  tibble::tibble(\n    x = (-(sampleSize-1)/2):((sampleSize-1)/2)\n    , y =  a + b * x + rnorm(n=sampleSize, mean=0, sd=sigma)\n  )\n\n\n\n\nprior = function(param){\n  aprior = dunif(param[1], min=0, max=10, log = T)\n  bprior = dnorm(param[2], sd = 5, log = T)\n  sdprior = dunif(param[3], min=0, max=30, log = T)\n  # return the log of the product of probabilities (parameters)\n  return(aprior+bprior+sdprior)\n}\n\n\n\n\nlikelihood = function(param, x, y){\n  pred = param[2] + param[1] * x \n  singlelikelihoods = dnorm(y, mean = pred, sd = param[3], log = T)\n  sumll = sum(singlelikelihoods)\n  # return the log of the product of probabilities (data)\n  return(sumll)\n}\n\n\n\n\nproposalfunction = function(param){\n  return( rnorm(3, mean = param, sd= c(0.1,0.5,0.3)) )\n}\n\n\n\n\nrun_metropolis_MCMC = function(startvalue, iterations, data){\n  # initialize\n  chain = array(dim = c(iterations+1,3))\n  chain[1,] &lt;- startvalue\n  \n  for (i in 1:iterations){\n    proposal = proposalfunction(chain[i,])\n    probab = \n      exp(\n        likelihood(proposal, data$x, data$y) + \n        prior(proposal) - \n        likelihood(chain[i,], data$x, data$y) - \n        prior(chain[i,])\n      )\n    if (runif(1) &lt; probab){\n      chain[i+1,] = proposal\n    }else{\n      chain[i+1,] = chain[i,]\n    }\n  }\n  return(chain)\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-4",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-4",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\n\nMH algorithm generating posterior parameter probabilities\nstartvalue = c(4,2,8)\nchain = run_metropolis_MCMC(startvalue, 20000, data)\n\nmcm_chain &lt;- coda::mcmc(chain)\nsmry &lt;- summary(mcm_chain)\nsmry$statistics |&gt; tibble::as_tibble() |&gt; \n  dplyr::bind_cols(smry$quantiles |&gt; tibble::as_tibble()) |&gt;\n  tibble::add_column(param = c('b','a','sigma')) |&gt; \n  gt::gt(\"param\") |&gt; \n  gt::tab_spanner(label = \"percentiles\", columns = ends_with(\"%\")) |&gt; \n  gt::tab_options( table.font.size = gt::px(48) ) %&gt;% \n  gtExtras::gt_theme_espn() |&gt;\n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\nSD\nNaive SE\nTime-series SE\n\npercentiles\n\n\n\n2.5%\n25%\n50%\n75%\n97.5%\n\n\n\n\nb\n4.8424\n0.1742\n0.001232\n0.005517\n4.498\n4.7268\n4.844\n4.959\n5.182\n\n\na\n0.6129\n1.4354\n0.010149\n0.070569\n-2.173\n-0.3511\n0.616\n1.560\n3.495\n\n\nsigma\n8.5691\n1.1006\n0.007783\n0.068802\n6.726\n7.7719\n8.496\n9.256\n10.982"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-5",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-5",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\n\nposterior parameter probability plots\nplot(mcm_chain)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-6",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-6",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#more",
    "href": "slides/BSMM_8740_lec_09.html#more",
    "title": "Monte Carlo Methods",
    "section": "More",
    "text": "More\n\nThe Random Walk Metropolis: Linking Theory and Practice Through a Case Study\n“The Metropolis-Hastings Algorithm.” April 12, 2023. https://blog.djnavarro.net/posts/2023-04-12_metropolis-hastings/.\nWhy Metropolis–Hastings Works"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#recap",
    "href": "slides/BSMM_8740_lec_09.html#recap",
    "title": "Monte Carlo Methods",
    "section": "Recap",
    "text": "Recap\n\nThe week we’ve introduced Monte Carlo (MC) methods for sampling from probability distributions, including using those sample to estimate expected values.\nWe discus importance sampling, rejection sampling, and Markov Chain Monte Carlo (MCMC) methods, particularily the particularly the Metropolis and Metropolis-Hastings methods.\nFinally, we applied the Metropolis-Hastings methods in Bayesian analytics, generating Bayesian estimates for the parameters of linear regression."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08inf.html#s1",
    "href": "slides/BSMM_8740_lec_08inf.html#s1",
    "title": "Causality Part INFINITY",
    "section": "S1",
    "text": "S1\nThis is some stuff\n#| code-annotations: below\n\n\nfit_ipw &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  if(\"rsplit\" %in% class(split)){\n    .df &lt;- rsample::analysis(split)\n  }else if(\"data.frame\" %in% class(split)){\n    .df &lt;- split\n  }\n\n  # fit propensity score model\n  propensity_model &lt;- glm(\n    net ~ income + health + temperature,\n    data = .df,\n    family = binomial()\n  )\n\n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\n  # fit correctly bootstrapped ipw model\n  lm(malaria_risk ~ net, data = .df, weights = wts) |&gt;\n    broom::tidy()\n}\n\n1. bollocks\n2. more bollocks"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08inf.html#s2",
    "href": "slides/BSMM_8740_lec_08inf.html#s2",
    "title": "Causality Part INFINITY",
    "section": "S2",
    "text": "S2\n\n\nCode\nfit_ipw &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  if(\"rsplit\" %in% class(split)){\n    .df &lt;- rsample::analysis(split)\n  }else if(\"data.frame\" %in% class(split)){\n    .df &lt;- split\n  }\n\n  # fit propensity score model\n  propensity_model &lt;- glm(\n    net ~ income + health + temperature,\n    data = .df,\n    family = binomial()\n  )\n\n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\n  # fit correctly bootstrapped ipw model\n  lm(malaria_risk ~ net, data = .df, weights = wts) |&gt;\n    broom::tidy()\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08inf.html#s3",
    "href": "slides/BSMM_8740_lec_08inf.html#s3",
    "title": "Causality Part INFINITY",
    "section": "S3",
    "text": "S3\n𝔼[N1(ATET̂−ATET)]=𝔼[N1(μ0(Xi)−μ0(Xj(i))]\n\\mathbb{E}\\left [\\sqrt{N_1}(\\hat{ATET} - ATET)\\right] = \n\\mathbb{E}\\left[\\sqrt{N_1}(\\mu_0(X_i) - \\mu_0(X_j(i))\\right]"
  },
  {
    "objectID": "slides/temp.html",
    "href": "slides/temp.html",
    "title": "temp",
    "section": "",
    "text": "Code\nsamples &lt;- matrix(runif(10000), ncol=2) |&gt; data.frame() |&gt; \n  dplyr::mutate(\n    normals = \n      purrr::map2(\n        X1, X2\n        ,(\\(x1,x2){\n          data.frame(\n            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)\n            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) \n          )\n        })\n      )\n  ) |&gt; \n  tidyr::unnest(normals)  \n  \n\nsamples |&gt; \n  tidyr::pivot_longer(-c(X1,X2)) |&gt; \n  ggplot(aes(x=value, color=name, fill=name)) + \n  geom_histogram(aes(y=..density..), bins = 60, position=\"identity\", alpha=0.3) + \n  labs(x=\"Value\", y=\"Density\") + theme_minimal()\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in\nggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nCode\nsamples |&gt; \nggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs"
  },
  {
    "objectID": "slides/temp.html#random-number-generation",
    "href": "slides/temp.html#random-number-generation",
    "title": "temp",
    "section": "",
    "text": "Code\nsamples &lt;- matrix(runif(10000), ncol=2) |&gt; data.frame() |&gt; \n  dplyr::mutate(\n    normals = \n      purrr::map2(\n        X1, X2\n        ,(\\(x1,x2){\n          data.frame(\n            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)\n            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) \n          )\n        })\n      )\n  ) |&gt; \n  tidyr::unnest(normals)  \n  \n\nsamples |&gt; \n  tidyr::pivot_longer(-c(X1,X2)) |&gt; \n  ggplot(aes(x=value, color=name, fill=name)) + \n  geom_histogram(aes(y=..density..), bins = 60, position=\"identity\", alpha=0.3) + \n  labs(x=\"Value\", y=\"Density\") + theme_minimal()\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in\nggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nCode\nsamples |&gt; \nggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs"
  },
  {
    "objectID": "slides/temp.html#random-number-generation-1",
    "href": "slides/temp.html#random-number-generation-1",
    "title": "temp",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nYour computer is only capable of producing pseudorandom numbers. These are made by running a pseudorandom number generator algorithm which is deterministic, e.g.\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\nOnce the RNG seed is set, the “random” numbers that R generates aren’t random at all. But someone looking at these random numbers would have a very hard time distinguishing these numbers from truly random numbers. That is what “statistical randomness” means!"
  },
  {
    "objectID": "slides/temp.html#central-limit-theorem",
    "href": "slides/temp.html#central-limit-theorem",
    "title": "temp",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT says that for f=ℙ(E)f=\\mathbb{P}(E) if σ2≡Var(f)\\sigma^2\\equiv\\mathrm{Var}\\left(f\\right) is finite then the error of the MC estimate\neN(f)=f‾−𝔼[f]\ne_N(f)=\\bar{f}-\\mathbb{E}[f]\n\nis approximately Normal in distribution for large MM, i.e.\neN(f)∼σM1/2Z\ne_N(f)\\sim\\sigma M^{1/2}Z\n where Z∼𝒩(0,1)Z\\sim\\mathscr{N}(0,1)"
  },
  {
    "objectID": "slides/temp.html#mc-methods",
    "href": "slides/temp.html#mc-methods",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\nSuppose we need to compute an expectation 𝔼[g(Z)]\\mathbb{E}[g(Z)] for some random variable ZZ and some function g:ℝ→ℝg:\\mathbb{R}\\to\\mathbb{R}. Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of ZZ, say z1,z2,…,zMz_1,z_2,\\ldots,z_M and estimate 𝔼[g(Z)]\\mathbb{E}[g(Z)] as 1M∑i=1Mg(zi)\\frac{1}{M}\\sum_{i=1}^Mg(z_i). The law of large numbers states that this sample mean should be close to 𝔼[g(Z)]\\mathbb{E}[g(Z)].\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/temp.html#mc-methods-1",
    "href": "slides/temp.html#mc-methods-1",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nWe can use Monte Carlo to estimate probabilities of the form ℙ[E]\\mathbb{P}\\left[E\\right] by approximating expectations of the form 𝔼[1X∈E]\\mathbb{E}[1_{X\\in E}].\nIf X∼𝒩(μ,σ)X\\sim\\mathscr{N}(\\mu,\\sigma) and we want to compute 𝔼[log|X|]\\mathbb{E}[\\log|X|], we could set up and solve the integral\n𝔼log|X|=∫−∞∞(log|t|)f(t;μ,σ)dt=∫−∞∞log|t|2πσ2exp{−(t−μ)22σ2}dt\n\\mathbb{E} \\log |X|\n= \\int_{-\\infty}^\\infty \\left( \\log |t| \\right) f( t; \\mu, \\sigma) dt\n= \\int_{-\\infty}^\\infty \\frac{ \\log |t| }{ \\sqrt{2\\pi \\sigma^2} }\n                  \\exp\\left\\{ \\frac{ -(t-\\mu)^2 }{ 2\\sigma^2 } \\right\\}dt\n\nAlternatively, we could just draw lots of Monte Carlo replicates X1,X2,⋯,XMX_1,X_2,\\cdots,X_M from a normal with mean μ\\mu and variance σ2\\sigma^2, and look at the sample mean M−1∑i=1Mlog|xi|M^{-1}\\sum_{i=1}^M\\log|x_i|, once again appealing to the law of large numbers to ensure that this sample mean is close to its expectation.\nMonte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/temp.html#mc-methods-2",
    "href": "slides/temp.html#mc-methods-2",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis idea can be pushed still further. Suppose that we want to compute an integral\n∫Dg(x)dx\n\\int_Dg(x)dx\n where DD is some domain of integration and g(.)g(.) is a function.\nLet f(x)f(x) be the density of some random variable with f(x)&gt;0,∀x∈Df(x)&gt;0, \\forall x\\in D. In other words, ff is the density of a random variable supported on DD. Then we can rewrite the integral as\n∫Dg(x)dx=∫Dg(x)f(x)f(x)dx=𝔼[h(x)]\n\\int_Dg(x)dx = \\int_D\\frac{g(x)}{f(x)}f(x)dx = \\mathbb{E}[h(x)]\n where h(x)=g(x)/f(x)h(x)=g(x)/f(x) and X∼fX\\sim f"
  },
  {
    "objectID": "slides/temp.html#mc-methods-3",
    "href": "slides/temp.html#mc-methods-3",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nNow suppose we are given h(x)h(x) and we want to compute 𝔼[h(X)]\\mathbb{E}[h(X)] where X∼f,x∈DX\\sim f,\\,x\\in D. So we need to sample from ff, but what if we could not do that directly?\nIf there were some other distribution g(x)g(x) we could sample from, such that g(x)&gt;0,x∈Dg(x)&gt;0,\\,x\\in D, then\n𝔼f[h(x)]=∫Dh(x)f(x)dx=∫Sh(x)f(x)g(x)g(x)dx=𝔼g[h(x)f(x)g(x)]=1n∑i=1nh(xi)f(xi)g(xi)xi∼g\n\\begin{align*}\n\\mathbb{E}_{f}\\left[h(x)\\right] & =\\int_{D}h(x)f(x)dx\\\\\n & =\\int_{S}h(x)\\frac{f(x)}{g(x)}g(x)dx=\\mathbb{E}_{g}\\left[h(x)\\frac{f(x)}{g(x)}\\right]\\\\\n & =\\frac{1}{n}\\sum_{i=1}^{n}h(x_{i})\\frac{f(x_{i})}{g(x_{i})}\\quad x_{i}\\sim g\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-4",
    "href": "slides/temp.html#mc-methods-4",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis is called importance sampling (IS).\n\ndraw iid x1,x2,…,xnx_1,x_2,\\ldots,x_n from g and calculate the importance weight w(xi)=f(xi)g(xi)\nw(x_i)=\\frac{f(x_{i})}{g(x_{i})}\n\nestimate 𝔼f(h)\\mathbb{E}_f(h) by μ̂h=1n∑i=1nw(xi)h(xi)\n\\hat{\\mu}_h=\\frac{1}{n}\\sum_{i=1}^nw(x_i)h(x_i)"
  },
  {
    "objectID": "slides/temp.html#mc-methods-5",
    "href": "slides/temp.html#mc-methods-5",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nexample\n\nEstimate 𝔼f(X)\\mathbb E_f(X) where f(x)=2/πe−x22;x≥0f(x) = \\sqrt{2/\\pi}e^{-\\frac{x^2}{2}};\\;x\\ge 0 (this is the half-Normal distribution)\n\n\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X) / sqrt(2 * pi)\n\nmu_h  &lt;- mean(W*X)\nvar_h &lt;- var(W*X)/n\nse_h  &lt;- sqrt(var_h)\n\ntibble::tibble(mean = mu_h,  variance = var_h, 'standard error' = se_h) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4)\n\n\n\n\n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.8030\n0.0003\n0.0178"
  },
  {
    "objectID": "slides/temp.html#mc-methods-6",
    "href": "slides/temp.html#mc-methods-6",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nunknown normalizing constant\n\nSuppose that q(x)&gt;0;x∈Dq(x)&gt;0;\\;x\\in D and ∫Dq(x)dx=Zq&lt;∞\\int_Dq(x)dx=Z_q&lt;\\infty The q()q() is an un-normalized density on DD whereas the corresponding normalized density is 1Zqq(x)\\frac{1}{Z_q}q(x).\nUsing IS, let g(x)=1Zrr(x);Zr=∫r(x)dxg(x) = \\frac{1}{Z_r}r(x);\\;Z_r=\\int r(x)dx, so rr is an un-normalized density with ZrZ_r possibly unknown.\n\nDraw x1,x2,…,xnx_1,x_2,\\ldots,x_n from g(x)g(x) and calculate importance weights w(xi)=g(xi)/r(xi)w(x_i)=g(x_i)/r(x_i)\nEstimate 𝔼f[h(X)]\\mathbb{E}_f\\left[h(X)\\right] by μĥ=∑i=1nw(xi)h(xi)∑i=1nw(xi)\n\\hat{\\mu_h}=\\frac{\\sum_{i=1}^nw(x_i)h(x_i)}{\\sum_{i=1}^nw(x_i)}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-7",
    "href": "slides/temp.html#mc-methods-7",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nunknown normalizing constant\n\nsince\n1n∑i=1nw(xi)→𝔼g[q(X)r(X)]=∫q(X)r(X)g(x)dx=ZqZr1n∑i=1nw(xi)→𝔼g[q(X)r(X)h(X)]=∫q(X)r(X)g(x)h(x)dx=1Zr∫g(x)h(x)dx\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}\\right]=\\int\\frac{q(X)}{r(X)}g(x)dx=\\frac{Z_{q}}{Z_{r}}\\\\\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}h(X)\\right]=\\int\\frac{q(X)}{r(X)}g(x)h(x)dx=\\frac{1}{Z_{r}}\\int g(x)h(x)dx\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-8",
    "href": "slides/temp.html#mc-methods-8",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrepeating the prior example, but un-normalized\n\nEstimate 𝔼f(X)\\mathbb E_f(X) where f(x)=e−x22;x≥0f(x) = e^{-\\frac{x^2}{2}};\\;x\\ge 0 (this is the half-Normal distribution, un-normalized)\n\n\n# un-normalized weights\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X)\n\nmu_h2  &lt;- sum(W*X)/sum(W)\nvar_h2 &lt;- var(W/mean(W))\nse_h2  &lt;- sqrt(var_h2)\n\ntibble::tibble(mean = mu_h2,  variance = var_h2, 'standard error' = se_h2) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4)\n\n\n\n\n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.7895\n0.4058\n0.6370"
  },
  {
    "objectID": "slides/temp.html#mc-methods-9",
    "href": "slides/temp.html#mc-methods-9",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling procedure\n\nAssume we have an un-normalized g(x)g(x), i.e. π(x)=cg(x)\\pi(x)=cg(x) but cc is unknown.\nWe want to generate iid samples x1,x2,…,xM∼πx_1,x_2,\\ldots,x_M\\sim \\pi to estimate 𝔼π[h]\\mathbb{E}_\\pi[h]\nNow assume we have an easily sampled density ff, and known K&gt;0K&gt;0, such that Kf(x)≥g(x),∀xKf(x)\\ge g(x),\\;\\forall x, i.e. Kf(x)≥π(x)/cKf(x)\\ge \\pi(x)/c ( or cKf(x)≥π(x)cKf(x)\\ge \\pi(x)).\nThen use the following procedure:\n\nsample X∼fX\\sim f and U∼uniform[0,1]U\\sim \\mathrm{uniform}[0,1]\nif U≤g(X)Kf(x)U\\le\\frac{g(X)}{Kf(x)}, the accept X as a draw from π\\pi\notherwise reject the sample and repeat"
  },
  {
    "objectID": "slides/temp.html#mc-methods-10",
    "href": "slides/temp.html#mc-methods-10",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\n\nSince 0≤g(x)Kf(x)≤10\\le\\frac{g(x)}{Kf(x)}\\le 1 we know that ℙ(U≤g(X)Kf(X)|X=x)=g(x)Kf(x)\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\nand so\n\n𝔼f[ℙ(U≤g(X)Kf(X)|X=x)]=𝔼f[g(X)Kf(X)]=∫−∞∞g(X)Kf(X)f(x)dx=∫−∞∞g(X)Kdx\n\\mathbb{E}_f\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]=\\mathbb{E}_f\\left[\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^\\infty\\frac{g(X)}{Kf(X)}f(x)dx=\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx"
  },
  {
    "objectID": "slides/temp.html#mc-methods-11",
    "href": "slides/temp.html#mc-methods-11",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\n\nSince 0≤g(x)Kf(x)≤10\\le\\frac{g(x)}{Kf(x)}\\le 1 we know that ℙ(U≤g(X)Kf(X)|X=x)=g(x)Kf(x)\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\nand so\n\n𝔼f[ℙ(U≤g(X)Kf(X)|X=x)]=𝔼f[g(X)Kf(X)]=∫−∞∞g(X)Kf(X)f(x)dx=∫−∞∞g(X)Kdx\n\\mathbb{E}_f\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]=\\mathbb{E}_f\\left[\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^\\infty\\frac{g(X)}{Kf(X)}f(x)dx=\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx"
  },
  {
    "objectID": "slides/temp.html#mc-methods-12",
    "href": "slides/temp.html#mc-methods-12",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\nSimilarly, for any y∈ℝy\\in\\mathbb{R}, we can calculate the joint probability\nℙ(X≤y,U≤g(X)Kf(X))=𝔼[1X≤y1U≤g(X)Kf(X)]=𝔼[1X≤yℙ(U≤g(X)Kf(X)|X)]=𝔼[1X≤yg(X)Kf(X)]=∫−∞yg(x)Kf(x)f(x)dx=∫−∞yg(x)Kdx\n\\begin{align*}\n\\mathbb{P}\\left(X\\le y,U\\le\\frac{g(X)}{Kf(X)}\\right) & =\\mathbb{E}\\left[1_{X\\le y}1_{U\\le\\frac{g(X)}{Kf(X)}}\\right]\\\\\n & =\\mathbb{E}\\left[1_{X\\le y}\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X\\right)\\right]\\\\\n & =\\mathbb{E}\\left[1_{X\\le y}\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^{y}\\frac{g(x)}{Kf(x)}f(x)dx\\\\\n & =\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx\n\\end{align*}\n\n\nand so we have the joint probability (above - ℙ(A,B)\\mathbb{P}(A,B)), and the probability of acceptance (previous slide - ℙ(B)\\mathbb{P}(B)), so the probability, conditional on acceptance (ℙ(A|B)\\mathbb{P}(A|B)) is ℙ(A,B)ℙ(B)\\frac{\\mathbb{P}(A,B)}{\\mathbb{P}(B)} by Bayes rule."
  },
  {
    "objectID": "slides/temp.html#mc-methods-13",
    "href": "slides/temp.html#mc-methods-13",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\nℙ(X≤y|U≤g(X)Kf(X))=∫−∞yg(x)Kdx∫−∞∞g(X)Kdx=∫−∞yπ(x)dx\n\\mathbb{P}\\left(X\\le y|U\\le\\frac{g(X)}{Kf(X)}\\right)=\\frac{\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx}{\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx}=\\int_{-\\infty}^{y}\\pi(x)dx"
  },
  {
    "objectID": "slides/temp.html#mc-methods-14",
    "href": "slides/temp.html#mc-methods-14",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\nconsider random variable XX with pdf/pmf q(x)&gt;0;x∈Dq(x)&gt;0;\\;x\\in D which is difficult to sample from\nwe will sample from qq using a proposal pdf/pmf ff which we can sample from\nif we can find a constant KK such that q(x)≤Kf(x);∀x∈Dq(x)\\le Kf(x); \\forall x\\in D. Alternatively q(x)f(x)≤K\\frac{q(x)}{f(x)}\\le K\nthen there is a rejection method that returns X∼qX\\sim q"
  },
  {
    "objectID": "slides/temp.html#mc-methods-15",
    "href": "slides/temp.html#mc-methods-15",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling method\n\ngiven a proposal pdf/pmf ff we can sample from, and constant KK such that q(x)f(x)≤K;∀x∈D\\frac{q(x)}{f(x)}\\le K; \\forall x\\in D\nsample Yi∼fY_i\\sim f and Ui∼U[0,1]U_i\\sim\\mathrm{U}[0,1]\nfor Ui≤q(Yi)Kf(Yi)U_i\\le\\frac{q(Y_i)}{Kf(Y_i)} return Xi=YiX_i=Y_i; otherwise return nothing and continue."
  },
  {
    "objectID": "slides/temp.html#mc-methods-16",
    "href": "slides/temp.html#mc-methods-16",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: proof for discrete rv\n\nWe have ℙ(X=x)=∑i=1nℙ(rejectY)n−1ℙ(drawY=xandaccept)\\mathbb{P}(X=x) = \\sum_{i=1}^n\\mathbb{P}(\\mathrm{reject }\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw }\\,Y=x\\,\\mathrm{and\\, accept})\nWe also have\nℙ(drawY=xandaccept)=ℙ(drawY=x)ℙ(acceptY|Y=x)=f(x)ℙ(U≤q(Y)Kf(Y)|Y=x)=q(x)K\n\\begin{align*}\n & \\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n= & \\mathbb{P}(\\mathrm{draw}\\,Y=x)\\mathbb{P}(\\left.\\mathrm{accept}\\,Y\\right|Y=x)\\\\\n= & f(x)\\mathbb{P}(\\left.U\\le\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n= & \\frac{q(x)}{K}\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-17",
    "href": "slides/temp.html#mc-methods-17",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: proof for discrete rv\n\nThe probability of rejection of a draw is\nℙ(rejectY)=∑x∈Dℙ(drawY=xandrejectit)=∑x∈Df(x)ℙ(U≥q(Y)Kf(Y)|Y=x)=∑x∈Df(x)(1−q(x)Kf(x))=1−1K\n\\begin{align*}\n\\mathbb{P}(\\mathrm{{reject}}\\,Y) & =\\sum_{x\\in D}\\mathbb{P}(\\mathrm{{draw}}\\,Y=x\\,\\mathrm{and\\,reject\\,it})\\\\\n & =\\sum_{x\\in D}f(x)\\mathbb{P}(\\left.U\\ge\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n & =\\sum_{x\\in D}f(x)(1-\\frac{q(x)}{Kf(x)})=1-\\frac{1}{K}\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-18",
    "href": "slides/temp.html#mc-methods-18",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: proof for discrete rv\n\nand so1\nℙ(X=x)=∑n=1∞ℙ(rejectY)n−1ℙ(drawY=xandaccept)=∑n=1∞(1−1K)n−1q(x)K=q(x)\n\\begin{align*}\n\\mathbb{P}(X=x) & =\\sum_{n=1}^{\\infty}\\mathbb{P}(\\mathrm{reject}\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n & =\\sum_{n=1}^{\\infty}\\left(1-\\frac{1}{K}\\right)^{n-1}\\frac{q(x)}{K}=q(x)\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-19",
    "href": "slides/temp.html#mc-methods-19",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: proof for continuous scalar rv\n\nRecal that we accept the proposal YY whenever (U,Y)∼qU,Y(U,Y)\\sim q_{U,Y} where qU,Y(u,y)=q(y)U0,1(u)q_{U,Y}\\left(u,y\\right)=q(y)U_{0,1}(u) such that U≤q(Y)/(Kf(Y))U\\le q(Y)/(Kf(Y))\nWe have\nℙ(X≤x)=ℙ(Y≤x|U≤q(Y)/Kf(Y)))=ℙ(Y≤x,U≤q(Y)/Kf(Y)))ℙ(U≤q(Y)/Kf(Y)))=∫−∞x∫0q(y)/Kf(y)fU,Y(u,y)dudy∫−∞∞∫0q(y)/Kf(y)fU,Y(u,y)dudy=∫−∞x∫0q(y)/Kf(y)f(y)dudy∫−∞∞∫0q(y)/Kf(y)f(y)dudy=∫−∞xq(y)dy\n\\begin{align*}\n\\mathbb{P}\\left(X\\le x\\right) & =\\mathbb{P}\\left(\\left.Y\\le x\\right|U\\le q(Y)/Kf(Y))\\right)\\\\\n & =\\frac{\\mathbb{P}\\left(Y\\le x,U\\le q(Y)/Kf(Y))\\right)}{\\mathbb{P}\\left(U\\le q(Y)/Kf(Y))\\right)}\\\\\n & =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}\\\\\n & =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}=\\int_{-\\infty}^{x}q(y)dy\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-20",
    "href": "slides/temp.html#mc-methods-20",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: unknown normalizing constants\n\nIn most practical scenarios, we know f(x)f(x) and q(x)q(x) only up to some normalizing constants\nf(x)=f‾(x)/Kfandq(x)=q‾(x)/Kq\nf(x)=\\bar{f}(x)/K_f\\;\\mathrm{and}\\;q(x)=\\bar{q}(x)/K_q\n We can still use rejection sampling since\nq(x)f(x)≤Kiffq‾(x)f‾(x)≤K̂≡KKqKf\n\\frac{q(x)}{f(x)}\\le K\\;\\mathrm{iff}\\;\\frac{\\bar{q}(x)}{\\bar{f}(x)}\\le\\hat{K}\\equiv  K\\frac{K_q}{K_f}\n In practice this means we can ignore the normalizing constants if we can find K̂\\hat{K} to bound q‾(x)f‾(x)\\frac{\\bar{q}(x)}{\\bar{f}(x)}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-21",
    "href": "slides/temp.html#mc-methods-21",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nSuppose we need to compute an expectation 𝔼[g(Z)]\\mathbb{E}[g(Z)] for some random variable ZZ and some function g:ℝ→ℝg:\\mathbb{R}\\to\\mathbb{R}. Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of ZZ, say z1,z2,…,zMz_1,z_2,\\ldots,z_M and estimate 𝔼[g(Z)]\\mathbb{E}[g(Z)] as 1M∑i=1Mg(zi)\\frac{1}{M}\\sum_{i=1}^Mg(z_i). The law of large numbers states that this sample mean should be close to 𝔼[g(Z)]\\mathbb{E}[g(Z)].\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/temp.html#stochastic-processes",
    "href": "slides/temp.html#stochastic-processes",
    "title": "temp",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nA stochastic process, which we will usually write as (Xn)(X_n), is an indexed sequence of random variables that are (usually) dependent on each other.\nEach random variable XnX_n takes a value in a state space 𝒮\\mathcal S which is the set of possible values for the process. As with usual random variables, the state space 𝒮\\mathcal S can be discrete or continuous. A discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, 𝒮={Heads,Tails}\\mathcal S = \\{\\text{Heads},\\text{Tails}\\} is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers 𝒮=ℤ+={0,1,2,…}\\mathcal S = \\mathbb Z_+ = \\{0,1,2,\\dots\\}."
  },
  {
    "objectID": "slides/temp.html#stochastic-processes-1",
    "href": "slides/temp.html#stochastic-processes-1",
    "title": "temp",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nFurther, the process has an index set that puts the random variables that make up the process in order. The index set is usually interpreted as a time variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at distinct points, often denoted by n=0,1,2,…n = 0,1,2,\\dots, while continuous time denotes a process monitored constantly over time, often denoted by t∈ℝ+={x∈ℝ:x≥0}t \\in \\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}."
  },
  {
    "objectID": "slides/temp.html#markov-property",
    "href": "slides/temp.html#markov-property",
    "title": "temp",
    "section": "Markov property",
    "text": "Markov property\nThink of a simple board game where we roll a dice and move that many squares forward on the board. Suppose we are currently on the square XnX_n. Then what can we say about which square Xn+1X_{n+1} we move to on our next turn?\n\nXn+1X_{n+1} is random, since it depends on the roll of the dice.\nXn+1X_{n+1} depends on where we are now XnX_n, since the score of dice will be added onto the number our current square,\nGiven the square XnX_n we are now, Xn+1X_{n+1} doesn’t depend any further on which sequence of squares X0,X1,…,Xn−1X_0, X_1, \\dots, X_{n-1} we used to get here.\n\nThe third point is called the Markov property or memoryless property. We say “memoryless”, because we only need to remember what square we’ve reached, not which squares we used to get here. The stochastic process before this moment has no bearing on the future, given where we are now. A mathematical way to say this is that “the past and the future are conditionally independent given the present.”"
  },
  {
    "objectID": "slides/temp.html#markov-chains",
    "href": "slides/temp.html#markov-chains",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider the following simple random walk on the integers ℤ\\mathbb Z: We start at 00, then at each time step, we go up by one with probability pp and down by one with probability q=1−pq = 1-p. When p=q=12p = q = \\frac12, we’re equally as likely to go up as down, and we call this the simple symmetric random walk.\nThe simple random walk is a simple but very useful model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In many modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the “drunkard’s walk”, suggesting it could model a drunk person trying to stagger home."
  },
  {
    "objectID": "slides/temp.html#markov-chains-1",
    "href": "slides/temp.html#markov-chains-1",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nrandom walks\nrequire(ggplot2, quietly = TRUE)\nset.seed(315)\n\nrrw &lt;- function(n, p = 1/2) {\n  q &lt;- 1 - p\n  Z &lt;- sample(c(1, -1), n, replace = TRUE, prob = c(p, q))\n  X &lt;- c(0, cumsum(Z))\n  c(0, cumsum(Z))\n}\n\nn &lt;- 2000\nrw_dat &lt;- tibble::tibble(x=0:n) |&gt; \n  dplyr::mutate(\n    \"p = 2/3\" = rrw(n, 2/3)\n    , \"p = 1/3\" = rrw(n, 1/3)\n    , \"p = 1/2\" = rrw(n, 1/2)\n  )\n\np0 &lt;- rw_dat |&gt; dplyr::slice_head(n=20) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np1 &lt;- rw_dat |&gt; dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np3 &lt;- rw_dat |&gt; #dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np0+p1+p3"
  },
  {
    "objectID": "slides/temp.html#markov-chains-2",
    "href": "slides/temp.html#markov-chains-2",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nWe can write this as a stochastic process (Xn)(X_n) with discrete time n={0,1,2,…}=ℤ+n = \\{0,1,2,\\dots\\} = \\mathbb Z_+ and discrete state space 𝒮=ℤ\\mathcal S = \\mathbb Z, where X0=0X_0 = 0 and, for n≥0n \\geq 0, we have Xn+1={Xn+1with probability p,Xn−1with probability q. X_{n+1} = \\begin{cases} X_n + 1 & \\text{with probability $p$,} \\\\\n                             X_n - 1 & \\text{with probability $q$.} \\end{cases} \nIt’s clear from this definition that Xn+1X_{n+1} (the future) depends on XnX_n (the present), but, given XnX_n, does not depend on Xn−1,…,X1,X0X_{n-1}, \\dots, X_1, X_0 (the past). Thus the Markov property holds, and the simple random walk is a discrete time Markov process or Markov chain."
  },
  {
    "objectID": "slides/temp.html#markov-chains-3",
    "href": "slides/temp.html#markov-chains-3",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nSo far we’ve seen a a few examples of stochastic processes in discrete time and discrete space with the Markov memoryless property. Now we will develop the theory more generally.\nTo define a so-called “Markov chain”, we first need to say where we start from, and second what the probabilities of transitions from one state to another are.\nIn our examples of the simple random walk and gambler’s ruin, we specified the start point X0=iX_0 = i exactly, but we could pick the start point at random according to some distribution λi=ℙ(X0=i)\\lambda_i = \\mathbb P(X_0 = i)."
  },
  {
    "objectID": "slides/temp.html#markov-chains-4",
    "href": "slides/temp.html#markov-chains-4",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nAfter that, we want to know the transition probabilities ℙ(Xn+1=j∣Xn=i)\\mathbb P(X_{n+1} = j \\mid X_n = i) for i,j∈𝒮i,j \\in \\mathcal S. Here, because of the Markov property, the transition probability only needs to condition on the state we’re in now Xn=iX_n = i, and not on the whole history of the process.\nIn the case of the simple random walk, for example, we had initial distribution λi=ℙ(X0=i)={1if i=00otherwise \\lambda_i = \\mathbb P(X_0 = i) = \\begin{cases} 1 & \\text{if $i = 0$} \\\\ 0 & \\text{otherwise} \\end{cases}  and transition probabilities ℙ(Xn+1=j∣Xn=i)={pif j=i+1qif j=i−10otherwise. \\mathbb P(X_{n+1} = j \\mid X_n = i) = \\begin{cases} p & \\text{if $j = i+1$} \\\\ q & \\text{if $j = i-1$} \\\\ 0 & \\text{otherwise.} \\end{cases} \nFor the random walk (and also the gambler’s ruin), the transition probabilities ℙ(Xn+1=j∣Xn=i)\\mathbb P(X_{n+1} = j \\mid X_n = i) don’t depend on nn; in other words, the transition probabilities stay the same over time. A Markov process with this property is called time homogeneous. We will always consider time homogeneous processes from now on (unless we say otherwise)."
  },
  {
    "objectID": "slides/temp.html#markov-chains-5",
    "href": "slides/temp.html#markov-chains-5",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nLet’s write pij=ℙ(Xn+1=j∣Xn=i)p_{ij} = \\mathbb P(X_{n+1} = j \\mid X_n = i) for the transition probabilities, which are independent of nn. We must have pij≥0p_{ij} \\geq 0, since it is a probability, and we must also have ∑jpij=1\\sum_j p_{ij} = 1 for all states ii, as this is the sum of the probabilities of all the places you can move to from state i.\nWhen the state space is finite (and even sometimes when it’s not), it’s convenient to write the transition probabilities (pij)(p_{ij}) as a matrix 𝖯\\mathsf P, called the transition matrix, whose (i,j)(i,j)th entry is pijp_{ij}. Then the condition that ∑jpij=1\\sum_j p_{ij} = 1 is the condition that each of the rows of 𝖯\\mathsf P add up to 11."
  },
  {
    "objectID": "slides/temp.html#markov-chains-6",
    "href": "slides/temp.html#markov-chains-6",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nConsider a simple two-state Markov chain with state space 𝒮={0,1}\\mathcal S = \\{0,1\\} and transition matrix 𝖯=(p00p01p10p11)=(1−ααβ1−β) \\mathsf P = \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\beta & 1-\\beta \\end{pmatrix}   for some 0&lt;α,β&lt;10 &lt; \\alpha, \\beta &lt; 1. Note that the rows of 𝖯\\mathsf P add up to 11, as they must.\nWe can illustrate 𝖯\\mathsf P by a transition diagram, where the blobs are the states and the arrows give the transition probabilities. (We don’t draw the arrow if pij=0p_{ij} = 0.) In this case, our transition diagram looks like this:"
  },
  {
    "objectID": "slides/temp.html#markov-chains-7",
    "href": "slides/temp.html#markov-chains-7",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\n\n\n\nTransition diagram for the two-state Markov chain"
  },
  {
    "objectID": "slides/temp.html#markov-chains-8",
    "href": "slides/temp.html#markov-chains-8",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nWe can use this as a simple model of customer churn, for example. If the customer has closed their account (state 0) on one period, then with probability α\\alpha we will be able to entice them to open their account again (state 1) by the next period; while if the customer has an account (state 1), then with probability β\\beta they will have closed their account (state 0) by the next period."
  },
  {
    "objectID": "slides/temp.html#markov-chains-9",
    "href": "slides/temp.html#markov-chains-9",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIf the customer has an account on period n, what’s the probability they also have an account on period n+2?\np11(2)=ℙ(Xn+2=1∣Xn=1)\np_{11}(2) = \\mathbb P (X_{n+2} = 1 \\mid X_n = 1)\n\nThe key to calculating this is to condition on the first step again – that is, on whether the printer is working on Tuesday. We have\np11(2)=ℙ(Xn+1=0∣Xn=1)ℙ(Xn+2=1∣Xn+1=0,Xn=1)+ℙ(Xn+1=1∣Xn=1)ℙ(Xn+2=1∣Xn+1=1,Xn=1)=ℙ(Xn+1=0∣Xn=1)ℙ(Xn+2=1∣Xn+1=0)+ℙ(Xn+1=1∣Xn=1)ℙ(Xn+2=1∣Xn+1=1)=p10p01+p11p11=βα+(1−β)2.\n\\begin{align*}\n  p_{11}(2) &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0, X_n = 1) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1, X_n = 1) \\\\\n  &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1) \\\\\n  &= p_{10}p_{01} + p_{11}p_{11} \\\\\n  &= \\beta\\alpha + (1-\\beta)^2 .\n\\end{align*} \n\nIn the second equality, we used the Markov property to mean conditional probabilities like ℙ(Xn+2=1∣Xn+1=k)\\mathbb P(X_{n+2} = 1 \\mid X_{n+1} = k) did not have to depend on XnX_n.\nAnother way to think of this as we summing the probabilities of all length-2 paths from 1 to 1, which are 1→0→11\\to 0\\to 1 with probability βα\\beta\\alpha and 1→1→11 \\to 1 \\to 1 with probability (1−β)2(1-\\beta)^2\n\nIn the above example, we calculated a two-step transition probability pij(2)=ℙ(Xn+2=j∣Xn=i)p_{ij}(2) = \\mathbb P (X_{n+2} = j \\mid X_n = i) by conditioning on the first step. That is, by considering all the possible intermediate steps kk, we have\npij(2)=∑k∈𝒮ℙ(Xn+1=k∣Xn=i)ℙ(Xn+2=j∣Xn+1=k)=∑k∈𝒮pikpkj \np_{ij}(2) = \\sum_{k\\in\\mathcal S} \\mathbb P (X_{n+1} = k \\mid X_n = i)\\mathbb P (X_{n+2} = j \\mid X_{n+1} = k) = \\sum_{k\\in\\mathcal S} p_{ik}p_{kj}\n\nBut this is exactly the formula for multiplying the matrix 𝖯\\mathsf P with itself! In other words, pij(2)=∑kpikpkjp_{ij}(2) = \\sum_{k} p_{ik}p_{kj} is the (i,j)(i,j)th entry of the matrix square 𝖯2=𝖯𝖯\\mathsf P^2 = \\mathsf{PP}. If we write 𝖯(2)=(pij(2))\\mathsf P(2)  = (p_{ij}(2)) for the matrix of two-step transition probabilities, we have 𝖯(2)=𝖯2\\mathsf P(2) = \\mathsf P^2.\nMore generally, we see that this rule holds over multiple steps, provided we sum over all the possible paths i→k1→k2→⋯→kn−1→ji\\to k_1 \\to k_2 \\to \\cdots \\to k_{n-1} \\to j of length nn from ii to jj."
  },
  {
    "objectID": "slides/temp.html#markov-chains-10",
    "href": "slides/temp.html#markov-chains-10",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTheorem 1 Let (Xn)(X_n) be a Markov chain with state space 𝒮\\mathcal S and transition matrix 𝖯=(pij)\\mathsf P = (p_{ij}). For i,j∈𝒮i,j \\in \\mathcal S, write pij(n)=ℙ(Xn=j∣X0=i) p_{ij}(n) = \\mathbb P(X_n = j \\mid X_0 = i)  for the nn-step transition probability. Then\npij(n)=∑k1,k2,…,kn−1∈𝒮pik1pk1k2⋯pkn−2kn−1pkn−1j \np_{ij}(n) = \\sum_{k_1, k_2, \\dots, k_{n-1} \\in \\mathcal S} p_{ik_1} p_{k_1k_2} \\cdots p_{k_{n-2}k_{n-1}} p_{k_{n-1}j}\n\nIn particular, pij(n)p_{ij}(n) is the (i,j)(i,j)th element of the matrix power 𝖯n\\mathsf P^n, and the matrix of nn-step transition probabilities is given by 𝖯(n)=𝖯n\\mathsf P(n) = \\mathsf P^n."
  },
  {
    "objectID": "slides/temp.html#markov-chains-11",
    "href": "slides/temp.html#markov-chains-11",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nThe so-called Chapman–Kolmogorov equations follow immediately from this.\n\nLet (Xn)(X_n) be a Markov chain with state space 𝒮\\mathcal S and transition matrix 𝖯=(pij)\\mathsf P = (p_{ij}). Then, for non-negative integers n,mn,m, we have pij(n+m)=∑k∈𝒮pik(n)pkj(m), p_{ij}(n+m) = \\sum_{k \\in \\mathcal S} p_{ik}(n)p_{kj}(m) ,  or, in matrix notation, 𝖯(n+m)=𝖯(n)𝖯(m)\\mathsf P(n+m) = \\mathsf P(n)\\mathsf P(m).\n\nIn other words, a trip of length n+mn + m from ii to jj is a trip of length nn from ii to some other state kk, then a trip of length mm from kk back to jj, and this intermediate stop kk can be any state, so we have to sum the probabilities.\nOf course, once we know that 𝖯(n)=𝖯n\\mathsf P(n) = \\mathsf P^n is given by the matrix power, it’s clear to see that 𝖯(n+m)=𝖯n+m=𝖯n𝖯m=𝖯(n)𝖯(m)\\mathsf P(n+m) = \\mathsf P^{n+m} = \\mathsf P^n \\mathsf P^m = \\mathsf P(n)\\mathsf P(m)."
  },
  {
    "objectID": "slides/temp.html#markov-chains-12",
    "href": "slides/temp.html#markov-chains-12",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nIf we start from a state given by a distribution 𝛑=(πi)\\boldsymbol \\pi = (\\pi_i), then after step 1 the probability we’re in state jj is ∑iπipij\\sum_i \\pi_i p_{ij}. So if πj=∑iπipij\\pi_j = \\sum_i \\pi_i p_{ij}, we stay in this distribution forever. We call such a distribution a stationary distribution. We again recognise this formula as a matrix-vector multiplication, so this is 𝛑=𝛑𝖯\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P, where 𝛑\\boldsymbol \\pi is a row vector.\n\nLet (Xn)(X_n) be a Markov chain on a state space 𝒮\\mathcal S with transition matrix 𝖯\\mathsf P. Let 𝛑=(πi)\\boldsymbol \\pi = (\\pi_i) be a distribution on 𝒮\\mathcal S, in that πi≥0\\pi_i \\geq 0 for all i∈𝒮i \\in \\mathcal S and ∑i∈𝒮πi=1\\sum_{i \\in \\mathcal S} \\pi_i = 1. We call 𝛑\\boldsymbol \\pi a stationary distribution if\nπj=*∑i∈𝒮πipijfor all j∈𝒮\n\\pi_j =* \\sum_{i\\in \\mathcal S} \\pi_i p_{ij} \\quad \\text{for all $j \\in \\mathcal S$} \n\nor, equivalently, if 𝛑=𝛑𝖯\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P.\n\nNote that we’re saying the distribution ℙ(Xn=i)\\mathbb P(X_n = i) stays the same; the Markov chain (Xn)(X_n) itself will keep moving. One way to think is that if we started off a thousand Markov chains, choosing each starting position to be ii with probability πi\\pi_i, then (roughly) 1000πj1000 \\pi_j of them would be in state jj at any time in the future – but not necessarily the same ones each time."
  },
  {
    "objectID": "slides/temp.html#markov-chains-13",
    "href": "slides/temp.html#markov-chains-13",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nproperties\nA Markov Chain is irreducible if you have positive probability of eventually getting from anywhere to anywhere else.\nA Markov Chain is aperiodic if there are no forced cycles, i.e. there do not exist disjoint non-empty subsets X1,X2,…,Xd ford≥2,suchthatP(x,Xi+1)=1 forallx∈Xi (1≤i≤d−1),andP(x,X1)=1forallx∈Xd. [Diagram.]"
  },
  {
    "objectID": "slides/temp.html#markov-chains-14",
    "href": "slides/temp.html#markov-chains-14",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\n\n\n\nMC with cycle"
  },
  {
    "objectID": "slides/temp.html#markov-chain-monte-carlo",
    "href": "slides/temp.html#markov-chain-monte-carlo",
    "title": "temp",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\nSuppose have complicated, high-dimensional density π=cg\\pi = cg and we want samples X1,X2,…∼πX_1, X_2,\\dots \\sim \\pi. (Then can do Monte Carlo.)\nDefine a Markovchain (dependent random process) X0,X1,X2…X_0, X_1,X_2\\dots in such a way that for large enough nn, Xn∼πX_n\\sim \\pi.\nThen we can estimate 𝔼π(h)≡∫h(x)π(x)dx\\mathbb{E}_{\\pi}(h) ≡ \\int h(x) \\pi(x) dx by:\n𝔼π[h]≈1M−B∑i=B+1M\n\\mathbb{E}_{\\pi}[h] \\approx \\frac{1}{M-B}\\sum_{i=B+1}^{M}\n\nwhere BB (“burn-in”) is chosen large enough so XB∼πX_B\\sim\\pi, and MM is chosen large enough to get good Monte Carlo estimates."
  },
  {
    "objectID": "slides/temp.html#mcmc-metropolis-algo",
    "href": "slides/temp.html#mcmc-metropolis-algo",
    "title": "temp",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\nchoose some initial value X0X_0, then\ngiven Xn−1X_{n-1}, choose a proposal state Yn∼MVN(Xn−1,σ2I)Y_n\\sim \\textrm{MVN}(X_{n-1},\\sigma^2\\textrm{I}) for some fixed σ2&gt;0\\sigma^2&gt;0\nlet An=π(Yn)/π(Xn−1=g(Yn)/g(Xn−1)A_n=\\pi(Y_n)/\\pi(X_{n-1}=g(Y_n)/g(X_{n-1}) and Un∼U[0,1]U_n\\sim\\textrm{U}[0,1], then\nid Un&lt;AnU_n&lt;A_n set Xn=YnX_n=Y_n (“accept”), otherwise set $X_n = X_{n-1} ) “reject”\nrepeat for n=1,2,3,…,Mn=1,2,3,\\ldots,M\n\nThis version is called random-walk Metropolis"
  },
  {
    "objectID": "slides/temp.html#mcmc-metropolis-algo-1",
    "href": "slides/temp.html#mcmc-metropolis-algo-1",
    "title": "temp",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\n\na simple Metropolis algorithm in one dimension\ng = function(y) {\n    if ( (y&lt;0) || (y&gt;1) )\n    return(0)\n    else\n    return( y^3 * sin(y^4) * cos(y^5) )\n}\n\nh = function(y) { return(y^2) }\n\nM = 11000  # run length\nB = 1000  # amount of burn-in\nX = runif(1)  # overdispersed starting distribution\nsigma = 1  # proposal scaling\nxlist = rep(0,M)  # for keeping track of chain values\nhlist = rep(0,M)  # for keeping track of h function values\nnumaccept = 0;\n\nfor (i in 1:M) {\n  Y = X + sigma * rnorm(1)  # proposal value\n  U = runif(1)              # for accept/reject\n  alpha = g(Y) / g(X)       # for accept/reject\n  if (U &lt; alpha) {\n    X = Y                   # accept proposal\n    numaccept = numaccept + 1;\n  }\n    xlist[i] = X;\n    hlist[i] = h(X);\n}\n\ncat(\"ran Metropolis algorithm for\", M, \"iterations, with burn-in\", B, \"\\n\");\n\n\nran Metropolis algorithm for 11000 iterations, with burn-in 1000 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"acceptance rate =\", numaccept/M, \"\\n\");\n\n\nacceptance rate = 0.1046 \n\n\na simple Metropolis algorithm in one dimension\nu = mean(hlist[(B+1):M])\ncat(\"mean of h is about\", u, \"\\n\")\n\n\nmean of h is about 0.773 \n\n\na simple Metropolis algorithm in one dimension\nse1 =  sd(hlist[(B+1):M]) / sqrt(M-B)\ncat(\"iid standard error would be about\", se1, \"\\n\")\n\n\niid standard error would be about 0.001658 \n\n\na simple Metropolis algorithm in one dimension\nvarfact &lt;- function(xxx) { 2 * sum(acf(xxx, plot=FALSE)$acf) - 1 }\nthevarfact = varfact(hlist[(B+1):M])\nse = se1 * sqrt( thevarfact )\ncat(\"varfact = \", thevarfact, \"\\n\")\n\n\nvarfact =  21.02 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"true standard error is about\", se, \"\\n\")\n\n\ntrue standard error is about 0.007601 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"approximate 95% confidence interval is (\", u - 1.96 * se, \",\",\n                        u + 1.96 * se, \")\\n\\n\")\n\n\napproximate 95% confidence interval is ( 0.7581 , 0.7879 )\n\n\na simple Metropolis algorithm in one dimension\nplot(xlist, type='l')\n\n\n\n\n\n\n\n\n\na simple Metropolis algorithm in one dimension\n# acf(xlist)"
  },
  {
    "objectID": "slides/temp.html#footnotes",
    "href": "slides/temp.html#footnotes",
    "title": "temp",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe geometric distribution is a discrete distribution that can be interpreted as the number of failures before the first success (ℙ(X=k)=(1−p)k−1p\\mathbb{P}(X=k)=(1-p)^{k-1}p, with mean pp).↩︎"
  },
  {
    "objectID": "slides/bayes.html",
    "href": "slides/bayes.html",
    "title": "Bayes",
    "section": "",
    "text": "We express the likelihood for our coin toss example as\nyi∼Bernoulli(θ)y_{i} \\sim \\operatorname{Bernoulli}(\\theta)\nOur prior will be\nθ∼Beta(α,β)\\theta \\sim \\operatorname{Beta}(\\alpha, \\beta)\n\n#|echo: true\ndat &lt;- readr::read_csv(\"data/z15N50.csv\", show_col_types = FALSE)\n\ndat |&gt; \n  dplyr::mutate(y = y |&gt; as.character()) |&gt; \n  ggplot(aes(x = y)) +\n  geom_bar() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nfit8.1 &lt;-\n  brms::brm(data = my_data, \n      family = brms::bernoulli(link = identity),\n      formula = y ~ 1,\n      brms::prior(beta(2, 2), class = Intercept, lb = 0, ub = 1),\n      iter = 500 + 3334, warmup = 500, chains = 3,\n      seed = 8,\n      file = \"fits/fit08.01\")\n\n\nplot(fit8.1)\n\n\n\n\n\n\n\n\n\ndraws &lt;- brms::as_draws_df(fit8.1) \ndraws\n\n\ndraws |&gt; \n  dplyr::mutate(chain = .chain) |&gt; \n  bayesplot::mcmc_dens_overlay(pars = vars(b_Intercept)) \n\n\n\n\n\n\n\n\n\ndraws |&gt; \n  dplyr::mutate(chain = .chain) |&gt; \n  bayesplot::mcmc_acf(pars = vars(b_Intercept), lags = 35) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nbrms::rhat(fit8.1)[\"b_Intercept\"]\n\nb_Intercept \n      1.002 \n\nbrms::neff_ratio(fit8.1)[\"b_Intercept\"]\n\nb_Intercept \n     0.3337 \n\n\n\nprint(fit8.1)\n\n Family: bernoulli \n  Links: mu = identity \nFormula: y ~ 1 \n   Data: dat (Number of observations: 50) \n  Draws: 3 chains, each with iter = 3834; warmup = 500; thin = 1;\n         total post-warmup draws = 10002\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept     0.32      0.06     0.20     0.44 1.00\n          Bulk_ESS Tail_ESS\nIntercept     3338     4646\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nbrms::posterior_summary(fit8.1, robust = T)\n\n            Estimate Est.Error      Q2.5    Q97.5\nb_Intercept   0.3128   0.06276   0.20028   0.4444\nIntercept     0.3128   0.06276   0.20028   0.4444\nlprior        0.2544   0.10549  -0.03978   0.3929\nlp__        -32.0707   0.30671 -34.35061 -31.8453\n\n\n\nbrms::posterior_summary(fit8.1, probs = c(.025, .25, .75, .975))\n\n            Estimate Est.Error      Q2.5      Q25\nb_Intercept   0.3155    0.0627   0.20028   0.2714\nIntercept     0.3155    0.0627   0.20028   0.2714\nlprior        0.2345    0.1137  -0.03978   0.1710\nlp__        -32.3491    0.7157 -34.35061 -32.5156\n                 Q75    Q97.5\nb_Intercept   0.3561   0.4444\nIntercept     0.3561   0.4444\nlprior        0.3190   0.3929\nlp__        -31.8984 -31.8453\n\n\n\nbayesplot::mcmc_areas(\n  draws, \n  pars = vars(b_Intercept),\n  prob = 0.5,\n  prob_outer = 0.95,\n  point_est = \"mean\"\n) +\n  scale_y_discrete(expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"Theta via bayesplot::mcmc_areas()\",\n       x = expression(theta)) +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\nScale for y is already present.\nAdding another scale for y, which will replace the\nexisting scale.\n\n\n\n\n\n\n\n\n\n\nlibrary(tidybayes)\n\n\nAttaching package: 'tidybayes'\n\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t,\n    rstudent_t\n\ndraws |&gt; \n  ggplot(aes(x = b_Intercept, y = 0)) +\n  tidybayes::stat_halfeye(point_interval = tidybayes::mode_hdi, .width = c(.95, .5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = expression(theta*\" via tidybayes::stat_halfeye()\"),\n       x = expression(theta)) +\n  theme_minimal()\n\n\n\n\n\n\n\ndraws |&gt; \n  ggplot(aes(x = b_Intercept, y = 0)) +\n  tidybayes::stat_histinterval(point_interval = tidybayes::mode_hdi, .width = c(.95, .5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = expression(theta*\" via tidybayes::stat_histinterval()\"),\n       x = expression(theta)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndat &lt;- readr::read_csv(\"data/z6N8z2N7.csv\", show_col_types = FALSE) |&gt; \n  dplyr::mutate(\n    s = \n      dplyr::case_when(s == \"Reginald\" ~ \"Windsor\", TRUE ~ \"London\")\n  )\n\n\ndat |&gt; \n  dplyr::mutate(y = y |&gt; as.character()) |&gt; \n  ggplot(aes(x = y, fill = s)) +\n  geom_bar(show.legend = F) +\n  ggthemes::scale_fill_colorblind() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal() +\n  facet_wrap(~ s)\n\n\n\n\n\n\n\n\n\nfit8.2 &lt;-\n  brms::brm(data = dat, \n      family = brms::bernoulli(identity),\n      y ~ 0 + s,\n      brms::prior(beta(2, 2), class = b, lb = 0, ub = 1),\n      iter = 2000, warmup = 500, cores = 4, chains = 4,\n      seed = 8,\n      file = \"fits/fit08.02\")\n\n\nplot(fit8.2, widths = c(2, 3))\n\n\n\n\n\n\n\n\n\nsummary(fit8.2)\n\n Family: bernoulli \n  Links: mu = identity \nFormula: y ~ 0 + s \n   Data: dat (Number of observations: 15) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n         Estimate Est.Error l-95% CI u-95% CI Rhat\nsLondon      0.37      0.14     0.12     0.65 1.00\nsWindsor     0.67      0.13     0.39     0.90 1.00\n         Bulk_ESS Tail_ESS\nsLondon      4942     4036\nsWindsor     5003     3408\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\npairs(fit8.2,\n      off_diag_args = list(size = 1/3, alpha = 1/3))\n\n\n\n\n\n\n\n\n\ndraws &lt;- brms::as_draws_df(fit8.2)\n\ndraws &lt;-\n  draws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  dplyr::mutate(`theta_Windsor - theta_London` = theta_Windsor - theta_London)\n\ndplyr::glimpse(draws |&gt; dplyr::slice_head(n=6))\n\nRows: 6\nColumns: 8\n$ theta_London                   &lt;dbl&gt; 0.5113, 0.2227…\n$ theta_Windsor                  &lt;dbl&gt; 0.7290, 0.6053…\n$ lprior                         &lt;dbl&gt; 0.5750, 0.3981…\n$ lp__                           &lt;dbl&gt; -11.86, -11.92…\n$ .chain                         &lt;int&gt; 1, 1, 1, 1, 1,…\n$ .iteration                     &lt;int&gt; 1, 2, 3, 4, 5,…\n$ .draw                          &lt;int&gt; 1, 2, 3, 4, 5,…\n$ `theta_Windsor - theta_London` &lt;dbl&gt; 0.217708, 0.38…\n\n\n\nlong_draws &lt;-\n  draws |&gt; \n  dplyr::select(starts_with(\"theta\")) |&gt; \n  tidyr::pivot_longer(everything()) |&gt; \n  dplyr::mutate(name = factor(name, levels = c(\"theta_Windsor\", \"theta_London\", \"theta_Windsor - theta_London\"))) \n  \nlong_draws |&gt; \n  ggplot(aes(x = value, y = 0, fill = name)) +\n  stat_histinterval(point_interval = mode_hdi, .width = .95,\n                    slab_color = \"white\", outline_bars = T,\n                    normalize = \"panels\") +\n  scale_fill_manual(values = ggthemes::colorblind_pal()(8)[2:4], breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\n\nlong_draws |&gt; \n  dplyr::group_by(name) |&gt; \n  tidybayes::mode_hdi()\n\n# A tibble: 3 × 7\n  name     value  .lower .upper .width .point .interval\n  &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 theta_W… 0.695  0.420   0.917   0.95 mode   hdi      \n2 theta_L… 0.317  0.105   0.626   0.95 mode   hdi      \n3 theta_W… 0.330 -0.0689  0.664   0.95 mode   hdi      \n\n\n\nfit8.3 &lt;-\n  brms::brm(data = dat, \n      family = brms::bernoulli(identity),\n      y ~ 0 + s,\n      prior =\n        c(brms::prior(beta(2, 2), class = b, coef = sWindsor),\n          brms::prior(beta(2, 2), class = b, coef = sLondon),\n          # this just sets the lower and upper bounds\n          brms::prior(beta(2, 2), class = b, lb = 0, ub = 1)),\n      iter = 2000, warmup = 500, cores = 4, chains = 4,\n      sample_prior = \"only\",\n      seed = 8,\n      file = \"fits/fit08.03\")\n\n\ndraws &lt;- brms::as_draws_df(fit8.3) |&gt; \n  dplyr::select(starts_with(\"b_\"))\n\nWarning: Dropping 'draws_df' class as required\nmetadata was removed.\n\nhead(draws)\n\n# A tibble: 6 × 2\n  b_sLondon b_sWindsor\n      &lt;dbl&gt;      &lt;dbl&gt;\n1     0.704      0.451\n2     0.178      0.429\n3     0.180      0.505\n4     0.819      0.237\n5     0.597      0.816\n6     0.347      0.185\n\n\n\ndat |&gt; \n  dplyr::group_by(s) |&gt; \n  dplyr::summarise(z = sum(y),\n            N = dplyr::n()) |&gt; \n  dplyr::mutate(`z/N` = z / N)\n\n# A tibble: 2 × 4\n  s           z     N `z/N`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 London      2     7 0.286\n2 Windsor     6     8 0.75 \n\nlevels &lt;- c(\"theta_Windsor\", \"theta_London\", \"theta_Windsor - theta_London\")\n\nd_line &lt;-\n  tibble::tibble(value = c(.75, .286, .75 - .286),\n         name  =  factor(c(\"theta_Windsor\", \"theta_London\", \"theta_Windsor - theta_London\"), \n                         levels = levels))\n\n\ndraws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  dplyr::mutate(\"theta_Windsor - theta_London\" = theta_Windsor - theta_London) |&gt; \n  tidyr::pivot_longer(contains(\"theta\")) |&gt; \n  dplyr::mutate(name = factor(name, levels = levels)) |&gt;\n  \n  ggplot(aes(x = value, y = 0)) +\n  stat_histinterval(point_interval = tidybayes::mode_hdi, .width = .95,\n                    fill = ggthemes::colorblind_pal()(8)[5], normalize = \"panels\") +\n  geom_vline(data = d_line, \n             aes(xintercept = value), \n             linetype = 2) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(\"The dashed vertical lines mark off \"*italic(z[s])/italic(N[s]))) +\n  cowplot::theme_cowplot() +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\n\ndraws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  \n  ggplot(aes(x = theta_Windsor, y = theta_London)) +\n  geom_point(alpha = 1/4, color = ggthemes::colorblind_pal()(8)[6]) +\n  coord_equal() +\n  cowplot::theme_minimal_grid()\n\n\n\n\n\n\n\n\n\ndraws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  \n  ggplot(aes(x = theta_Windsor, y = theta_London)) +\n  stat_density_2d(aes(fill = stat(density)), \n                  geom = \"raster\", contour = F) +\n  scale_fill_viridis_c(option = \"B\", breaks = NULL) +\n  scale_x_continuous(expression(theta[1]), \n                     expand = c(0, 0), limits = c(0, 1),\n                     breaks = 0:4 / 4, labels = c(\"0\", \".25\", \".5\", \".75\", \"1\")) +\n  scale_y_continuous(expression(theta[2]), \n                     expand = c(0, 0), limits = c(0, 1),\n                     breaks = 0:4 / 4, labels = c(\"0\", \".25\", \".5\", \".75\", \"1\")) +\n  coord_equal() +\n  cowplot::theme_minimal_grid()\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 396 rows containing missing values or values\noutside the scale range (`geom_raster()`)."
  },
  {
    "objectID": "slides/bayes.html#basic-model",
    "href": "slides/bayes.html#basic-model",
    "title": "Bayes",
    "section": "",
    "text": "We express the likelihood for our coin toss example as\nyi∼Bernoulli(θ)y_{i} \\sim \\operatorname{Bernoulli}(\\theta)\nOur prior will be\nθ∼Beta(α,β)\\theta \\sim \\operatorname{Beta}(\\alpha, \\beta)\n\n#|echo: true\ndat &lt;- readr::read_csv(\"data/z15N50.csv\", show_col_types = FALSE)\n\ndat |&gt; \n  dplyr::mutate(y = y |&gt; as.character()) |&gt; \n  ggplot(aes(x = y)) +\n  geom_bar() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nfit8.1 &lt;-\n  brms::brm(data = my_data, \n      family = brms::bernoulli(link = identity),\n      formula = y ~ 1,\n      brms::prior(beta(2, 2), class = Intercept, lb = 0, ub = 1),\n      iter = 500 + 3334, warmup = 500, chains = 3,\n      seed = 8,\n      file = \"fits/fit08.01\")\n\n\nplot(fit8.1)\n\n\n\n\n\n\n\n\n\ndraws &lt;- brms::as_draws_df(fit8.1) \ndraws\n\n\ndraws |&gt; \n  dplyr::mutate(chain = .chain) |&gt; \n  bayesplot::mcmc_dens_overlay(pars = vars(b_Intercept)) \n\n\n\n\n\n\n\n\n\ndraws |&gt; \n  dplyr::mutate(chain = .chain) |&gt; \n  bayesplot::mcmc_acf(pars = vars(b_Intercept), lags = 35) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nbrms::rhat(fit8.1)[\"b_Intercept\"]\n\nb_Intercept \n      1.002 \n\nbrms::neff_ratio(fit8.1)[\"b_Intercept\"]\n\nb_Intercept \n     0.3337 \n\n\n\nprint(fit8.1)\n\n Family: bernoulli \n  Links: mu = identity \nFormula: y ~ 1 \n   Data: dat (Number of observations: 50) \n  Draws: 3 chains, each with iter = 3834; warmup = 500; thin = 1;\n         total post-warmup draws = 10002\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept     0.32      0.06     0.20     0.44 1.00\n          Bulk_ESS Tail_ESS\nIntercept     3338     4646\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nbrms::posterior_summary(fit8.1, robust = T)\n\n            Estimate Est.Error      Q2.5    Q97.5\nb_Intercept   0.3128   0.06276   0.20028   0.4444\nIntercept     0.3128   0.06276   0.20028   0.4444\nlprior        0.2544   0.10549  -0.03978   0.3929\nlp__        -32.0707   0.30671 -34.35061 -31.8453\n\n\n\nbrms::posterior_summary(fit8.1, probs = c(.025, .25, .75, .975))\n\n            Estimate Est.Error      Q2.5      Q25\nb_Intercept   0.3155    0.0627   0.20028   0.2714\nIntercept     0.3155    0.0627   0.20028   0.2714\nlprior        0.2345    0.1137  -0.03978   0.1710\nlp__        -32.3491    0.7157 -34.35061 -32.5156\n                 Q75    Q97.5\nb_Intercept   0.3561   0.4444\nIntercept     0.3561   0.4444\nlprior        0.3190   0.3929\nlp__        -31.8984 -31.8453\n\n\n\nbayesplot::mcmc_areas(\n  draws, \n  pars = vars(b_Intercept),\n  prob = 0.5,\n  prob_outer = 0.95,\n  point_est = \"mean\"\n) +\n  scale_y_discrete(expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"Theta via bayesplot::mcmc_areas()\",\n       x = expression(theta)) +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\nScale for y is already present.\nAdding another scale for y, which will replace the\nexisting scale.\n\n\n\n\n\n\n\n\n\n\nlibrary(tidybayes)\n\n\nAttaching package: 'tidybayes'\n\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t,\n    rstudent_t\n\ndraws |&gt; \n  ggplot(aes(x = b_Intercept, y = 0)) +\n  tidybayes::stat_halfeye(point_interval = tidybayes::mode_hdi, .width = c(.95, .5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = expression(theta*\" via tidybayes::stat_halfeye()\"),\n       x = expression(theta)) +\n  theme_minimal()\n\n\n\n\n\n\n\ndraws |&gt; \n  ggplot(aes(x = b_Intercept, y = 0)) +\n  tidybayes::stat_histinterval(point_interval = tidybayes::mode_hdi, .width = c(.95, .5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = expression(theta*\" via tidybayes::stat_histinterval()\"),\n       x = expression(theta)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndat &lt;- readr::read_csv(\"data/z6N8z2N7.csv\", show_col_types = FALSE) |&gt; \n  dplyr::mutate(\n    s = \n      dplyr::case_when(s == \"Reginald\" ~ \"Windsor\", TRUE ~ \"London\")\n  )\n\n\ndat |&gt; \n  dplyr::mutate(y = y |&gt; as.character()) |&gt; \n  ggplot(aes(x = y, fill = s)) +\n  geom_bar(show.legend = F) +\n  ggthemes::scale_fill_colorblind() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal() +\n  facet_wrap(~ s)\n\n\n\n\n\n\n\n\n\nfit8.2 &lt;-\n  brms::brm(data = dat, \n      family = brms::bernoulli(identity),\n      y ~ 0 + s,\n      brms::prior(beta(2, 2), class = b, lb = 0, ub = 1),\n      iter = 2000, warmup = 500, cores = 4, chains = 4,\n      seed = 8,\n      file = \"fits/fit08.02\")\n\n\nplot(fit8.2, widths = c(2, 3))\n\n\n\n\n\n\n\n\n\nsummary(fit8.2)\n\n Family: bernoulli \n  Links: mu = identity \nFormula: y ~ 0 + s \n   Data: dat (Number of observations: 15) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n         Estimate Est.Error l-95% CI u-95% CI Rhat\nsLondon      0.37      0.14     0.12     0.65 1.00\nsWindsor     0.67      0.13     0.39     0.90 1.00\n         Bulk_ESS Tail_ESS\nsLondon      4942     4036\nsWindsor     5003     3408\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\npairs(fit8.2,\n      off_diag_args = list(size = 1/3, alpha = 1/3))\n\n\n\n\n\n\n\n\n\ndraws &lt;- brms::as_draws_df(fit8.2)\n\ndraws &lt;-\n  draws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  dplyr::mutate(`theta_Windsor - theta_London` = theta_Windsor - theta_London)\n\ndplyr::glimpse(draws |&gt; dplyr::slice_head(n=6))\n\nRows: 6\nColumns: 8\n$ theta_London                   &lt;dbl&gt; 0.5113, 0.2227…\n$ theta_Windsor                  &lt;dbl&gt; 0.7290, 0.6053…\n$ lprior                         &lt;dbl&gt; 0.5750, 0.3981…\n$ lp__                           &lt;dbl&gt; -11.86, -11.92…\n$ .chain                         &lt;int&gt; 1, 1, 1, 1, 1,…\n$ .iteration                     &lt;int&gt; 1, 2, 3, 4, 5,…\n$ .draw                          &lt;int&gt; 1, 2, 3, 4, 5,…\n$ `theta_Windsor - theta_London` &lt;dbl&gt; 0.217708, 0.38…\n\n\n\nlong_draws &lt;-\n  draws |&gt; \n  dplyr::select(starts_with(\"theta\")) |&gt; \n  tidyr::pivot_longer(everything()) |&gt; \n  dplyr::mutate(name = factor(name, levels = c(\"theta_Windsor\", \"theta_London\", \"theta_Windsor - theta_London\"))) \n  \nlong_draws |&gt; \n  ggplot(aes(x = value, y = 0, fill = name)) +\n  stat_histinterval(point_interval = mode_hdi, .width = .95,\n                    slab_color = \"white\", outline_bars = T,\n                    normalize = \"panels\") +\n  scale_fill_manual(values = ggthemes::colorblind_pal()(8)[2:4], breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\n\nlong_draws |&gt; \n  dplyr::group_by(name) |&gt; \n  tidybayes::mode_hdi()\n\n# A tibble: 3 × 7\n  name     value  .lower .upper .width .point .interval\n  &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 theta_W… 0.695  0.420   0.917   0.95 mode   hdi      \n2 theta_L… 0.317  0.105   0.626   0.95 mode   hdi      \n3 theta_W… 0.330 -0.0689  0.664   0.95 mode   hdi      \n\n\n\nfit8.3 &lt;-\n  brms::brm(data = dat, \n      family = brms::bernoulli(identity),\n      y ~ 0 + s,\n      prior =\n        c(brms::prior(beta(2, 2), class = b, coef = sWindsor),\n          brms::prior(beta(2, 2), class = b, coef = sLondon),\n          # this just sets the lower and upper bounds\n          brms::prior(beta(2, 2), class = b, lb = 0, ub = 1)),\n      iter = 2000, warmup = 500, cores = 4, chains = 4,\n      sample_prior = \"only\",\n      seed = 8,\n      file = \"fits/fit08.03\")\n\n\ndraws &lt;- brms::as_draws_df(fit8.3) |&gt; \n  dplyr::select(starts_with(\"b_\"))\n\nWarning: Dropping 'draws_df' class as required\nmetadata was removed.\n\nhead(draws)\n\n# A tibble: 6 × 2\n  b_sLondon b_sWindsor\n      &lt;dbl&gt;      &lt;dbl&gt;\n1     0.704      0.451\n2     0.178      0.429\n3     0.180      0.505\n4     0.819      0.237\n5     0.597      0.816\n6     0.347      0.185\n\n\n\ndat |&gt; \n  dplyr::group_by(s) |&gt; \n  dplyr::summarise(z = sum(y),\n            N = dplyr::n()) |&gt; \n  dplyr::mutate(`z/N` = z / N)\n\n# A tibble: 2 × 4\n  s           z     N `z/N`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 London      2     7 0.286\n2 Windsor     6     8 0.75 \n\nlevels &lt;- c(\"theta_Windsor\", \"theta_London\", \"theta_Windsor - theta_London\")\n\nd_line &lt;-\n  tibble::tibble(value = c(.75, .286, .75 - .286),\n         name  =  factor(c(\"theta_Windsor\", \"theta_London\", \"theta_Windsor - theta_London\"), \n                         levels = levels))\n\n\ndraws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  dplyr::mutate(\"theta_Windsor - theta_London\" = theta_Windsor - theta_London) |&gt; \n  tidyr::pivot_longer(contains(\"theta\")) |&gt; \n  dplyr::mutate(name = factor(name, levels = levels)) |&gt;\n  \n  ggplot(aes(x = value, y = 0)) +\n  stat_histinterval(point_interval = tidybayes::mode_hdi, .width = .95,\n                    fill = ggthemes::colorblind_pal()(8)[5], normalize = \"panels\") +\n  geom_vline(data = d_line, \n             aes(xintercept = value), \n             linetype = 2) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(\"The dashed vertical lines mark off \"*italic(z[s])/italic(N[s]))) +\n  cowplot::theme_cowplot() +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\n\ndraws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  \n  ggplot(aes(x = theta_Windsor, y = theta_London)) +\n  geom_point(alpha = 1/4, color = ggthemes::colorblind_pal()(8)[6]) +\n  coord_equal() +\n  cowplot::theme_minimal_grid()\n\n\n\n\n\n\n\n\n\ndraws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  \n  ggplot(aes(x = theta_Windsor, y = theta_London)) +\n  stat_density_2d(aes(fill = stat(density)), \n                  geom = \"raster\", contour = F) +\n  scale_fill_viridis_c(option = \"B\", breaks = NULL) +\n  scale_x_continuous(expression(theta[1]), \n                     expand = c(0, 0), limits = c(0, 1),\n                     breaks = 0:4 / 4, labels = c(\"0\", \".25\", \".5\", \".75\", \"1\")) +\n  scale_y_continuous(expression(theta[2]), \n                     expand = c(0, 0), limits = c(0, 1),\n                     breaks = 0:4 / 4, labels = c(\"0\", \".25\", \".5\", \".75\", \"1\")) +\n  coord_equal() +\n  cowplot::theme_minimal_grid()\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 396 rows containing missing values or values\noutside the scale range (`geom_raster()`)."
  },
  {
    "objectID": "slides/bayes.html#hierarchical-model",
    "href": "slides/bayes.html#hierarchical-model",
    "title": "Bayes",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\nCoin flip"
  },
  {
    "objectID": "slides/bayes.html#a-single-coin-from-a-single-mint",
    "href": "slides/bayes.html#a-single-coin-from-a-single-mint",
    "title": "Bayes",
    "section": "A single coin from a single mint",
    "text": "A single coin from a single mint\nRecall from the last chapter that our likelihood is the Bernoulli distribution,\nyi∼Bernoulli(θ).y_i \\sim \\operatorname{Bernoulli}(\\theta).\nWe’ll use the beta density for our prior distribution for θ\\theta,\nθ∼Beta(α,β).\\theta \\sim \\operatorname{Beta}(\\alpha, \\beta).\nAnd we can re-express α\\alpha and β\\beta in terms of the mode ω\\omega and concentration κ\\kappa, such that\nα=ω(κ−2)+1andβ=(1−ω)(κ−2)+1.\\alpha = \\omega(\\kappa - 2) + 1 \\;\\;\\; \\textrm{and} \\;\\;\\; \\beta = (1 - \\omega)(\\kappa - 2) + 1.\nAs a consequence, we can re-express θ\\theta as\nθ∼Beta(ω(κ−2)+1,(1−ω)(κ−2)+1).\\theta \\sim \\operatorname{Beta}(\\omega(\\kappa - 2) + 1, (1 - \\omega)(\\kappa - 2) + 1).\nThe value of κ\\kappa governs how near θ\\theta is to ω\\omega, with larger values of κ\\kappa generating values of θ\\theta more concentrated near ω\\omega.\nUsing ss for shape and rr for rate, Kruschke’s Equations 9.7 and 9.8 are as follows:\n$$\ns = \\frac{\\mu^2}{\\sigma^2} \\;\\;\\; \\text{and} \\;\\;\\; r = \\frac{\\mu}{\\sigma^2} \\;\\;\\; \\text{for mean} \\;\\;\\; \\mu &gt; 0 \\\\\ns = 1 + \\omega r \\;\\;\\; \\text{where} \\;\\;\\; r = \\frac{\\omega + \\sqrt{\\omega^2 + 4\\sigma^2}}{2\\sigma^2} \\;\\;\\; \\text{for mode} \\;\\;\\; \\omega &gt; 0.\n$$\n\ngamma_s_and_r_from_mean_sd &lt;- function(mean, sd) {\n  if (mean &lt;= 0) stop(\"mean must be &gt; 0\")\n  if (sd   &lt;= 0) stop(\"sd must be &gt; 0\")\n  shape &lt;- mean^2 / sd^2\n  rate  &lt;- mean   / sd^2\n  return(list(shape = shape, rate = rate))\n}\n\ngamma_s_and_r_from_mode_sd &lt;- function(mode, sd) {\n  if (mode &lt;= 0) stop(\"mode must be &gt; 0\")\n  if (sd   &lt;= 0) stop(\"sd must be &gt; 0\")\n  rate  &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)\n  shape &lt;- 1 + mode * rate\n  return(list(shape = shape, rate = rate))\n}\n\n\ngamma_s_and_r_from_mean_sd(mean = 10, sd = 100)\n\n$shape\n[1] 0.01\n\n$rate\n[1] 0.001\n\ngamma_s_and_r_from_mode_sd(mode = 10, sd = 100)\n\n$shape\n[1] 1.105\n\n$rate\n[1] 0.01051\n\n\n\ndat &lt;- readr::read_csv(\"data/TherapeuticTouchData.csv\", show_col_types = FALSE)\n\ndplyr::glimpse(dat)\n\nRows: 280\nColumns: 2\n$ y &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ s &lt;chr&gt; \"S01\", \"S01\", \"S01\", \"S01\", \"S01\", \"S01\", \"…\n\n\n\ndat |&gt; \n  dplyr::mutate(y = y |&gt; as.character()) |&gt; \n  \n  ggplot(aes(y = y)) +\n  geom_bar(aes(fill = after_stat(count))) +\n  scale_fill_viridis_c(option = \"A\", end = .7, breaks = NULL) +\n  scale_x_continuous(breaks = 0:4 * 2, expand = c(0, NA), limits = c(0, 9)) +\n  cowplot::theme_minimal_vgrid() +\n  cowplot::panel_border() +\n  facet_wrap(~ s, ncol = 7)\n\n\n\n\n\n\n\n\n\n\nCode\na_purple &lt;- viridis::viridis_pal(option = \"A\")(9)[4]\ndat |&gt; \n  dplyr::group_by(s) |&gt; \n  dplyr::summarize(mean = mean(y)) |&gt;\n  \n  ggplot(aes(x = mean)) +\n  geom_histogram(color = \"white\", fill = a_purple,\n                 linewidth = .2, binwidth = .1) +\n  scale_x_continuous(\"Proportion Correct\", limits = c(0, 1)) +\n  scale_y_continuous(\"# Practitioners\", expand = c(0, NA)) +\n  cowplot::theme_minimal_hgrid()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfit9.1 &lt;-\n  brms::brm(data = dat,\n      family = brms::bernoulli(link = logit),\n      y ~ 1 + (1 | s),\n      prior = c(brms::prior(normal(0, 1.5), class = Intercept),\n                brms::prior(normal(0, 1), class = sd)),\n      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4,\n      seed = 9,\n      file = \"fits/fit09.01\")\n\n\n\nplot(fit9.1, widths = c(2, 3))\n\n\n\n\n\n\n\n\n\ndraws &lt;- brms::as_draws_df(fit9.1)\n\n\ndraws |&gt; \n  dplyr::mutate(chain = .chain) |&gt; \n  bayesplot::mcmc_acf(pars = vars(b_Intercept, sd_s__Intercept), lags = 10) +\n  cowplot::theme_cowplot()\n\n\n\n\n\n\n\n\n\nbayesplot::neff_ratio(fit9.1) |&gt; \n  bayesplot::mcmc_neff() +\n  cowplot::theme_cowplot(font_size = 12)\n\n\n\n\n\n\n\n\nThe N𝑒𝑓𝑓/NN_\\textit{eff}/N ratio values for our model parameters were excellent. Here’s a numeric summary of the model.\n\nprint(fit9.1)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: y ~ 1 + (1 | s) \n   Data: dat (Number of observations: 280) \n  Draws: 4 chains, each with iter = 20000; warmup = 1000; thin = 10;\n         total post-warmup draws = 7600\n\nMultilevel Hyperparameters:\n~s (Number of levels: 28) \n              Estimate Est.Error l-95% CI u-95% CI\nsd(Intercept)     0.28      0.18     0.01     0.68\n              Rhat Bulk_ESS Tail_ESS\nsd(Intercept) 1.00     7103     7350\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept    -0.25      0.14    -0.54     0.02 1.00\n          Bulk_ESS Tail_ESS\nIntercept     7885     7200\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe’ll need brms::inv_logit_scaled() to convert the model parameters to predict θ\\theta rather than logit(θ)\\operatorname{logit}(\\theta). After the conversions, we’ll be ready to make the histograms in the lower portion of Figure 9.10.\n\n# wrangle\ndraws_small &lt;-\n  draws |&gt; \n  # convert the linear model parameters to the probability space with `inv_logit_scaled()`\n  dplyr::mutate(`theta[1]`  = (b_Intercept + `r_s[S01,Intercept]`) |&gt; brms::inv_logit_scaled(),\n         `theta[14]` = (b_Intercept + `r_s[S14,Intercept]`) |&gt; brms::inv_logit_scaled(),\n         `theta[28]` = (b_Intercept + `r_s[S28,Intercept]`) |&gt; brms::inv_logit_scaled()) |&gt; \n  # make the difference distributions\n  dplyr::mutate(`theta[1] - theta[14]`  = `theta[1]`  - `theta[14]`,\n         `theta[1] - theta[28]`  = `theta[1]`  - `theta[28]`,\n         `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) |&gt; \n  dplyr::select(starts_with(\"theta\"))\n\nWarning: Dropping 'draws_df' class as required\nmetadata was removed.\n\ndraws_small |&gt; \n  tidyr::pivot_longer(everything()) |&gt; \n  # this line is unnecessary, but will help order the plots \n  dplyr::mutate(name = factor(name, levels = c(\"theta[1]\", \"theta[14]\", \"theta[28]\", \n                                        \"theta[1] - theta[14]\", \"theta[1] - theta[28]\", \"theta[14] - theta[28]\"))) |&gt; \n\n  ggplot(aes(x = value, y = 0)) +\n  stat_histinterval(point_interval = mode_hdi, .width = .95,\n                    fill = a_purple, breaks = 40, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(NULL) +\n  cowplot::theme_minimal_hgrid() +\n  facet_wrap(~ name, scales = \"free\", ncol = 3)\n\n\n\n\n\n\n\n\n\ndraws_small |&gt; \n  tidyr::pivot_longer(everything()) |&gt;\n  dplyr::group_by(name) |&gt; \n  tidybayes::mode_hdi(value) |&gt; \n  dplyr::mutate_if(is.double, round, digits = 3)\n\n# A tibble: 6 × 7\n  name      value .lower .upper .width .point .interval\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 theta[1]  0.421  0.206  0.521   0.95 mode   hdi      \n2 theta[1…  0     -0.274  0.12    0.95 mode   hdi      \n3 theta[1… -0.002 -0.428  0.068   0.95 mode   hdi      \n4 theta[1…  0.437  0.289  0.579   0.95 mode   hdi      \n5 theta[1… -0.001 -0.328  0.105   0.95 mode   hdi      \n6 theta[2…  0.463  0.363  0.71    0.95 mode   hdi      \n\n\n\np1 &lt;-\n  draws_small |&gt; \n  ggplot(aes(x = `theta[1]`, y = `theta[14]`)) +\n  geom_abline(linetype = 2) +\n  geom_point(color = a_purple, size = 1/8, alpha = 1/8)\n\np2 &lt;-\n  draws_small |&gt; \n  ggplot(aes(x = `theta[1]`, y = `theta[28]`)) +\n  geom_abline(linetype = 2) +\n  geom_point(color = a_purple, size = 1/8, alpha = 1/8)\n\np3 &lt;-\n  draws_small |&gt; \n  ggplot(aes(x = `theta[14]`, y = `theta[28]`)) +\n  geom_abline(linetype = 2) +\n  geom_point(color = a_purple, size = 1/8, alpha = 1/8)\n\n(p1 + p2 + p3) &\n  coord_cartesian(xlim = c(0, 1),\n                  ylim = c(0, 1)) & \n  cowplot::theme_minimal_grid()\n\n\n\n\n\n\n\n\n\n\nCode\n# this part makes it easier to set the break points in `scale_x_continuous()` \nlabels &lt;-\n  draws |&gt; \n  dplyr::mutate(theta = b_Intercept |&gt; inv_logit_scaled(), .keep = \"none\") |&gt;\n  tidybayes::mode_hdi() |&gt; \n  tidyr::pivot_longer(theta:.upper) |&gt; \n  dplyr::mutate(label = value |&gt; round(3) |&gt; as.character())\n  \ndraws |&gt; \n  mutate(theta = b_Intercept |&gt; inv_logit_scaled()) |&gt; \n\n  ggplot(aes(x = theta, y = 0)) +\n  stat_histinterval(point_interval = mode_hdi, .width = .95,\n                    fill = a_purple, breaks = 40) +\n  scale_x_continuous(expression(theta), \n                     breaks = labels$value,\n                     labels = labels$label) +  \n  scale_y_continuous(NULL, breaks = NULL) +\n  cowplot::theme_minimal_hgrid()\n\n\n\n\n\n\n\n\n\n\n# the tibble of the primary data\nsigmas &lt;-\n  stats::coef(fit9.1, summary = F)$s |&gt; \n  tibble::as_tibble() |&gt; \n  dplyr::mutate(iter = 1:n()) |&gt; \n  dplyr::group_by(iter) |&gt; \n  tidyr::pivot_longer(-iter) |&gt; \n  dplyr::mutate(theta = brms::inv_logit_scaled(value)) |&gt; \n  dplyr::summarise(sd = sd(theta))\n\n# this, again, is just to customize `scale_x_continuous()`\nlabels &lt;-\n  sigmas |&gt; \n  tidybayes::mode_hdi(sd) |&gt; \n  tidyr::pivot_longer(sd:.upper) |&gt; \n  dplyr::mutate(label = value |&gt; round(3) |&gt; as.character())\n  \n# the plot\nsigmas |&gt; \n  ggplot(aes(x = sd, y = 0)) +\n  stat_histinterval(point_interval = mode_hdi, .width = .95,\n                    fill = a_purple, breaks = 40) +\n  scale_x_continuous(expression(paste(sigma, \" of \", theta, \" in a probability metric\")),\n                     breaks = labels$value,\n                     labels = labels$label) +  \n  scale_y_continuous(NULL, breaks = NULL) +\n  cowplot::theme_minimal_hgrid()\n\n\n\n\n\n\n\n\n\n\nCode\ncolor_scheme_set(\"purple\")\nbayesplot::bayesplot_theme_set(theme_default() + cowplot::theme_minimal_grid())\n\nstats::coef(fit9.1, summary = F)$s |&gt; \n  brms::inv_logit_scaled() |&gt; \n  data.frame() |&gt; \n  rename(`theta[1]`  = S01.Intercept, \n         `theta[14]` = S14.Intercept, \n         `theta[28]` = S28.Intercept) |&gt; \n  dplyr::select(`theta[1]`, `theta[14]`, `theta[28]`) |&gt; \n  \n  bayesplot::mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8))\n\n\n\n\n\n\n\n\n\nDid you see how we slipped in the color_scheme_set() and bayesplot_theme_set() lines at the top? Usually, the plots made with bayesplot are easy to modify with ggplot2 syntax. Plots made with mcmc_pairs() function are one notable exception. On the back end, these made by combining multiple ggplot into a grid, a down-the-line result of which is they are difficult to modify. Happily, one can make some modifications beforehand by altering the global settings with the color_scheme_set() and bayesplot_theme_set() functions. You can learn more in the discussion on issue #128 on the bayesplot GitHub repo."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#recap-of-last-lecture",
    "href": "slides/BSMM_8740_lec_03.html#recap-of-last-lecture",
    "title": "Regression methods",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\nLast time we worked with the recipes package to develop workflows for pre-processing our data.\nToday we look at regression methods we might apply to understand relationships between measurements in our data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nIn the simple linear regression (SLR) model, we have \\(N\\) observations of a single outcome variable \\(Y\\) along with \\(D\\) predictor (aka co-variate) variables \\(\\mathbf{x}\\) where the likelihood1 of observing \\(Y=y\\) is conditional on the predictor values \\(x\\) and parameters \\(\\theta=\\{\\beta,\\sigma^2\\}\\):\n\\[\n\\pi\\left(Y=y|\\mathbf{x,\\theta}\\right)=\\mathscr{N}\\left(\\left.y\\right|\\mu(\\mathbf{x};\\beta),\\sigma^{2}\\right)\n\\]\nIn SLR models we assume a Normal (Gaussian) probability model for the data generation process. For non-Normal data generation processes we use generalized linear models, which we’ll discuss later."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-1",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-1",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nIn the SLR model, \\(\\mathscr{N}\\left(\\left.y\\right|\\mu(\\mathbf{x};\\beta),\\sigma^{2}\\right)\\) is a Normal probability density with mean \\(\\mu(\\mathbf{x};\\beta)\\) and variance \\(\\sigma^2\\), where \\(\\sigma^2\\) is a constant and the mean is a function of the predictors \\(\\mathbf{x}\\) and a vector of parameters \\(\\beta\\).\nThe mean function \\(\\mu(\\mathbf{x};\\beta)\\) is often assumed to be continuous, i.e. a small change in the predictors implies a small change in the outcome. In addition, it is often convenient to decompose the mean function into a sum of simpler functions, e.g. polynomial functions (like straight lines, parabolas, and more)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#taylor-series",
    "href": "slides/BSMM_8740_lec_03.html#taylor-series",
    "title": "Regression methods",
    "section": "Taylor Series",
    "text": "Taylor Series\nThe decomposition of a function \\(f\\) of a single variable \\(x\\) into a sum of simpler polynomial functions is called a Taylor series and is defined as follows:\n\n\\[\nf(x;x_0)=\\sum_{n=0}\\beta_n(x-x_0)^n=\\beta_0+\\beta_1(x-x_0)+\\beta_2(x-x_0)^2+\\beta_3(x-x_0)^3+\\ldots\n\\]\n\nwhere \\(\\frac{d^nf(x)}{dx^n}|_{x=x_0} \\equiv f^{(n)}(x_0) = n!\\beta_n\\;\\rightarrow \\beta_n=\\frac{1}{n!}f^{(n)}(x_0)\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#taylor-series-1",
    "href": "slides/BSMM_8740_lec_03.html#taylor-series-1",
    "title": "Regression methods",
    "section": "Taylor Series",
    "text": "Taylor Series\n\n\\[\nf(x;x_0)=\\sum_n\\beta_n(x-x_0)^n=\\beta_0+\\beta_1(x-x_0)+\\beta_2(x-x_0)^2+\\beta_3(x-x_0)^3+\\ldots\n\\]\n\nWe can use the following constructive proof to find the coefficients in the series:\n\n\n\\(0^{th}\\) derivative at \\(x=x_0\\): \\(f(x_0) = \\beta_0\\)\n\\(1^{st}\\) derivative at \\(x=x_0\\): \\(f'(x_0) = \\beta_1\\)\n\\(2^{nd}\\) derivative at \\(x=x_0\\): \\(f''(x_0) = 2\\beta_2\\)\n\\(3^{rd}\\) derivative at \\(x=x_0\\): \\(f'''(x_0) = 6\\beta_3\\)\n\\(n^{th}\\) derivative at \\(x=x_0\\): \\(f^{(n)}(x_0) = n!\\beta_n\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-2",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-2",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nSimilarly, when the number of variables is \\(D=2\\) the Taylor series is (writing \\(f_{x}\\equiv\\frac{\\partial f}{\\partial x}\\), \\(f_{y}\\equiv\\frac{\\partial f}{\\partial y}\\), \\(f_{x,y}\\equiv\\frac{\\partial^2 f}{\\partial x,\\partial x}\\) and so on):\n\n\\[\n\\begin{align*}\nf(x,y;x_0,y_0) & =f(x_{0},y_{0})+f_{x}(x_{0},y_{0})(x-x_{0})+f_{y}(x_{0},y_{0})(y-y_{0})\\\\\n& = + \\frac{1}{2}f_{x,x}(x_{0},y_{0})(x-x_{0})^{2}+\\frac{1}{2}f_{y,y}(x_{0},y_{0})(y-y_{0})^{2}\\\\\n& = + \\frac{1}{2}f_{x,y}(x_{0},y_{0})(x-x_{0})(y-y_{0})+\\ldots\n\\end{align*}\n\\]\nwhich decomposes a function of two variables into a sum of simpler functions, e.g. polynomial functions (like 2-D straight lines, parabolas, and more)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-3",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-3",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nFor one co-variate, if the mean function is smooth (i.e. not changing rapidly with the co-variate) then \\(\\mu^{(n)}\\) will be decreasing in \\(n\\), and furthermore \\(\\beta_n\\) decreases as \\(1/n!\\), so SLR models in one co-variate typically use only the first two or at most three \\(\\beta\\) coefficients.\nThus the likelihood of observing \\(Y=y\\) is conditional on the predictor values \\(x\\) and parameters \\(\\theta=\\{\\beta_0,\\beta_1,\\sigma^2\\}\\):\n\\(\\theta=\\{\\beta_{0},\\mathbf{\\beta},\\sigma^{2}\\}\\) are the parameters of the model, where \\(\\beta_0\\) is a constant and \\(\\beta_1\\) is the co-variate weight or regression coefficient."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-4",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-4",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nFor multiple co-variates, If the mean function is smooth (i.e. not changing rapidly with the co-variates) then under similar (reasonable) assumptions on the differentials, it is common to see SLR models using only the first order coefficients, i.e. a constant and one coefficient for each co-variate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-5",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-5",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\n\n\n\n\n\n\nNote\n\n\nIt is common to add the unit constant to the covariate vector \\(x=(1,x_1,x_2,\\ldots)\\) so that the coefficient vector is \\(\\mathbf{\\beta}=(\\beta_0,\\beta_{x_1},\\beta_{x_2},\\ldots)\\), and the model can be expressed1 as a vector equation: \\(y=\\mathbf{\\beta}\\cdot \\mathbf{x}\\).\n\n\n\nsometime written as \\(\\beta'\\mathbf{x}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-6",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-6",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nRecall that the one dimensional Normal/Gaussian probability density (aka likelihood) is:\n\\[\n\\mathscr{N}\\left(x;\\mu,\\sigma^2\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\frac{1}{2}\\frac{x-\\mu}{\\sigma^2}}\n\\]\nThe likelihood of multiple observations is the product of the likelihoods for each observation."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-7",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-7",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nTo fit the 1D linear regression model given \\(N\\) data samples, we minimize the negative log-likelihood (NLL) on the training set.\n\n\\[\n\\begin{align*}\n\\text{NLL}\\left(\\beta,\\sigma^{2}\\right) & =-\\sum_{n=1}^{N}\\log\\left[\\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{\\frac{1}{2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(y_{n}-\\beta'x_{n}\\right)^{2}\\right)\\right]\\\\\n& =\\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{N}\\left(y_{n}-\\hat{y}_{n}\\right)^{2}-\\frac{N}{2}\\log\\left(2\\pi\\sigma^{2}\\right)\n\\end{align*}\n\\]\n\nwhere the predicted response is \\(\\hat{y}\\equiv\\beta'x_{n}\\). This is also the maximum likelihood estimation (MLE) method."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-8",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-8",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nMinimizing the NLL by minimizing the residual sum of squares (RSS) is the same as minimizing\n\nthe mean squared error \\(\\text{MSE}\\left(\\beta\\right) = \\frac{1}{N}\\text{RSS}\\left(\\beta\\right)\\)\nthe root mean squared error \\(\\text{RMSE}\\left(\\beta\\right) = \\sqrt{\\text{MSE}\\left(\\beta\\right)}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#aside-empirical-risk-minimization",
    "href": "slides/BSMM_8740_lec_03.html#aside-empirical-risk-minimization",
    "title": "Regression methods",
    "section": "Aside: empirical risk minimization",
    "text": "Aside: empirical risk minimization\nThe MLE can be generalized by replacing the NLL (\\(\\ell\\left(y_{n},\\theta;x_{n}\\right)=-\\log\\pi\\left(y_n|x_n,\\theta\\right)\\)) with any other loss function to get\n\\[\n\\mathscr{L}\\left(\\theta\\right)=\\frac{1}{N}\\sum_{n=1}^{N}\\ell\\left(y_{n},\\theta;x_{n}\\right)\n\\]\nThis is known as the empirical risk minimization (ERM) - the expected loss taken with respect to the empirical distribution."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-9",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-9",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nFocusing on just the coefficients \\(\\beta\\), the minimum NLL is (up to a constant) the minimum of the residual sum of squares (RSS)1 with coefficient estimates \\(\\hat\\beta\\) :\n\\[\n\\begin{align*}\\text{RSS}\\left(\\beta\\right) & =\\frac{1}{2}\\sum_{n=1}^{N}\\left(y_{n}-\\beta'x_{n}\\right)^{2}=\\frac{1}{2}\\left\\Vert y_{n}-\\beta'x_{n}\\right\\Vert ^{2}\\\\\n& =\\frac{1}{2}\\left(y_{n}-\\beta'x_{n}\\right)'\\left(y_{n}-\\beta'x_{n}\\right)\\\\\n\\\\\n\\end{align*}\n\\]\ni.e. minimizing the squared prediction error, aka ordinary least squares."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-10",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-10",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nNote that, given the assumption that the data generation process is Normal/Gaussian, we can write our regression equation in terms of individual observations as\n\\[\ny_i=\\beta_0+\\beta_1 x_i + u_i\n\\]\nwhere error term \\(u_i\\) is a sample from \\(\\mathscr{N}\\left(0,\\sigma^{2}\\right)\\) which in turn implies \\(\\mathbb{E}\\left[u\\right]=0;\\;\\mathbb{E}\\left[\\left.u\\right|x\\right]=0\\)\nThe independence of the covariates and the errors/residuals is a testable assumption."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-11",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-11",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nIt follows independence of the covariates and the errors that\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[y-\\beta_{0}-\\beta_{1}x\\right] & =0\\\\\n\\mathbb{E}\\left[x\\left(y-\\beta_{0}-\\beta_{1}x\\right)\\right] & =0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-12",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-12",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nWriting these equations in terms of our samples (where \\(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}\\) are our coefficient estimates)\n\\[\n\\begin{align*}\n\\frac{1}{N}\\sum_{i-1}^{N}y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i} & =0\\\\\n\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i}\\right) & =0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-13",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-13",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nFrom the first equation\n\\[\n\\begin{align*}\n\\bar{y}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}\\bar{x} & =0\\\\\n\\bar{y}-\\hat{\\beta}_{1}\\bar{x} & =\\hat{\\beta}_{0}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-14",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-14",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nSubstituting the expression for \\(\\hat{\\beta}_{0}\\) in the independence equation\n\n\\[\n\\begin{align*}\n\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(y_{i}-\\left(\\bar{y}-\\hat{\\beta}_{1}\\bar{x}\\right)-\\hat{\\beta}_{1}x_{i}\\right) & =0\\\\\n\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(y_{i}-\\bar{y}\\right) & =\\hat{\\beta}_{1}\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(\\bar{x}-x_{i}\\right)\\\\\n\\frac{1}{N}\\sum_{i-1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right) & =\\hat{\\beta}_{1}\\frac{1}{N}\\sum_{i-1}^{N}\\left(\\bar{x}-x_{i}\\right)^2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-15",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-15",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nSo as long as \\(\\sum_{i-1}^{N}\\left(\\bar{x}-x_{i}\\right)^2\\ne 0\\)\n\\[\n\\begin{align*}\n\\hat{\\beta}_{1} & =\\frac{\\sum_{i-1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum_{i-1}^{N}\\left(\\bar{x}_{i}-x_{i}\\right)^2}\\\\\n& =\\frac{\\text{sample covariance}(x_{i}y_{i})}{\\text{sample variance}(x_{i})}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-16",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-16",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nSimilarly, in the vector equation, the minimum of the RSS is solved by (assuming \\(N&gt;D\\)):\n\\[\n\\hat{\\mathbf{\\beta}}_{OLS}=\\left(X'X\\right)^{-1}\\left(X'Y\\right) = \\frac{\\text{cov}(X,Y)}{\\text{var}(X)}\n\\]\nThere are algorithmic issues with computing \\(\\left(X'X\\right)^{-1}\\) though, so we could instead start with \\(X\\beta=y\\) and write \\(\\hat{\\mathbf{\\beta}}_{OLS}=X^{-1}y\\) ."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\nComputing the inverse of \\(X'X\\) directly, while theoretically possible, can be numerically unstable.\nIn R, the \\(QR\\) decomposition is used to solve for \\(\\beta\\). Let \\(X=QR\\) where \\(Q'Q=I\\) and write:\n\\[\n\\begin{align*}\n(QR)\\beta & = y\\\\\nQ'QR\\beta & = Q'y\\\\\n\\beta & = R^{-1}(Q'y)\n\\end{align*}\n\\]\nSince \\(R\\) is upper triangular, the last equation can be solved by back-substitution."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-1",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-1",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\n\n&gt; A &lt;- matrix(c(1,2,5, 2,4,6, 3, 3, 3), nrow=3)\n&gt; QR &lt;- qr(A)\n\n\nQRA\n\n\n\n&gt; Q &lt;- qr.Q(QR); Q\n\n           [,1]       [,2]          [,3]\n[1,] -0.1825742 -0.4082483 -8.944272e-01\n[2,] -0.3651484 -0.8164966  4.472136e-01\n[3,] -0.9128709  0.4082483  2.593051e-16\n\n\n\n&gt; Q %*% t(Q) |&gt; round()\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n\n\n\n&gt; R &lt;- qr.R(QR); R\n\n          [,1]      [,2]      [,3]\n[1,] -5.477226 -7.302967 -4.381780\n[2,]  0.000000 -1.632993 -2.449490\n[3,]  0.000000  0.000000 -1.341641\n\n\n\n\n\n&gt; Q %*% R\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    2    4    3\n[3,]    5    6    3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-2",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-2",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\n\nCode\n&gt; # A linear system of equations y = Ax\n&gt; cat(\"matrix A\\n\")\n&gt; A &lt;- matrix(c(3, 2, -1, 2, -2, .5, -1, 4, -1), nrow=3); A\n&gt; cat(\"vector x\\n\")\n&gt; x &lt;- c(1, -2, -2); x\n&gt; cat(\"vector y\\n\")\n&gt; y &lt;- A %*% x ; y\n\n\n\n\nmatrix A\n\n\n     [,1] [,2] [,3]\n[1,]    3  2.0   -1\n[2,]    2 -2.0    4\n[3,]   -1  0.5   -1\n\n\n\n\nvector x\n\n\n[1]  1 -2 -2\n\n\n\n\nvector y\n\n\n     [,1]\n[1,]    1\n[2,]   -2\n[3,]    0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-3",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-3",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\n\n&gt; # Compute the QR decomposition of A\n&gt; QR &lt;- qr(A)\n&gt; Q &lt;- qr.Q(QR)\n&gt; R &lt;- qr.R(QR)\n&gt; \n&gt; # Compute b=Q'y\n&gt; b &lt;- crossprod(Q, y); b\n\n           [,1]\n[1,]  0.2672612\n[2,]  2.1472519\n[3,] -0.5638092\n\n&gt; # Solve the upper triangular system Rx=b\n&gt; backsolve(R, b)\n\n     [,1]\n[1,]    1\n[2,]   -2\n[3,]   -2"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#collinearity",
    "href": "slides/BSMM_8740_lec_03.html#collinearity",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\n\nOne of the important assumptions of the classical linear regression models is that there is no exact collinearity among the regressors.\nWhile high correlation between regressors is a necessary indicator of the collinearity problem, a direct linear relationship beween regressors is sufficient."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#collinearity-1",
    "href": "slides/BSMM_8740_lec_03.html#collinearity-1",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\n\nData collection methods, constraints on the fitted regression model, model specification error, an overdefined model, may be some potential sources of multicollinearity.\nIn other cases it is an artifact caused by creating new predictors from other predictors."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#collinearity-2",
    "href": "slides/BSMM_8740_lec_03.html#collinearity-2",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\nThe problem of collinearity has potentially serious effect on the regression estimates such as:\n\nimplausible coefficient signs,\nimpossible inversion of matrix \\(X'X\\) as it becomes near or exactly singular,\nlarge magnitude of coefficients in absolute value,\nlarge variance or standard errors with wider confidence intervals."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#collinearity-3",
    "href": "slides/BSMM_8740_lec_03.html#collinearity-3",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\nMitigating Collinearity:\n\nRemove Highly Correlated Variables: If two variables are highly correlated, consider removing one of them.\nCombine Variables: Create a new variable that combines the collinear variables\nPrincipal Component Analysis (PCA): Use PCA to transform the correlated variables into a smaller set of uncorrelated variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-vs-variance",
    "href": "slides/BSMM_8740_lec_03.html#bias-vs-variance",
    "title": "Regression methods",
    "section": "Bias vs Variance",
    "text": "Bias vs Variance\nWe introduced truncated Taylor series approximations to motivate using simplified models of the mean function when using regression.\nBut bias is the error introduced by approximating a real-world problem, which may be complex, by a simplified model.\nSo to reduce bias, why not include more Taylor series terms, or more covariates in a first-order model?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-vs-variance-1",
    "href": "slides/BSMM_8740_lec_03.html#bias-vs-variance-1",
    "title": "Regression methods",
    "section": "Bias vs Variance",
    "text": "Bias vs Variance\nNote that for random variables in general and Gaussian random variables in particular\n\nthe mean of the sum of random variables is the sum of the means of the random variables.\nthe variance of the sum of random variables is the sum of the variances of the random variables.\n\nSo adding more terms or more covariates may reduce bias by improving the mean estimate, but will certainly increase the variance of the estimate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-b-vs-variance-v-tradeoffs",
    "href": "slides/BSMM_8740_lec_03.html#bias-b-vs-variance-v-tradeoffs",
    "title": "Regression methods",
    "section": "Bias (B) vs Variance (V) tradeoffs",
    "text": "Bias (B) vs Variance (V) tradeoffs\n\n\\(\\downarrow\\) B \\(\\uparrow\\) V\\(\\uparrow\\) B \\(\\downarrow\\) V\\(-\\) B \\(-\\) V\n\n\nLow Bias and High Variance\n\nA model with low bias fits the training data very closely, capturing all the details and fluctuations.\nThis leads to overfitting, where the model performs well on the training data but poorly on new data because it has learned the noise in the training data as if it were a signal.\n\n\n\nHigh Bias and Low Variance\n\nA model with high bias makes oversimplified assumptions about the data, ignoring relevant complexities.\nThis leads to underfitting, where the model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and new data.\n\n\n\nBalancing Bias and Variance\n\nThe goal is to find a sweet spot where the model is complex enough to capture the underlying patterns (low bias) but simple enough not to capture the noise (low variance).\nAchieving this balance ensures the model generalizes well to new data, providing good performance overall."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-b-vs-variance-v-examples",
    "href": "slides/BSMM_8740_lec_03.html#bias-b-vs-variance-v-examples",
    "title": "Regression methods",
    "section": "Bias (B) vs Variance (V) examples",
    "text": "Bias (B) vs Variance (V) examples\n\nUnderfittingOverfittingBalanced\n\n\nUnderfitting (High Bias, Low Variance)\n\nSuppose you’re predicting house prices using just the size of the house (one variable) in a linear regression model.\nIf the true relationship is complex (e.g., non-linear, involving multiple factors), this simple model will have high bias and underfit the data, missing important patterns.\n\n\n\nOverfitting (Low Bias, High Variance)\n\nNow, imagine you use a very complex model, like a high-degree polynomial regression, that uses many variables and interactions.\nThis model fits the training data very well but captures noise as well. When applied to new data, its performance drops because it has learned patterns that don’t generalize (high variance).\n\n\n\nBalanced Model\n\nA balanced model might use a moderate number of relevant variables and a reasonable complexity (like a linear regression with interaction terms or a low-degree polynomial).\nThis model captures the essential patterns without fitting the noise, resulting in good generalization to new data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-vs-variance-2",
    "href": "slides/BSMM_8740_lec_03.html#bias-vs-variance-2",
    "title": "Regression methods",
    "section": "Bias vs Variance",
    "text": "Bias vs Variance\nThe following regression models techniques with the higher variance that follows from a large number of covariates by adding a bit of bias. The variance is reduced by penalizing covariate coefficients, shrinking then towards zero.\nThe resulting simpler models may not fully capture the patterns in the data, thus underfitting the data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression",
    "title": "Regression methods",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nRidge regression is an example of a penalized regression model; in this case the magnitude of the weights are penalized by adding the \\(\\ell_2\\) norm of the weights to the loss function. In particular, the ridge regression weights are:\n\\[\n\\hat{\\beta}_{\\text{ridge}}=\\arg\\!\\min\\text{RSS}\\left(\\beta\\right)+\\lambda\\left\\Vert \\beta\\right\\Vert _{2}^{2}\n\\]\nwhere \\(\\lambda\\) is the strength of the penalty term.\nThe Ridge objective function is\n\\[\n\\mathscr{L}\\left(\\beta,\\lambda\\right)=\\text{NLL}+\\lambda\\left\\Vert \\beta\\right\\Vert_2^2\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-1",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-1",
    "title": "Regression methods",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nThe solution is:\n\\[\n\\begin{align*}\n\\hat{\\mathbf{\\beta}}_{ridge} & =\\left(X'X-\\lambda I_{D}\\right)^{-1}\\left(X'Y\\right)\\\\\n& =\\left(\\sum_{n}x_{n}x'_{n}+\\lambda I_{D}\\right)^{-1}\\left(\\sum_{n}y_{n}x_{n}\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-2",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-2",
    "title": "Regression methods",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nAs for un-penalized linear regression, using matrix inversion to solve for \\(\\hat{\\mathbf{\\beta}}_{ridge}\\) can be a bad idea. The QR transformation can be used here, however, ridge regression is often used when \\(D&gt;N\\), in which case the SVD transformation is faster."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-example",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-example",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n&gt; #define response variable\n&gt; y &lt;- mtcars %&gt;% dplyr::pull(hp)\n&gt; \n&gt; #define matrix of predictor variables\n&gt; x &lt;- mtcars %&gt;% dplyr::select(mpg, wt, drat, qsec) %&gt;% data.matrix()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-example-1",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-example-1",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n&gt; # fit ridge regression model\n&gt; model &lt;- glmnet::glmnet(x, y, alpha = 0)\n&gt; \n&gt; # get coefficients when lambda = 7.6\n&gt; coef(model, s = 7.6)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 477.91365858\nmpg          -3.29697140\nwt           20.31745927\ndrat         -0.09524492\nqsec        -18.48934710"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-example-2",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-example-2",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n\nglmnet example\n&gt; # perform k-fold cross-validation to find optimal lambda value\n&gt; cv_model &lt;- glmnet::cv.glmnet(x, y, alpha = 0)\n&gt; \n&gt; # find optimal lambda value that minimizes test MSE\n&gt; best_lambda &lt;- cv_model$lambda.min\n&gt; \n&gt; # produce plot of test MSE by lambda value\n&gt; cv_model %&gt;% broom::tidy() %&gt;% \n+ ggplot(aes(x=lambda, y = estimate)) +\n+   geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#00ABFD\", alpha=0.5) +\n+   geom_point() +\n+   geom_vline(xintercept=best_lambda) +\n+   labs(title='Ridge Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}\"\n+          )\n+   ) +\n+   ggplot2::scale_x_log10()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-example-3",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-example-3",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n\nglmnet coefficients\n&gt; model$beta %&gt;% \n+   as.matrix() %&gt;% \n+   t() %&gt;% \n+   tibble::as_tibble() %&gt;% \n+   tibble::add_column(lambda = model$lambda, .before = 1) %&gt;% \n+   tidyr::pivot_longer(-lambda, names_to = 'parameter') %&gt;% \n+   ggplot(aes(x=lambda, y=value, color=parameter)) +\n+   geom_line() + geom_point() +\n+   xlim(0,2000) +\n+   labs(title='Ridge Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"Parameters as a function of lambda\"\n+          )\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#lasso-regression",
    "href": "slides/BSMM_8740_lec_03.html#lasso-regression",
    "title": "Regression methods",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nLasso regression is another example of a penalized regression model; in this case both the magnitude of the weights and the number of parameters are penalized by using the \\(\\ell_1\\) norm of the weights to the loss function of the lasso regression. In particular, the lasso regression weights are:\n\\[\n\\hat{\\beta}_{\\text{lasso}}=\\arg\\!\\min\\text{RSS}\\left(\\beta\\right)+\\lambda\\left\\Vert \\beta\\right\\Vert _{1}\n\\]\nThe Lasso objective function is\n\\[\n\\mathscr{L}\\left(\\beta,\\lambda\\right)=\\text{NLL}+\\lambda\\left\\Vert \\beta\\right\\Vert _{1}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#lasso-regression-example",
    "href": "slides/BSMM_8740_lec_03.html#lasso-regression-example",
    "title": "Regression methods",
    "section": "Lasso Regression Example",
    "text": "Lasso Regression Example\n\n\nlasso model\n&gt; # define response variable\n&gt; y &lt;- mtcars %&gt;% dplyr::pull(hp)\n&gt; \n&gt; # define matrix of predictor variables\n&gt; x &lt;- mtcars %&gt;% dplyr::select(mpg, wt, drat, qsec) %&gt;% data.matrix()\n&gt; \n&gt; # fit ridge regression model\n&gt; model &lt;- glmnet::glmnet(x, y, alpha = 1)\n&gt; \n&gt; # get coefficients when lambda = 3.53\n&gt; coef(model, s = 3.53)\n\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) 480.761125\nmpg          -3.036337\nwt           20.222451\ndrat          .       \nqsec        -18.944318"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#lasso-regression-example-1",
    "href": "slides/BSMM_8740_lec_03.html#lasso-regression-example-1",
    "title": "Regression methods",
    "section": "Lasso Regression Example",
    "text": "Lasso Regression Example\n\n\nlasso example\n&gt; #perform k-fold cross-validation to find optimal lambda value\n&gt; cv_model &lt;- glmnet::cv.glmnet(x, y, alpha = 1)\n&gt; \n&gt; #find optimal lambda value that minimizes test MSE\n&gt; best_lambda &lt;- cv_model$lambda.min\n&gt; \n&gt; #produce plot of test MSE by lambda value\n&gt; cv_model %&gt;% broom::tidy() %&gt;% \n+ ggplot(aes(x=lambda, y = estimate)) +\n+   geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#00ABFD\", alpha=0.5) +\n+   geom_point() +\n+   geom_vline(xintercept=best_lambda) +\n+   labs(title='Lasso Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}\"\n+          )\n+   ) +\n+   xlim(0,exp(4)) + ggplot2::scale_x_log10()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#lasso-regression-example-2",
    "href": "slides/BSMM_8740_lec_03.html#lasso-regression-example-2",
    "title": "Regression methods",
    "section": "Lasso Regression Example",
    "text": "Lasso Regression Example\n\n\nlasso coefficients\n&gt; model %&gt;%\n+   broom::tidy() %&gt;%\n+   tidyr::pivot_wider(names_from=term, values_from=estimate) %&gt;%\n+   dplyr::select(-c(step,dev.ratio, `(Intercept)`)) %&gt;%\n+   dplyr::mutate_all(dplyr::coalesce, 0) %&gt;% \n+   tidyr::pivot_longer(-lambda, names_to = 'parameter') %&gt;% \n+   ggplot(aes(x=lambda, y=value, color=parameter)) +\n+   geom_line() + geom_point() +\n+   xlim(0,70) +\n+   labs(title='Ridge Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"Parameters as a function of lambda\"\n+          )\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#elastic-net-regression",
    "href": "slides/BSMM_8740_lec_03.html#elastic-net-regression",
    "title": "Regression methods",
    "section": "Elastic Net Regression",
    "text": "Elastic Net Regression\nElastic Net regression is a hybrid of ridge and lasso regression.\nThe elastic net objective function is\n\\[\n\\mathscr{L}\\left(\\beta,\\lambda,\\alpha\\right)=\\text{NLL}+\\lambda\\left(\\left(1-\\alpha\\right)\\left\\Vert \\beta\\right\\Vert _{2}^{2}+\\alpha\\left\\Vert \\beta\\right\\Vert _{1}\\right)\n\\]\nso that \\(\\alpha=0\\) is ridge regression and \\(\\alpha=1\\) is lasso regression and \\(\\alpha\\in\\left(0,1\\right)\\) is the general elastic net."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#elastic-net-regression-example",
    "href": "slides/BSMM_8740_lec_03.html#elastic-net-regression-example",
    "title": "Regression methods",
    "section": "Elastic Net Regression Example",
    "text": "Elastic Net Regression Example\n\n\nelastic net example\n&gt; # set length of data and seed for reproducability\n&gt; n &lt;- 50\n&gt; set.seed(2467)\n&gt; # create the dataset\n&gt; dat &lt;- tibble::tibble(\n+   a = sample(1:20, n, replace = T)/10\n+   , b = sample(1:10, n, replace = T)/10\n+   , c = sort(sample(1:10, n, replace = T))\n+ ) %&gt;% \n+   dplyr::mutate(\n+     z = (a*b)/2 + c + sample(-10:10, n, replace = T)/10\n+     , .before = 1\n+   )\n&gt; # cross validate to get the best alpha\n&gt; alpha_dat &lt;- tibble::tibble( alpha = seq(0.01, 0.99, 0.01) ) %&gt;% \n+   dplyr::mutate(\n+     mse =\n+       purrr::map_dbl(\n+         alpha\n+         , (\\(a){\n+           cvg &lt;- \n+            glmnet::cv.glmnet(\n+              x = dat %&gt;% dplyr::select(-z) %&gt;% as.matrix() \n+              , y = dat$z \n+              , family = \"gaussian\"\n+              , gamma = a\n+           )\n+           min(cvg$cvm)\n+         })\n+       )\n+   ) \n&gt; \n&gt; best_alpha &lt;- alpha_dat %&gt;% \n+   dplyr::filter(mse == min(mse)) %&gt;% \n+   dplyr::pull(alpha)\n&gt; \n&gt; cat(\"best alpha:\", best_alpha)\n\n\nbest alpha: 0.64\n\n\n\n\nelastic net example, part 2\n&gt; elastic_cv &lt;- \n+   glmnet::cv.glmnet(\n+     x = dat %&gt;% dplyr::select(-z) %&gt;% as.matrix() \n+     , y = dat$z \n+     , family = \"gaussian\"\n+     , gamma = best_alpha)\n&gt; \n&gt; best_lambda &lt;- elastic_cv$lambda.min\n&gt; cat(\"best lambda:\", best_lambda)\n\n\nbest lambda: 0.01015384\n\n\nelastic net example, part 2\n&gt; elastic_mod &lt;- glmnet::glmnet(\n+   x = dat %&gt;% dplyr::select(-z) %&gt;% as.matrix() \n+   , y = dat$z \n+   , family = \"gaussian\"\n+   , gamma = best_alpha, lambda = best_lambda)\n&gt; \n&gt; elastic_mod %&gt;% broom::tidy()\n\n\n# A tibble: 4 × 5\n  term         step estimate lambda dev.ratio\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1   -0.467 0.0102     0.963\n2 a               1    0.221 0.0102     0.963\n3 b               1    0.560 0.0102     0.963\n4 c               1    1.03  0.0102     0.963"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#elastic-net-regression-example-1",
    "href": "slides/BSMM_8740_lec_03.html#elastic-net-regression-example-1",
    "title": "Regression methods",
    "section": "Elastic Net Regression Example",
    "text": "Elastic Net Regression Example\n\n\nelastic net example, part 3\n&gt; pred &lt;- predict(elastic_mod, dat %&gt;% dplyr::select(-z) %&gt;% as.matrix())\n&gt; \n&gt; rmse &lt;- sqrt(mean( (pred - dat$z)^2 ))\n&gt; R2 &lt;- 1 - (sum((dat$z - pred )^2)/sum((dat$z - mean(y))^2))\n&gt; mse &lt;- mean((dat$z - pred)^2)\n&gt; \n&gt; cat(\" RMSE:\", rmse, \"\\n\", \"R-squared:\", R2, \"\\n\", \"MSE:\", mse)\n\n\n RMSE: 0.5817823 \n R-squared: 0.9999828 \n MSE: 0.3384707\n\n\n\n\nelastic net example, part 4\n&gt; dat %&gt;% \n+   tibble::as_tibble() %&gt;% \n+   tibble::add_column(pred = pred[,1]) %&gt;% \n+   tibble::rowid_to_column(\"ID\") %&gt;% \n+   ggplot(aes(x=ID, y=z)) +\n+   geom_point() +\n+   geom_line(aes(y=pred),color='red')"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA generalized linear model (GLM) is a flexible generalization of ordinary linear regression.\nOrdinary linear regression predicts the expected value of the outcome variable, a random variable, as a linear combination of a set of observed values (predictors). In a generalized linear model (GLM), each outcome \\(Y\\) is assumed to be generated from a particular distribution in an exponential family, The mean, \\(\\mu\\), of the distribution depends on the independent variables, \\(X\\), through:\n\\[\n\\mathbb{E}\\left[\\left.Y\\right|X\\right]=\\mu=\\text{g}^{-1}\\left(X\\beta\\right)\n\\] where \\(g\\) is called the link function."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models-1",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models-1",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nFor example, if \\(Y\\) is Poisson distributed, then\n\\[\n\\mathbb{P}\\left[\\left.Y=y\\right|X,\\lambda\\right]=\\frac{\\lambda^{y}}{y!}e^{-\\lambda}=e^{y\\log\\lambda-\\lambda-\\log y!}\n\\]\nWhere \\(\\lambda\\) is both the mean and the variance. In the glm the link function is \\(\\log\\) and\n\\[\n\\log\\mathbb{E}\\left[\\left.Y\\right|X\\right] = \\beta X=\\log\\lambda\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models-2",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models-2",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nKey Components of GLMs\n\nRandomSystemicLink\n\n\nRandom Component:\n\nSpecifies the probability distribution of the data generation process of the response variable (\\(Y\\)). Examples include Normal, Binomial, Poisson, etc.\n\n\n\nSystematic Component:\n\nSpecifies the linear predictor (\\(\\eta = X\\beta)\\), where (\\(X\\)) is the matrix of predictors and (\\(\\beta\\)) is the vector of coefficients.\n\n\n\nLink Function:\n\nConnects the mean of the response variable (\\(\\mathbb{E}(Y)\\)) to the linear predictor (\\(\\eta\\)). It transforms the expected value of the response variable to the linear predictor scale."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models-3",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models-3",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nCommon Types of GLMs\n\nLinearLogisticPoissonGamma\n\n\n\nLinear Regression (Binomial Distribution)\n\nResponse Variable: Continuous\nLink Function: Identity (\\((g(\\mu) = \\mu\\)))\nExample: Predicting house prices based on square footage, number of bedrooms, etc.\nFormula: \\((Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\epsilon)\\), where \\((Y\\)) is normally distributed.\n\n\n\n\n\nLogistic Regression (binomial Distribution)\n\nResponse Variable: Binary (0 or 1)\nLink Function: Logit (\\((g(\\mu) = \\log(\\frac{\\mu}{1-\\mu})\\)))\nExample: Predicting whether a customer will buy a product (yes/no) based on age, income, etc.\nFormula: \\((\\log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots\\)), where (\\(p\\)) is the probability of the event occurring.\n\n\n\n\n\nPoisson Regression (Poisson Distribution)\n\nResponse Variable: Count data (non-negative integers)\nLink Function: Log (\\((g(\\mu) = \\log(\\mu)\\)))\nExample: Predicting the number of insurance claims in a year based on driver age, vehicle type, etc.\nFormula: \\((\\log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots)\\), where \\((\\lambda)\\) is the expected count.\n\n\n\n\n\nGamma Regression (Gamma Distribution)\n\nResponse Variable: Continuous and positive\nLink Function: Inverse (\\((g(\\mu) = \\frac{1}{\\mu}\\)))\nExample: Predicting the time until failure of a machine based on temperature, pressure, etc.\nFormula: \\((\\frac{1}{\\mu} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots)\\), where (\\(\\mu\\)) is the mean of the response variable."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models-4",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models-4",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nExamples of GLMs\n\nLogisticPoissonGamma\n\n\n\nLogistic Regression Example:\n\nScenario: A marketing team wants to predict whether a customer will buy a product.\nVariables: Customer age, income, and previous purchase history.\nModel: \\((\\log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 \\text{Age} + \\beta_2 \\text{Income} + \\beta_3 \\text{History}\\))\nInterpretation: The coefficients (_1, _2, _3) indicate how each predictor affects the log odds of making a purchase.\n\n\n\n\n\nPoisson Regression Example:\n\nScenario: An insurance company wants to predict the number of claims a policyholder will file.\nVariables: Age of the policyholder, type of vehicle, and driving experience.\nModel: \\((\\log(\\lambda) = \\beta_0 + \\beta_1 \\text{Age} + \\beta_2 \\text{VehicleType} + \\beta_3 \\text{Experience}\\))\nInterpretation: The coefficients \\((\\beta_1, \\beta_2, \\beta_3)\\) indicate how each predictor affects the expected number of claims.\n\n\n\n\n\nGamma Regression Example:\n\nScenario: A manufacturing company wants to predict the lifetime of a machine part.\nVariables: Operating temperature, pressure, and usage frequency.\nModel: \\((\\frac{1}{\\mu} = \\beta_0 + \\beta_1 \\text{Temperature} + \\beta_2 \\text{Pressure} + \\beta_3 \\text{Frequency})\\)\nInterpretation: The coefficients \\((\\beta_1, \\beta_2, \\beta_3)\\) indicate how each predictor affects the inverse of the expected lifetime."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\n\n\nCode\n&gt; dat &lt;- MASS::Boston\n\n\nThere are many methodologies for constructing regression trees but one of the oldest is known as the classification and regression tree (CART) approach.\nBasic regression trees partition a data set into smaller subgroups and then fit a simple constant for each observation in the subgroup. The partitioning is achieved by successive binary partitions (aka recursive partitioning) based on the different predictors."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-1",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nAs a simple example, consider a continuous response variable \\(y\\) with two covariates \\(x_1,x_2\\) and the support of \\(x_1,x_2\\) partitioned into three regions. Then we write the tree regression model for \\(y\\) as:\n\\[\n\\hat{y} = \\hat{f}(x_1,x_2)=\\sum_{i=1}^{3}c_1\\times I_{(x_1,x_2)\\in R_i}\n\\]\nTree algorithms differ in how they grow the regression tree, i.e. partition the space of the covariates."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-2",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nAll partitioning of variables is done in a top-down, greedy fashion. This just means that a partition performed earlier in the tree will not change based on later partitions. In general the partitions are made to minimize following objective function (support initially partitioned into 2 regions, i.e. a binary tree):\n\\[\n\\text{SSE}=\\left\\{ \\sum_{i\\in R_{1}}\\left(y_{i}-c_{i}\\right)^{2}+\\sum_{i\\in R_{2}}\\left(y_{i}-c_{i}\\right)^{2}\\right\\}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-3",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nHaving found the best split, we repeat the splitting process on each of the two regions.\nThis process is continued until some stopping criterion is reached. What typically results is a very deep, complex tree that may produce good predictions on the training set, but is likely to overfit the data, particularly at the lower nodes.\nBy pruning these lower level nodes, we can introduce a little bit of bias in our model that help to stabilize predictions and will tend to generalize better to new, unseen data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-4",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-4",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nAs with penalized linear regression, we can use a complexity parameter \\(\\alpha\\) to penalize the number of terminal nodes of the tree (\\(T\\)), like the lasso \\(L_1\\) norm penalty, and find the smallest tree with lowest penalized error, i.e. the minimizing the following objective function:\n\\[\n\\text{SSE}+\\alpha\\left|T\\right|\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-5",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-5",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\n\n\nStrengths\n\nThey are very interpretable.\nMaking predictions is fast; just lookup constants in the tree.\nVariables importance is easy; those variables that most reduce the SSE.\nTree models give a non-linear response; better if the true regression surface is not smooth.\nThere are fast, reliable algorithms to learn these trees.\n\n\nWeaknesses\n\nSingle regression trees have high variance, resulting in unstable predictions (an alternative subsample of training data can significantly change the terminal nodes).\nDue to the high variance single regression trees have poor predictive accuracy."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\nAs mentioned, single tree models suffer from high variance. Although pruning the tree helps reduce this variance, there are alternative methods that actually exploite the variability of single trees in a way that can significantly improve performance over and above that of single trees. Bootstrap aggregating (bagging) is one such approach.\nBagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-1",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\nBagging combines and averages multiple tree models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, improving predictive performance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-2",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\nBagging follows three steps:\n\nCreate \\(m\\) bootstrap samples from the training data. Bootstrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.\nFor each bootstrap sample train a single, unpruned regression tree.\nAverage individual predictions from each tree to create an overall average predicted value."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-3",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\n\nFig: The bagging process."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\nBagging trees introduces a random component into the tree building process that reduces the variance of a single tree’s prediction and improves predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree.\nSo trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships. They are correlated."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-1",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\nTree correlation prevents bagging from optimally reducing the variance of the predictive values. Reducing variance further can be achieved by injecting more randomness into the tree-growing process. Random forests achieve this in two ways:\n\n\nBootstrap: similar to bagging - each tree is grown from a bootstrap resampled data set, which somewhat decorrelates them.\nSplit-variable randomization: each time a split is made, the search for the split variable is limited to a random subset of \\(m\\) of the \\(p\\) variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-2",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\nFor regression trees, typical default values used in split-value randomization are \\(m=\\frac{p}{3}\\) but this should be considered a tuning parameter.\nWhen \\(m=p\\), the randomization amounts to using only step 1 and is the same as bagging."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-3",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\n\n\nStrengths\n\nTypically have very good performance\nRemarkably good “out-of-the box” - very little tuning required\nBuilt-in validation set - don’t need to sacrifice data for extra validation\nNo pre-processing required\nRobust to outliers\n\n\nWeaknesses\n\nCan become slow on large data sets\nAlthough accurate, often cannot compete with advanced boosting algorithms\nLess interpretable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nGradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-1",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nWhereas random forests build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-2",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nThe main idea of boosting is to add new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.\n\nSequential ensemble approach."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-3",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nBoosting is a framework that iteratively improves any weak learning model. Many gradient boosting applications allow you to “plug in” various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-4",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-4",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nA weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. With regards to decision trees, shallow trees represent a weak learner. Commonly, trees with only 1-6 splits are used."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-5",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-5",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nCombining many weak models (versus strong ones) has a few benefits:\n\n\nSpeed: Constructing weak models is computationally cheap.\nAccuracy improvement: Weak models allow the algorithm to learn slowly; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.\nAvoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-6",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-6",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nHere is the algorithm for boosted regression trees with features \\(x\\) and response \\(y\\):\n\n\nFit a decision tree to the data: \\(F_1(x)=y\\),\nWe then fit the next decision tree to the residuals of the previous: \\(h_1(x)=y−F_1(x)\\)\nAdd this new tree to our algorithm: \\(F_2(x)=F_1(x)+h_1(x)\\),\nFit the next decision tree to the residuals of \\(F_2: h_2(x)=y−F_2(x)\\),\nAdd this new tree to our algorithm: \\(F_3(x)=F_2(x)+h_1(x)\\),\nContinue this process until some mechanism (i.e. cross validation) tells us to stop."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#xgboost-example",
    "href": "slides/BSMM_8740_lec_03.html#xgboost-example",
    "title": "Regression methods",
    "section": "XGBoost Example",
    "text": "XGBoost Example\nXGBoost is short for eXtreme Gradient Boosting package.\nWhile the XGBoost model often achieves higher accuracy than a single decision tree, it sacrifices the intrinsic interpretability of decision trees. For example, following the path that a decision tree takes to make its decision is trivial and self-explained, but following the paths of hundreds or thousands of trees is much harder.\nWe will work with XGBoost in today’s lab."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#kernel-regression",
    "href": "slides/BSMM_8740_lec_03.html#kernel-regression",
    "title": "Regression methods",
    "section": "Kernel Regression",
    "text": "Kernel Regression\nKernel Regression is a non-parametric technique in machine learning used to estimate the relationship between a dependent variable and one or more independent variables.\nUnlike linear regression, Kernel Regression does not assume a specific form for the relationship between the variables. Instead, it uses a weighted average of nearby observed data points to make predictions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#kernel-regression-1",
    "href": "slides/BSMM_8740_lec_03.html#kernel-regression-1",
    "title": "Regression methods",
    "section": "Kernel Regression",
    "text": "Kernel Regression\n\n\nSelect a Kernel Function:\n\nChoose a kernel function that will determine how weights are assigned to nearby data points. The Gaussian kernel is a common choice, where weights decrease with distance according to a normal distribution.\n\nChoose a Bandwidth:\n\nDecide on the bandwidth parameter that will control the spread of the kernel function. This affects the smoothness of the regression curve.\n\nCompute Weights:\n\nFor each point where you want to estimate the dependent variable, compute the weights for all observed data points using the kernel function.\n\nCalculate Weighted Average:\n\nUse the weights to compute a weighted average of the dependent variable values, giving more influence to points closer to the point of interest."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#kernel-regression-example",
    "href": "slides/BSMM_8740_lec_03.html#kernel-regression-example",
    "title": "Regression methods",
    "section": "Kernel Regression: example",
    "text": "Kernel Regression: example\n\n\nCode\n&gt; #Kernel regression\n&gt; # from https://towardsdatascience.com/kernel-regression-made-easy-to-understand-86caf2d2b844\n&gt; Kdata &lt;- \n+   tibble::tibble(\n+     Area = c(11,22,33,44,50,56,67,70,78,89,90,100)\n+     , RiverFlow = c(2337,2750,2301,2500,1700,2100,1100,1750,1000,1642, 2000,1932)\n+   )\n&gt; \n&gt; #function to calculate Gaussian kernel\n&gt; gausinKernel &lt;- function(x,b){exp(-0.5 *(x/b)^2)/(sqrt(2*pi))}\n&gt; #plotting function\n&gt; plt_fit &lt;- function(bandwidth = 10, support = seq(5,110,1)){\n+   tibble::tibble(x_hat = support) |&gt; \n+   dplyr::mutate(\n+     y_hat =\n+       purrr::map_dbl(\n+         x_hat\n+         , (\n+         \\(x){\n+           K &lt;- gausinKernel(Kdata$Area-x, bandwidth)\n+           sum( Kdata$RiverFlow * K/sum(K) )\n+         })\n+       )\n+   ) |&gt; \n+   ggplot(aes(x=x_hat, y=y_hat)) + \n+   geom_line(color=\"blue\") +\n+   geom_point(data = Kdata, aes(x=Area, y=RiverFlow), size=4, color=\"red\") +\n+   labs(title = \"Kernel regression\", subtitle = stringr::str_glue(\"bandwith = {bandwidth}; data = red | fit = blue\") ) +\n+   theme_minimal()\n+ }\n\n\n\nB = 5B=10B=15\n\n\n\n&gt; plt_fit(bandwidth = 5)\n\n\n\n\n\n\n\n\n\n\n\n&gt; plt_fit(bandwidth = 10)\n\n\n\n\n\n\n\n\n\n\n\n&gt; plt_fit(bandwidth = 15)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#kernel-regression-2",
    "href": "slides/BSMM_8740_lec_03.html#kernel-regression-2",
    "title": "Regression methods",
    "section": "Kernel Regression:",
    "text": "Kernel Regression:\n\nAdvantages of Kernel Regression\n\nFlexibility: Can capture complex, non-linear relationships between variables.\nNo Assumptions: Does not require the assumption of a specific functional form for the relationship.\n\nDisadvantages of Kernel Regression\n\nComputationally Intensive: Can be slow, especially with large datasets, since it requires calculating weights for all data points for each estimate.\nChoice of Parameters: The results can be sensitive to the choice of kernel function and bandwidth, requiring careful tuning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\nRegression with neural nets involves using artificial neural networks (ANNs) to predict a continuous output variable based on one or more input variables. Neural nets are powerful, flexible models that can capture complex relationships and patterns in the data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns-components",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns-components",
    "title": "Regression methods",
    "section": "Regression with ANNs: Components",
    "text": "Regression with ANNs: Components\n\n\nNeurons:\n\nThe building blocks of neural networks. Each neuron takes an input, processes it, and passes the output to the next layer.\n\nLayers:\n\nInput Layer: Receives the input data.\nHidden Layers: Intermediate layers that process the input data through neurons. There can be one or more hidden layers.\nOutput Layer: Produces the final prediction.\n\nWeights and Biases:\n\nEach connection between neurons has a weight, which adjusts the strength of the signal.\nEach neuron has a bias, which adjusts the output along with the weighted sum of inputs."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns-components-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns-components-1",
    "title": "Regression methods",
    "section": "Regression with ANNs: Components",
    "text": "Regression with ANNs: Components\n\n\nActivation Functions:\n\nFunctions applied to the output of each neuron in hidden layers to introduce non-linearity. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n\nLoss Function:\n\nMeasures the difference between the predicted output and the actual output. For regression tasks, common loss functions include Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n\nOptimization Algorithm:\n\nAdjusts the weights and biases to minimize the loss function. The most common optimization algorithm is Gradient Descent and its variants like Adam."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns-algorithm",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns-algorithm",
    "title": "Regression methods",
    "section": "Regression with ANNs: Algorithm",
    "text": "Regression with ANNs: Algorithm\n\n\nForward Propagation:\n\nInput data is passed through the network, layer by layer, with each neuron applying its weights, bias, and activation function, until the output layer produces the prediction.\n\nLoss Calculation:\n\nThe loss function calculates the error between the predicted output and the actual target value.\n\nBackward Propagation:\n\nThe network uses the error to adjust the weights and biases. This involves calculating the gradient of the loss function with respect to each weight and bias (using the chain rule), and then updating the weights and biases to reduce the error.\n\nIterative Training:\n\nThe process of forward propagation, loss calculation, and backward propagation is repeated for many iterations (epochs) until the loss converges to a minimum value."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns",
    "title": "Regression methods",
    "section": "Regression with ANNs:",
    "text": "Regression with ANNs:\n\nAdvantagesDisadvantages\n\n\nAdvantages of ANNs for Regression\n\nFlexibility: Can model complex, non-linear relationships between inputs and outputs.\nHigh Performance: Can achieve high accuracy with sufficient data and proper tuning.\nFeature Learning: Automatically learns relevant features from raw input data.\n\n\n\nDisadvantages of ANNs for Regression\n\nComputationally Intensive: Requires significant computational resources.\nData Hungry: Needs a large amount of training data to perform well.\nComplexity: Requires careful tuning of hyperparameters (e.g., number of layers, neurons, learning rate) and can be prone to overfitting if not properly regularized."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-1",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\nArchitecture of an ANN\n\nSingle layer NN architecture"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-2",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\n\nCommon Activation Functions"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns-example",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns-example",
    "title": "Regression methods",
    "section": "Regression with ANNs: example",
    "text": "Regression with ANNs: example\n\n\nCode\n&gt; set.seed(500)\n&gt;   \n&gt; # Boston dataset from MASS\n&gt; data &lt;- MASS::Boston\n&gt; \n&gt; # Normalize the data\n&gt; maxs &lt;- data %&gt;% dplyr::summarise_all(max) %&gt;% as.matrix() %&gt;% as.vector()\n&gt; mins &lt;- data %&gt;% dplyr::summarise_all(min) %&gt;% as.matrix() %&gt;% as.vector()\n&gt; data_scaled &lt;- data %&gt;% \n+   scale(center = mins, scale = maxs - mins) %&gt;% \n+   tibble::as_tibble()\n&gt;   \n&gt; # Split the data into training and testing set\n&gt; data_split &lt;- data_scaled %&gt;% rsample::initial_split(prop = .75)\n&gt; # extracting training data and test data as two seperate dataframes\n&gt; data_train &lt;- rsample::training(data_split)\n&gt; data_test  &lt;- rsample::testing(data_split)\n&gt; \n&gt; nn &lt;- data_train %&gt;% \n+   neuralnet::neuralnet(\n+     medv ~ .\n+     , data = .\n+     , hidden = c(5, 3)\n+     , linear.output = TRUE\n+   )\n&gt;   \n&gt; # Predict on test data\n&gt; pr.nn &lt;- neuralnet::compute( nn, data_test %&gt;% dplyr::select(-medv) )\n&gt;   \n&gt; # Compute mean squared error\n&gt; pr.nn_ &lt;- \n+   pr.nn$net.result * \n+   (max(data$medv) - min(data$medv)) +\n+   min(data$medv)\n&gt; test.r &lt;- \n+   data_test$medv * \n+   (max(data$medv) - min(data$medv)) + \n+   min(data$medv)\n&gt; MSE.nn &lt;- sum((test.r - pr.nn_)^2) / nrow(data_test)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-3",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\n\nNNRegression\n\n\n\n\n\n\n\n\n\n\n&gt; tibble::tibble(test = data_test$medv, predicted = pr.nn$net.result) %&gt;% \n+   ggplot(aes(x=test, y=predicted)) +\n+   geom_point(color='red') +\n+   geom_abline(intercept = 0, slope = 1, color=\"blue\", \n+                  linetype=\"dashed\", linewidth=1.5) +\n+   labs(title='Neural Net Regression'\n+      , subtitle = \n+        stringr::str_glue(\n+          \"Mean squared prediction error is {scales::number(MSE.nn, accuracy=0.01)}\"\n+        )\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#recap",
    "href": "slides/BSMM_8740_lec_03.html#recap",
    "title": "Regression methods",
    "section": "Recap",
    "text": "Recap\n\nToday we worked though a parametric and non-parametric regression methods that are useful for predicting a value given a set of covariates.\nNext week we will look in detail at the tidymodels package which will give a way to develop a workflow for fitting and comparing our models across different feature sets."
  },
  {
    "objectID": "slides/recipes.html#r-model-formulas",
    "href": "slides/recipes.html#r-model-formulas",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "R Model Formulas",
    "text": "R Model Formulas\nA simple formula in a linear model to predict house prices:\n\n&gt; mod1 &lt;- stats::lm(\n+   log(price) ~ type + sqft\n+   , data = Sacramento\n+   , subset = beds &gt; 2\n+   )\n\n\nThe purpose of this code chunk:\n\nsubset some of the data points (subset)\ncreate a design matrix for 2 predictor variable (but 3 model terms)\nlog transform the outcome variable\nfit a linear regression model\n\n\nThe first two steps create the design matrix."
  },
  {
    "objectID": "slides/recipes.html#summary-model-formula-method",
    "href": "slides/recipes.html#summary-model-formula-method",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Summary: Model Formula Method",
    "text": "Summary: Model Formula Method\n\nModel formulas are very expressive in that they can represent model terms easily\nThe formula/terms framework does some elegant functional programming\nFunctions can be embedded inline to do fairly complex things (on single variables) and these can be applied to new data sets."
  },
  {
    "objectID": "slides/recipes.html#summary-model-formula-method-1",
    "href": "slides/recipes.html#summary-model-formula-method-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Summary: Model Formula Method",
    "text": "Summary: Model Formula Method\nHowever, there are significant limitations to what this framework can do and, in some cases, it can be very inefficient.\nThis is mostly due of being written well before large scale modeling and machine learning were commonplace."
  },
  {
    "objectID": "slides/recipes.html#limitations-of-the-current-system",
    "href": "slides/recipes.html#limitations-of-the-current-system",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Limitations of the Current System",
    "text": "Limitations of the Current System\n\nFormulas are not very extensible especially with nested or sequential operations (e.g. y ~ scale(center(knn_impute(x)))).\nWhen used in modeling functions, you cannot recycle the previous computations.\nFor wide data sets, the formula method can be very inefficient and consume a significant proportion of the total execution time."
  },
  {
    "objectID": "slides/recipes.html#limitations-of-the-current-system-1",
    "href": "slides/recipes.html#limitations-of-the-current-system-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Limitations of the Current System",
    "text": "Limitations of the Current System\n\nMultivariate outcomes are kludgy by requiring cbind\nFormulas have a limited set of roles (next two slides)\n\nA more in-depth discussion of these issues can be found in this blog post."
  },
  {
    "objectID": "slides/recipes.html#variable-roles",
    "href": "slides/recipes.html#variable-roles",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Variable Roles",
    "text": "Variable Roles\nFormulas have been re-implemented in different packages for a variety of different reasons:\n\n&gt; # ?lme4::lmer\n&gt; # Subjects need to be in the data but are not part of the model\n&gt; lme4::lmer(Reaction ~ Days + (Days | Subject), data = lme4::sleepstudy)\n&gt; \n&gt; # BradleyTerry2\n&gt; # We want to make the outcomes to be a function of a \n&gt; # competitor-specific function of reach \n&gt; BradleyTerry2::BTm(outcome = 1, player1 = winner, player2 = loser,\n+     formula = ~ SVL[..] + (1|..), \n+     data = BradleyTerry2::flatlizards)\n&gt; \n&gt; # modeltools::ModelEnvFormula (using the modeltools package for formulas)\n&gt; # mob\n&gt; data(PimaIndiansDiabetes, package = 'mlbench')\n&gt; modeltools::ModelEnvFormula(diabetes ~ glucose | pregnant + mass +  age,\n+     data = PimaIndiansDiabetes)"
  },
  {
    "objectID": "slides/recipes.html#variable-roles-1",
    "href": "slides/recipes.html#variable-roles-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Variable Roles",
    "text": "Variable Roles\n\nA general list of possible variables roles could be\n\noutcomes\npredictors\nstratification\nmodel performance data (e.g. loan amount to compute expected loss)\nconditioning or faceting variables (e.g. lattice or ggplot2)\nrandom effects or hierarchical model ID variables\ncase weights (*)\noffsets (*)\nerror terms (limited to Error in the aov function)(*)\n\n(*) Can be handled in formulas but are hard-coded into functions."
  },
  {
    "objectID": "slides/recipes.html#recipes",
    "href": "slides/recipes.html#recipes",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Recipes",
    "text": "Recipes\nWe can approach the design matrix and preprocessing steps by first specifying a sequence of steps.\n\nprice is an outcome\ntype and sqft are predictors\nlog transform price\nconvert type to dummy variables"
  },
  {
    "objectID": "slides/recipes.html#recipes-1",
    "href": "slides/recipes.html#recipes-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Recipes",
    "text": "Recipes\nA recipe is a specification of intent.\nOne issue with the formula method is that it couples the specification for your predictors along with the implementation.\nRecipes, as you’ll see, separates the planning from the doing.\nWebsite: https://topepo.github.io/recipes"
  },
  {
    "objectID": "slides/recipes.html#recipes-2",
    "href": "slides/recipes.html#recipes-2",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Recipes",
    "text": "Recipes\nA recipe can be trained then applied to any data.\n\n&gt; ## Create an initial recipe with only predictors and outcome\n&gt; rec &lt;- recipes::recipe(price ~ type + sqft, data = Sacramento)\n&gt; \n&gt; rec &lt;- rec %&gt;% \n+   recipes::step_log(price) %&gt;%\n+   recipes::step_dummy(type)\n&gt; \n&gt; rec_trained &lt;- recipes::prep(rec, training = Sacramento, retain = TRUE)\n&gt; \n&gt; design_mat  &lt;- recipes::bake(rec_trained, new_data = Sacramento)"
  },
  {
    "objectID": "slides/recipes.html#selecting-variables",
    "href": "slides/recipes.html#selecting-variables",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Selecting Variables",
    "text": "Selecting Variables\nIn the last slide, we used dplyr-like syntax for selecting variables such as step_dummy(type).\nIn some cases, the names of the predictors may not be known at the time when you construct a recipe (or model formula). For example:\n\ndummy variable columns\nPCA feature extraction when you keep components that capture \\(X\\)% of the variability.\ndiscretized predictors with dynamic bins"
  },
  {
    "objectID": "slides/recipes.html#selecting-variables-1",
    "href": "slides/recipes.html#selecting-variables-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Selecting Variables",
    "text": "Selecting Variables\ndplyr selectors can also be used on variables names, such as\n\n&gt; rec %&gt;% \n+   recipes::step_spatialsign(\n+     matches(\"^PC[1-9]\")\n+     , all_numeric()\n+     , -all_outcomes()\n+   )\n\nVariables can be selected by name, role, data type, or any combination of these."
  },
  {
    "objectID": "slides/recipes.html#extending",
    "href": "slides/recipes.html#extending",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Extending",
    "text": "Extending\nNeed to add more preprocessing or other operations?\n\n&gt; standardized &lt;- rec_trained %&gt;%\n+   recipes::step_center(recipes::all_numeric()) %&gt;%\n+   recipes::step_scale(recipes::all_numeric()) %&gt;%\n+   recipes::step_pca(recipes::all_numeric())\n&gt;           \n&gt; ## Only estimate the new parts:\n&gt; standardized &lt;- recipes::prep(standardized)\n\nIf an initial step is computationally expensive, you don’t have to redo those operations to add more."
  },
  {
    "objectID": "slides/recipes.html#available-steps",
    "href": "slides/recipes.html#available-steps",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Available Steps",
    "text": "Available Steps\n\nBasic: logs, roots, polynomials, logits, hyperbolics\nEncodings: dummy variables, “other” factor level collapsing, discretization\nDate Features: Encodings for day/doy/month etc, holiday indicators\nFilters: correlation, near-zero variables, linear dependencies\nImputation: K-nearest neighbors, bagged trees, mean/mode imputation,"
  },
  {
    "objectID": "slides/recipes.html#available-steps-1",
    "href": "slides/recipes.html#available-steps-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Available Steps",
    "text": "Available Steps\n\nNormalization/Transformations: center, scale, range, Box-Cox, Yeo-Johnson\nDimension Reduction: PCA, kernel PCA, ICA, Isomap, data depth features, class distances\nOthers: spline basis functions, interactions, spatial sign\n\nMore on the way (i.e. autoencoders, more imputation methods, etc.)\nOne of the package vignettes shows how to write your own step functions."
  },
  {
    "objectID": "slides/recipes.html#extending-1",
    "href": "slides/recipes.html#extending-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Extending",
    "text": "Extending\nRecipes can also be created with different roles manually\n\n&gt; rec &lt;- \n+   recipes::recipe(x  = Sacramento) %&gt;%\n+   recipes::update_role(price, new_role = \"outcome\") %&gt;%\n+   recipes::update_role(type, sqft, new_role = \"predictor\") %&gt;%\n+   recipes::update_role(zip, new_role = \"strata\")\n\nAlso, the sequential nature of steps means that they don’t have to be R operations and could call other compute engines (e.g. Weka, scikit-learn, Tensorflow, etc. )"
  },
  {
    "objectID": "slides/recipes.html#extending-2",
    "href": "slides/recipes.html#extending-2",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Extending",
    "text": "Extending\nWe can create wrappers to work with recipes too:\n\n&gt; lin_reg.recipe &lt;- function(rec, data, ...) {\n+   trained &lt;- recipes::prep(rec, training = data)\n+   lm.fit(\n+     x = recipes::bake(trained, newdata = data, all_predictors())\n+     , y = recipes::bake(trained, newdata = data, all_outcomes())\n+     , ...\n+   )\n+ }"
  },
  {
    "objectID": "slides/recipes.html#an-example",
    "href": "slides/recipes.html#an-example",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "An Example",
    "text": "An Example\nKuhn and Johnson (2013) analyze a data set where thousands of cells are determined to be well-segmented (WS) or poorly segmented (PS) based on 58 image features. We would like to make predictions of the segmentation quality based on these features."
  },
  {
    "objectID": "slides/recipes.html#an-example-1",
    "href": "slides/recipes.html#an-example-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "An Example",
    "text": "An Example\n\n&gt; data(segmentationData, package = \"caret\")\n&gt; \n&gt; seg_train &lt;- segmentationData %&gt;% \n+   dplyr::filter(Case == \"Train\") %&gt;% \n+   dplyr::select(-Case, -Cell)\n&gt; \n&gt; seg_test  &lt;- segmentationData %&gt;% \n+   dplyr::filter(Case == \"Test\")  %&gt;% \n+   dplyr::select(-Case, -Cell)"
  },
  {
    "objectID": "slides/recipes.html#a-simple-recipe",
    "href": "slides/recipes.html#a-simple-recipe",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "A Simple Recipe",
    "text": "A Simple Recipe\n\n&gt; rec &lt;- recipes::recipe(Class  ~ ., data = seg_train)\n&gt; \n&gt; basic &lt;- rec %&gt;%\n+   # Correct some predictors for skewness\n+   recipes::step_YeoJohnson(recipes::all_predictors()) %&gt;%\n+   # Standardize the values\n+   recipes::step_center(recipes::all_predictors()) %&gt;%\n+   recipes::step_scale(recipes::all_predictors())\n&gt; \n&gt; # Estimate the transformation and standardization parameters \n&gt; basic &lt;- \n+   recipes::prep(\n+     basic\n+     , training = seg_train\n+     , verbose = FALSE\n+     , retain = TRUE\n+   )"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis",
    "href": "slides/recipes.html#principal-component-analysis",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca &lt;- basic %&gt;% \n+   recipes::step_pca(\n+     recipes::all_predictors()\n+     , threshold = .9\n+   )\n&gt; \n&gt; summary(pca)\n\n# A tibble: 59 × 4\n   variable                type      role      source  \n   &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 AngleCh1                &lt;chr [2]&gt; predictor original\n 2 AreaCh1                 &lt;chr [2]&gt; predictor original\n 3 AvgIntenCh1             &lt;chr [2]&gt; predictor original\n 4 AvgIntenCh2             &lt;chr [2]&gt; predictor original\n 5 AvgIntenCh3             &lt;chr [2]&gt; predictor original\n 6 AvgIntenCh4             &lt;chr [2]&gt; predictor original\n 7 ConvexHullAreaRatioCh1  &lt;chr [2]&gt; predictor original\n 8 ConvexHullPerimRatioCh1 &lt;chr [2]&gt; predictor original\n 9 DiffIntenDensityCh1     &lt;chr [2]&gt; predictor original\n10 DiffIntenDensityCh3     &lt;chr [2]&gt; predictor original\n# ℹ 49 more rows"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis-1",
    "href": "slides/recipes.html#principal-component-analysis-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca %&lt;&gt;% recipes::prep() \n&gt; \n&gt; pca %&gt;% summary()\n\n# A tibble: 16 × 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Class    &lt;chr [3]&gt; outcome   original\n 2 PC01     &lt;chr [2]&gt; predictor derived \n 3 PC02     &lt;chr [2]&gt; predictor derived \n 4 PC03     &lt;chr [2]&gt; predictor derived \n 5 PC04     &lt;chr [2]&gt; predictor derived \n 6 PC05     &lt;chr [2]&gt; predictor derived \n 7 PC06     &lt;chr [2]&gt; predictor derived \n 8 PC07     &lt;chr [2]&gt; predictor derived \n 9 PC08     &lt;chr [2]&gt; predictor derived \n10 PC09     &lt;chr [2]&gt; predictor derived \n11 PC10     &lt;chr [2]&gt; predictor derived \n12 PC11     &lt;chr [2]&gt; predictor derived \n13 PC12     &lt;chr [2]&gt; predictor derived \n14 PC13     &lt;chr [2]&gt; predictor derived \n15 PC14     &lt;chr [2]&gt; predictor derived \n16 PC15     &lt;chr [2]&gt; predictor derived"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis-2",
    "href": "slides/recipes.html#principal-component-analysis-2",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca &lt;-\n+   recipes::bake(\n+     pca\n+     , new_data = seg_test\n+     , everything()\n+   )\n&gt; pca[1:4, 1:8]\n\n# A tibble: 4 × 8\n  Class  PC01  PC02   PC03   PC04  PC05   PC06   PC07\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 PS     4.86 -5.85 -0.891 -4.13  1.84  -2.29  -3.88 \n2 PS     3.28 -1.51  0.353 -2.24  0.441 -0.911  0.800\n3 WS    -7.03 -1.77 -2.42  -0.652 3.22  -0.212  0.118\n4 WS    -6.96 -2.08 -2.89  -1.79  3.20  -0.845 -0.204"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis-3",
    "href": "slides/recipes.html#principal-component-analysis-3",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n\nPCA1\n&gt; ggplot(pca, aes(x = PC01, y = PC02, color = Class)) + \n+   geom_point(alpha = .4)"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis-4",
    "href": "slides/recipes.html#principal-component-analysis-4",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n\nPCA2\n&gt; rngs &lt;- extendrange(c(pca$PC01, pca$PC02))\n&gt; ggplot(pca, aes(x = PC01, y = PC02, color = Class)) + \n+   geom_point(alpha = .4) + \n+   xlim(rngs) + ylim(rngs) + \n+   theme(legend.position = \"top\")"
  },
  {
    "objectID": "slides/recipes.html#kernel-principal-component-analysis",
    "href": "slides/recipes.html#kernel-principal-component-analysis",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Kernel Principal Component Analysis",
    "text": "Kernel Principal Component Analysis\n\n&gt; kern_pca &lt;- basic %&gt;% \n+   recipes::step_kpca(\n+     recipes::all_predictors()\n+     , num_comp = 2\n+     , options = \n+       list(\n+         kernel = \"rbfdot\"\n+         , kpar = list(sigma = 0.05)\n+       )\n+   )\n&gt; \n&gt; kern_pca &lt;- recipes::prep(kern_pca)\n&gt; \n&gt; kern_pca &lt;- recipes::bake(kern_pca, new_data = seg_test, everything())"
  },
  {
    "objectID": "slides/recipes.html#kernel-principal-component-analysis-1",
    "href": "slides/recipes.html#kernel-principal-component-analysis-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Kernel Principal Component Analysis",
    "text": "Kernel Principal Component Analysis\n\n\nKernel PCA\n&gt; rngs &lt;- extendrange(c(kern_pca$kPC1, kern_pca$kPC2))\n&gt; ggplot(kern_pca, aes(x = kPC1, y = kPC2, color = Class)) + \n+   geom_point(alpha = .4) + \n+   xlim(rngs) + ylim(rngs) + \n+   theme(legend.position = \"top\")"
  },
  {
    "objectID": "slides/recipes.html#distance-to-each-class-centroid",
    "href": "slides/recipes.html#distance-to-each-class-centroid",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Distance to Each Class Centroid",
    "text": "Distance to Each Class Centroid\n\n\nKernel PCA - distance to each centroid\n&gt; dist_to_classes &lt;- basic %&gt;% \n+   recipes::step_classdist(recipes::all_predictors(), class = \"Class\") %&gt;%\n+   # Take log of the new distance features\n+   recipes::step_log(starts_with(\"classdist\"))\n&gt; \n&gt; dist_to_classes &lt;- recipes::prep(dist_to_classes, verbose = FALSE)\n&gt; \n&gt; # All variables are retained plus an additional one for each class\n&gt; dist_to_classes &lt;- recipes::bake(dist_to_classes, new_data = seg_test, matches(\"[Cc]lass\"))\n&gt; dist_to_classes\n\n\n# A tibble: 1,010 × 3\n   Class classdist_PS classdist_WS\n   &lt;fct&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 PS            1.53         1.74\n 2 PS            1.35         1.46\n 3 WS            1.71         1.53\n 4 WS            1.75         1.61\n 5 PS            1.47         1.65\n 6 WS            1.48         1.47\n 7 WS            1.49         1.55\n 8 WS            1.55         1.40\n 9 PS            1.54         1.71\n10 PS            1.55         1.57\n# ℹ 1,000 more rows"
  },
  {
    "objectID": "slides/recipes.html#distance-to-each-class",
    "href": "slides/recipes.html#distance-to-each-class",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Distance to Each Class",
    "text": "Distance to Each Class\n\n\nKernel PCA - distance to each class\n&gt; rngs &lt;- extendrange(c(dist_to_classes$classdist_PS, dist_to_classes$classdist_WS))\n&gt; ggplot(dist_to_classes, aes(x = classdist_PS, y = classdist_WS, color = Class)) + \n+   geom_point(alpha = .4) + \n+   xlim(rngs) + ylim(rngs) + \n+   theme(legend.position = \"top\") + \n+   xlab(\"Distance to PS Centroid (log scale)\") + \n+   ylab(\"Distance to WS Centroid (log scale)\")"
  },
  {
    "objectID": "slides/recipes.html#next-steps",
    "href": "slides/recipes.html#next-steps",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Next Steps",
    "text": "Next Steps\n\nGet it on CRAN once tidyselect is on CRAN\nAdd more steps\ncaret methods for recipes (instead of using preProcess):\n\nmodel1 &lt;- train(recipe, data = data, method, ...)\nas an alternative to\nmodel2 &lt;- train(x, y, method, preProcess, ...) # or\nmodel3 &lt;- train(y ~ x1 + x2, data = data, method, preProcess, ...)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_06.html#recap-of-last-week",
    "title": "Time series methods",
    "section": "Recap of last week:",
    "text": "Recap of last week:\n\nLast week we introduced classification and clustering methods within the Tidymodels framework in R.\nToday we look at methods for analysing time series, and we use the timetk and modeltime packages in conjunction with Tidymodels to create and evaluate time series models."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#this-week",
    "href": "slides/BSMM_8740_lec_06.html#this-week",
    "title": "Time series methods",
    "section": "This week:",
    "text": "This week:\n\nToday we will explore time series - data where each observation includes a time measurement and the time measurements are ordered.\nWe’ll look at how to manipulate our time values, create time-based features, plot our time series, and decompose time series into components.\nFinally we will use our time series for forecasting, using regression, exponential smoothing and ARIMA1 models\n\nAuto Regressive Integrated Moving Average"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series",
    "href": "slides/BSMM_8740_lec_06.html#time-series",
    "title": "Time series methods",
    "section": "Time series",
    "text": "Time series\nA time series data is a data frame (tibble) with an ordered temporal measurement.\nWhy is this a separate area of study? Consider the simple linear regression model\n\\[\ny_t=x_t^\\top\\beta + \\epsilon_t;\\;t=1,\\ldots,R\n\\]\nerrors should not be serially correlated for OLS estimates:\n\n\\(\\mathbb{E}[\\epsilon_t]=\\mathbb{E}[\\epsilon_t|\\epsilon_{t-1},\\epsilon_{t-1},\\ldots]\\), and\n\\(\\mathbb{E}[\\epsilon_t\\epsilon_{t-j}]=0,\\forall j\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series-characteristics",
    "href": "slides/BSMM_8740_lec_06.html#time-series-characteristics",
    "title": "Time series methods",
    "section": "Time series: characteristics",
    "text": "Time series: characteristics\n\n\nMost economic and financial time series exhibit some form of serial correlation\n\nIf economic output is large during the previous quarter then there is a good chance that it is going to be large in the current quarter\n\nA change that arises in the current period may only affect other variables in the distant future\nA particular shock may affect variables over successive quarters\n\nHence, we need to start thinking about the dynamic structure of the system that we are investigating"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series-dynamics",
    "href": "slides/BSMM_8740_lec_06.html#time-series-dynamics",
    "title": "Time series methods",
    "section": "Time series: dynamics",
    "text": "Time series: dynamics\nWhether time series data is used for forecasting or for testing various theories/hypotheses, we always need to identify the dynamic evolution of the variables, e.g.\n\\[\n\\begin{align*}\n\\text{(trend)}\\qquad T_{t} & =1+0.05t\\qquad\\\\\n\\text{(seasonality)}\\qquad S_{t} & =1.5\\cos(t\\pi\\times0.166)\\\\\n\\text{(noise)}\\qquad I_{t} & =0.5I_{t-1}+\\epsilon_{t}\\quad\\epsilon_t\\sim\\mathscr{N}(0,\\sigma^2)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series-generation",
    "href": "slides/BSMM_8740_lec_06.html#time-series-generation",
    "title": "Time series methods",
    "section": "Time series: generation",
    "text": "Time series: generation\n\n\ncreating a time series\n&gt; set.seed(8740)\n&gt; \n&gt; dat &lt;- tibble::tibble(\n+   date = seq(as.Date('2015-04-7'),as.Date('2020-03-22'),'2 weeks')\n+ ) |&gt; \n+   tibble::rowid_to_column(\"t\") |&gt; \n+   dplyr::mutate(\n+     trend = 1 + 0.05 * t\n+     , seasonality = 1.5 * cos(pi * t * 0.166)\n+     , noise = rnorm(length(t))\n+     , temp = dplyr::lag(noise)\n+   ) |&gt; \n+   tidyr::replace_na(list(temp = 0)) |&gt; \n+   dplyr::mutate(\n+     noise =\n+       purrr::map2_dbl(\n+         noise\n+         , temp\n+         , ~ .x + 0.5 * .y\n+       )\n+   ) |&gt; \n+   dplyr::select(-temp)\n&gt; dat\n\n\n# A tibble: 130 × 5\n       t date       trend seasonality   noise\n   &lt;int&gt; &lt;date&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 2015-04-07  1.05     1.30     1.59  \n 2     2 2015-04-21  1.1      0.755    0.937 \n 3     3 2015-05-05  1.15     0.00942 -0.941 \n 4     4 2015-05-19  1.2     -0.739   -0.460 \n 5     5 2015-06-02  1.25    -1.29     0.318 \n 6     6 2015-06-16  1.3     -1.50    -0.0151\n 7     7 2015-06-30  1.35    -1.31    -1.23  \n 8     8 2015-07-14  1.4     -0.772   -1.55  \n 9     9 2015-07-28  1.45    -0.0283   0.697 \n10    10 2015-08-11  1.5      0.723   -0.0404\n# ℹ 120 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series-generation-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series-generation-1",
    "title": "Time series methods",
    "section": "Time series: generation",
    "text": "Time series: generation\n\ncomponentscombined\n\n\n\n\nCode\n&gt; dat |&gt; \n+   tidyr::pivot_longer(-c(t, date)) |&gt; \n+   ggplot(aes(x=date, y=value, color=name)) + geom_line(linewidth=1) + \n+   labs(\n+     title = 'trend, seasonality, and noise'\n+     , subtitle = \"deterministic: trend, seasonality | stochastic: noise\"\n+     , color=NULL) + theme_minimal() + theme(plot.margin = margin(0,0,0,0, \"cm\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n&gt; dat |&gt; \n+   dplyr::mutate(y = trend + seasonality + noise) |&gt; \n+   ggplot(aes(x=date, y=y)) + geom_line(linewidth=1) + geom_point(color=\"red\") +\n+   labs(\n+     title = 'trend + seasonality + noise'\n+     , subtitle = \"deterministic: trend, seasonality | stochastic: noise\") + theme_minimal() +\n+   theme(plot.margin = margin(0,0,0,0, \"cm\"))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\n\nTime series is a collection of observations indexed by the date of each observation\nUsing notation that starts at time, \\(t=1\\), and using the end point, \\(t=T\\) the time series is a set of observations:\n\n\\[\\{y_1,y_2,y_3,…,y_T\\}\\]\n\nTime index can be of any frequency (e.g. daily, quarterly, etc.)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-1",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nDeterministic and Stochastic Processes\n\ndeterministic processes always produce the same output from a given starting point or state\nstochastic processes have indeterminacy\n\nUsually described by some form of statistical distribution\nExamples include: white noise processes, random walks, Brownian motions, Markov chains, martingale difference sequences, etc."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-2",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-2",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - White noise\n\n\nA white noise series is made of serially uncorrelated random variables with zero mean and finite variance\nFor example, errors may be characterised by a Gaussian white noise process, where such a variable has a normal distribution\nSlightly stronger condition is that they are independent from one another\n\n\\[\\epsilon_{t}\\sim\\text{{i.i.d.}}\\mathscr{N}\\left(0,\\sigma_{\\epsilon_{t}}^{2}\\right)\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-3",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-3",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - White noise\n\nimplications:\n\n\\(\\mathbb{E}\\left[\\epsilon_{t}\\right]=\\mathbb{E}\\left[\\left.\\epsilon_{t}\\right|\\epsilon_{t-1},\\epsilon_{t-2},\\ldots\\right]=0\\)\n\\(\\mathbb{E}\\left[\\epsilon_{t}\\epsilon_{t-j}\\right]=\\text{cov}\\left(\\epsilon_{t},\\epsilon_{t-j}\\right)=0\\)\n\\(\\text{var}\\left(\\epsilon_{t}\\right)=\\text{cov}\\left(\\epsilon_{t},\\epsilon_{t}\\right)=\\sigma_{\\epsilon_{t}}^{2}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-4",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-4",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Random walk\n\n\nRandom walk definition implies that the effect of a shock is permanent. \\[y_t=y_{t-1}+\\epsilon_t\\]\nGiven the starting value \\(y_0\\), and using recursive substitution, this process could be represented as \\[y_t=y_0+\\sum_{j=1}^t\\epsilon_t\\]\nSince the effect of past shocks do not dissipate we say it has an infinite memory"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-5",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-5",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Random walk + drift\n\n\nRandom walk plus a constant term. \\[y_t=\\mu+y_{t-1}+\\epsilon_t\\]\nGiven the starting value \\(y_0=0\\), and using recursive substitution, this process could be represented as \\[y_t=\\mu t+\\sum_{j=1}^t\\epsilon_t\\]\nShocks have permanent effects and are influenced by drift"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-6",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-6",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Random walks\n\nCharacteristics:\n\nIndependence:\n\nThe steps in a random walk are independent of each other. The future position depends only on the current position and a random step.\n\nTypes:\n\nSimple Random Walk: The step sizes are often \\(\\pm 1\\), with equal probability.\nGeneral Random Walk: The step sizes can follow any distribution.\n\nApplications:\n\nStock price movements, particle diffusion, and population genetics."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-7",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-7",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Markov chains\n\nCharacteristics:\n\nState Space: - A Markov chain consists of a set of states and transition probabilities between these states.\nMarkov Property:\n\nThe probability of transitioning to the next state depends only on the current state: \\(\\mathbb{P}(X_{t+1} = s' | X_t = s, X_{t-1}, ..., X_0) = \\mathbb{P}(X_{t+1} = s' | X_t = s)\\).\n\nTransition Matrix:\n\nThe transitions are governed by a matrix of probabilities, where each entry \\(\\mathsf{P}_{i,j}\\) represents the probability of moving from state \\(i\\) to state \\(j\\).\n\nTypes:\n\nDiscrete-Time Markov Chain: The process is observed at discrete time intervals.\nContinuous-Time Markov Chain: The process evolves continuously over time.\n\nApplications:\n\nWeather modeling, queueing theory, board games (like Monopoly), speech recognition and speech generation."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-8",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-8",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Markov chains\n\nIn the discrete case, given a state transition matrix \\(A\\) and an initial state \\(\\pi_0\\), then\n\\[\n\\begin{align*}\n\\pi_1 & = \\pi_0A\\\\\n\\pi_2 & = \\pi_1A = \\pi_0A^2\\\\\n&\\vdots\\\\\n\\pi_n & = \\pi_0A^n\n\\end{align*}\n\\]\nIf the markov chain has a stationary distribution \\(\\pi^{s}\\), then \\(\\pi^{s}A=\\pi^{s}\\) by definition and \\(\\pi^{s}(I-A)=0\\) determines \\(\\pi^{s}\\) up to a constant."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-9",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-9",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\n\nIn an AR(1) process the current value is a linear function of the previous value.\n\n\\[\ny_t=\\phi_1 y_{t-1}+\\epsilon_t\n\\]\n\nFixing the starting value at \\(y_0=0\\), and with repeated substitution, this process could be represented as\n\n\\[\ny_t=\\sum_{j=1}^t\\phi_1^{t-j}\\epsilon_j\n\\]\n\nThe distribution of each error term is \\(\\epsilon_t=\\mathscr{N}(0,\\sigma^2)\\) with \\(\\mathbb{E}[\\epsilon_i\\epsilon_j]=0,\\,\\forall i\\ne j\\), and we can generalize to several lags (i.e. an AR(p) model)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-10",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-10",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nMoments of distribution\n\n\nThe first moment of a stochastic process is the average of \\(y_t\\) over all possible realisations \\[\\hat{y}=\\mathbb{E}[y_t];\\;\\;t=1,\\ldots,T\\]\nThe second moment is defined as the variance \\[\\text{var}(y_t)=\\mathbb{E}[y_t\\times y_t]=\\mathbb{E}[y_t- \\mathbb{E}[y_t]^2];\\;\\;t=1,\\ldots,T\\]\nThe covariance, for \\(j\\) \\[\\text{cov}(y_t,y_{t-j})=\\mathbb{E}[y_t\\times y_{t-j}]=\\mathbb{E}[(y_t- \\mathbb{E}[y_t])(y_{t-j}- \\mathbb{E}[y_{t-j}])];\\;\\;t=1,\\ldots,T\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-11",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-11",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nConditional moments\n\n\nConditional distribution is based on past realisations of a random variable\nFor the AR(1) model (where \\(\\epsilon_t\\) are iid Gausian and \\(|\\phi|&lt;0\\)) \\[y_t=\\phi y_{t-1}+\\epsilon_t\\]\nThe conditional moments are \\[\n\\begin{align*}\n\\mathbb{E}\\left[\\left.y_{t}\\right|y_{t-1}\\right] & =\\phi y_{t-1}\\\\\n\\text{var}\\left(\\left.y_{t}\\right|y_{t-1}\\right) & =\\mathbb{E}\\left[\\phi y_{t-1}+\\epsilon_{t}-\\phi y_{t-1}\\right]^{2}=\\mathbb{E}\\left[\\epsilon\\right]^{2}=\\sigma^{2}\\\\\n\\text{cov}\\left(\\left.y_{t}\\right|y_{t-1},\\left.y_{t-j}\\right|y_{t-j-1}\\right) & =0;\\;j&gt;1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-12",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-12",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nConditional moments\n\n\nConditioning on \\(y_{t-2}\\) for \\(y_t\\) \\[\n\\begin{align*}\n\\mathbb{E}\\left[\\left.y_{t}\\right|y_{t-2}\\right] & =\\phi^{2}y_{t-2}\\\\\n\\text{var}\\left(\\left.y_{t}\\right|y_{t-2}\\right) & =\\left(1+\\phi^{2}\\right)\\sigma^{2}\\\\\n\\text{cov}\\left(\\left.y_{t}\\right|y_{t-2},\\left.y_{t-j}\\right|y_{t-j-2}\\right) & =\\phi\\sigma^{2};\\;j=1\\\\\n& =0;\\;j&gt;1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-13",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-13",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nUnconditional moments\n\n\nFor the AR(1) model (where \\(\\epsilon_t\\) are iid Gausian and \\(|\\phi|&lt;1\\)) \\[y_t=\\phi y_{t-1}+\\epsilon_t\\]\nThe unconditional moments are (assuming stationarity and \\(\\phi&lt;1\\)) \\[\n\\begin{align*}\n\\mathbb{E}\\left[y_{t}\\right] & =0\\\\\n\\text{var}\\left(y_{t}\\right) & =\\text{var}\\left(\\phi y_{t-1}+\\epsilon_t\\right)\\\\\n& = \\phi^2 \\text{var}\\left(y_{t-1}\\right) + \\text{var}\\left(\\epsilon_{t}\\right)\\\\\n& = \\frac{\\sigma^2}{1-\\phi^2}\\\\\n\\text{cov}\\left(y_{t},y_{t-k}\\right) & =\\phi^k\\frac{\\sigma^2}{1-\\phi^2}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-14",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-14",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStationarity\n\n\nA time series is strictly stationary if for any \\(\\{j_1,j_2,\\ldots,j_n\\}\\)\nthe joint distribution of \\(\\{y_t,y_{t+j_1},y_{t+j_2},\\ldots,y_{t+j_n}\\}\\)\ndepends only on the intervals separating the dates \\(\\{j_1,j_2,\\ldots,j_n\\}\\)\nand not on the date \\(t\\) itself"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-15",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-15",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nCovariance stationary\n\n\nIf neither the mean \\(\\hat{y}\\) nor the covariance \\(\\text{cov}(y_t,y_{t-j})\\) depend on \\(t\\)\nThe the process for \\(y_t\\) is said to be covariance (weakly) stationary, where \\(\\forall t,j\\) \\[\n\\begin{align*}\n\\mathbb{E}\\left[y_{t}\\right] & =\\bar{y}\\\\\n\\mathbb{E}\\left[\\left(y_{t}-\\bar{y}\\right)\\left(y_{t-j}-\\bar{y}\\right)\\right] & =\\text{cov}\\left(y_{t},y_{t-j}\\right)\n\\end{align*}\n\\]\nNote that the process \\(y_t=\\alpha t+\\epsilon_t\\) would not be stationary, as the mean clearly depends on \\(t\\)\nWe saw that the unconditional moments of the AR(1) with \\(|ϕ|&lt;1\\) had a mean and covariance that did not depend on time"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-16",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-16",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutocorrelation function (ACF)\n\n\nFor a stationary process we can plot the standardised covariance of the process over successive lags\nThe autocovariance function is denoted by \\(\\gamma (j)\\equiv\\text{cov}(y_t,y_{t-j})\\) for \\(t=1,\\ldots,T\\)\nTha autocovariance function is standardized by dividing each function by the variance, giving the ACF for successive values of \\(j\\)\n\\[\\rho(j)\\equiv\\frac{\\gamma(j)}{\\gamma(0)}\\]\nTo display the results of the ACF we usually plot \\(ρ(j)\\) against (non-negative) \\(j\\) to illustrate the degree of persistence in a variable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-17",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-17",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nPartial autocorrelation function (PACF)\n\n\nWith an AR(1) process \\(y_t=\\phi y_{t-1}+\\epsilon_t\\), the ACF would suggest \\(y_t\\) and \\(y_{t-2}\\) are correlated, even though \\(y_{t-2}\\) does not appear in the model.\nThis is due to the pass through, where we noted that \\(y_t=\\phi^2y_{t-2}\\) when performing recursive substitution\nPACF eliminates the effects of passthrough and puts the focus on the independent relationship between \\(y_t\\) and \\(y_{t_2}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-18",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-18",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nPartial autocorrelation function (PACF)\n\ndemonstration of an AR(1) process and its ACF\n&gt; library(ggfortify)\n\n\nRegistered S3 method overwritten by 'ggfortify':\n  method          from   \n  autoplot.glmnet parsnip\n\n\ndemonstration of an AR(1) process and its ACF\n&gt; # Parameters for the AR(1) process\n&gt; phi = 0.8  # Autoregressive coefficient (should be less than 1 in absolute value)\n&gt; n = 100    # Number of observations\n&gt; \n&gt; # Simulate AR(1) process\n&gt; set.seed(123)  # For reproducibility\n&gt; epsilon = rnorm(n)  # White noise\n&gt; X = rep(0, n)  # Initialize the series\n&gt; \n&gt; # Generate the AR(1) series\n&gt; for (t in 2:n) {\n+   X[t] = phi * X[t-1] + epsilon[t]\n+ }\n&gt; \n&gt; # Plot the AR(1) series\n&gt; tibble::tibble(y=X, x=1:length(X)) |&gt; ggplot(aes(x=x, y=y)) + geom_line() + \n+   labs(title=\"AR(1) Process\", x = \"Time\", y = \"Value\")\n&gt; # Plot the autocorrelation function (ACF)\n&gt; autoplot(stats::acf(X, plot = FALSE)) + labs(title = \"Autocorrelation of AR(1) Process\")\n&gt; # Plot the autocorrelation function (ACF)\n&gt; autoplot(stats::pacf(X, plot = FALSE)) + labs(title = \"Partial Autocorrelation of AR(1) Process\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-19",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-19",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\n\nThe characteristic equation for an AR(p) process is derived from the autoregressive parameters \\((\\phi_1, \\phi_2, \\ldots, \\phi_p)\\).\nThe characteristic equation is: \\(1-\\phi_1 z-\\phi_2 z^2-\\cdots-\\phi_p z^p = 0\\)\nA process \\({y_t}\\) is strictly stationary if for each \\(k\\) and \\(t\\), and \\(n\\), the distribution of \\({y_t,\\ldots,y_{t+k}}\\) is the same as the distribution of \\({y_{t+n},\\ldots,y_{t+k+n}}\\)\nFor an AR process to be stationary (its statistical properties do not change over time), the parameters \\(\\phi_1, \\phi_2, \\ldots, \\phi_p\\) must satisfy certain conditions (typically related to the roots of the characteristic equation lying outside the unit circle)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-20",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-20",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\n\nThe AR(p) model is sometimes expressed in terms of the Lag operator \\(\\mathrm{L}\\), where \\(\\mathrm{L}y_t=y_{t-1}\\).\nThe lag operator can be raised to powers, e.g. \\(\\mathrm{L}^2y_t=y_{t-2}\\) and its powers can be combined into polynomials to form a new operator: \\(a(\\mathrm{L})=a_0+a_1\\mathrm{L}+a_2\\mathrm{L}^2+\\ldots+a_p\\mathrm{L}^p\\) such that \\(a(\\mathrm{L})y_t=a_0y_t+a_1y_{t-1}+a_2y_{t-2}+\\ldots+a_py_{t-p}\\)\nLag polymonials can be multiplied and multiplication commutes: \\(a(\\mathrm{L})b(\\mathrm{L})=b(\\mathrm{L})a(\\mathrm{L})\\).\nWe define \\((1-\\rho\\mathrm{L})^{-1}\\) by \\((1-\\rho\\mathrm{L})(1-\\rho\\mathrm{L})^{-1}\\equiv1\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-21",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-21",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\n\nper the formula for the geometric series, if \\(|\\rho|&lt;1, then\\)\n\n\\[\n(1-\\rho\\mathrm{L})^{-1} = \\sum_{i=0}^\\infty\\rho^i\\mathrm{L}^i\n\\]\n\nEXERCISE: check that \\((1-\\rho\\mathrm{L})(1-\\rho\\mathrm{L})^{-1}\\equiv1\\) holds on both sides of the equation above.\nIf the sum is to converge, then we need \\(|\\rho|&lt;1\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-22",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-22",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\nFor the AR(1) process, using the lag operator we can write:\n\\[\n\\begin{align*}\ny_t & = \\phi y_{t-1}+\\epsilon_t\\\\\n(1-\\phi L)y_t & =  \\epsilon_t\\\\\ny_t & = (1-\\phi L)^{-1} \\epsilon_t\\\\\n  & = (\\sum_{i=0}^\\infty\\phi^i\\mathrm{L}^i)\\epsilon_t\\\\\n  & = \\sum_{j=1}^t\\phi^{t-j}\\epsilon_j\n\\end{align*}\n\\] - EXERCISE: check that the last equality holds."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-23",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-23",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\nAR(p) processes can be written as AR(1) vector processes, e.g. for AR(2)\n\\[\n\\left(\\begin{array}{c}\ny_{t}\\\\\ny_{t-1}\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n\\phi_{1} & \\phi_{2}\\\\\n1 & 0\n\\end{array}\\right)\\left(\\begin{array}{c}\ny_{t-1}\\\\\ny_{t-2}\n\\end{array}\\right)+\\left(\\begin{array}{c}\n\\epsilon_{t}\\\\\n0\n\\end{array}\\right)\n\\] where the matrix \\(A\\) here is (i.e. AR(2)):\n\\[\nA=\\left(\\begin{array}{cc}\n\\phi_{1} & \\phi_{2}\\\\\n1 & 0\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-24",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-24",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\nRepeating the argument for AR(1) processes\n\\[\n\\begin{align*}\n\\vec{y}_t & = (1-A L)^{-1} \\vec{\\epsilon}_t\\\\\n  & = (\\sum_{i=0}^\\infty A^i\\mathrm{L}^i)\\vec{\\epsilon}_t\\\\\n\\end{align*}\n\\]\nAnd we only converge if \\(A^i\\rightarrow0 \\;\\mathrm{as}\\;i\\rightarrow\\infty\\), i.e. all eigenvalues are &lt; 1."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-25",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-25",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\nIn general if an \\(n\\times n\\) matrix \\(A\\) has \\(n\\) distinct eigenvalues, then all eigenvalues must have magnitude \\(&lt;1\\) for \\(A^i\\rightarrow0 \\;\\mathrm{as}\\;i\\rightarrow\\infty\\).\nIn this case we have \\(A=X\\Lambda X^{-1}\\) where the columns of \\(x\\) are the eigenvectors of \\(A\\) and \\(\\Lambda\\) is a diagonal matrix with the eigenvalues on the diagonal.\n\\(A^i=X\\Lambda^i X^{-1}\\) so \\(A^i\\rightarrow0 \\;\\mathrm{as}\\;i\\rightarrow\\infty\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-26",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-26",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nComputing determinants\n\n\nMinor:\n\nThe minor \\(M_{ij}\\) of an element \\(a_{ij}\\) in an \\(n \\times n\\) matrix \\(A\\) is the determinant of the \\((n-1) \\times (n-1)\\) matrix that results from removing the \\(i\\)-th row and \\(j\\)-th column from \\(A\\).\n\nCofactor:\n\nThe cofactor \\(C_{ij}\\) of an element \\(a_{ij}\\) is given by: \\(C_{ij} = (-1)^{i+j} M_{ij}\\)\nThe sign factor \\((-1)^{i+j}\\) alternates according to the position of the element in the matrix.\n\n\nCofactor Expansion\nThe determinant of an \\(n \\times n\\) matrix \\(A\\) can be computed by expanding along any row or column. The cofactor expansion along the \\(i\\)-th row is given by:\n\\[\n\\det(A) = \\sum_{j=1}^n a_{ij} C_{ij}\n\\] Similarly, the cofactor expansion along the \\(j\\)-th column is: \\(\\det(A) = \\sum_{i=1}^n a_{ij} C_{ij}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-27",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-27",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nComputing determinants\nThe structure of the AR(1) vector process makes it easy to compute the polynomial for the determinant in terms of the coefficients of the process.\nThe roots of the polynomial can be computed using polyroot, passing in a vector of polynomial coefficients in increasing order. The magnitudes of the eigenvalues can using the base function Mod.\npolyroot(c(1,.2,3,.6)) |&gt; Mod()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-28",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-28",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nMoving Average Processes\n\n\nIn an MA(q) process the present value is a weighted sum on the current and previous errors. \\[y_t= \\epsilon_t +\\theta_1\\epsilon_{t-1};\\;\\text(MA(1))\\]\nMA(q) models describe processes where it takes a bit of time for the error (or “shock”) to dissipate\nThis type of expression may be used to describe a wide variety of stationary time series processes"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-29",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-29",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nMoving Average Processes\n\n\nThere is a duality of AR() and MA() processes.\nWe have already seen how AR processes can be expressed as MA() processes:\n\n\\[\n\\vec{y}_t = (1-A L)^{-1} \\vec{\\epsilon}_t = (\\sum_{i=0}^\\infty A^i\\mathrm{L}^i)\\vec{\\epsilon}_t\\\\\n\\]\nand MA() processes can be expressed as AR processes by a similar argument."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-30",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-30",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nARMA Processes\n\n\nA combination of an AR(1) and a MA(1) process is termed an ARMA(1,1) observation. \\[y_t=\\phi y_{t-1}+\\epsilon_t+\\theta\\epsilon_{t-1}\\]\nAn ARMA(p,q) process takes the form \\[y_t=\\sum_{j=1}^p\\phi_jy_{t-j}+\\epsilon_t+\\sum_{i=1}^q\\theta_i\\epsilon_{t-i}\\]\nThis model was popularized by Box & Jenkins, who developed a methodology that may be used to identify the terms that should be included in the model"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-31",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-31",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nLong memory & fractional differencing\n\n\n\nMost AR(p), MA(q) and ARMA(p,q) processes are termed short-memory process because the coefficients in the representation are dominated by exponential decay\nLong-memory (or persistent) time series are considered intermediate compromises between the short-memory models and integrated nonstationary processes"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-32",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-32",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nARIMA\n\n\nAutoRegressive Integrated Moving Average combines three key concepts: autoregression (AR), differencing (I - for Integrated), and moving average (MA).\nAR (AutoRegressive): captures the relationship between an observation and lagged observations.\nI (Integrated): Differencing is used to make the time series stationary.\nMA (Moving Average): captures the relationship between an observation and a residual error from a moving average model.\nAn ARIMA model is denoted as ARIMA(p, d, q), where: - p is the number of lag observations in the model (AR part), - d is the number of times that the raw observations are differenced (I part), - q is the size of the moving average window (MA part)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing",
    "href": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing",
    "title": "Time series methods",
    "section": "Time series - exponential smoothing",
    "text": "Time series - exponential smoothing\nExponential smoothing or exponential moving average (EMA) is a rule of thumb technique for smoothing time series data using the exponential window function.\nFor a raw data series \\(\\{x_t\\}\\) the output of the exponential smoothing process is \\(\\{y_t\\}\\), where, for \\(0\\le\\alpha\\le1\\) and \\(t&gt;0\\)\n\\[\n\\begin{align*}\ny_0 & = x_0 \\\\\ny_t & = \\alpha x_t + (1-\\alpha)y_{t-1}\n\\end{align*}\n\\]\n\\(\\alpha\\) is called the smoothing factor."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-1",
    "title": "Time series methods",
    "section": "Time series - exponential smoothing",
    "text": "Time series - exponential smoothing\nThe time constant \\(\\tau\\) is the amount of time for the smoothed response of a unit step function to reach \\(1-1/e\\approx 63.2\\%\\) of the original signal.\n\\[\n\\begin{align*}\n\\alpha & = 1-e^{-\\Delta T/\\tau}\\\\\n\\tau & = -\\frac{\\Delta T}{\\log(1-\\alpha)}\n\\end{align*}\n\\]\nwhere \\(\\Delta T\\) is the time interval."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-2",
    "href": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-2",
    "title": "Time series methods",
    "section": "Time series - exponential smoothing",
    "text": "Time series - exponential smoothing\nWe can also include a trend term\n\\[\n\\begin{align*}\ny_0 & = x_0 \\\\\nb_0 & = x_1 - x_0\\\\\n\\end{align*}\n\\]\nand for \\(t&gt;0\\), \\(0\\le\\alpha\\le1\\) and \\(0\\le\\beta\\le1\\)\n\\[\n\\begin{align*}\ny_t & = \\alpha x_t + (1-\\alpha)(y_{t-1} + b_{t-1})\\\\\nb_t & = \\beta(y_t-y_{t-1}) + (1-\\beta)b_{t-1}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-3",
    "href": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-3",
    "title": "Time series methods",
    "section": "Time series - exponential smoothing",
    "text": "Time series - exponential smoothing\nFinally we can also include a seasonality term, with a cycle length of \\(K\\) time intervals.\nand for \\(t&gt;0\\), \\(0\\le\\alpha\\le1\\), \\(0\\le\\beta\\le1\\) and \\(0\\le\\gamma\\le1\\)\n\\[\n\\begin{align*}\ny_t & = \\alpha \\frac{x_t}{c_{t-L}} + (1-\\alpha)(y_{t-1} + b_{t-1})\\\\\nb_t & = \\beta(y_t-y_{t-1}) + (1-\\beta)b_{t-1}\\\\\nc_t & = \\gamma\\frac{x_t}{y_t} + (1-\\gamma)c_{t-L}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---ets-models",
    "href": "slides/BSMM_8740_lec_06.html#time-series---ets-models",
    "title": "Time series methods",
    "section": "Time series - ETS models",
    "text": "Time series - ETS models\n\nETS stands for Error-Trend-Seasonality, and the exponential smoothing model is clearly in this class, with each component additive. The more general taxonomy is:\n\nError: “Additive” (A), or “Multiplicative” (M);\nTrend: “None” (N), “Additive” (A), “Additive damped” (Ad), “Multiplicative” (M), or “Multiplicative damped” (Md);\nSeasonality: “None” (N), or “Additive” (A), or “Multiplicative” (M).\n\nIn this taxonomy, the exponential smoothing model is denoted as ETS(A,A,A)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---ets-models-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---ets-models-1",
    "title": "Time series methods",
    "section": "Time series - ETS models",
    "text": "Time series - ETS models\nThe additive ETS models are shown below, and more detailed discussion can be found here"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---plotting",
    "href": "slides/BSMM_8740_lec_06.html#time-series---plotting",
    "title": "Time series methods",
    "section": "Time series - plotting",
    "text": "Time series - plotting\n\n&gt; timetk::bike_sharing_daily |&gt; \n+   dplyr::slice_head(n=5) |&gt; \n+   dplyr::glimpse()\n\nRows: 5\nColumns: 16\n$ instant    &lt;dbl&gt; 1, 2, 3, 4, 5\n$ dteday     &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-05\n$ season     &lt;dbl&gt; 1, 1, 1, 1, 1\n$ yr         &lt;dbl&gt; 0, 0, 0, 0, 0\n$ mnth       &lt;dbl&gt; 1, 1, 1, 1, 1\n$ holiday    &lt;dbl&gt; 0, 0, 0, 0, 0\n$ weekday    &lt;dbl&gt; 6, 0, 1, 2, 3\n$ workingday &lt;dbl&gt; 0, 0, 1, 1, 1\n$ weathersit &lt;dbl&gt; 2, 2, 1, 1, 1\n$ temp       &lt;dbl&gt; 0.344167, 0.363478, 0.196364, 0.200000, 0.226957\n$ atemp      &lt;dbl&gt; 0.363625, 0.353739, 0.189405, 0.212122, 0.229270\n$ hum        &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957\n$ windspeed  &lt;dbl&gt; 0.160446, 0.248539, 0.248309, 0.160296, 0.186900\n$ casual     &lt;dbl&gt; 331, 131, 120, 108, 82\n$ registered &lt;dbl&gt; 654, 670, 1229, 1454, 1518\n$ cnt        &lt;dbl&gt; 985, 801, 1349, 1562, 1600"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---plotting-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---plotting-1",
    "title": "Time series methods",
    "section": "Time series - plotting",
    "text": "Time series - plotting\nThe timetk::plot_time_series() function is a good way to to get a quick timeseries plot. From a tidy table we\n\nselect the time value and the columns we want to plot\npivot (longer) the columns we want to plot\nplot\n\nThe timetk::plot_time_series() function has many options that can be used to customize the plot."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---plotting-2",
    "href": "slides/BSMM_8740_lec_06.html#time-series---plotting-2",
    "title": "Time series methods",
    "section": "Time series - plotting",
    "text": "Time series - plotting\n\n\nCode\n&gt; timetk::bike_sharing_daily |&gt; \n+   dplyr::select(dteday, casual, registered) |&gt; \n+   tidyr::pivot_longer(-dteday) |&gt; \n+   timetk::plot_time_series(\n+     .date_var = dteday\n+     , .value = value\n+     , .color_var = name\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime downscaling\n\n\ntime-downscale the bike sharing data\n&gt; timetk::bike_sharing_daily |&gt; \n+   timetk::summarise_by_time(\n+     .date_var = dteday\n+     , .by = \"week\"\n+     , .week_start = 7\n+     , causal = sum(casual)\n+     , registered = mean(registered)\n+     , max_cnt = max(cnt)\n+   )\n\n\n# A tibble: 106 × 4\n   dteday     causal registered max_cnt\n   &lt;date&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 2010-12-26    331       654      985\n 2 2011-01-02    745      1235.    1606\n 3 2011-01-09    477      1167.    1421\n 4 2011-01-16    706      1183.    1927\n 5 2011-01-23    632       994.    1985\n 6 2011-01-30    550      1314.    1708\n 7 2011-02-06   1075      1450.    1746\n 8 2011-02-13   2333      1734.    2927\n 9 2011-02-20   1691      1405.    1969\n10 2011-02-27   2120      1631.    2402\n# ℹ 96 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-1",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime upscaling\n\n\ntime-upscale the bike sharing data\n&gt; timetk::bike_sharing_daily |&gt; \n+   dplyr::select(dteday, casual) |&gt; \n+   timetk::pad_by_time(.date_var = dteday, .by = \"hour\") |&gt; \n+   timetk::mutate_by_time(\n+     .date_var = dteday\n+     , .by = \"day\"\n+     , casual = sum(casual,na.rm=T)/24\n+   )\n\n\n# A tibble: 17,521 × 2\n   dteday              casual\n   &lt;dttm&gt;               &lt;dbl&gt;\n 1 2011-01-01 00:00:00   13.8\n 2 2011-01-01 01:00:00   13.8\n 3 2011-01-01 02:00:00   13.8\n 4 2011-01-01 03:00:00   13.8\n 5 2011-01-01 04:00:00   13.8\n 6 2011-01-01 05:00:00   13.8\n 7 2011-01-01 06:00:00   13.8\n 8 2011-01-01 07:00:00   13.8\n 9 2011-01-01 08:00:00   13.8\n10 2011-01-01 09:00:00   13.8\n# ℹ 17,511 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-2",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-2",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime filtering\n\n\nfilter the bike sharing data by date range\n&gt; timetk::bike_sharing_daily |&gt;\n+   timetk::filter_by_time(\n+     .date_var = dteday\n+     , .start_date=\"2012-01-15\"\n+     , .end_date = \"2012-07-01\"\n+   ) |&gt; \n+   timetk::plot_time_series(.date_var = dteday, casual)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-3",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-3",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime offsets\n\n\nfilter the bike sharing data by offset\n&gt; require(timetk, quietly = FALSE)\n\n\nLoading required package: timetk\n\n\nfilter the bike sharing data by offset\n&gt; timetk::bike_sharing_daily |&gt;\n+   timetk::filter_by_time(\n+     .date_var = dteday\n+     , .start_date=\"2012-01-15\"\n+     , .end_date = \"2012-01-15\" %+time% \"12 weeks\"\n+   ) |&gt; \n+   timetk::plot_time_series(.date_var = dteday, casual)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-4",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-4",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\nmutate by period\n\n\nadd columns using a period (rolling window)\n&gt; timetk::bike_sharing_daily |&gt;\n+   dplyr::select(dteday, casual) |&gt; \n+   timetk::mutate_by_time(\n+     .date_var = dteday\n+     , .by = \"7 days\"\n+     , casual_mean = mean(casual)\n+     , casual_median = median(casual)\n+     , casual_max = max(casual)\n+     , casual_min = min(casual)\n+   )\n\n\n# A tibble: 731 × 6\n   dteday     casual casual_mean casual_median casual_max casual_min\n   &lt;date&gt;      &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 2011-01-01    331       144             120        331         82\n 2 2011-01-02    131       144             120        331         82\n 3 2011-01-03    120       144             120        331         82\n 4 2011-01-04    108       144             120        331         82\n 5 2011-01-05     82       144             120        331         82\n 6 2011-01-06     88       144             120        331         82\n 7 2011-01-07    148       144             120        331         82\n 8 2011-01-08     68        46.1            43         68         25\n 9 2011-01-09     54        46.1            43         68         25\n10 2011-01-10     41        46.1            43         68         25\n# ℹ 721 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-5",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-5",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\nsummarize by period\n\n\nadd columns that summarize a period (rolling window)\n&gt; timetk::bike_sharing_daily |&gt;\n+   timetk::summarize_by_time(\n+     .date_var = dteday\n+     , .by = \"7 days\"\n+     , casual_mean = mean(casual)\n+     , registered_mean = mean(registered)\n+     , windspeed_max = max(windspeed)\n+   )\n\n\n# A tibble: 119 × 4\n   dteday     casual_mean registered_mean windspeed_max\n   &lt;date&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1 2011-01-01       144             1201.         0.249\n 2 2011-01-08        46.1           1147.         0.362\n 3 2011-01-15       119.            1203.         0.353\n 4 2011-01-22        86              981.         0.294\n 5 2011-01-29       102.            1130          0.187\n 6 2011-02-01       120.            1377.         0.278\n 7 2011-02-08       172.            1455.         0.418\n 8 2011-02-15       366             1618.         0.507\n 9 2011-02-22       233.            1546.         0.347\n10 2011-03-01       243.            1495          0.343\n# ℹ 109 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-6",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-6",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ncreate a timeseries\n\n\ncreate a timeseries\n&gt; tibble::tibble(\n+   date = \n+     timetk::tk_make_timeseries(\n+       start_date = \"2024\"\n+       , length_out = 100\n+       , by = \"month\"\n+     )\n+   , values=1:100\n+ )\n\n\n# A tibble: 100 × 2\n   date       values\n   &lt;date&gt;      &lt;int&gt;\n 1 2024-01-01      1\n 2 2024-02-01      2\n 3 2024-03-01      3\n 4 2024-04-01      4\n 5 2024-05-01      5\n 6 2024-06-01      6\n 7 2024-07-01      7\n 8 2024-08-01      8\n 9 2024-09-01      9\n10 2024-10-01     10\n# ℹ 90 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-7",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-7",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ncreate a timeseries\n\n\nadd columns for holidays\n&gt; timetk::tk_make_holiday_sequence(\n+   start_date = \"2024\"\n+   , end_date = \"2026\"\n+   , calendar = \"TSX\"\n+ ) %&gt;% \n+   timetk::tk_get_holiday_signature(holiday_pattern = \"Thanksgiving\",locale_set = \"CA\", exchange = \"TSX\") |&gt; \n+   dplyr::slice_head(n = 6) |&gt; \n+   dplyr::glimpse()\n\n\nRows: 6\nColumns: 6\n$ index              &lt;date&gt; 2024-01-01, 2024-02-19, 2024-02-19, 2024-02-19, 2024-03-29, 2024-…\n$ exch_TSX           &lt;dbl&gt; 1, 1, 1, 1, 1, 1\n$ locale_CA          &lt;dbl&gt; 0, 1, 1, 1, 0, 1\n$ CA_ThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0\n$ JP_ThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0\n$ US_ThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-8",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-8",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\n\nrawbox-cox transformed\n\n\n\n\nplot raw windspeed data\n&gt; # plot wind speed\n&gt; timetk::bike_sharing_daily |&gt; \n+   timetk::plot_time_series(dteday, windspeed, .title = \"Time Series - Raw\")\n\n\n\n\n\n\n\n\n\n\nplot transformed windspeed data\n&gt; # plot transformed speed\n&gt; timetk::bike_sharing_daily |&gt; \n+   timetk::plot_time_series(\n+     dteday\n+     , timetk::box_cox_vec(windspeed, lambda=\"auto\",  silent = T)\n+     , .title = \"Time Series - Box Cox Tranformed\")\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nRegistered S3 methods overwritten by 'forecast':\n  method                 from     \n  autoplot.Arima         ggfortify\n  autoplot.acf           ggfortify\n  autoplot.ar            ggfortify\n  autoplot.bats          ggfortify\n  autoplot.decomposed.ts ggfortify\n  autoplot.ets           ggfortify\n  autoplot.forecast      ggfortify\n  autoplot.stl           ggfortify\n  autoplot.ts            ggfortify\n  fitted.ar              ggfortify\n  fortify.ts             ggfortify\n  residuals.ar           ggfortify"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-9",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-9",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntimeseries transformations\nSee Also\n\n\nLag Transformation: lag_vec()\nDifferencing Transformation: diff_vec()\nRolling Window Transformation: slidify_vec()\nLoess Smoothing Transformation: smooth_vec()\nFourier Series: fourier_vec()\nMissing Value Imputation for Time Series: ts_impute_vec(), ts_clean_vec()\n\nOther common transformations to reduce variance: log(), log1p() and sqrt()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---feature-engineering",
    "href": "slides/BSMM_8740_lec_06.html#time-series---feature-engineering",
    "title": "Time series methods",
    "section": "Time series - Feature engineering",
    "text": "Time series - Feature engineering\n\nfeature engineeringplot\n\n\n\n\nfeature engineering\n&gt; subscribers_tbl   &lt;- readRDS(\"data/00_data/mailchimp_users.rds\")\n&gt; \n&gt; data_prepared_tbl &lt;- subscribers_tbl |&gt; \n+   timetk::summarize_by_time(optin_time, .by=\"day\", optins=dplyr::n()) |&gt; \n+   timetk::pad_by_time(.pad_value=0) |&gt; \n+   # preprocessing\n+   dplyr::mutate(optins_trans=timetk::log_interval_vec(optins, limit_lower=0, offset=1)) |&gt; \n+   dplyr::mutate(optins_trans=timetk::standardize_vec(optins_trans)) |&gt; \n+   # fix missing vals at start\n+   timetk::filter_by_time(.start_date = \"2018-07-03\") |&gt; \n+   # outliers clean\n+   dplyr::mutate(optins_trans_cleaned = timetk::ts_clean_vec(optins_trans, period=7)) |&gt; \n+   dplyr::mutate(optins_trans=ifelse(optin_time |&gt; timetk::between_time(\"2018-11-18\",\"2018-11-20\")\n+                              , optins_trans_cleaned\n+                              , optins_trans\n+                              )) |&gt; \n+   dplyr::select(-optins, -optins_trans_cleaned)\n&gt; # show the dt  \n&gt; data_prepared_tbl     \n\n\n# A tibble: 609 × 2\n   optin_time optins_trans\n   &lt;date&gt;            &lt;dbl&gt;\n 1 2018-07-03      -0.492 \n 2 2018-07-04      -0.153 \n 3 2018-07-05      -0.578 \n 4 2018-07-06      -0.413 \n 5 2018-07-07      -1.20  \n 6 2018-07-08      -1.66  \n 7 2018-07-09      -0.274 \n 8 2018-07-10      -0.212 \n 9 2018-07-11      -0.0986\n10 2018-07-12      -0.274 \n# ℹ 599 more rows\n\n\n\n\n\n\nplot the new data\n&gt; data_prepared_tbl |&gt; # plot the table\n+   tidyr::pivot_longer(contains(\"trans\")) |&gt; \n+   timetk::plot_time_series(optin_time,value,name)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#date-features",
    "href": "slides/BSMM_8740_lec_06.html#date-features",
    "title": "Time series methods",
    "section": "Date Features",
    "text": "Date Features\n\n\nadd date features\n&gt; data_prep_signature_tbl &lt;- \n+   data_prepared_tbl |&gt; \n+   timetk::tk_augment_timeseries_signature(\n+     .date_var = optin_time\n+   ) \n\n\n\n\nRows: 4\nColumns: 30\n$ optin_time   &lt;date&gt; 2018-07-03, 2018-07-04, 2018-07-05, 2018-07-06\n$ optins_trans &lt;dbl&gt; -0.4919060, -0.1534053, -0.5779424, -0.4133393\n$ index.num    &lt;dbl&gt; 1530576000, 1530662400, 1530748800, 1530835200\n$ diff         &lt;dbl&gt; NA, 86400, 86400, 86400\n$ year         &lt;int&gt; 2018, 2018, 2018, 2018\n$ year.iso     &lt;int&gt; 2018, 2018, 2018, 2018\n$ half         &lt;int&gt; 2, 2, 2, 2\n$ quarter      &lt;int&gt; 3, 3, 3, 3\n$ month        &lt;int&gt; 7, 7, 7, 7\n$ month.xts    &lt;int&gt; 6, 6, 6, 6\n$ month.lbl    &lt;ord&gt; July, July, July, July\n$ day          &lt;int&gt; 3, 4, 5, 6\n$ hour         &lt;int&gt; 0, 0, 0, 0\n$ minute       &lt;int&gt; 0, 0, 0, 0\n$ second       &lt;int&gt; 0, 0, 0, 0\n$ hour12       &lt;int&gt; 0, 0, 0, 0\n$ am.pm        &lt;int&gt; 1, 1, 1, 1\n$ wday         &lt;int&gt; 3, 4, 5, 6\n$ wday.xts     &lt;int&gt; 2, 3, 4, 5\n$ wday.lbl     &lt;ord&gt; Tuesday, Wednesday, Thursday, Friday\n$ mday         &lt;int&gt; 3, 4, 5, 6\n$ qday         &lt;int&gt; 3, 4, 5, 6\n$ yday         &lt;int&gt; 184, 185, 186, 187\n$ mweek        &lt;int&gt; 1, 1, 1, 1\n$ week         &lt;int&gt; 27, 27, 27, 27\n$ week.iso     &lt;int&gt; 27, 27, 27, 27\n$ week2        &lt;int&gt; 1, 1, 1, 1\n$ week3        &lt;int&gt; 0, 0, 0, 0\n$ week4        &lt;int&gt; 3, 3, 3, 3\n$ mday7        &lt;int&gt; 1, 1, 1, 1"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#trend",
    "href": "slides/BSMM_8740_lec_06.html#trend",
    "title": "Time series methods",
    "section": "Trend",
    "text": "Trend\n\n\nadd regression\n&gt; data_prep_signature_tbl |&gt; \n+   timetk::plot_time_series_regression(\n+     .date_var = optin_time\n+     , .formula = optins_trans ~ index.num\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#non-linear-trends",
    "href": "slides/BSMM_8740_lec_06.html#non-linear-trends",
    "title": "Time series methods",
    "section": "Non-linear trends",
    "text": "Non-linear trends\n\nB-spline, degree 3Cubic spline, 3 knots\n\n\n\n\nCode\n&gt; data_prep_signature_tbl |&gt; \n+   timetk::plot_time_series_regression(\n+     .date_var = optin_time\n+     , .formula = optins_trans ~ splines::bs(index.num, degree=3)\n+     , .show_summary = FALSE\n+     , .title = \"B-spline, degree 3\"\n+   )\n\n\n\n\n\n\n\n\n\n\nCode\n&gt; data_prep_signature_tbl |&gt; \n+   timetk::plot_time_series_regression(\n+     .date_var=optin_time\n+     , .formula=\n+       optins_trans ~ splines::ns(\n+         index.num\n+         , knots=quantile(index.num, probs=c(0.25, 0.5, 0.75)))\n+     , .show_summary = FALSE\n+     , .title = \"Cubic spline, 3 knots\"\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#seasonality",
    "href": "slides/BSMM_8740_lec_06.html#seasonality",
    "title": "Time series methods",
    "section": "Seasonality",
    "text": "Seasonality\n\nweeklymonthly\n\n\n\n\nCode\n&gt; # Weekly Seasonality\n&gt; data_prep_signature_tbl |&gt; \n+   timetk::plot_time_series_regression(\n+     .date_var=optin_time\n+     , .formula=optins_trans ~ wday.lbl + splines::bs(index.num, degree=3)\n+     , .show_summary = FALSE\n+     , .title = \"Weekday seasonality\"\n+   )\n\n\n\n\n\n\n\n\n\n\nCode\n&gt; data_prep_signature_tbl |&gt; \n+   timetk::plot_time_series_regression(\n+     .date_var=optin_time\n+     , .formula=optins_trans ~ month.lbl + splines::bs(index.num, degree=3)\n+     , .show_summary = FALSE\n+     , .title = \"Monthly seasonality\"\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#seasonality-1",
    "href": "slides/BSMM_8740_lec_06.html#seasonality-1",
    "title": "Time series methods",
    "section": "Seasonality",
    "text": "Seasonality\n\n\nregress with a formula\n&gt; # ** Together with Trend\n&gt; model_formula_seasonality &lt;- as.formula(\n+   optins_trans ~ wday.lbl + month.lbl +\n+     splines::ns(index.num\n+                 , knots=quantile(index.num, probs=c(0.25, 0.5, 0.75))) + .\n+ )\n&gt; data_prep_signature_tbl |&gt; \n+   timetk::plot_time_series_regression(\n+     .date_var=optin_time\n+     , .formula = model_formula_seasonality\n+     , .show_summary = FALSE\n+     , .title = \"Day and Month seasonality + Cubic spline, 3 knots\"\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#fourier-series",
    "href": "slides/BSMM_8740_lec_06.html#fourier-series",
    "title": "Time series methods",
    "section": "Fourier series",
    "text": "Fourier series\n\n\nshow autocorrelations\n&gt; data_prep_signature_tbl |&gt; \n+   timetk::plot_acf_diagnostics(optin_time,optins_trans)\n\n\nMax lag exceeds data available. Using max lag: 608"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#fourier-series-1",
    "href": "slides/BSMM_8740_lec_06.html#fourier-series-1",
    "title": "Time series methods",
    "section": "Fourier series",
    "text": "Fourier series\n\n\nadd seasonality using sin and cos\n&gt; data_prep_fourier_tbl &lt;- \n+   data_prep_signature_tbl |&gt; \n+   timetk::tk_augment_fourier(optin_time, .periods=c(7,14,30,90,365), .K=2)\n&gt; \n&gt; data_prep_fourier_tbl |&gt; dplyr::slice_head(n=3) |&gt; dplyr::glimpse()\n\n\nRows: 3\nColumns: 50\n$ optin_time           &lt;date&gt; 2018-07-03, 2018-07-04, 2018-07-05\n$ optins_trans         &lt;dbl&gt; -0.4919060, -0.1534053, -0.5779424\n$ index.num            &lt;dbl&gt; 1530576000, 1530662400, 1530748800\n$ diff                 &lt;dbl&gt; NA, 86400, 86400\n$ year                 &lt;int&gt; 2018, 2018, 2018\n$ year.iso             &lt;int&gt; 2018, 2018, 2018\n$ half                 &lt;int&gt; 2, 2, 2\n$ quarter              &lt;int&gt; 3, 3, 3\n$ month                &lt;int&gt; 7, 7, 7\n$ month.xts            &lt;int&gt; 6, 6, 6\n$ month.lbl            &lt;ord&gt; July, July, July\n$ day                  &lt;int&gt; 3, 4, 5\n$ hour                 &lt;int&gt; 0, 0, 0\n$ minute               &lt;int&gt; 0, 0, 0\n$ second               &lt;int&gt; 0, 0, 0\n$ hour12               &lt;int&gt; 0, 0, 0\n$ am.pm                &lt;int&gt; 1, 1, 1\n$ wday                 &lt;int&gt; 3, 4, 5\n$ wday.xts             &lt;int&gt; 2, 3, 4\n$ wday.lbl             &lt;ord&gt; Tuesday, Wednesday, Thursday\n$ mday                 &lt;int&gt; 3, 4, 5\n$ qday                 &lt;int&gt; 3, 4, 5\n$ yday                 &lt;int&gt; 184, 185, 186\n$ mweek                &lt;int&gt; 1, 1, 1\n$ week                 &lt;int&gt; 27, 27, 27\n$ week.iso             &lt;int&gt; 27, 27, 27\n$ week2                &lt;int&gt; 1, 1, 1\n$ week3                &lt;int&gt; 0, 0, 0\n$ week4                &lt;int&gt; 3, 3, 3\n$ mday7                &lt;int&gt; 1, 1, 1\n$ optin_time_sin7_K1   &lt;dbl&gt; -9.749279e-01, -7.818315e-01, 2.256296e-13\n$ optin_time_cos7_K1   &lt;dbl&gt; -0.2225209, 0.6234898, 1.0000000\n$ optin_time_sin7_K2   &lt;dbl&gt; 4.338837e-01, -9.749279e-01, 4.512593e-13\n$ optin_time_cos7_K2   &lt;dbl&gt; -0.9009689, -0.2225209, 1.0000000\n$ optin_time_sin14_K1  &lt;dbl&gt; 7.818315e-01, 4.338837e-01, -1.128148e-13\n$ optin_time_cos14_K1  &lt;dbl&gt; -0.6234898, -0.9009689, -1.0000000\n$ optin_time_sin14_K2  &lt;dbl&gt; -9.749279e-01, -7.818315e-01, 2.256296e-13\n$ optin_time_cos14_K2  &lt;dbl&gt; -0.2225209, 0.6234898, 1.0000000\n$ optin_time_sin30_K1  &lt;dbl&gt; 1.126564e-13, -2.079117e-01, -4.067366e-01\n$ optin_time_cos30_K1  &lt;dbl&gt; -1.0000000, -0.9781476, -0.9135455\n$ optin_time_sin30_K2  &lt;dbl&gt; -2.253127e-13, 4.067366e-01, 7.431448e-01\n$ optin_time_cos30_K2  &lt;dbl&gt; 1.0000000, 0.9135455, 0.6691306\n$ optin_time_sin90_K1  &lt;dbl&gt; -0.8660254, -0.8290376, -0.7880108\n$ optin_time_cos90_K1  &lt;dbl&gt; 0.5000000, 0.5591929, 0.6156615\n$ optin_time_sin90_K2  &lt;dbl&gt; -0.8660254, -0.9271839, -0.9702957\n$ optin_time_cos90_K2  &lt;dbl&gt; -0.5000000, -0.3746066, -0.2419219\n$ optin_time_sin365_K1 &lt;dbl&gt; -0.2135209, -0.2303057, -0.2470222\n$ optin_time_cos365_K1 &lt;dbl&gt; -0.9769385, -0.9731183, -0.9690098\n$ optin_time_sin365_K2 &lt;dbl&gt; 0.4171936, 0.4482293, 0.4787338\n$ optin_time_cos365_K2 &lt;dbl&gt; 0.9088176, 0.8939186, 0.8779601"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#visualization",
    "href": "slides/BSMM_8740_lec_06.html#visualization",
    "title": "Time series methods",
    "section": "Visualization",
    "text": "Visualization\n\n\nregress with a formula again\n&gt; # Model\n&gt; model_formula_fourier &lt;- \n+   as.formula(\n+     optins_trans ~ . +\n+       splines::ns(index.num\n+                   , knots=quantile(index.num, probs=c(0.25, 0.5, 0.75)))\n+   )\n&gt; \n&gt; # Visualize\n&gt; data_prep_fourier_tbl |&gt; \n+   timetk::filter_by_time(.start_date=\"2018-09-13\") |&gt; \n+   timetk::plot_time_series_regression(\n+     .date_var = optin_time\n+     , .formula = model_formula_fourier\n+     , .show_summary = FALSE\n+     , .title = \"Fourier + Cubic spline, 3 knots\"\n+   )\n\n\n.date_var is missing. Using: optin_time"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#test-train-splits",
    "href": "slides/BSMM_8740_lec_06.html#test-train-splits",
    "title": "Time series methods",
    "section": "Test-train splits",
    "text": "Test-train splits\n\n\ntest/train splits in timeseries\n&gt; dat &lt;- subscribers_tbl |&gt; \n+   timetk::summarize_by_time(optin_time, .by=\"day\", optins=dplyr::n()) |&gt; \n+   timetk::pad_by_time(.pad_value=0) |&gt; \n+   timetk::filter_by_time(.start_date = \"2018-12-15\")\n&gt; \n&gt; # Split Data 80/20\n&gt; splits &lt;- \n+   timetk::time_series_split(\n+     data = dat\n+     , initial = \"12 months\"\n+     , assess = \"1 months\"\n+   )\n&gt; \n&gt; splits |&gt;\n+   timetk::tk_time_series_cv_plan() |&gt;\n+   timetk::plot_time_series_cv_plan(.date_var = optin_time, .value = optins)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#feature-engineering-w-recipes",
    "href": "slides/BSMM_8740_lec_06.html#feature-engineering-w-recipes",
    "title": "Time series methods",
    "section": "Feature engineering w/ recipes",
    "text": "Feature engineering w/ recipes\n\n\nusing recipes to engineer features\n&gt; time_rec &lt;- dat |&gt; \n+   recipes::recipe(optins ~ ., data = rsample::training(splits)) |&gt; \n+   timetk::step_log_interval(optins, limit_lower = 0, offset = 1) |&gt; \n+   recipes::step_normalize(recipes::all_outcomes()) |&gt; \n+   timetk::step_timeseries_signature(optin_time) |&gt; \n+   timetk::step_fourier(optin_time, period = c(7,14,30,90,365), K=2)\n&gt; \n&gt; time_rec |&gt; recipes::prep(training = rsample::training(splits)) |&gt; \n+   recipes::bake(new_data = NULL) |&gt; \n+   timetk::plot_time_series_regression(\n+     .date_var = optin_time\n+     , .formula = optins ~ .\n+     , .show_summary = FALSE\n+   )\n\n\n$optins\n[1] 0\n\n$optins\n[1] 400"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#workflows",
    "href": "slides/BSMM_8740_lec_06.html#workflows",
    "title": "Time series methods",
    "section": "Workflows",
    "text": "Workflows\n\n&gt; # process engineering with workflows: ARIMA model\n&gt; model_spec_arima &lt;- modeltime::arima_reg() |&gt;\n+     parsnip::set_engine(\"auto_arima\")\n&gt; \n&gt; recipe_spec_fourier &lt;- \n+   recipes::recipe(\n+     optins ~ optin_time\n+     , data = rsample::training(splits)\n+   ) |&gt;\n+     timetk::step_fourier(optin_time, period = c(7, 14, 30, 90), K = 1) \n&gt; \n&gt; workflow_fit_arima &lt;- workflows::workflow() |&gt;\n+   workflows::add_recipe(recipe_spec_fourier) |&gt;\n+   workflows::add_model(model_spec_arima) |&gt;\n+   parsnip::fit(rsample::training(splits))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#workflows-1",
    "href": "slides/BSMM_8740_lec_06.html#workflows-1",
    "title": "Time series methods",
    "section": "Workflows",
    "text": "Workflows\n\n&gt; # process engineering with workflows: linear model\n&gt; model_spec_lm &lt;- parsnip::linear_reg() |&gt;\n+   parsnip::set_engine(\"lm\") \n&gt; \n&gt; recipe_spec_linear &lt;- \n+   recipes::recipe(\n+     optins ~ optin_time\n+     , data = rsample::training(splits)\n+   ) |&gt;\n+     timetk::step_fourier(optin_time, period = c(7, 14, 30, 90), K = 1) \n&gt; \n&gt; workflow_fit_linear &lt;- workflows::workflow() |&gt;\n+   workflows::add_recipe(recipe_spec_linear) |&gt;\n+   workflows::add_model(model_spec_lm) |&gt;\n+   parsnip::fit(rsample::training(splits))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#predict",
    "href": "slides/BSMM_8740_lec_06.html#predict",
    "title": "Time series methods",
    "section": "Predict",
    "text": "Predict\n\n\nforecasting with different models\n&gt; models_tbl &lt;- \n+   modeltime::modeltime_table(workflow_fit_arima, workflow_fit_linear)\n&gt; \n&gt; calibration_tbl &lt;- models_tbl |&gt;\n+   modeltime::modeltime_calibrate(new_data = rsample::testing(splits))\n&gt; \n&gt; calibration_tbl |&gt;\n+   modeltime::modeltime_forecast(\n+     new_data    = rsample::testing(splits),\n+     actual_data = dat\n+   ) |&gt;\n+   modeltime::plot_modeltime_forecast(\n+     .legend_max_width = 25, # For mobile screens\n+     .interactive      = TRUE\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#evaluate",
    "href": "slides/BSMM_8740_lec_06.html#evaluate",
    "title": "Time series methods",
    "section": "Evaluate",
    "text": "Evaluate\n\n\nevaluate model accuracy with test data\n&gt; calibration_tbl |&gt;\n+   modeltime::modeltime_accuracy() |&gt;\n+   modeltime::table_modeltime_accuracy(\n+     .interactive = FALSE\n+   )\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nREGRESSION WITH ARIMA(2,0,1)(2,0,0)[7] ERRORS\nTest\n60.82\n65.18\n0.6\n72.87\n124.54\n0.08\n\n\n2\nLM\nTest\n60.80\n65.60\n0.6\n68.92\n126.34\n0.02"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#re-fit",
    "href": "slides/BSMM_8740_lec_06.html#re-fit",
    "title": "Time series methods",
    "section": "Re-fit",
    "text": "Re-fit\n\n\nforecast out of sample\n&gt; refit_tbl &lt;- calibration_tbl |&gt;\n+   # use all the data\n+   modeltime::modeltime_refit(data = dat)\n&gt; \n&gt; refit_tbl |&gt;\n+   modeltime::modeltime_forecast(h = \"3 months\", actual_data = dat) |&gt;\n+   modeltime::plot_modeltime_forecast(\n+     .legend_max_width = 12, # For mobile screens\n+     .interactive      = TRUE\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#recap",
    "href": "slides/BSMM_8740_lec_06.html#recap",
    "title": "Time series methods",
    "section": "Recap",
    "text": "Recap\n\nIn this section we have worked with the tidymodels and timetk packages to build a workflow that facilitates building and evaluating multiple models.\nCombined with the recipes package we now have a complete data modeling framework for timeseries."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_05.html#recap-of-last-week",
    "title": "Classification & clustering methods",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast time we introduced the Tidymodels framework in R\nWe showed how we can use the Tidymodels framework to create a workflow for data prep, feature engineering, model fitting and model evaluation.\nToday we look at the using the Tidymodels package to build classification and clustering models."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#classification",
    "href": "slides/BSMM_8740_lec_05.html#classification",
    "title": "Classification & clustering methods",
    "section": "Classification",
    "text": "Classification\n\nClassification is a supervised machine learning method where the model tries to predict a categorical outcome for given input data.\nThese models are essential in various business applications, such as credit scoring, customer segmentation, fraud detection, and more. Classification methods can broadly be categorized into two types: eager learners and lazy (instance-based) learners."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#eager-learners",
    "href": "slides/BSMM_8740_lec_05.html#eager-learners",
    "title": "Classification & clustering methods",
    "section": "Eager Learners",
    "text": "Eager Learners\nEager learners are machine learning algorithms that first build a model from the training dataset before making any prediction on future datasets. They spend more time on the training process to better generalize from the data.\nThey usually require less time to make predictions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#eager-learners-1",
    "href": "slides/BSMM_8740_lec_05.html#eager-learners-1",
    "title": "Classification & clustering methods",
    "section": "Eager Learners",
    "text": "Eager Learners\nExample eager learners are:\n\nLRDTRFSVMANNGMB\n\n\n\nLogistic Regression:\n\nOverview: A statistical method for binary classification that models the probability of a binary outcome based on one or more predictor variables.\nAdvantages: Simple, interpretable, and works well with linear decision boundaries.\nDisadvantages: Assumes linearity between predictors and the log-odds of the outcome.\n\n\n\n\n\nDecision Trees:\n\nOverview: A model that splits the data into subsets based on feature values, resulting in a tree structure where each leaf node represents a class label.\nAdvantages: Easy to interpret and visualize, handles both numerical and categorical data.\nDisadvantages: Prone to overfitting, especially with deep trees.\n\n\n\n\n\nRandom Forests:\n\nOverview: An ensemble method that builds multiple decision trees and aggregates their predictions to improve accuracy and reduce overfitting.\nAdvantages: Robust to overfitting, handles large datasets well.\nDisadvantages: Less interpretable than single decision trees.\n\n\n\n\n\nSupport Vector Machines (SVM):\n\nOverview: A model that finds the hyperplane that best separates the classes in the feature space.\nAdvantages: Effective in high-dimensional spaces, works well with a clear margin of separation.\nDisadvantages: Computationally intensive, less effective with noisy data or overlapping classes.\n\n\n\n\n\nNeural Networks:\n\nOverview: Models inspired by the human brain, consisting of layers of interconnected neurons that learn to map inputs to outputs.\nAdvantages: Capable of capturing complex patterns and relationships.\nDisadvantages: Requires large amounts of data and computational power, less interpretable.\n\n\n\n\n\nGradient Boosting Machines (GBM):\n\nOverview: An ensemble technique that builds trees sequentially, where each new tree corrects errors made by the previous ones.\nAdvantages: High accuracy, effective at handling various data types.\nDisadvantages: Computationally intensive, requires careful tuning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#lazy-learners",
    "href": "slides/BSMM_8740_lec_05.html#lazy-learners",
    "title": "Classification & clustering methods",
    "section": "Lazy Learners",
    "text": "Lazy Learners\nLazy learners or instance-based learners, do not create any model immediately from the training data, and this where the lazy aspect comes from. They just memorize the training data, and each time there is a need to make a prediction, they predict based on similarity between the query instance and stored instances."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#lazy-learners-1",
    "href": "slides/BSMM_8740_lec_05.html#lazy-learners-1",
    "title": "Classification & clustering methods",
    "section": "Lazy Learners",
    "text": "Lazy Learners\nExample lazy learners are:\n\nKNNCase-based\n\n\n\nk-Nearest Neighbors (k-NN):\n\nOverview: Predicts the class of a query instance based on the majority class among its k nearest neighbors in the training data.\nAdvantages: Simple, intuitive, and effective for small datasets.\nDisadvantages: Computationally intensive during prediction, performance degrades with high-dimensional data.\n\n\n\n\n\nCase-based Learning:\n\nOverview: Makes predictions by fitting simple models (like linear regression) to localized regions of the data.\nAdvantages: Flexible, can adapt to complex patterns locally.\nDisadvantages: Computationally expensive, sensitive to the choice of kernel and bandwidth parameters."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#comparing-eager-and-lazy-learners",
    "href": "slides/BSMM_8740_lec_05.html#comparing-eager-and-lazy-learners",
    "title": "Classification & clustering methods",
    "section": "Comparing Eager and Lazy Learners",
    "text": "Comparing Eager and Lazy Learners\n\n\nTraining vs. Prediction Time:\n\nEager learners invest time in building the model during training, resulting in faster predictions.\nLazy learners have negligible training time but are computationally intensive during prediction.\n\nModel Interpretability:\n\nEager learners like decision trees and logistic regression are generally more interpretable.\nLazy learners like k-NN are less interpretable as they rely on instance-based comparisons."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#comparing-eager-and-lazy-learners-1",
    "href": "slides/BSMM_8740_lec_05.html#comparing-eager-and-lazy-learners-1",
    "title": "Classification & clustering methods",
    "section": "Comparing Eager and Lazy Learners",
    "text": "Comparing Eager and Lazy Learners\n\n\nHandling of High-Dimensional Data:\n\nEager learners like SVMs and neural networks can handle high-dimensional data effectively.\nLazy learners like k-NN can struggle with the curse of dimensionality.\n\nFlexibility and Complexity:\n\nEager learners can capture complex relationships and interactions through models like neural networks and gradient boosting.\nLazy learners are simpler but can be flexible in capturing local patterns."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#types-of-classification",
    "href": "slides/BSMM_8740_lec_05.html#types-of-classification",
    "title": "Classification & clustering methods",
    "section": "Types of classification",
    "text": "Types of classification\n\nBinary classification\nMulti-Class Classification (mutually exclusive)\n\nmulticlass\n\nMulti-Label Classification (not mutually exclusive)\n\nmultilabel\n\nImbalanced Classification\n\nclass imbalance"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-logistic-regression",
    "href": "slides/BSMM_8740_lec_05.html#binary-logistic-regression",
    "title": "Classification & clustering methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\nLogistic regression is a Generalized Linear Model where the dependent (categorical) variable \\(y\\) is binary, i.e. takes values in \\(\\{0,1\\}\\) (e.g., yes/no, success/failure).\nThis can be interpreted as identifying two classes, and logistic regression provides a prediction for class membership based on a linear combination of the explanatory variables.\nLogistic regression is an example of supervised learning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-1",
    "href": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-1",
    "title": "Classification & clustering methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\nFor the logistic GLM:\n\nthe distribution of the observations is Binomial with parameter \\(\\pi\\equiv\\mathbb{P}(\\left.Y=1\\right|\\eta)\\)\nthe explanatory variables are linear in the parameters: \\(\\eta=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_2 x_2\\ldots+\\beta_n x_n\\)\nthe link function is the logit: \\(\\eta=\\text{logit}(\\pi) = \\log(\\frac{\\pi}{1-\\pi})\\)\n\nIt follows that \\(\\pi = \\frac{e^\\eta}{1+e^\\eta} = \\frac{1}{1+e^{-\\eta}}\\), which is a sigmoid function in the explanatory variables. The equation \\(\\eta=0\\) defines a linear decision boundary or classification threshold."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-2",
    "href": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-2",
    "title": "Classification & clustering methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\n\n\n\n&gt; tibble::tibble(eta = seq(-5,5,0.2)) %&gt;% dplyr::mutate(pi = 1/(1+exp(-eta))) %&gt;% \n+ ggplot(aes(x=eta, y=pi)) + geom_point() + theme_bw(base_size = 38)\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\n\\pi & =\\frac{1}{1+e^{-\\eta}}\\;\\text{(logistic function)}\\\\\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) & =\\log\\left(\\frac{\\frac{1}{1+e^{-\\eta}}}{1-\\frac{1}{1+e^{-\\eta}}}\\right)\\\\\n& =\\log\\left(\\frac{\\frac{1}{1+e^{-\\eta}}}{\\frac{e^{-\\eta}}{1+e^{-\\eta}}}\\right)=\\log\\left(e^{\\eta}\\right)=\\eta\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-3",
    "href": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-3",
    "title": "Classification & clustering methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\nThe term \\(\\frac{\\pi}{1-\\pi}\\) is called the the odds-ratio. By its definition:\n\\[\n\\frac{\\pi}{1-\\pi}=e^{\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_2 x_2\\ldots+\\beta_n x_n}\n\\]\nSo if \\(x_1\\) changes by one unit (\\(x_1\\rightarrow x_1+1\\)), then the odds ratio changes by \\(e^{\\beta_1}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nConfusion matrix\nThe confusion matrix is a 2x2 table summarizing the number of correct predictions of the model (a function of the decision boundary): It is the foundation for understanding other evaluation metrics.\n\n\n\n\npredict 1\npredict 0\n\n\n\n\ndata = 1\ntrue positives (TP)\nfalse negatives (FN)1\n\n\ndata = 0\nfalse positives (FP)2\ntrue negatives (TN)\n\n\n\nType II errorType I error"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-1",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-1",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nAccuracy - the simplest metric:\nAccuracy measures the percent of correct predictions:\n\\[\n\\begin{align*}\n\\frac{\\text{TP}+\\text{TN}}{\\text{observation count}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-2",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-2",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nAccuracy:\n\nUsefulnessLimitations\n\n\n\nAccuracy is a useful metric in evaluating classification models under certain conditions:\n\nBalanced Datasets: When the classes in the dataset are roughly equal in number, accuracy can be a reliable indicator of model performance. For example, if you have a dataset where 50% of the samples are class A and 50% are class B, accuracy is a good measure of whether the model correctly predicts the classes.\nGeneral Performance: For an overall sense of a model’s performance, accuracy provides a straightforward, easy-to-understand measure, giving the proportion of correct predictions out of all predictions made.\n\n\n\n\n\nAccuracy has several limitations, especially in the context of imbalanced datasets:\n\n\nImbalanceImportanceBehaviour\n\n\n\nImbalanced Datasets:\n\nFalse Sense of Performance: When one class significantly outnumbers the other, accuracy can be misleading: if 95% of the samples belong to class A and only 5% to class B, always predicting class A will have a high accuracy (95%) but may fail to correctly identify any instances of class B.\n\n\n\n\n\nIgnoring Class Importance:\n\nPrecision and Recall: Accuracy does not take into account the importance of different classes or the costs of different types of errors (false positives and false negatives). In many business contexts, these costs are not equal, e.g., in medical diagnosis, a false -ve might be much more serious than a false +ve.\n\n\n\n\n\nLack of Insight into Model Behavior:\n\nDetailed Performance: Accuracy give no insight into the types of errors the model is making. Precision and recall, on the other hand, offer more detailed information about the performance on individual classes - crucial for understanding and improving the model."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-3",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-3",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nAccuracy: examples of limitation\n\nFraud Detection:\n\nScenario: Suppose you have a dataset with 1,000 transactions, of which 990 are legitimate and 10 are fraudulent. A model that always predicts “legitimate” will have an accuracy of 99% but fails to identify fraudulent transactions, making it useless for fraud detection purposes.\n\nSpam Detection:\n\nScenario: Consider an email classification problem where 95% of emails are legitimate and 5% are spam. A model that always predicts “legitimate” will have high accuracy but will not catch any spam emails, which is the primary goal."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-4",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-4",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nPrecision\nPrecision measures the percent of positive predictions that are correct (true positives / all positives predicted):\n\\[\n\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-5",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-5",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nRecall / Sensitivity\n\nMeasures the success at predicting the first class (true positives predicted / actual positives):\n\n\\[\n\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\\qquad\\text{(True Positive Rate - TPR)}\n\\]\nRecall / Specificity\n\nMeasures the success at predicting the second class (true negatives predicted / actual negative):\n\n\\[\n\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}\\qquad\\text{(True Negative Rate - TNR)}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-6",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-6",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nReceiver Operating Characteristic (ROC) curve & the Area Under the Curve (AUC)\n\n\nROC Curve: Plot of the true positive rate (Recall) against the false positive rate (1 - Specificity) at various threshold settings.\nAUC: The area under the ROC curve, representing the probability that the model ranks a randomly chosen positive instance higher than a randomly chosen negative instance.\n\nAUC values range from 0.5 (no discrimination) to 1 (perfect discrimination)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-7",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-7",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nROC Curves\nConsider plotting the TPR against the FPR (1-TNR) at different classification thresholds.\n\nthe diagonal (TPR = 1-TNR) describes a process equivalent to tossing a fair coin (i.e. no predictive power)\nour method should have a curve above the diagonal; which shape is better depends on the purpose of our classifier.\n\nSo, how to compute?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#example-create-the-workflow",
    "href": "slides/BSMM_8740_lec_05.html#example-create-the-workflow",
    "title": "Classification & clustering methods",
    "section": "Example: create the workflow",
    "text": "Example: create the workflow\nWorkflow to model credit card default\n\n\nCode\n&gt; data &lt;- ISLR::Default %&gt;% tibble::as_tibble()\n&gt; set.seed(8740)\n&gt; \n&gt; # split data\n&gt; data_split &lt;- rsample::initial_split(data)\n&gt; default_train &lt;- rsample::training(data_split)\n&gt; \n&gt; # create a recipe\n&gt; default_recipe &lt;- default_train %&gt;% \n+   recipes::recipe(formula = default ~ student + balance + income) %&gt;% \n+   recipes::step_dummy(recipes::all_nominal_predictors())\n&gt; \n&gt; # create a linear regression model\n&gt; default_model &lt;- parsnip::logistic_reg() %&gt;% \n+   parsnip::set_engine(\"glm\") %&gt;% \n+   parsnip::set_mode(\"classification\")\n&gt; \n&gt; # create a workflow\n&gt; default_workflow &lt;- workflows::workflow() %&gt;%\n+   workflows::add_recipe(default_recipe) %&gt;%\n+   workflows::add_model(default_model)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#example-fit-the-model-using-the-data",
    "href": "slides/BSMM_8740_lec_05.html#example-fit-the-model-using-the-data",
    "title": "Classification & clustering methods",
    "section": "Example: fit the model using the data",
    "text": "Example: fit the model using the data\n\n&gt; # fit the model\n&gt; lm_fit &lt;- \n+   default_workflow %&gt;% \n+   parsnip::fit(default_train)\n&gt; \n&gt; # augment the data with the predictions using the model fit\n&gt; training_results &lt;- \n+   broom::augment(lm_fit , default_train) \n&gt; \n&gt; training_results %&gt;% dplyr::slice_head(n=6)\n\n# A tibble: 6 × 7\n  .pred_class .pred_No .pred_Yes default student balance income\n  &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 No             0.998  0.00164  No      No         759. 45774.\n2 No             1.00   0.000145 No      Yes        452. 19923.\n3 No             0.770  0.230    Yes     No        1666. 30070.\n4 No             1.00   0.000256 No      No         434. 57146.\n5 No             1.00   0.000449 No      No         536. 32994.\n6 No             0.973  0.0269   No      No        1252. 32721."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#example-compute-the-auc",
    "href": "slides/BSMM_8740_lec_05.html#example-compute-the-auc",
    "title": "Classification & clustering methods",
    "section": "Example: compute the AUC",
    "text": "Example: compute the AUC\n\n\n\n&gt; training_results %&gt;% \n+   yardstick::roc_curve(\n+     truth = default\n+     , .pred_No\n+   ) %&gt;% \n+   autoplot() + \n+   theme_bw(base_size = 38) \n\n\n\n\n\n\n\n\n\n\n&gt; training_results %&gt;% \n+    yardstick::roc_auc(\n+      truth = default\n+      , .pred_No\n+     )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.949"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier",
    "text": "Binary Classifier\nClassification Threshold\nRecall:\n\nIn binary classification, the model predicts the probability of an instance belonging to the positive class. The classification threshold is the probability value above which an instance is classified as positive.\nCommonly, this threshold is set at 0.5, meaning any instance with a predicted probability above 0.5 is classified as positive, and below 0.5 as negative."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-1",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-1",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier",
    "text": "Binary Classifier\nClassification Threshold Impact\n\nTrue Positives (TP) and False Positives (FP):\n\nLower Threshold: A lower threshold increases the number of instances classified as positive, which increases both true positives and false positives.\nHigher Threshold: A higher threshold decreases the number of instances classified as positive, which decreases both true positives and false positives.\n\nTrue Negatives (TN) and False Negatives (FN):\n\nLower Threshold: A lower threshold decreases the number of instances classified as negative, which decreases both true negatives and increases false negatives.\nHigher Threshold: A higher threshold increases the number of instances classified as negative, which increases true negatives and decreases false negatives.\n\nTrade-offs:\n\nAdjusting the threshold affects the trade-off between sensitivity (recall) and specificity. A lower threshold improves sensitivity but reduces specificity, and vice versa."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-2",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-2",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier",
    "text": "Binary Classifier\nROC Curve and AUC\n\n\nROC Curve:\n\nThe ROC curve plots the true positive rate (TPR, or recall) against the false positive rate (FPR) at various threshold settings.\nEach point on the ROC curve represents a TPR/FPR pair corresponding to a specific threshold.\n\nAUC (Area Under the Curve):\n\nThe AUC represents the model’s ability to discriminate between positive and negative classes across all threshold values.\nA higher AUC indicates better overall performance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-optimal-threshold",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-optimal-threshold",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier: optimal threshold",
    "text": "Binary Classifier: optimal threshold\n\n\nBusiness Context:\n\nThe optimal threshold depends on the specific costs of false positives and false negatives in the business context. For example, in fraud prediction, the cost of missing a positive case (false negative) might be much higher than a false alarm (false positive).\n\nMaximize Specific Metrics:\n\nYou can choose a threshold that maximizes a specific metric such as F1 score, which balances precision and recall.\nAlternatively, you might want to maximize precision, recall, or minimize a cost function that accounts for both false positives and false negatives.\n\nYouden’s Index:\n\nOne method to select an optimal threshold is to maximize Youden’s Index (\\(J\\)), which is defined as: \\(J=\\mathrm{Sensitivity}+\\mathrm{Specificity}-1\\)\nThis index helps to find a threshold that maximizes the difference between true positive rate and false positive rate.\n\nCost-Benefit Analysis:\n\nPerform a cost-benefit analysis by assigning costs to false positives and false negatives and choosing the threshold that minimizes the total expected cost."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-threshold-examples",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-threshold-examples",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier: threshold examples",
    "text": "Binary Classifier: threshold examples\n\nExample Scenario\nFraud Detection: - In fraud detection, missing a fraudulent transaction (false negative) might be more costly than flagging a legitimate transaction as fraud (false positive). - You might choose a lower threshold to ensure higher sensitivity (recall), even if it means a higher false positive rate, thereby catching more fraudulent transactions.\nConclusion\nThe choice of classification threshold in computing the ROC curve is crucial for balancing the trade-offs between sensitivity and specificity, and ultimately for optimizing the model’s performance in a way that aligns with business goals and context. Understanding and carefully selecting the appropriate threshold ensures that the model’s predictions are most useful and cost-effective for the specific application."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nBayes Rule - using the rules of conditional probability:\n\\[\n\\mathbb{P}(A,B) = \\mathbb{P}(A|B)\\mathbb{P}(B) = \\mathbb{P}(B|A)\\mathbb{P}(A)\n\\] We can write:\n\\[\n\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A|B)\\mathbb{P}(B)}{\\mathbb{P}(A)}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-1",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-1",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nThis method starts with Bayes rule:\nfor \\(K\\) classes and an observation \\(x\\) consisting of \\(N\\) features \\(\\{x_1,\\ldots,x_N\\}\\), since \\(\\mathbb{P}\\left[\\left.C_{k}\\right|x_{1},\\ldots,x_{N}\\right]\\times\\mathbb{P}\\left[x_{1},\\ldots,x_{N}\\right]\\) is equal to \\(\\mathbb{P}\\left[\\left.x_{1},\\ldots,x_{N}\\right|C_{k}\\right]\\times\\mathbb{P}\\left[C_{k}\\right]\\), we can write\n\\[\n\\mathbb{P}\\left[\\left.C_{k}\\right|x_{1},\\ldots,x_{N}\\right]=\\frac{\\mathbb{P}\\left[\\left.x_{1},\\ldots,x_{N}\\right|C_{k}\\right]\\times\\mathbb{P}\\left[C_{k}\\right]}{\\mathbb{P}\\left[x_{1},\\ldots,x_{N}\\right]}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-2",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-2",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nIf we assume that the features are all independent we can write Bayes rule as\n\\[\n\\mathbb{P}\\left[\\left.C_{k}\\right|x_{1},\\ldots,x_{N}\\right]=\\frac{\\mathbb{P}\\left[C_{k}\\right]\\times\\prod_{n=1}^{N}\\mathbb{P}\\left[\\left.x_{n}\\right|C_{k}\\right]}{\\prod_{n=1}^{N}\\mathbb{P}\\left[x_{n}\\right]}\n\\]\nand since the denominator is independent of \\(C_{k}\\), our classifier is\n\\[\nC_{k}=\\arg\\max_{C_{k}}\\mathbb{P}\\left[C_{k}\\right]\\prod_{n=1}^{N}\\mathbb{P}\\left[\\left.x_{n}\\right|C_{k}\\right]\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-3",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-3",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nSo it remains to calculate the class probability \\(\\mathbb{P}\\left[C_{k}\\right]\\) and the conditional probabilities \\(\\mathbb{P}\\left[\\left.x_{n}\\right|C_{k}\\right]\\)\nThe different naive Bayes classifiers differ mainly by the assumptions they make regarding the conditional probabilities."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-4",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-4",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nIf our features are all ordinal, then\n\nThe class probabilities1 are simply the frequency of observations that belong to each class divided by the total number of observations.\nThe conditional probabilities are the frequency of each feature value for a given class value divided by the frequency of measurements with that class value.\n\ni.e. empirical probabilities"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-5",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-5",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nIf any features are numeric, we can estimate conditional probabilities by assuming that the numeric features have a Gaussian distribution for each class"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-6",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-6",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\n\nCode\n&gt; library(discrim)\n&gt; # create a naive bayes classifier\n&gt; default_model_nb &lt;- parsnip::naive_Bayes() %&gt;% \n+   parsnip::set_engine(\"klaR\") %&gt;% \n+   parsnip::set_mode(\"classification\")\n&gt; \n&gt; # create a workflow\n&gt; default_workflow_nb &lt;- workflows::workflow() %&gt;%\n+   workflows::add_recipe(default_recipe) %&gt;%\n+   workflows::add_model(default_model_nb)\n&gt; \n&gt; # fit the model\n&gt; lm_fit_nb &lt;- \n+   default_workflow_nb %&gt;% \n+   parsnip::fit(\n+     default_train\n+   , control = \n+     workflows::control_workflow(parsnip::control_parsnip(verbosity = 1L))\n+   )\n&gt; \n&gt; # augment the data with the predictions using the model fit\n&gt; training_results_nb &lt;- \n+   broom::augment(lm_fit_nb , default_train)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-7",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-7",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\nAUCROC\n\n\n\n&gt; training_results_nb %&gt;% \n+   yardstick::roc_auc(.pred_No, truth = default)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.942"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-uses",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-uses",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification: uses",
    "text": "Naive Bayes Classification: uses\n\n\nText Classification:\n\nSentiment Analysis: It can classify text data into categories such as positive, negative, or neutral sentiment.\n\nDocument Categorization:\n\nUseful in classifying news articles, blog posts, or any document into predefined categories based on content.\n\nMedical Diagnosis:\n\nCan be used to predict the likelihood of a disease based on patient symptoms and medical history, assuming independence between symptoms.\n\nRecommender Systems:\n\nHelps in predicting user preferences and recommending items such as movies, books, or products based on previous user behavior.\n\nReal-time Prediction:\n\nDue to its simplicity and speed, Naive Bayes is suitable for real-time prediction tasks where quick decisions are essential."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-abuses",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-abuses",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification: abuses",
    "text": "Naive Bayes Classification: abuses\n\n\nAssumption of Feature Independence:\n\nMisuse: The model assumes that all features are independent given the class.\nImpact: Poor performance if features are highly correlated.\n\nImbalanced Data:\n\nMisuse: Naive Bayes can struggle with imbalanced class datasets.\nImpact: Bias towards the majority class, leads to high accuracy but poor minority recall.\n\nZero Probability Problem:\n\nMisuse: If a feature \\(x_j\\) missing for class \\(k\\), then \\(\\mathbb{P}\\left[\\left.x_{j}\\right|C_{k}\\right]=0\\), which can skew results.\nSolution: Use techniques like Laplace Smoothing to handle zero probabilities.\n\nOverfitting on Small Datasets:\n\nMisuse: Naive Bayes may overfit if trained on a small dataset with noise or outliers.\nImpact: This can result in poor generalization to new data.\n\nIgnoring Feature Scaling:\n\nMisuse: The model does not inherently handle features with different scales or units.\nImpact: Features with larger scales can disproportionately influence the model"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-best-practices",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-best-practices",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Best Practices",
    "text": "Naive Bayes Best Practices\n\n\nFeature Engineering:\n\nProper feature selection and engineering can help mitigate some of the independence assumption issues. For instance, combining related features can improve performance.\n\nHandling Correlated Features:\n\nWhile Naive Bayes assumes independence, it can still perform well with moderately correlated features. In cases of strong correlation, consider using other models.\n\nEvaluation Metrics:\n\nUse appropriate metrics such as precision, recall, and ROC-AUC, especially for imbalanced datasets, to get a comprehensive understanding of model performance.\n\nCross-Validation:\n\nEmploy cross-validation techniques to ensure that the model generalizes well to unseen data and to avoid overfitting.\n\nComparative Analysis:\n\nCompare Naive Bayes with other classifiers (e.g., logistic regression, SVM, random forest) to ensure that it is the best choice for your specific problem."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification",
    "href": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification",
    "title": "Classification & clustering methods",
    "section": "Nearest Neighbour Classification",
    "text": "Nearest Neighbour Classification\nThe k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.\nIt is typically used as a classification algorithm, working off the assumption that similar class predictions can be made by predictors near one another."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification-1",
    "href": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification-1",
    "title": "Classification & clustering methods",
    "section": "Nearest Neighbour Classification",
    "text": "Nearest Neighbour Classification\nFor classification problems, a class label is assigned on the basis of a majority vote—i.e. the label that is most frequently represented around a given data point is used.\nBefore a classification can be made, the distance between points must be defined. Euclidean distance is most commonly used."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification-2",
    "href": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification-2",
    "title": "Classification & clustering methods",
    "section": "Nearest Neighbour Classification",
    "text": "Nearest Neighbour Classification\nNote that the KNN algorithm is also part of a family of “lazy learning” models, meaning that it only stores a training dataset versus undergoing a training stage. This also means that all the computation occurs when a classification or prediction is being made.\nThe k value in the k-NN algorithm determines how many neighbors will be checked to determine the classification of a specific query point."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-distance-measures",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-distance-measures",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: distance measures",
    "text": "KNN Classification: distance measures\n\nEuclidean: \\(\\text{d}(x,y)=\\sqrt{\\sum_i(y_i- x_i)^2}\\)\nManhattan: \\(\\text{d}(x,y)=\\sum_{i}\\left|y_{i}-x_{i}\\right|\\)\nMinkowski: \\(\\text{d}(x,y;p)=\\left(\\sum_{i}\\left|y_{i}-x_{i}\\right|\\right)^{1/p}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-algorithm",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-algorithm",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: algorithm",
    "text": "KNN Classification: algorithm\n\nChoose the value of K, which is the number of nearest neighbors that will be used to make the prediction.\nCalculate the distance between the observation you want to classify and all the observations in the training set.\nSelect the K nearest neighbors based on the distances calculated.\nAssign the label of the majority class to the new data point.\nRepeat steps 2 to 4 for all the data points in the test set."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-model",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-model",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: model",
    "text": "KNN Classification: model\nOur classification workflow only differs by the model, e.g.:\n\n&gt; default_model_knn &lt;- parsnip::nearest_neighbor(neighbors = 4) %&gt;% \n+   parsnip::set_engine(\"kknn\") %&gt;% \n+   parsnip::set_mode(\"classification\")\n\n\n\n\n\n\n\nk-NN regression\n\n\nTo use k-NN for a regression problem, calculate the mean or median (or another aggregate measure) of the dependent variable among the k neighbors."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-metrics",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-metrics",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: metrics",
    "text": "KNN Classification: metrics\nIn the context of k-Nearest Neighbors (kNN) classification, while the general evaluation metrics like accuracy, precision, recall, F1 score, and others are commonly used, there are no unique metrics that are exclusively specific to kNN. However, there are certain considerations and additional analyses that are particularly relevant when evaluating a kNN model."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-metrics-1",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-metrics-1",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: metrics",
    "text": "KNN Classification: metrics\n\n\nChoice of k (Number of Neighbors): The value of k affects the performance of a kNN model. Testing the model with various values of k and evaluating the performance using standard metrics (like accuracy, F1 score) can to select the best k.\nFeature Scaling Sensitivity: kNN is sensitive to the scale of the features because it relies on calculating distances. Evaluate the model’s performance before and after feature scaling (like Min-Max scaling or Z-score normalization)\nCurse of Dimensionality: kNN can perform poorly with high-dimensional data (many features). Evaluating the model’s performance in relation to the number of features (dimensionality) can be important. Dimensionality reduction techniques like PCA might be used with kNN."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-uses",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-uses",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: uses",
    "text": "KNN Classification: uses\n\n\nPattern Recognition:\n\nHandwriting Recognition: kNN can classify handwritten digits or letters using their features.\nImage Classification: Categorize by comparing pixels/features with known images.\n\nRecommender Systems:\n\nContent-Based Filtering: kNN can recommend items using similarities between user preferences and the attributes of items.\nCollaborative Filtering: Can also recommend items based on preferences of similar users.\n\nMedical Diagnosis:\n\nDisease Prediction: kNN can predict likelihood of diseases by comparing patient data to historical patient data with known diagnoses.\n\nAnomaly Detection:\n\nFraud Detection: kNN can identify unusual transactions by comparing to known legitimate and fraudulent transactions.\n\nCustomer Segmentation:\n\nMarket Analysis: It can segment customers based on purchasing behavior, demographics, or other attributes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-abuses",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-abuses",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: abuses",
    "text": "KNN Classification: abuses\n\n\nHigh Dimensionality:\n\nMisuse: kNN can struggle with high-dimensional data as distances become less meaningful (curse of dimensionality).\nImpact: Performance degrades as irrelevant or noisy features overshadow important ones.\nSolution: Use dimensionality reduction (e.g., PCA, t-SNE) or feature selection before using kNN.\n\nLarge Datasets:\n\nMisuse: kNN stores all training data and calculates all distances during prediction, & can be computationally expensive.\nImpact: It becomes slow and impractical for large datasets.\nSolution: Approximate kNN techniques or other algorithms suited for large datasets.\n\nImbalanced Datasets:\n\nMisuse: kNN can be biased towards the majority class in imbalanced datasets because the majority class neighbors dominate.\nImpact: Low recall for minority class, & to poor performance in e.g. fraud detection or rare disease diagnosis.\nSolution: Use resampling, synthetic data generation (SMOTE), or adjusting the decision rule to account for class imbalance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-abuses-1",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-abuses-1",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: abuses",
    "text": "KNN Classification: abuses\n\n\nChoice of k and Distance Metric:\n\nMisuse: An inappropriate choice of ( k ) (too small or too large) or an unsuitable distance metric can lead to poor classification performance.\nImpact: A small ( k ) can make the model sensitive to noise (overfitting), while a large ( k ) can oversmooth the decision boundary (underfitting).\nSolution: Use cross-validation to choose the optimal ( k ) and experiment with different distance metrics (e.g., Euclidean, Manhattan).\n\nScalability:\n\nMisuse: without optimization, kNN does not scale well to datasets with a large number of features or samples.\nImpact: Slow predictions and high memory usage.\nSolution: Use k-d trees, ball trees, or locality-sensitive hashing to speed up nearest neighbor searches."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-best-practices",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-best-practices",
    "title": "Classification & clustering methods",
    "section": "KNN Classification Best Practices",
    "text": "KNN Classification Best Practices\n\n\nNormalization and Scaling:\n\nImportance: Features should be on a similar scale for kNN to perform well, as distance calculations are sensitive to feature scales.\nPractice: Apply normalization (e.g., Min-Max scaling) or standardization (mean=0, variance=1) to the features.\n\nHandling Missing Data:\n\nImportance: kNN cannot handle missing values directly.\nPractice: Impute missing values before applying kNN, using mean/mode imputation or kNN-based imputation.\n\nChoosing k:\n\nImportance: The choice of \\(k\\) can significantly affect model performance.\nPractice: Use cross-validation to determine the optimal value of \\(k\\).\n\nEvaluating Model Performance:\n\nImportance: Use appropriate metrics to evaluate the model, especially with imbalanced data.\nPractice: Evaluate using precision, recall, and ROC-AUC, not just accuracy."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#support-vector-machine-classification",
    "href": "slides/BSMM_8740_lec_05.html#support-vector-machine-classification",
    "title": "Classification & clustering methods",
    "section": "Support Vector Machine Classification",
    "text": "Support Vector Machine Classification\nThe SVM assumes a training set of the form \\((x_1,y_1),\\ldots,(x_n,y_n)\\) where the \\(y_i\\) are either \\(-1\\) or \\(1\\), indicating the class to which each \\(x_i\\) belongs.\nThe SVM algorithm looks to find the maximum-margin hyperplane that divides the group of points \\(x_i\\) for which \\(y_1=-1\\) from the group for which \\(Y_1=1\\), such that the distance between the hyperplane and the nearest point \\(x_i\\) from either group is maximized."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification",
    "title": "Classification & clustering methods",
    "section": "SVM Classification",
    "text": "SVM Classification\nBasic Concepts\n\n\nSeparating Hyperplane: The core idea of SVM is to find a hyperplane (in two-dimensional space, this would be a line) that best separates the classes in the feature space.\nSupport Vectors: Support vectors are the data points that are closest to the separating hyperplane. These points are critical in defining the position and orientation of the hyperplane.\nMargin: The algorithm aims to maximize the margin, which is the distance between the hyperplane and the nearest points from both classes. A larger margin is considered better as it may lead to lower generalization error of the classifier."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-large-margin",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-large-margin",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: large margin",
    "text": "SVM Classification: large margin\nIllustration of SVM large-margin principle"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-1",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-1",
    "title": "Classification & clustering methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\n\n\n\nLet our decision boundary be given by \\(f\\left(x\\right)=w^{\\top}x+w_{0}=0\\), for a vector \\(w\\) perpendicular to the boundary1.\nWe can express any point as \\(x=x_{\\bot}+r\\frac{w}{\\left\\Vert w\\right\\Vert }\\)\nNote that \\(f\\left(x\\right)=\\left(w^{\\top}x_{\\bot}+w_{0}\\right)+r\\left\\Vert w\\right\\Vert\\)\n\n\n\n\n\n\n\n\nany point \\(x\\) on the red line satisfies \\(w^\\top x+w_0=0\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-2",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-2",
    "title": "Classification & clustering methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\n\n\n\nSince \\(f\\left(x_{\\bot}\\right)=w^{\\top}x_{\\bot}+w_{0}=0\\), we have \\(f\\left(x\\right)=r\\left\\Vert w\\right\\Vert\\).\nWe also require \\(f\\left(x_{n}\\right)\\tilde{y}_{n}&gt;0\\)\nTo maximize the distance to the closest point, the objective is \\(\\max_{w,w_{0}}\\min_{n}\\left[\\tilde{y}_{n}\\left(w^{\\top}x_{n}+w_{0}\\right)\\right]\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-3",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-3",
    "title": "Classification & clustering methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\nIt is common to scale the vector \\(w\\) and the offset \\(w_0\\) such that \\(f_n\\hat{y}_n=1\\) for the point nearest the decision boundary, such that \\(f_n\\hat{y}_n\\ge1\\) for all \\(n\\).\nIn addition, since minimizing \\(1/ \\left\\Vert w\\right\\Vert\\) is equivalent to minimizing \\(\\left\\Vert w\\right\\Vert^2\\), we can state the objective as\n\\[\n\\min_{w,w_{0}}\\frac{1}{2}\\left\\Vert w\\right\\Vert ^{2}\\quad\\text{s.t.}\\quad\\tilde{y}_{n}\\left(w^{\\top}x_{n}+w_{0}\\right)\\ge 1, \\forall n\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-4",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-4",
    "title": "Classification & clustering methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\n\n\nIf there is no solution to the objective we can add slack variables \\(\\xi_n\\ge0\\) to replace the hard constraints that \\(f_n\\hat{y}_n\\ge1\\) with the soft margin constraints that \\(f_n\\hat{y}_n\\ge1-\\xi_n\\).\nThe new objective is\n\\[\n\\min_{w,w_{0},\\xi}\\frac{{1}}{2}\\left\\Vert w\\right\\Vert ^{2} + C\\sum_n \\xi_n \\\\\n\\text{s.t.}\\xi_n\\ge0, \\quad\\tilde{y}_{n}\\left(w^{\\top}x_{n}+w_{0}\\right)\\ge 1, \\forall n\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-data-not-separable",
    "href": "slides/BSMM_8740_lec_05.html#svm-data-not-separable",
    "title": "Classification & clustering methods",
    "section": "SVM: data not separable",
    "text": "SVM: data not separable\n\n\nIf the data is not separable:\n\na transformation of data may make them separable\nan embedding in a higher dimensional space might make them separable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-support-vectors",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-support-vectors",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: Support Vectors",
    "text": "SVM Classification: Support Vectors\n\nSupport vectors are the data points that lie closest to the decision surface (or hyperplane)\nThey are the data points most difficult to classify\nThey have direct bearing on the optimum location of the decision surface\nSupport vectors are the elements of the training set that would change the position of the dividing hyperplane if\nremoved"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-example",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-example",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: example",
    "text": "SVM Classification: example\n\n\nCode\n&gt; # show_engines(\"svm_linear\")\n&gt; default_model_svm &lt;- parsnip::svm_linear() %&gt;% \n+   parsnip::set_engine(\"svm_linear\") %&gt;% \n+   parsnip::set_mode(\"classification\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-variants",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-variants",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: variants",
    "text": "SVM Classification: variants\nThe linear SVM constructs a linear decision boundary (hyperplane) to separate classes in the feature space. It aims to find the hyperplane that maximizes the margin between the closest points (support vectors) of the classes. The decision function is \\(f(x) = w \\cdot x + b\\), where \\(w\\) is the weight vector and \\(b\\) is the bias term.\nThere are similar SVM methods that are adapted to more complex boundaries."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-polynomial",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-polynomial",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: polynomial",
    "text": "SVM Classification: polynomial\nPolynomial SVM uses a polynomial kernel to create a non-linear decision boundary. It transforms the input features into higher-dimensional space where a linear separation is possible.\nThe polynomial kernel is \\(K(x, x') = (w \\cdot x + b)^d\\), where \\(d\\) is the degree of the polynomial. This kernel is implemented though parsnip::svm_poly and engine kernlab."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-radial-basis",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-radial-basis",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: radial basis",
    "text": "SVM Classification: radial basis\nRBF SVM uses the Radial Basis Function (Gaussian) kernel to handle non-linear classification problems. It maps the input space into an infinite-dimensional space where a linear separation is possible.\nThe RBF kernel is \\(K(x, x') = \\exp\\left(-\\gamma \\left\\Vert x - x'\\right\\Vert^2\\right)\\), where \\(\\gamma\\) controls the width of the Gaussian function. This kernel is implemented though parsnip::svm_rbf and engine kernlab."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-comparisons",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-comparisons",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: Comparisons",
    "text": "SVM Classification: Comparisons\n\n\n\n\n\n\n\n\n\n\nFeature\nLinear SVM\nPolynomial SVM\nRBF SVM\n\n\n\n\nKernel Function\nLinear\nPolynomial\nRadial Basis Function\n\n\nEquation\n\\(w \\cdot x + b\\)\n\\((w \\cdot x + b)^d\\)\n\\(\\exp\\left(-\\gamma \\left\\Vert x - x'\\right\\Vert^2\\right)\\)\n\n\nComplexity\nLow\nMedium to High (depending on \\(d\\))\nHigh\n\n\nInterpretability\nHigh\nMedium\nLow\n\n\nComputational Cost\nLow\nMedium to High (higher with increasing \\(d\\))\nHigh\n\n\nFlexibility\nLow\nMedium to High\nHigh\n\n\nRisk of Overfitting\nLow\nMedium to High (higher with increasing \\(d\\))\nMedium to High (depends on \\(\\gamma\\) and \\(C\\))\n\n\nTypical Use Cases\nLinearly separable, high-dimensional spaces (e.g., text)\nData with polynomial relationships\nHighly non-linear data, complex patterns"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices",
    "title": "Classification & clustering methods",
    "section": "SVM Classification Best Practices",
    "text": "SVM Classification Best Practices\n\n1. Data Preparation\na. Feature Scaling\n\nImportance: SVM is sensitive to the scale of the features. Features with larger ranges can dominate the distance calculations, leading to suboptimal boundaries.\nBest Practice: Standardize or normalize the features so that they have similar scales. Common techniques include Min-Max scaling (to a [0, 1] range) and StandardScaler (to zero mean and unit variance).\n\nb. Handling Missing Data\n\nImportance: SVM cannot handle missing values directly.\nBest Practice: Impute missing values using methods like mean/mode/median imputation or more sophisticated techniques such as k-NN imputation or using models to predict missing values.\n\n2. Choosing the Kernel\na. Linear Kernel\n\nWhen to Use: If the data is approximately linearly separable or when the number of features is very large compared to the number of samples.\nBest Practice: Start with a linear kernel as a baseline, especially for high-dimensional data like text classification.\n\nb. Non-linear Kernels (Polynomial, RBF)\n\nWhen to Use: If the data is not linearly separable. The RBF kernel is generally a good first choice for non-linear problems.\nBest Practice: Experiment with different kernels. Use cross-validation to compare performance and select the most appropriate kernel."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices-1",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices-1",
    "title": "Classification & clustering methods",
    "section": "SVM Classification Best Practices",
    "text": "SVM Classification Best Practices\n\n3. Hyperparameter Tuning\na. Regularization Parameter (cost \\(C\\))\n\nImportance: Controls the trade-off between achieving a low training error and a low testing error.\nBest Practice: Use cross-validation to find the optimal value of cost \\(C\\). Start with a wide range and then narrow down.\n\nb. Kernel-specific Parameters\n\nFor RBF Kernel: Tune the \\(\\sigma\\) parameter, which defines the influence of individual data points.\nFor Polynomial Kernel: Tune the degree \\(d\\), the margin coefficient \\(r\\), and the scale_factor.\nBest Practice: Use grid search or random search for hyperparameter tuning. Cross-validation is crucial to avoid overfitting and to find the best combination of parameters.\n\n4. Handling Imbalanced Data\na. Class Weights\n\nImportance: Imbalanced classes can bias the SVM towards the majority class.\nBest Practice: Adjust the class weights to give more importance to the minority class. Most SVM implementations allow setting the class_weight parameter to ‘balanced’ or manually specifying weights.\n\nb. Resampling Techniques\n\nImportance: Can further help in dealing with imbalanced datasets.\nBest Practice: Use oversampling (e.g., themis::step_smote()) or undersampling techniques to balance the class distribution before training the model."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices-2",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices-2",
    "title": "Classification & clustering methods",
    "section": "SVM Classification Best Practices",
    "text": "SVM Classification Best Practices\n\n5. Model Evaluation\na. Performance Metrics\n\nImportance: Accuracy alone may not be sufficient, especially for imbalanced datasets.\nBest Practice: Evaluate using metrics like precision, recall,and ROC-AUC to get a comprehensive understanding of model performance.\n\nb. Cross-validation\n\nImportance: Ensures that the model generalizes well to unseen data.\nBest Practice: Use k-fold cross-validation to assess the model’s performance and robustness. This helps in reducing the variance in performance estimates.\n\n6. Implementation Considerations\na. Choosing the Right Software/Library\n\nImportance: Efficient and reliable implementation is crucial for performance.\nBest Practice: Use well-established libraries like those in parsnip.\n\nb. Handling Large Datasets\n\nImportance: SVM can be computationally intensive for large datasets.\nBest Practice: Use linear SVMs for very large datasets or use dimensionality reduction techniques (e.g., PCA) to reduce the feature space."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#clustering",
    "href": "slides/BSMM_8740_lec_05.html#clustering",
    "title": "Classification & clustering methods",
    "section": "Clustering",
    "text": "Clustering\nClassification and clustering serve different purposes in machine learning. Classification is a supervised learning technique used for predicting predefined labels, requiring labeled data and focusing on accuracy and interpretability.\nClustering, on the other hand, is an unsupervised learning technique used for discovering natural groupings in data, requiring no labeled data and focusing on exploratory data analysis and pattern discovery. Understanding the strengths and limitations of each method is crucial for applying them effectively to solve real-world problems."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#clustering-1",
    "href": "slides/BSMM_8740_lec_05.html#clustering-1",
    "title": "Classification & clustering methods",
    "section": "Clustering",
    "text": "Clustering\nCluster analysis refers to algorithms that group similar objects into groups called clusters. The endpoint of cluster analysis is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\nThe purpose of cluster analysis is to help reveal patterns and structures within a dataset that may provide insights into underlying relationships and associations."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#clustering-applications",
    "href": "slides/BSMM_8740_lec_05.html#clustering-applications",
    "title": "Classification & clustering methods",
    "section": "Clustering Applications",
    "text": "Clustering Applications\n\n\nMarket Segmentation: Cluster analysis is often used in marketing to segment customers into groups based on their buying behavior, demographics, or other characteristics.\nImage Processing: In image processing, cluster analysis is used to group pixels with similar properties together, allowing for the identification of objects and patterns in images.\nBiology and Medicine: Cluster analysis is used in biology and medicine to identify genes associated with specific diseases or to group patients with similar clinical characteristics together.\nSocial Network Analysis: In social network analysis, cluster analysis is used to group individuals with similar social connections and characteristics together, allowing for the identification of subgroups within a larger network.\nAnomaly Detection: Cluster analysis can be used to detect anomalies in data, such as fraudulent financial transactions, unusual patterns in network traffic, or outliers in medical data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-clustering",
    "href": "slides/BSMM_8740_lec_05.html#k-means-clustering",
    "title": "Classification & clustering methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nk-means is a method of unsupervised learning that produces a partitioning of observations into k unique clusters.\nThe goal of k-means is to minimize the sum of squared Euclidian distances between observations in a cluster and the centroid, or geometric mean, of that cluster."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-clustering-1",
    "href": "slides/BSMM_8740_lec_05.html#k-means-clustering-1",
    "title": "Classification & clustering methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nIn k-means clustering, observed variables (columns) are considered to be locations on axes in multidimensional space.\n\nThe basic k-means algorithm has the following steps.\n\npick the number of clusters k\nChoose k observations in the dataset. These locations in space are declared to be the initial centroids.\nAssign each observation to the nearest centroid.\nCompute the new centroids of each cluster (the mean of each measurement over all observations in the cluster).\nRepeat steps 3 and 4 until the centroids do not change."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-clustering-2",
    "href": "slides/BSMM_8740_lec_05.html#k-means-clustering-2",
    "title": "Classification & clustering methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nThere are three common methods for selecting initial centers:\n\nRandom observations: Chosing random observations to act as our initial centers is the most commonly used approach, implemented in the Forgy, Lloyd, and MacQueen methods.\nRandom partition: The observations are assigned to a cluster uniformly at random. The centroid of each cluster is computed, and these are used as the initial centers. This approach is implemented in the Hartigan-Wong method.\nk-means++: Beginning with one random set of the observations, further observations are sampled via probability-weighted sampling until \\(k\\) clusters are formed. The centroids of these clusters are used as the initial centers."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-clustering-3",
    "href": "slides/BSMM_8740_lec_05.html#k-means-clustering-3",
    "title": "Classification & clustering methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nBecause the initial conditions are based on random selection in both approaches, the k-means algorithm is not deterministic.\nRunning the clustering twice on the same data may not result in the same cluster assignments."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-example",
    "href": "slides/BSMM_8740_lec_05.html#k-means-example",
    "title": "Classification & clustering methods",
    "section": "K-means Example",
    "text": "K-means Example\n\n\nCode\n&gt; # create recipe for 2-D clustering\n&gt; cluster_recipe &lt;- data |&gt; \n+   recipes::recipe(~ x1 + x2, data = _)\n&gt; \n&gt; # specify the workflows\n&gt; all_workflows &lt;- \n+   workflowsets::workflow_set(\n+     preproc = list(base = cluster_recipe),\n+     models = list(tidyclust::k_means( num_clusters = parsnip::tune() ) )\n+   )\n&gt; # create bootstrap samples\n&gt; dat_resamples &lt;- data |&gt; rsample::bootstraps(apparent = TRUE)\n&gt; \n&gt; tuned_results &lt;-\n+    all_workflows |&gt; \n+    workflow_map(\n+       fn = \"tune_cluster\"\n+       , resamples = dat_resamples\n+       , grid = dials::grid_regular(dials::num_clusters(), levels = 10)\n+       , metrics = tidyclust::cluster_metric_set(sse_within_total, sse_total, sse_ratio)\n+       , control = tune::control_grid(save_pred = TRUE, extract = identity)\n+    )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-example-1",
    "href": "slides/BSMM_8740_lec_05.html#k-means-example-1",
    "title": "Classification & clustering methods",
    "section": "K-means Example",
    "text": "K-means Example\n\n\nCode\n&gt; set.seed(8740)\n&gt; \n&gt; centers &lt;- tibble::tibble(\n+   cluster = factor(1:4), \n+   num_points = c(100, 150, 50, 90),  # number points in each cluster\n+   x1 = c(5, 0, -3, -4),              # x1 coordinate of cluster center\n+   x2 = c(-1, 1, -2, 1.5),               # x2 coordinate of cluster center\n+ )\n&gt; \n&gt; labelled_points &lt;- \n+   centers |&gt;\n+   dplyr::mutate(\n+     x1 = purrr::map2(num_points, x1, rnorm),\n+     x2 = purrr::map2(num_points, x2, rnorm)\n+   ) |&gt; \n+   dplyr::select(-num_points) |&gt; \n+   tidyr::unnest(cols = c(x1, x2))\n&gt; \n&gt; p &lt;- ggplot(labelled_points, aes(x1, x2, color = cluster)) +\n+   geom_point(alpha = 0.3) + \n+   geom_point(data = centers, size = 10, shape = \"o\")\n&gt; p"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-example-2",
    "href": "slides/BSMM_8740_lec_05.html#k-means-example-2",
    "title": "Classification & clustering methods",
    "section": "K-means Example",
    "text": "K-means Example\n\n\nCode\n&gt; # create recipe\n&gt; labelled_points_recipe &lt;- labelled_points |&gt; \n+   recipes::recipe(~ x1 + x2, data = _)\n&gt; # create model spec\n&gt; kmeans_spec &lt;- tidyclust::k_means( num_clusters = 4 )\n&gt; # create workflow\n&gt; wflow &lt;- workflows::workflow() |&gt;\n+   workflows::add_model(kmeans_spec) |&gt;\n+   workflows::add_recipe(labelled_points_recipe)\n&gt; # fit workflow & extract centroids\n&gt; cluster_centers &lt;- wflow |&gt;\n+   parsnip::fit(labelled_points) %&gt;% tidyclust::extract_centroids() |&gt; \n+   dplyr::mutate( cluster = stringr::str_extract(.cluster,\"\\\\d\") )\n&gt; # plot\n&gt; p + geom_point(data = cluster_centers, size = 10, shape = \"x\")\n\n\n\n\n&gt; # all_workflows &lt;- all_workflows %&gt;% \n&gt; #   workflowsets::workflow_map(\n&gt; #     resamples = train_resamples, grid = 5, verbose = TRUE\n&gt; #   )\n\n\n&gt; # all_workflows &lt;- \n&gt; #   workflowsets::workflow_set(\n&gt; #     preproc = list(base = labelled_points_recipe),\n&gt; #     models = \n&gt; #       3:20 %&gt;% purrr::map( ~tidyclust::k_means( num_clusters = .x ) )\n&gt; #   )\n\n\n&gt; all_workflows &lt;- \n+   workflowsets::workflow_set(\n+     preproc = list(base = labelled_points_recipe),\n+     models = list(tidyclust::k_means( num_clusters = parsnip::tune() ) )\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#hierarchical-clustering",
    "href": "slides/BSMM_8740_lec_05.html#hierarchical-clustering",
    "title": "Classification & clustering methods",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical Clustering, sometimes called Agglomerative Clustering, is a method of unsupervised learning that produces a dendrogram, which can be used to partition observations into clusters (see tidyclust)\nFor other clustering algorithms, see\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nMean Shift Clustering\nSpectral Clustering"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#clustering-general-considerations",
    "href": "slides/BSMM_8740_lec_05.html#clustering-general-considerations",
    "title": "Classification & clustering methods",
    "section": "Clustering: General Considerations",
    "text": "Clustering: General Considerations\n\nFeature Scaling: Most clustering algorithms benefit from feature scaling.\nChoosing the Right Algorithm: Depends on the size, dimensionality of data, and the nature of the clusters.\nEvaluation: Since clustering is unsupervised, evaluating the results can be subjective and is often based on domain knowledge."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#more",
    "href": "slides/BSMM_8740_lec_05.html#more",
    "title": "Classification & clustering methods",
    "section": "More",
    "text": "More\n\nRead An Idiot’s Guide to Support Vector Machines"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#recap",
    "href": "slides/BSMM_8740_lec_05.html#recap",
    "title": "Classification & clustering methods",
    "section": "Recap",
    "text": "Recap\n\nWe have looked at several classification algorithms in the context of tidymodels workflows.\nWe also looked at clustering and several algorithms in the tidyclust package."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_04.html#recap-of-last-week",
    "title": "The Tidymodels Framework",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we looked at several regression models that are useful for predictions.\nHowever, R packages that implement these models have different conventions on how they accept data and specify the model.\nToday we look at the tidymodels package which will give us a workflow to describe, fit and compare models, using the same approach across methods."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\n\nTidymodels is a collection of R packages that provides a unified and consistent framework for modeling and machine learning tasks.\nIt is built on top of the tidyverse, making it easy to integrate with other tidyverse packages.\nTidymodels promotes best practices, repeatability, and clear documentation in your data analysis and modeling workflow."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-2",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-2",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nKey Components of Tidymodels\n\nModel Building: the parsnip package provides various modeling engines for different algorithms like lm(), glm(), randomForest(), xgboost(), etc.\nPreprocessing: Easy and flexible data preprocessing using the recipes package, allowing for seamless data transformation and feature engineering."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-3",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-3",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nKey Components of Tidymodels\n\nResampling: The rsample package supplies efficient methods for handling data splitting, cross-validation, bootstrapping, and more.\nMetrics: the yardstick package gives a wide range of evaluation metrics to assess model performance and choose the best model.\nTuning: the tune package facilitates hyperparameter tuning for the tidymodels packages."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-4",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-4",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nKey Components of Tidymodels\n\n\nWorkflows: functions in the workflows package can bundle together your pre-processing, modeling, and post-processing requests. The advantages are:\n\nYou don’t have to keep track of separate objects in your workspace.\nThe recipe prepping and model fitting can be executed using a single call to fit().\nIf you have custom tuning parameter settings, these can be defined using a simpler interface when combined with tune.\n\nWorkflowsets: The workflowsets package facilitates multiple workwflows - applying different types of models and preprocessing methods on a given data set."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-5",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-5",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nIn base R, the predict function returns results in a format that depends on the models.\nBy contrast, parsnip and workflows conforms to the following rules:\n\nThe results are always a tibble.\nThe column names of the tibble are always predictable.\nThere are always as many rows in the result tibble as there are in the input data set, and in the same order."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-6",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-6",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-parsnip",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-parsnip",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: parsnip",
    "text": "Tidymodels: parsnip\nWe’ve seen how the form of the arguments to linear models in R can be very different1.\nParsnip is one of the tidymodels packages that provides a standardized interface across models.\nWe look at how to fit and predict with parsnip in the next few slides, given data that has been preprocessed.\ne.g. stats::lm takes a formula, while glmnet::glmnet takes separate outcome and co-variate matrices"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nFor example we call stats::lm, specifying the model using a formula. By contrast glmnet::glmnet specify the model with separate outcome and co-variate matrices\nBy contrast, the tidymodels permits using both models with a uniform model specification."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-1",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-1",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\n\nSpecify the type of model based on its algorithm (e.g., linear regression, random forest, KNN, etc).\nSpecify the engine for fitting the model. Most often this reflects the software package and function that should be used, like lm or glmnet.\nWhen required, declare the mode of the model. The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-2",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-2",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nWith parsnip specifications are built without referencing the data:\n\nlmglmnet\n\n\n\n&gt; # basic linear model\n&gt; parsnip::linear_reg() %&gt;% \n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n\n&gt; # basic penalized linear model\n&gt; parsnip::linear_reg() %&gt;% \n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::set_engine(\"glmnet\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-3",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-3",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nThe translate function can be used to see how the parsnip spec is converted to the correct syntax for the underlying package / functions.\n\nlmglmnet\n\n\n\n\n&gt; parsnip::linear_reg() %&gt;% \n+   parsnip::set_engine(\"lm\") %&gt;% \n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\n\n\n\n\n\n&gt; parsnip::linear_reg(penalty = 1) %&gt;% \n+   parsnip::set_engine(\"glmnet\") %&gt;% \n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    family = \"gaussian\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-4",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-4",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nWe can specify the model with either a formula or outcome and model matrix:\n\n&gt; # prep data\n&gt; data_split &lt;- rsample::initial_split(modeldata::ames, strata = \"Sale_Price\")\n&gt; ames_train &lt;- rsample::training(data_split)\n&gt; ames_test  &lt;- rsample::testing(data_split)\n&gt; # spec model\n&gt; lm_model &lt;- parsnip::linear_reg() %&gt;%\n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::set_engine(\"lm\")\n&gt; # fit model\n&gt; lm_form_fit &lt;- lm_model %&gt;% \n+   # Recall that Sale_Price has been pre-logged\n+    parsnip::fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n&gt; # fit model with data in (x,y) form\n&gt; lm_xy_fit &lt;- \n+   lm_model %&gt;% parsnip::fit_xy(\n+     x = ames_train %&gt;% dplyr::select(Longitude, Latitude),\n+     y = ames_train %&gt;% dplyr::pull(Sale_Price)\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-5",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-5",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nModel results can be extracted from the fit object\n\n&gt; lm_form_fit %&gt;% parsnip::extract_fit_engine()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n -134223828      -810988      1390808  \n\n&gt; lm_form_fit %&gt;% parsnip::extract_fit_engine() %&gt;% stats::vcov()\n\n              (Intercept)    Longitude      Latitude\n(Intercept)  4.664768e+13 355373289509 -318058799276\nLongitude    3.553733e+11   3734037641    -135776456\nLatitude    -3.180588e+11   -135776456    7264101260"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-6",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-6",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nA list of all parsnip-type models can be found here."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflows",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflows",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflows",
    "text": "Tidymodels: workflows\nThe model workflow collects all the steps of the analysis, including any pre-processing steps, the specification of the model, the model fit itself, as well as potential post-processing activities.\nSimilar collections of steps are sometimes called pipelines."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-example",
    "href": "slides/BSMM_8740_lec_04.html#workflows-example",
    "title": "The Tidymodels Framework",
    "section": "Workflows example",
    "text": "Workflows example\nWorkflows always require a parsnip model object\n\n&gt; # create test/train splits\n&gt; ames &lt;- modeldata::ames %&gt;% dplyr::mutate( Sale_Price = log10(Sale_Price) )\n&gt; \n&gt; set.seed(502)\n&gt; ames_split &lt;- \n+   rsample::initial_split(\n+     ames, prop = 0.80, strata = \"Sale_Price\"\n+   )\n&gt; ames_train &lt;- rsample::training(ames_split)\n&gt; ames_test  &lt;- rsample::testing(ames_split)\n&gt; \n&gt; # Create a linear regression model\n&gt; lm_model &lt;- parsnip::linear_reg() %&gt;% \n+   parsnip::set_engine(\"lm\") \n&gt; \n&gt; # Create a workflow: adding a parsnip model\n&gt; lm_wflow &lt;- \n+   workflows::workflow() %&gt;% \n+   workflows::add_model(lm_model)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-example-1",
    "href": "slides/BSMM_8740_lec_04.html#workflows-example-1",
    "title": "The Tidymodels Framework",
    "section": "Workflows example",
    "text": "Workflows example\nIf our model is very simple, a standard R formula can be used as a preprocessor:\n\n\nCode\n&gt; # preprocessing not specified; a formula is sufficient\n&gt; lm_wflow %&lt;&gt;% \n+   workflows::add_formula(Sale_Price ~ Longitude + Latitude)\n&gt; # fit the model ( can be written as fit(lm_wflow, ames_train) )\n&gt; lm_fit &lt;- lm_wflow %&gt;% parsnip::fit(ames_train)\n&gt; # tidy up the fitted coefficients\n&gt; lm_fit %&gt;%\n+   # pull the parsnip object\n+   workflows::extract_fit_parsnip() %&gt;% \n+   # tidy up the fit results\n+   broom::tidy() %&gt;% \n+   # show the first n rows\n+   dplyr::slice_head(n=3)\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -303.      14.4       -21.0 3.64e-90\n2 Longitude      -2.07     0.129     -16.1 1.40e-55\n3 Latitude        2.71     0.180      15.0 9.29e-49"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-prediction",
    "href": "slides/BSMM_8740_lec_04.html#workflows-prediction",
    "title": "The Tidymodels Framework",
    "section": "Workflows prediction",
    "text": "Workflows prediction\nWhen using predict(workflow, new_data), no model or preprocessor parameters like those from recipes are re-estimated using the values in new_data.\nTake centering and scaling using step_normalize() as an example.\nUsing this step, the means and standard deviations from the appropriate columns are determined from the training set; new samples at prediction time are standardized using these values from training when predict() is invoked."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-prediction-1",
    "href": "slides/BSMM_8740_lec_04.html#workflows-prediction-1",
    "title": "The Tidymodels Framework",
    "section": "Workflows prediction",
    "text": "Workflows prediction\nThe fitted workflow can be used to predict outcomes given a new dataset of co-variates. Alternatively, the parsnip::augment function can be used to augment the new data with the prediction and other information about the prediction.\n\npredictaugment\n\n\n\n&gt; # predict on the fitted workflow\n&gt; lm_fit %&gt;% stats::predict(ames_test %&gt;% dplyr::slice(1:3))\n\n# A tibble: 3 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.22\n2  5.21\n3  5.28\n\n\n\n\n\n&gt; lm_fit |&gt; \n+   parsnip::augment(new_data = ames_test %&gt;% dplyr::slice(1:3)) |&gt; \n+   dplyr::select(starts_with(\".\"))\n\n# A tibble: 3 × 2\n  .pred   .resid\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  5.22 -0.202  \n2  5.21  0.174  \n3  5.28 -0.00629"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-updating",
    "href": "slides/BSMM_8740_lec_04.html#workflows-updating",
    "title": "The Tidymodels Framework",
    "section": "Workflows updating",
    "text": "Workflows updating\nThe model and data pre-processor can be removed or updated:\n\n&gt; # remove the formula and use add_variables instead\n&gt; lm_wflow %&lt;&gt;% \n+   workflows::remove_formula() %&gt;% \n+   workflows::add_variables(\n+     outcome = Sale_Price, predictors = c(Longitude, Latitude)\n+   )\n\nPredictors can be selected using tidyselect selectors, e.g. everything(), ends_with(“tude”), etc."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-use-of-formulas",
    "href": "slides/BSMM_8740_lec_04.html#workflows-use-of-formulas",
    "title": "The Tidymodels Framework",
    "section": "Workflows use of formulas",
    "text": "Workflows use of formulas\nWe’ve noted that R formulas can specify a good deal of preprocessing, including inline transformations and creating dummy variables, interactions and other column expansions. But some R packages extend the formula in ways that base R functions cannot parse or execute.\nWhen add_formula is executed, since preprocessing is model dependent, workflows attempts to emulate what the underlying model would do whenever possible."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-use-of-formulas-1",
    "href": "slides/BSMM_8740_lec_04.html#workflows-use-of-formulas-1",
    "title": "The Tidymodels Framework",
    "section": "Workflows use of formulas",
    "text": "Workflows use of formulas\nIf a random forest model is fit using the ranger or randomForest packages, the workflow knows predictor columns that are factors should be left as is (not converted to dummy vaiables).\nBy contrast, a boosted tree created with the xgboost package requires the user to create dummy variables from factor predictors (since xgboost::xgb.train() will not). A workflow using xgboost will create the indicator columns for this engine. Also note that a different engine for boosted trees, C5.0, does not require dummy variables so none are made by the workflow."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-special-formulas",
    "href": "slides/BSMM_8740_lec_04.html#workflows-special-formulas",
    "title": "The Tidymodels Framework",
    "section": "Workflows: special formulas",
    "text": "Workflows: special formulas\nSome packages have specilized formula specification, i.e. the lme4 package allows random effects per\nlme4::lmer(distance ~ Sex + (age | Subject), data = Orthodont)\nThe effect of this is that each subject will have an estimated intercept and slope parameter for age. Standard R methods can’t properly process this formula."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-special-formulas-1",
    "href": "slides/BSMM_8740_lec_04.html#workflows-special-formulas-1",
    "title": "The Tidymodels Framework",
    "section": "Workflows: special formulas",
    "text": "Workflows: special formulas\nIn this case The add_variables() specification provides the bare column names, and then the actual formula given to the model is set within add_model():\n\n&gt; multilevel_spec &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(\"lmer\")\n&gt; \n&gt; multilevel_workflow &lt;- \n+   workflows::workflow() %&gt;% \n+   # Pass the data along as-is: \n+   workflows::add_variables(outcome = distance, predictors = c(Sex, age, Subject)) %&gt;% \n+   workflows::add_model(multilevel_spec, \n+             # This formula is given to the model\n+             formula = distance ~ Sex + (age | Subject))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflowsets",
    "text": "Tidymodels: workflowsets\nCreating multiple workflows at once\nThe workflowset package creates combinations of workflow components.\nA list of preprocessors (e.g., formulas, dplyr selectors, or feature engineering recipe objects) can be combined (i.e. a crossproduct) with a list of model specifications, resulting in a set of workflows."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflowsets",
    "text": "Tidymodels: workflowsets\nCreate a set of preprocessors by formula\n\n&gt; # set up a list of formulas\n&gt; location &lt;- list(\n+   longitude = Sale_Price ~ Longitude,\n+   latitude = Sale_Price ~ Latitude,\n+   coords = Sale_Price ~ Longitude + Latitude,\n+   neighborhood = Sale_Price ~ Neighborhood\n+ )\n&gt; \n&gt; # create a workflowset\n&gt; location_models &lt;- \n+   workflowsets::workflow_set(\n+     preproc = location, models = list(lm = lm_model)\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-2",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-2",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflowsets",
    "text": "Tidymodels: workflowsets\nA workflow set is a data structure\n\n&gt; # view\n&gt; location_models\n\n# A workflow set/tibble: 4 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 longitude_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 latitude_lm     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 coords_lm       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 neighborhood_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nYou can extract the elements of the workflowset using tidy::unnest, dplyr::filter, etc. Alternatively the are a number of workflowsets::extract_X functions that will do the job, e.g.\nworkflowsets::extract_workflow(location_models, id = \"coords_lm\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-3",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-3",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflowsets",
    "text": "Tidymodels: workflowsets\nCreate model fits\n\n&gt; # create a new column (fit) by mapping fit \n&gt; # against the data in the info column\n&gt; location_models %&lt;&gt;%\n+    dplyr::mutate(\n+      fit = purrr::map(\n+        info\n+        , ~ parsnip::fit(.x$workflow[[1]], ames_train)\n+       )\n+    )\n&gt; \n&gt; # view\n&gt; location_models$fit[[1]] |&gt; broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -184.      12.6       -14.6 1.97e-46\n2 Longitude      -2.02     0.135     -15.0 7.01e-49"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-evaluation",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-evaluation",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: evaluation",
    "text": "Tidymodels: evaluation\n\nOnce we’ve settled on a final model there is a convenience function called last_fit() that will fit the model to the entire training set and evaluate it with the testing set. Notice that last_fit() takes a data split as an input, not a dataframe.\n\n&gt; # pull \n&gt; final_lm_res &lt;- tune::last_fit(lm_wflow, ames_split)\n\n\nmetricspredictions\n\n\n\n&gt; final_lm_res %&gt;% workflowsets::collect_metrics() \n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.164 Preprocessor1_Model1\n2 rsq     standard       0.189 Preprocessor1_Model1\n\n\n\n\n\n&gt; workflowsets::collect_predictions(final_lm_res) %&gt;% dplyr::slice(1:3)\n\n# A tibble: 3 × 5\n  .pred id                .row Sale_Price .config             \n  &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n1  5.22 train/test split     2       5.02 Preprocessor1_Model1\n2  5.21 train/test split     4       5.39 Preprocessor1_Model1\n3  5.28 train/test split     5       5.28 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nPerformance metrics and inference\nAn inferential model is used primarily to understand relationships, and typically emphasizes the choice (and validity) of probabilistic distributions and other generative qualities that define the model.\nFor a model used primarily for prediction, by contrast, predictive strength is of primary importance and other concerns about underlying statistical qualities may be less important."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nThe point of this analysis is to demonstrate the idea that optimization of statistical characteristics of the model does not imply that the model fits the data well.\nEven for purely inferential models, some measure of fidelity to the data should accompany the inferential results. With empirical validation, the users of the analyses can calibrate their expectations of the results."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-2",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-2",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\n\n\nCode\n&gt; ames &lt;- dplyr::mutate(modeldata::ames, Sale_Price = log10(Sale_Price))\n&gt; \n&gt; set.seed(502)\n&gt; ames_split &lt;- rsample::initial_split(ames, prop = 0.80, strata = Sale_Price)\n&gt; ames_train &lt;- rsample::training(ames_split)\n&gt; ames_test  &lt;- rsample::testing(ames_split)\n&gt; \n&gt; ames_rec &lt;- \n+   recipes::recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n+            Latitude + Longitude, data = ames_train) %&gt;%\n+   recipes::step_log(Gr_Liv_Area, base = 10) %&gt;% \n+   recipes::step_other(Neighborhood, threshold = 0.01) %&gt;% \n+   recipes::step_dummy(\n+     recipes::all_nominal_predictors()\n+   ) %&gt;% \n+   recipes::step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n+   recipes::step_ns(Latitude, Longitude, deg_free = 20)\n&gt;   \n&gt; lm_model &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(\"lm\")\n&gt; \n&gt; lm_wflow &lt;- \n+   workflows::workflow() %&gt;% \n+   workflows::add_model(lm_model) %&gt;% \n+   workflows::add_recipe(ames_rec)\n&gt; \n&gt; lm_fit &lt;- parsnip::fit(lm_wflow, ames_train)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-3",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-3",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics\n\nCode\n&gt; # fit with new data\n&gt; ames_test_res &lt;- \n+   stats::predict(\n+     lm_fit\n+     , new_data = ames_test %&gt;% dplyr::select(-Sale_Price)\n+   )\n&gt; ames_test_res\n&gt; # compare predictions with corresponding data\n&gt; ames_test_res &lt;- \n+   dplyr::bind_cols(\n+     ames_test_res, ames_test %&gt;% dplyr::select(Sale_Price))\n&gt; ames_test_res\n&gt; # there is a standard output format for yardstick functions\n&gt; yardstick::rmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n\n\n\n\n# A tibble: 588 × 1\n   .pred\n   &lt;dbl&gt;\n 1  5.22\n 2  5.21\n 3  5.28\n 4  5.27\n 5  5.28\n 6  5.28\n 7  5.26\n 8  5.26\n 9  5.26\n10  5.24\n# ℹ 578 more rows\n\n\n# A tibble: 588 × 2\n   .pred Sale_Price\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1  5.22       5.02\n 2  5.21       5.39\n 3  5.28       5.28\n 4  5.27       5.28\n 5  5.28       5.28\n 6  5.28       5.26\n 7  5.26       5.73\n 8  5.26       5.60\n 9  5.26       5.32\n10  5.24       4.98\n# ℹ 578 more rows\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.164"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-4",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-4",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multiple metrics at once\n\n&gt; # NOTE: yardstick::metric_set returns a function\n&gt; ames_metrics &lt;- \n+   yardstick::metric_set(\n+     yardstick::rmse\n+     , yardstick::rsq\n+     , yardstick::mae\n+   )\n&gt; \n&gt; ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.164\n2 rsq     standard       0.189\n3 mae     standard       0.124"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-5",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-5",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nClassification metrics: binary class targets\n\nconfusion mataccuracymccF-metric\n\n\n\n&gt; # compute the confusion matrix: \n&gt; yardstick::conf_mat(\n+   modeldata::two_class_example, truth = truth, estimate = predicted\n+ )\n\n          Truth\nPrediction Class1 Class2\n    Class1    227     50\n    Class2     31    192\n\n\n\n\n\n&gt; # compute the accuracy:\n&gt; yardstick::accuracy(\n+   modeldata::two_class_example, truth, predicted\n+ )\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838\n\n\n\n\n\n&gt; # Matthews correlation coefficient:\n&gt; yardstick::mcc(\n+   modeldata::two_class_example, truth, predicted\n+ )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mcc     binary         0.677\n\n\n\n\n\n&gt; # F1 metric: (The measure \"F\" is a combination of precision and recall )  \n&gt; yardstick::f_meas(modeldata::two_class_example, truth, predicted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  binary         0.849"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-6",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-6",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nClassification metrics: binary class targets\n\nCode\n&gt; # Combining three classification metrics together\n&gt; classification_metrics &lt;- \n+   yardstick::metric_set(\n+     yardstick::accuracy\n+     , yardstick::mcc\n+     , yardstick::f_meas\n+   )\n&gt; \n&gt; classification_metrics(\n+   modeldata::two_class_example\n+   , truth = truth\n+   , estimate = predicted\n+ )\n\n\n\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838\n2 mcc      binary         0.677\n3 f_meas   binary         0.849"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-7",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-7",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nClassification metrics: class probabilities\n\n\nCode\n&gt; two_class_curve &lt;- \n+   yardstick::roc_curve(\n+     modeldata::two_class_example\n+     , truth\n+     , Class1\n+   )\n&gt; \n&gt; parsnip::autoplot(two_class_curve) +\n+   labs(\n+     title = stringr::str_glue(\"roc_auc = {round(yardstick::roc_auc(modeldata::two_class_example, truth, Class1)[1,3],4)}\") \n+     , subtitle = \"There are other functions that use probability estimates, including gain_curve, lift_curve, and pr_curve.\"\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-8",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-8",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\n\nThe functions for metrics that use the discrete class predictions are identical to their binary counterparts.\nMetrics designed to handle outcomes with only two classes are extended for outcomes with more than two classes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-9",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-9",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\nTake sensitivity for example:\n\n\nMacro-averaging computes a set of one-versus-all metrics using the standard two-class statistics. These are averaged.\nMacro-weighted averaging does the same but the average is weighted by the number of samples in each class.\nMicro-averaging computes the contribution for each class, aggregates them, then computes a single metric from the aggregates."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-10",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-10",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\n\n&gt; modeldata::hpc_cv\n\n     obs pred           VF            F            M            L Resample\n1     VF   VF 9.136340e-01 7.786694e-02 8.479147e-03 1.991225e-05   Fold01\n2     VF   VF 9.380672e-01 5.710623e-02 4.816447e-03 1.011557e-05   Fold01\n3     VF   VF 9.473710e-01 4.946767e-02 3.156287e-03 4.999849e-06   Fold01\n4     VF   VF 9.289077e-01 6.528949e-02 5.787179e-03 1.564496e-05   Fold01\n5     VF   VF 9.418764e-01 5.430830e-02 3.808013e-03 7.294581e-06   Fold01\n6     VF   VF 9.510978e-01 4.618223e-02 2.716177e-03 3.841455e-06   Fold01\n7     VF   VF 9.141380e-01 7.815186e-02 7.674706e-03 3.540712e-05   Fold01\n8     VF   VF 9.183508e-01 7.437284e-02 7.260583e-03 1.572869e-05   Fold01\n9     VF   VF 8.426395e-01 1.276057e-01 2.956273e-02 1.921091e-04   Fold01\n10    VF   VF 9.201676e-01 7.278837e-02 7.029299e-03 1.470748e-05   Fold01\n11    VF    F 3.761772e-01 5.456339e-01 7.679576e-02 1.393113e-03   Fold01\n12    VF    F 3.885041e-01 5.088634e-01 1.014177e-01 1.214754e-03   Fold01\n13    VF   VF 4.947838e-01 4.567374e-01 4.809350e-02 3.853588e-04   Fold01\n14    VF   VF 8.805588e-01 9.875896e-02 2.062370e-02 5.849500e-05   Fold01\n15    VF   VF 9.143142e-01 7.116937e-02 1.449491e-02 2.150462e-05   Fold01\n16    VF   VF 8.299298e-01 1.315213e-01 3.845993e-02 8.899835e-05   Fold01\n17    VF   VF 9.172870e-01 7.200110e-02 1.068621e-02 2.567282e-05   Fold01\n18    VF   VF 9.133611e-01 7.247206e-02 1.412105e-02 4.578425e-05   Fold01\n19    VF   VF 9.388037e-01 5.393117e-02 7.258208e-03 6.908569e-06   Fold01\n20    VF   VF 9.842818e-01 1.496544e-02 7.517025e-04 1.017422e-06   Fold01\n21    VF   VF 9.837444e-01 1.555593e-02 6.986590e-04 1.004350e-06   Fold01\n22    VF   VF 9.839162e-01 1.530611e-02 7.766103e-04 1.089092e-06   Fold01\n23    VF   VF 9.868004e-01 1.262830e-02 5.707369e-04 5.733360e-07   Fold01\n24    VF   VF 9.774908e-01 2.096118e-02 1.545566e-03 2.429809e-06   Fold01\n25    VF   VF 9.882188e-01 1.136615e-02 4.146959e-04 3.498133e-07   Fold01\n26    VF   VF 9.886027e-01 1.092521e-02 4.718386e-04 2.975579e-07   Fold01\n27    VF   VF 9.752615e-01 2.307828e-02 1.657259e-03 2.961357e-06   Fold01\n28    VF   VF 9.824464e-01 1.657506e-02 9.769593e-04 1.559296e-06   Fold01\n29    VF   VF 9.884003e-01 1.115648e-02 4.428435e-04 3.642457e-07   Fold01\n30    VF   VF 9.324751e-01 5.554159e-02 1.127280e-02 7.104905e-04   Fold01\n31    VF   VF 9.788454e-01 1.986417e-02 1.287635e-03 2.808330e-06   Fold01\n32    VF   VF 9.854944e-01 1.386496e-02 6.398649e-04 7.660727e-07   Fold01\n33    VF   VF 9.868995e-01 1.237899e-02 7.209161e-04 5.714906e-07   Fold01\n34    VF   VF 9.830121e-01 1.625257e-02 7.339278e-04 1.400940e-06   Fold01\n35    VF   VF 9.866763e-01 1.255464e-02 7.684679e-04 6.264748e-07   Fold01\n36    VF   VF 6.362303e-01 3.202032e-01 4.324050e-02 3.260450e-04   Fold01\n37    VF   VF 5.988413e-01 3.412932e-01 5.919116e-02 6.742845e-04   Fold01\n38    VF   VF 6.696001e-01 2.988741e-01 3.129559e-02 2.301224e-04   Fold01\n39    VF   VF 5.583367e-01 3.786000e-01 6.252555e-02 5.377030e-04   Fold01\n40    VF   VF 6.572574e-01 3.050531e-01 3.741023e-02 2.792341e-04   Fold01\n41    VF   VF 7.610149e-01 2.024816e-01 3.644132e-02 6.214494e-05   Fold01\n42    VF   VF 8.316829e-01 1.550445e-01 1.325248e-02 2.011195e-05   Fold01\n43    VF   VF 8.032623e-01 1.756865e-01 2.102688e-02 2.424176e-05   Fold01\n44    VF   VF 8.634132e-01 1.266075e-01 9.969700e-03 9.634219e-06   Fold01\n45    VF    F 8.783413e-02 5.539494e-01 3.493844e-01 8.832123e-03   Fold01\n46    VF    F 1.612212e-01 5.930725e-01 2.441203e-01 1.585951e-03   Fold01\n47    VF    F 8.451150e-02 5.770391e-01 3.307289e-01 7.720507e-03   Fold01\n48    VF   VF 9.875622e-01 1.213416e-02 3.034030e-04 2.158980e-07   Fold01\n49    VF   VF 9.769174e-01 2.217658e-02 9.050344e-04 1.032298e-06   Fold01\n50    VF   VF 9.801830e-01 1.915377e-02 6.624589e-04 7.661263e-07   Fold01\n51    VF   VF 9.575459e-01 4.054433e-02 1.904713e-03 5.046420e-06   Fold01\n52    VF   VF 9.872374e-01 1.242886e-02 3.334950e-04 2.386789e-07   Fold01\n53    VF   VF 9.596211e-01 3.815929e-02 2.216034e-03 3.592323e-06   Fold01\n54    VF   VF 9.267618e-01 6.894315e-02 4.283164e-03 1.189249e-05   Fold01\n55    VF   VF 9.264446e-01 6.921441e-02 4.329014e-03 1.200612e-05   Fold01\n56    VF   VF 9.263946e-01 6.933977e-02 4.253789e-03 1.180854e-05   Fold01\n57    VF   VF 9.887127e-01 1.102153e-02 2.655891e-04 1.851411e-07   Fold01\n58    VF   VF 9.707098e-01 2.785877e-02 1.429527e-03 1.951670e-06   Fold01\n59    VF   VF 9.355224e-01 6.101654e-02 3.453754e-03 7.340268e-06   Fold01\n60    VF   VF 8.982292e-01 9.129933e-02 1.044626e-02 2.516838e-05   Fold01\n61    VF   VF 9.908595e-01 8.875058e-03 2.652782e-04 1.450390e-07   Fold01\n62    VF   VF 9.914449e-01 8.367227e-03 1.878131e-04 7.722762e-08   Fold01\n63    VF   VF 8.711986e-01 1.026056e-01 2.552014e-02 6.756286e-04   Fold01\n64    VF   VF 9.758033e-01 2.327581e-02 9.199892e-04 8.851335e-07   Fold01\n65    VF   VF 5.337455e-01 2.703335e-01 1.807857e-01 1.513533e-02   Fold01\n66    VF   VF 5.977373e-01 2.730663e-01 1.204233e-01 8.773181e-03   Fold01\n67    VF    F 3.124458e-01 3.388463e-01 3.068530e-01 4.185484e-02   Fold01\n68    VF   VF 9.123433e-01 8.108853e-02 6.544284e-03 2.387595e-05   Fold01\n69    VF   VF 9.159462e-01 7.509021e-02 8.055620e-03 9.079857e-04   Fold01\n70    VF   VF 9.141032e-01 7.661989e-02 8.321049e-03 9.558380e-04   Fold01\n71    VF   VF 9.714111e-01 2.684255e-02 1.744117e-03 2.260981e-06   Fold01\n72    VF   VF 9.816820e-01 1.748758e-02 8.295249e-04 8.487934e-07   Fold01\n73    VF   VF 9.816855e-01 1.750611e-02 8.075625e-04 8.407729e-07   Fold01\n74    VF   VF 9.523548e-01 4.447296e-02 3.164154e-03 8.068876e-06   Fold01\n75    VF   VF 9.721119e-01 2.666614e-02 1.218474e-03 3.508354e-06   Fold01\n76    VF   VF 9.745002e-01 2.449543e-02 1.002777e-03 1.591891e-06   Fold01\n77    VF   VF 7.986738e-01 1.774858e-01 2.365189e-02 1.885431e-04   Fold01\n78    VF   VF 9.885608e-01 1.120334e-02 2.356735e-04 1.522772e-07   Fold01\n79    VF   VF 9.889737e-01 1.078012e-02 2.460542e-04 1.529213e-07   Fold01\n80    VF   VF 9.713789e-01 2.745702e-02 1.160417e-03 3.632274e-06   Fold01\n81    VF   VF 9.753267e-01 2.362954e-02 1.041529e-03 2.191413e-06   Fold01\n82    VF   VF 9.703061e-01 2.831243e-02 1.376846e-03 4.613122e-06   Fold01\n83    VF   VF 9.917060e-01 8.057310e-03 2.365507e-04 1.091971e-07   Fold01\n84    VF   VF 9.424027e-01 5.468923e-02 2.901550e-03 6.549774e-06   Fold01\n85    VF   VF 8.921387e-01 1.034190e-01 4.433228e-03 9.138659e-06   Fold01\n86    VF   VF 9.110022e-01 8.566141e-02 3.331579e-03 4.802896e-06   Fold01\n87    VF   VF 8.538234e-01 1.371248e-01 9.031568e-03 2.019706e-05   Fold01\n88    VF   VF 9.144959e-01 8.145326e-02 4.047646e-03 3.239909e-06   Fold01\n89    VF   VF 8.994292e-01 9.702336e-02 3.541015e-03 6.409939e-06   Fold01\n90    VF   VF 6.351969e-01 3.581087e-01 6.668649e-03 2.581245e-05   Fold01\n91    VF   VF 8.717157e-01 1.227939e-01 5.471967e-03 1.840272e-05   Fold01\n92    VF   VF 8.863817e-01 1.091790e-01 4.429300e-03 9.980779e-06   Fold01\n93    VF   VF 8.859842e-01 1.095496e-01 4.456089e-03 1.010652e-05   Fold01\n94    VF   VF 9.150898e-01 8.239322e-02 2.513611e-03 3.372488e-06   Fold01\n95    VF   VF 9.030168e-01 9.319102e-02 3.786256e-03 5.949777e-06   Fold01\n96    VF   VF 8.862154e-01 1.078361e-01 5.935736e-03 1.275886e-05   Fold01\n97    VF   VF 8.468075e-01 1.450910e-01 8.070122e-03 3.139134e-05   Fold01\n98    VF   VF 9.028579e-01 9.387851e-02 3.258046e-03 5.581293e-06   Fold01\n99    VF   VF 8.494848e-01 1.406018e-01 9.889040e-03 2.435532e-05   Fold01\n100   VF   VF 8.980186e-01 9.765205e-02 4.321778e-03 7.525800e-06   Fold01\n101   VF   VF 6.215213e-01 3.707613e-01 7.685053e-03 3.234477e-05   Fold01\n102   VF   VF 8.845148e-01 1.082052e-01 7.268204e-03 1.182380e-05   Fold01\n103   VF   VF 8.340881e-01 1.562953e-01 9.589948e-03 2.659680e-05   Fold01\n104   VF   VF 6.175360e-01 3.749005e-01 7.530447e-03 3.308563e-05   Fold01\n105   VF   VF 8.400409e-01 1.494389e-01 1.049174e-02 2.843310e-05   Fold01\n106   VF   VF 8.324296e-01 1.564529e-01 1.108633e-02 3.120914e-05   Fold01\n107   VF   VF 8.790226e-01 1.144739e-01 6.487872e-03 1.564006e-05   Fold01\n108   VF   VF 8.429517e-01 1.466936e-01 1.032700e-02 2.778209e-05   Fold01\n109   VF   VF 6.114596e-01 3.805150e-01 7.988716e-03 3.665608e-05   Fold01\n110   VF   VF 8.808949e-01 1.132671e-01 5.823290e-03 1.468807e-05   Fold01\n111   VF   VF 8.993866e-01 9.677692e-02 3.829533e-03 6.986108e-06   Fold01\n112   VF   VF 8.394917e-01 1.491751e-01 1.130168e-02 3.156773e-05   Fold01\n113   VF   VF 5.306079e-01 3.738703e-01 8.239941e-02 1.312247e-02   Fold01\n114   VF   VF 8.875119e-01 1.080252e-01 4.450524e-03 1.238820e-05   Fold01\n115   VF   VF 8.847947e-01 1.097347e-01 5.458649e-03 1.196155e-05   Fold01\n116   VF   VF 6.001405e-01 3.897366e-01 1.007204e-02 5.088708e-05   Fold01\n117   VF   VF 5.917984e-01 3.975205e-01 1.062426e-02 5.685470e-05   Fold01\n118   VF   VF 8.965044e-01 9.936515e-02 4.122732e-03 7.722178e-06   Fold01\n119   VF   VF 8.679495e-01 1.239747e-01 8.051185e-03 2.467113e-05   Fold01\n120   VF   VF 5.817390e-01 4.068520e-01 1.134381e-02 6.512504e-05   Fold01\n121   VF   VF 5.737648e-01 4.142328e-01 1.193005e-02 7.238104e-05   Fold01\n122   VF   VF 8.404990e-01 1.496568e-01 9.792677e-03 5.155171e-05   Fold01\n123   VF   VF 5.676901e-01 4.198411e-01 1.239048e-02 7.828540e-05   Fold01\n124   VF   VF 5.680875e-01 4.194708e-01 1.236376e-02 7.796300e-05   Fold01\n125   VF   VF 8.981045e-01 9.741964e-02 4.467601e-03 8.236929e-06   Fold01\n126   VF   VF 8.845382e-01 1.106182e-01 4.829816e-03 1.377258e-05   Fold01\n127   VF   VF 7.962954e-01 1.865537e-01 1.707704e-02 7.380697e-05   Fold01\n128   VF   VF 9.150219e-01 8.186970e-02 3.104184e-03 4.199818e-06   Fold01\n129   VF   VF 8.609217e-01 1.309869e-01 8.074670e-03 1.676395e-05   Fold01\n130   VF   VF 9.115107e-01 8.528893e-02 3.195881e-03 4.510884e-06   Fold01\n131   VF   VF 9.102747e-01 8.641357e-02 3.306981e-03 4.710561e-06   Fold01\n132   VF   VF 9.002321e-01 9.466994e-02 5.091202e-03 6.807249e-06   Fold01\n133   VF   VF 9.004551e-01 9.446465e-02 5.073455e-03 6.745023e-06   Fold01\n134   VF   VF 8.967222e-01 9.756846e-02 5.701562e-03 7.811409e-06   Fold01\n135   VF   VF 8.591927e-01 1.327091e-01 8.080800e-03 1.743518e-05   Fold01\n136   VF   VF 9.122513e-01 8.446578e-02 3.278240e-03 4.701450e-06   Fold01\n137   VF   VF 9.213432e-01 7.709308e-02 1.546999e-03 1.675097e-05   Fold01\n138   VF   VF 8.493043e-01 1.407255e-01 9.947932e-03 2.229406e-05   Fold01\n139   VF   VF 9.586919e-01 4.027547e-02 1.031713e-03 9.598498e-07   Fold01\n140   VF   VF 8.955489e-01 9.334821e-02 1.109101e-02 1.190543e-05   Fold01\n141   VF   VF 9.662497e-01 3.233910e-02 1.409680e-03 1.510032e-06   Fold01\n142   VF   VF 9.798559e-01 1.954576e-02 5.980660e-04 2.505611e-07   Fold01\n143   VF   VF 9.287899e-01 6.621300e-02 4.991993e-03 5.120976e-06   Fold01\n144   VF   VF 9.661810e-01 3.237346e-02 1.444697e-03 8.581622e-07   Fold01\n145   VF   VF 9.786443e-01 2.054377e-02 8.110262e-04 8.725714e-07   Fold01\n146   VF    F 1.187766e-01 7.771076e-01 1.037824e-01 3.333928e-04   Fold01\n147   VF    F 1.096314e-01 7.856875e-01 1.043047e-01 3.764069e-04   Fold01\n148   VF    F 1.304592e-01 7.820385e-01 8.725116e-02 2.510808e-04   Fold01\n149   VF    F 1.185873e-01 7.503935e-01 1.306802e-01 3.390273e-04   Fold01\n150   VF   VF 8.049013e-01 1.833819e-01 1.170656e-02 1.028389e-05   Fold01\n151   VF   VF 6.350021e-01 3.112810e-01 5.360323e-02 1.135919e-04   Fold01\n152   VF   VF 6.318738e-01 3.133561e-01 5.465184e-02 1.182631e-04   Fold01\n153   VF   VF 7.389381e-01 2.424602e-01 1.856696e-02 3.474295e-05   Fold01\n154   VF   VF 7.316738e-01 2.492195e-01 1.906803e-02 3.867170e-05   Fold01\n155   VF   VF 7.275003e-01 2.508938e-01 2.156650e-02 3.939398e-05   Fold01\n156   VF   VF 7.379694e-01 2.399291e-01 2.208295e-02 1.851327e-05   Fold01\n157   VF   VF 7.454390e-01 2.322463e-01 2.228534e-02 2.939182e-05   Fold01\n158   VF   VF 7.222103e-01 2.557280e-01 2.202501e-02 3.667674e-05   Fold01\n159   VF   VF 7.419233e-01 2.399265e-01 1.812666e-02 2.352907e-05   Fold01\n160   VF   VF 7.084196e-01 2.661835e-01 2.533445e-02 6.253189e-05   Fold01\n161   VF   VF 7.726722e-01 2.090576e-01 1.825533e-02 1.487307e-05   Fold01\n162   VF   VF 7.411961e-01 2.391737e-01 1.960385e-02 2.638083e-05   Fold01\n163   VF   VF 7.124028e-01 2.565346e-01 3.102221e-02 4.036243e-05   Fold01\n164   VF   VF 7.192077e-01 2.579253e-01 2.282319e-02 4.377537e-05   Fold01\n165   VF   VF 7.418596e-01 2.318153e-01 2.629117e-02 3.394051e-05   Fold01\n166   VF   VF 6.079875e-01 3.464456e-01 4.545964e-02 1.072807e-04   Fold01\n167   VF   VF 7.392631e-01 2.395576e-01 2.114679e-02 3.245415e-05   Fold01\n168   VF   VF 7.704830e-01 2.063473e-01 2.315137e-02 1.834767e-05   Fold01\n169   VF   VF 7.681594e-01 2.166717e-01 1.515070e-02 1.821612e-05   Fold01\n170   VF   VF 6.903768e-01 2.677582e-01 4.179722e-02 6.776893e-05   Fold01\n171   VF   VF 7.123066e-01 2.633203e-01 2.432734e-02 4.574183e-05   Fold01\n172   VF   VF 5.790094e-01 3.656105e-01 5.521148e-02 1.686258e-04   Fold01\n173   VF   VF 6.836925e-01 2.833857e-01 3.283596e-02 8.583775e-05   Fold01\n174   VF   VF 6.977413e-01 2.763481e-01 2.587546e-02 3.515187e-05   Fold01\n175   VF   VF 7.131733e-01 2.590263e-01 2.776507e-02 3.537681e-05   Fold01\n176   VF   VF 7.268013e-01 2.508101e-01 2.236202e-02 2.659466e-05   Fold01\n177   VF    F 9.643483e-02 7.878652e-01 1.130202e-01 2.679799e-03   Fold01\n178    F    F 3.085202e-01 5.743957e-01 1.152658e-01 1.818341e-03   Fold01\n179    F    L 3.629908e-03 8.967472e-02 2.747283e-01 6.319671e-01   Fold01\n180    F   VF 8.162113e-01 1.384098e-01 4.487402e-02 5.048867e-04   Fold01\n181    F   VF 8.054500e-01 1.443777e-01 4.980389e-02 3.683336e-04   Fold01\n182    F    F 2.425761e-01 4.550394e-01 2.182175e-01 8.416698e-02   Fold01\n183    F    F 1.799199e-01 4.485544e-01 1.949528e-01 1.765729e-01   Fold01\n184    F   VF 7.184922e-01 2.430724e-01 3.828628e-02 1.490647e-04   Fold01\n185    F   VF 7.870749e-01 1.899762e-01 2.291767e-02 3.117119e-05   Fold01\n186    F   VF 7.924986e-01 1.864006e-01 2.107759e-02 2.318856e-05   Fold01\n187    F   VF 8.838839e-01 1.090490e-01 7.062073e-03 4.941168e-06   Fold01\n188    F    F 1.316681e-01 6.202153e-01 2.452239e-01 2.892700e-03   Fold01\n189    F    F 1.265108e-01 6.096967e-01 2.608381e-01 2.954476e-03   Fold01\n190    F    M 1.456121e-02 3.262319e-01 5.954977e-01 6.370922e-02   Fold01\n191    F    F 1.590519e-01 6.312172e-01 2.078950e-01 1.835944e-03   Fold01\n192    F    F 1.092097e-01 5.990014e-01 2.881367e-01 3.652271e-03   Fold01\n193    F    M 3.138967e-03 1.425370e-01 5.875186e-01 2.668054e-01   Fold01\n194    F    F 9.425470e-02 5.827994e-01 3.182536e-01 4.692287e-03   Fold01\n195    F    F 1.854731e-01 6.403700e-01 1.729089e-01 1.248068e-03   Fold01\n196    F    F 1.051158e-01 6.099695e-01 2.817733e-01 3.141422e-03   Fold01\n197    F    F 1.514262e-01 6.238219e-01 2.225621e-01 2.189807e-03   Fold01\n198    F    F 1.515560e-01 6.264246e-01 2.198439e-01 2.175476e-03   Fold01\n199    F    F 1.066972e-01 6.181348e-01 2.707183e-01 4.449714e-03   Fold01\n200    F    F 8.135952e-02 6.016845e-01 3.097005e-01 7.255453e-03   Fold01\n201    F    F 6.759671e-02 5.726734e-01 3.469888e-01 1.274106e-02   Fold01\n202    F    F 1.744948e-01 6.318562e-01 1.921652e-01 1.483871e-03   Fold01\n203    F    M 4.319906e-02 4.659033e-01 4.751903e-01 1.570737e-02   Fold01\n204    F   VF 5.835503e-01 2.489801e-01 1.272985e-01 4.017110e-02   Fold01\n205    F   VF 8.712551e-01 1.207056e-01 8.016429e-03 2.277681e-05   Fold01\n206    F   VF 8.754459e-01 1.174705e-01 7.066784e-03 1.679573e-05   Fold01\n207    F   VF 8.832072e-01 1.109875e-01 5.791971e-03 1.335235e-05   Fold01\n208    F   VF 8.674563e-01 1.248704e-01 7.650685e-03 2.262082e-05   Fold01\n209    F   VF 8.182991e-01 1.681319e-01 1.352264e-02 4.635748e-05   Fold01\n210    F   VF 8.746294e-01 1.167866e-01 8.565726e-03 1.822287e-05   Fold01\n211    F   VF 8.317975e-01 1.577410e-01 1.040307e-02 5.841863e-05   Fold01\n212    F   VF 5.595845e-01 4.291561e-01 1.118578e-02 7.356257e-05   Fold01\n213    F   VF 5.531884e-01 4.350879e-01 1.164363e-02 8.003413e-05   Fold01\n214    F   VF 7.879990e-01 1.939855e-01 1.790277e-02 1.128042e-04   Fold01\n215    F   VF 8.673445e-01 1.266219e-01 6.013753e-03 1.984644e-05   Fold01\n216    F   VF 8.910100e-01 9.462423e-02 1.431747e-02 4.831503e-05   Fold01\n217    F   VF 8.264881e-01 1.394620e-01 3.380100e-02 2.489328e-04   Fold01\n218    F    F 1.222172e-01 5.260073e-01 3.462782e-01 5.497361e-03   Fold01\n219    F   VF 4.609566e-01 3.313034e-01 1.988226e-01 8.917323e-03   Fold01\n220    F   VF 8.762492e-01 1.116796e-01 1.204600e-02 2.521629e-05   Fold01\n221    F    F 1.374266e-01 3.954473e-01 3.753199e-01 9.180629e-02   Fold01\n222    F   VF 4.450757e-01 2.892204e-01 2.310014e-01 3.470251e-02   Fold01\n223    F    F 9.929779e-02 7.870347e-01 1.131600e-01 5.074847e-04   Fold01\n224    F    F 7.948567e-02 7.737098e-01 1.458932e-01 9.113317e-04   Fold01\n225    F    F 9.076328e-02 7.391177e-01 1.695726e-01 5.464904e-04   Fold01\n226    F    F 8.762716e-02 7.817623e-01 1.299781e-01 6.324745e-04   Fold01\n227    F    F 1.221888e-01 7.542483e-01 1.233339e-01 2.289787e-04   Fold01\n228    F    F 1.092334e-01 7.787982e-01 1.115848e-01 3.836295e-04   Fold01\n229    F    F 1.016567e-01 7.833472e-01 1.145654e-01 4.306604e-04   Fold01\n230    F    F 6.481820e-02 7.547046e-01 1.795216e-01 9.556250e-04   Fold01\n231    F    F 4.099488e-02 7.390468e-01 2.114576e-01 8.500737e-03   Fold01\n232    F    F 7.790251e-02 7.128219e-01 2.086333e-01 6.422048e-04   Fold01\n233    F    F 6.050997e-02 7.566477e-01 1.817530e-01 1.089333e-03   Fold01\n234    F    F 1.061616e-01 7.776771e-01 1.157066e-01 4.546342e-04   Fold01\n235    F    F 1.049795e-01 7.788066e-01 1.157522e-01 4.618084e-04   Fold01\n236    F    F 8.347306e-02 7.797370e-01 1.361209e-01 6.690213e-04   Fold01\n237    F    F 1.109299e-01 7.482205e-01 1.404773e-01 3.722136e-04   Fold01\n238    F    F 8.548400e-02 7.173201e-01 1.963948e-01 8.010348e-04   Fold01\n239    F    F 9.210791e-02 8.235494e-01 7.913478e-02 5.207895e-03   Fold01\n240    F    F 6.025764e-02 7.622342e-01 1.754322e-01 2.076004e-03   Fold01\n241    F    F 8.993485e-02 7.874067e-01 1.218853e-01 7.730759e-04   Fold01\n242    F    F 1.224717e-01 8.203716e-01 5.499967e-02 2.156998e-03   Fold01\n243    F    F 1.077506e-01 7.769704e-01 1.149425e-01 3.365003e-04   Fold01\n244    F    F 1.139636e-01 7.919489e-01 9.378241e-02 3.051207e-04   Fold01\n245    F    F 1.198684e-01 7.805807e-01 9.924446e-02 3.064149e-04   Fold01\n246    F   VF 7.392725e-01 2.423529e-01 1.833872e-02 3.594056e-05   Fold01\n247    F   VF 8.329427e-01 1.588675e-01 8.185231e-03 4.556007e-06   Fold01\n248    F   VF 7.410474e-01 2.304667e-01 2.845997e-02 2.585193e-05   Fold01\n249    F    F 4.301165e-01 4.701842e-01 9.325885e-02 6.440513e-03   Fold01\n250    F    F 4.323225e-01 4.691488e-01 9.224870e-02 6.280041e-03   Fold01\n251    F    F 4.302306e-01 4.701662e-01 9.318210e-02 6.421094e-03   Fold01\n252    F   VF 6.814849e-01 2.844926e-01 3.394474e-02 7.779644e-05   Fold01\n253    F    F 3.830151e-01 4.541171e-01 1.534624e-01 9.405432e-03   Fold01\n254    F    F 3.034164e-01 4.998574e-01 1.795911e-01 1.713518e-02   Fold01\n255    F   VF 6.972304e-01 2.702410e-01 3.247484e-02 5.377830e-05   Fold01\n256    F   VF 6.471921e-01 3.147512e-01 3.791467e-02 1.420286e-04   Fold01\n257    F   VF 6.353334e-01 3.256375e-01 3.887164e-02 1.575544e-04   Fold01\n258    F   VF 5.979454e-01 3.548225e-01 4.702182e-02 2.102580e-04   Fold01\n259    F   VF 6.665615e-01 3.029924e-01 3.040042e-02 4.567723e-05   Fold01\n260    F   VF 5.943269e-01 3.481596e-01 5.732233e-02 1.910942e-04   Fold01\n261    F    F 7.759463e-02 7.482968e-01 1.692793e-01 4.829242e-03   Fold01\n262    F    F 7.869524e-02 7.888136e-01 1.275408e-01 4.950419e-03   Fold01\n263    F    F 6.928254e-02 7.320389e-01 1.921349e-01 6.543636e-03   Fold01\n264    F    F 6.977566e-02 7.347491e-01 1.889911e-01 6.484129e-03   Fold01\n265    F    F 6.978968e-02 7.325013e-01 1.911785e-01 6.530551e-03   Fold01\n266    F    F 1.165881e-02 4.198061e-01 1.863739e-01 3.821611e-01   Fold01\n267    F    F 1.179715e-02 4.224329e-01 1.866518e-01 3.791182e-01   Fold01\n268    F    F 6.879429e-02 7.335038e-01 1.910055e-01 6.696364e-03   Fold01\n269    F    F 3.922435e-02 7.348875e-01 1.862365e-01 3.965173e-02   Fold01\n270    F    F 6.336183e-02 7.717532e-01 1.563192e-01 8.565765e-03   Fold01\n271    F    F 6.358290e-02 7.799444e-01 1.483085e-01 8.164159e-03   Fold01\n272    F    F 7.616875e-02 7.848653e-01 1.335304e-01 5.435470e-03   Fold01\n273    F    F 6.758068e-02 7.285866e-01 1.968608e-01 6.971862e-03   Fold01\n274    F    F 6.678626e-02 7.300235e-01 1.960342e-01 7.156119e-03   Fold01\n275    F    F 9.341467e-02 7.597267e-01 1.435434e-01 3.315189e-03   Fold01\n276    F    F 6.065368e-02 7.718283e-01 1.581569e-01 9.361049e-03   Fold01\n277    F    F 6.003814e-02 7.657586e-01 1.644387e-01 9.764570e-03   Fold01\n278    F    F 2.474163e-02 6.413660e-01 2.278819e-01 1.060104e-01   Fold01\n279    F    F 1.585978e-02 5.379014e-01 3.480197e-01 9.821916e-02   Fold01\n280    F    F 7.277590e-02 7.835962e-01 1.375826e-01 6.045321e-03   Fold01\n281    F    F 4.199578e-02 7.310529e-01 2.122579e-01 1.469349e-02   Fold01\n282    F    F 4.763636e-02 7.584089e-01 1.751642e-01 1.879050e-02   Fold01\n283    F    F 7.644313e-02 7.928165e-01 1.241020e-01 6.638411e-03   Fold01\n284    F    F 9.255571e-02 7.518003e-01 1.520139e-01 3.630120e-03   Fold01\n285    F    F 3.599022e-02 7.116566e-01 2.165581e-01 3.579506e-02   Fold01\n286    M    F 3.656146e-01 5.590954e-01 7.413402e-02 1.156016e-03   Fold01\n287    M   VF 5.046640e-01 2.059424e-01 2.784793e-01 1.091432e-02   Fold01\n288    M    M 2.557471e-01 1.845101e-01 4.788544e-01 8.088831e-02   Fold01\n289    M    M 9.554212e-02 1.441559e-01 5.819675e-01 1.783344e-01   Fold01\n290    M    M 1.048358e-01 1.434331e-01 5.806475e-01 1.710836e-01   Fold01\n291    M    F 3.411136e-01 5.263217e-01 1.303257e-01 2.239069e-03   Fold01\n292    M   VF 4.395510e-01 4.173478e-01 1.409447e-01 2.156456e-03   Fold01\n293    M    F 1.903181e-01 6.431888e-01 1.656219e-01 8.712275e-04   Fold01\n294    M    L 2.324416e-04 2.496363e-02 3.128318e-01 6.619722e-01   Fold01\n295    M    L 1.035633e-04 1.678893e-02 2.191687e-01 7.639388e-01   Fold01\n296    M   VF 7.472499e-01 2.260186e-01 2.644872e-02 2.828239e-04   Fold01\n297    M    L 2.836057e-03 1.726291e-02 2.014179e-02 9.597593e-01   Fold01\n298    M    F 7.382566e-02 7.554666e-01 1.700694e-01 6.383945e-04   Fold01\n299    M    M 1.087517e-02 4.594597e-01 4.745927e-01 5.507241e-02   Fold01\n300    M    F 5.319387e-02 7.528510e-01 1.927023e-01 1.252863e-03   Fold01\n301    M    F 9.919670e-03 5.064792e-01 3.540276e-01 1.295734e-01   Fold01\n302    M    F 1.360221e-02 5.446233e-01 2.978724e-01 1.439022e-01   Fold01\n303    M    F 8.337477e-02 7.314479e-01 1.843469e-01 8.304285e-04   Fold01\n304    M    F 8.005903e-02 7.716926e-01 1.474005e-01 8.478554e-04   Fold01\n305    M    F 5.844718e-02 7.613169e-01 1.780749e-01 2.160968e-03   Fold01\n306    M    F 7.676709e-02 7.697531e-01 1.525668e-01 9.129818e-04   Fold01\n307    M    F 7.079268e-02 7.623972e-01 1.657485e-01 1.061655e-03   Fold01\n308    M    F 4.934766e-02 7.448912e-01 2.045572e-01 1.204006e-03   Fold01\n309    M   VF 6.832505e-01 2.598014e-01 5.690574e-02 4.234545e-05   Fold01\n310    M    F 3.200191e-01 5.064819e-01 1.548760e-01 1.862297e-02   Fold01\n311    M    F 2.111733e-01 4.941866e-01 2.828528e-01 1.178732e-02   Fold01\n312    M    L 1.570983e-03 1.718572e-02 3.021684e-02 9.510265e-01   Fold01\n313    M   VF 7.153636e-01 2.528645e-01 3.173166e-02 4.028389e-05   Fold01\n314    M    M 2.347342e-02 2.255326e-01 5.576506e-01 1.933433e-01   Fold01\n315    M   VF 5.519460e-01 3.857331e-01 6.210817e-02 2.127387e-04   Fold01\n316    M   VF 4.699252e-01 4.253436e-01 1.043000e-01 4.313003e-04   Fold01\n317    M   VF 5.838790e-01 3.439192e-01 7.196280e-02 2.390813e-04   Fold01\n318    M    F 4.874664e-02 7.658500e-01 1.759823e-01 9.421111e-03   Fold01\n319    M    F 4.825635e-02 7.513312e-01 1.902829e-01 1.012964e-02   Fold01\n320    M    F 1.923061e-02 5.966262e-01 2.890027e-01 9.514040e-02   Fold01\n321    M    F 3.699025e-02 7.021783e-01 2.143871e-01 4.644432e-02   Fold01\n322    M    F 2.126716e-02 6.014982e-01 2.717149e-01 1.055197e-01   Fold01\n323    M    F 3.681704e-02 7.030506e-01 2.124787e-01 4.765367e-02   Fold01\n324    M    F 3.926112e-02 7.267427e-01 2.178923e-01 1.610396e-02   Fold01\n325    M    F 3.369507e-02 7.037111e-01 2.344293e-01 2.816456e-02   Fold01\n326    M    F 2.868885e-02 6.424514e-01 2.988210e-01 3.003871e-02   Fold01\n327    L    L 7.846397e-04 4.045227e-02 1.782932e-01 7.804699e-01   Fold01\n328    L    L 1.285633e-03 5.211588e-02 2.229619e-01 7.236366e-01   Fold01\n329    L    M 7.234951e-03 5.150661e-02 5.271950e-01 4.140634e-01   Fold01\n330    L    L 5.999027e-03 4.912893e-02 3.981381e-01 5.467339e-01   Fold01\n331    L    M 1.970209e-02 8.066688e-02 5.678960e-01 3.317350e-01   Fold01\n332    L    M 4.961505e-03 4.301445e-02 4.958910e-01 4.561330e-01   Fold01\n333    L    L 3.648615e-03 3.803177e-02 3.633666e-01 5.949531e-01   Fold01\n334    L    F 7.910197e-02 5.809876e-01 3.348174e-01 5.093085e-03   Fold01\n335    L    L 1.976828e-04 2.281506e-02 3.065967e-01 6.703905e-01   Fold01\n336    L    F 5.946487e-02 5.421973e-01 3.897932e-01 8.544651e-03   Fold01\n337    L   VF 6.122185e-01 3.218876e-01 6.483275e-02 1.061143e-03   Fold01\n338    L    L 8.204597e-16 2.131698e-11 3.001437e-07 9.999997e-01   Fold01\n339    L    F 2.997362e-01 4.165304e-01 2.601360e-01 2.359742e-02   Fold01\n340    L    F 2.619184e-02 6.451174e-01 3.224154e-01 6.275384e-03   Fold01\n341    L    F 5.340559e-02 7.462599e-01 1.975554e-01 2.779066e-03   Fold01\n342    L    L 3.207882e-03 2.488212e-01 3.371387e-01 4.108322e-01   Fold01\n343    L    L 1.379048e-06 4.330500e-04 1.394348e-03 9.981712e-01   Fold01\n344    L    L 1.381118e-06 4.332419e-04 1.395443e-03 9.981699e-01   Fold01\n345    L    F 2.629506e-02 6.768979e-01 2.404450e-01 5.636200e-02   Fold01\n346    L    F 2.604416e-02 6.754036e-01 2.413222e-01 5.723005e-02   Fold01\n347    L    L 6.960036e-08 5.436411e-05 3.210546e-04 9.996245e-01   Fold01\n348   VF   VF 9.412219e-01 5.435180e-02 4.413963e-03 1.229696e-05   Fold02\n349   VF   VF 9.482549e-01 4.826368e-02 3.473475e-03 7.924652e-06   Fold02\n350   VF   VF 9.581118e-01 3.952440e-02 2.360657e-03 3.096142e-06   Fold02\n351   VF   VF 9.095277e-01 7.808625e-02 1.230020e-02 8.587749e-05   Fold02\n352   VF   VF 8.914812e-01 8.601302e-02 2.239651e-02 1.092957e-04   Fold02\n353   VF   VF 9.109269e-01 7.371181e-02 1.531922e-02 4.204513e-05   Fold02\n354   VF   VF 9.604986e-01 3.598692e-02 3.510844e-03 3.661911e-06   Fold02\n355   VF   VF 9.543906e-01 3.883720e-02 6.767580e-03 4.584495e-06   Fold02\n356   VF   VF 9.828308e-01 1.634729e-02 8.207968e-04 1.091500e-06   Fold02\n357   VF   VF 9.858069e-01 1.358387e-02 6.083039e-04 9.110665e-07   Fold02\n358   VF   VF 9.850278e-01 1.403103e-02 9.402040e-04 9.969921e-07   Fold02\n359   VF   VF 9.845470e-01 1.448080e-02 9.713373e-04 8.669615e-07   Fold02\n360   VF   VF 9.885874e-01 1.100112e-02 4.111571e-04 3.696151e-07   Fold02\n361   VF   VF 9.885936e-01 1.098762e-02 4.184856e-04 2.750856e-07   Fold02\n362   VF   VF 9.816863e-01 1.736159e-02 9.504412e-04 1.693428e-06   Fold02\n363   VF   VF 9.822054e-01 1.686104e-02 9.316683e-04 1.893113e-06   Fold02\n364   VF   VF 9.884880e-01 1.109655e-02 4.150801e-04 3.641179e-07   Fold02\n365   VF   VF 9.775021e-01 2.134114e-02 1.153370e-03 3.394507e-06   Fold02\n366   VF   VF 9.726112e-01 2.502926e-02 2.352862e-03 6.720409e-06   Fold02\n367   VF   VF 9.805811e-01 1.819343e-02 1.223488e-03 2.016982e-06   Fold02\n368   VF   VF 9.801749e-01 1.857453e-02 1.248723e-03 1.884647e-06   Fold02\n369   VF   VF 9.729258e-01 2.544547e-02 1.621842e-03 6.850999e-06   Fold02\n370   VF   VF 9.706357e-01 2.738039e-02 1.973417e-03 1.052861e-05   Fold02\n371   VF   VF 9.814356e-01 1.777971e-02 7.830491e-04 1.668492e-06   Fold02\n372   VF   VF 9.244614e-01 6.351502e-02 1.063448e-02 1.389127e-03   Fold02\n373   VF   VF 9.886241e-01 1.094139e-02 4.339418e-04 5.656572e-07   Fold02\n374   VF   VF 9.668749e-01 3.021923e-02 2.892037e-03 1.379920e-05   Fold02\n375   VF   VF 9.791999e-01 1.943186e-02 1.366067e-03 2.203352e-06   Fold02\n376   VF   VF 9.830276e-01 1.628674e-02 6.841259e-04 1.575653e-06   Fold02\n377   VF   VF 7.014406e-01 2.713657e-01 2.709992e-02 9.376695e-05   Fold02\n378   VF   VF 6.369721e-01 3.257907e-01 3.702708e-02 2.101082e-04   Fold02\n379   VF   VF 6.322773e-01 3.369105e-01 3.056442e-02 2.476871e-04   Fold02\n380   VF   VF 6.203508e-01 3.319245e-01 4.746032e-02 2.643606e-04   Fold02\n381   VF   VF 7.151039e-01 2.631021e-01 2.171415e-02 7.981983e-05   Fold02\n382   VF   VF 6.191431e-01 3.238063e-01 5.678272e-02 2.678440e-04   Fold02\n383   VF   VF 5.627792e-01 3.864002e-01 5.002291e-02 7.976363e-04   Fold02\n384   VF   VF 6.605081e-01 3.080120e-01 3.127552e-02 2.044106e-04   Fold02\n385   VF   VF 7.276204e-01 2.515287e-01 2.076431e-02 8.663783e-05   Fold02\n386   VF   VF 6.633374e-01 3.057436e-01 3.072903e-02 1.900385e-04   Fold02\n387   VF   VF 7.439829e-01 2.218717e-01 3.405797e-02 8.743769e-05   Fold02\n388   VF   VF 8.212639e-01 1.640595e-01 1.465166e-02 2.499219e-05   Fold02\n389   VF   VF 8.873386e-01 1.058930e-01 6.761658e-03 6.718742e-06   Fold02\n390   VF   VF 8.266728e-01 1.581429e-01 1.514921e-02 3.500786e-05   Fold02\n391   VF   VF 8.565567e-01 1.347502e-01 8.673446e-03 1.969678e-05   Fold02\n392   VF   VF 8.942613e-01 9.976400e-02 5.968923e-03 5.776085e-06   Fold02\n393   VF    F 1.319817e-01 6.532315e-01 2.124915e-01 2.295341e-03   Fold02\n394   VF    F 1.578868e-01 6.487568e-01 1.909684e-01 2.388031e-03   Fold02\n395   VF    F 1.120367e-01 6.140491e-01 2.701824e-01 3.731767e-03   Fold02\n396   VF   VF 9.623908e-01 3.551062e-02 2.093784e-03 4.810624e-06   Fold02\n397   VF   VF 9.935280e-01 6.349190e-03 1.227248e-04 4.955180e-08   Fold02\n398   VF   VF 9.899627e-01 9.796279e-03 2.408796e-04 1.477022e-07   Fold02\n399   VF   VF 9.881459e-01 1.145432e-02 3.996262e-04 1.284713e-07   Fold02\n400   VF   VF 9.907346e-01 9.065933e-03 1.993730e-04 1.021842e-07   Fold02\n401   VF   VF 9.640417e-01 3.456528e-02 1.391007e-03 2.033527e-06   Fold02\n402   VF   VF 9.348205e-01 5.506303e-02 1.005462e-02 6.184648e-05   Fold02\n403   VF   VF 9.556890e-01 4.225700e-02 2.048080e-03 5.968322e-06   Fold02\n404   VF   VF 9.931306e-01 6.737728e-03 1.316406e-04 4.927377e-08   Fold02\n405   VF   VF 9.590621e-01 3.903457e-02 1.897683e-03 5.607987e-06   Fold02\n406   VF   VF 9.825806e-01 1.683898e-02 5.797981e-04 6.387659e-07   Fold02\n407   VF   VF 9.623393e-01 3.549000e-02 2.166718e-03 3.948187e-06   Fold02\n408   VF   VF 9.491463e-01 4.847133e-02 2.376224e-03 6.166182e-06   Fold02\n409   VF   VF 9.924491e-01 7.331232e-03 2.196052e-04 6.728706e-08   Fold02\n410   VF   VF 9.472081e-01 5.022245e-02 2.561338e-03 8.074188e-06   Fold02\n411   VF   VF 9.500427e-01 4.735223e-02 2.596009e-03 9.028863e-06   Fold02\n412   VF   VF 9.790948e-01 2.009903e-02 8.050499e-04 1.131054e-06   Fold02\n413   VF   VF 9.788184e-01 2.032442e-02 8.555545e-04 1.613734e-06   Fold02\n414   VF   VF 9.894083e-01 1.015608e-02 4.354148e-04 2.403840e-07   Fold02\n415   VF   VF 9.774361e-01 2.156228e-02 1.000656e-03 9.436831e-07   Fold02\n416   VF   VF 9.841789e-01 1.535520e-02 4.654007e-04 5.013316e-07   Fold02\n417   VF   VF 9.799535e-01 1.947840e-02 5.670985e-04 9.906829e-07   Fold02\n418   VF   VF 9.132297e-01 7.880853e-02 7.855828e-03 1.059837e-04   Fold02\n419   VF   VF 9.798805e-01 1.953308e-02 5.858438e-04 5.383499e-07   Fold02\n420   VF   VF 9.721206e-01 2.652469e-02 1.349283e-03 5.415316e-06   Fold02\n421   VF   VF 8.869423e-01 1.068298e-01 6.216196e-03 1.168894e-05   Fold02\n422   VF   VF 8.655679e-01 1.259751e-01 8.431411e-03 2.562060e-05   Fold02\n423   VF   VF 8.949100e-01 1.002857e-01 4.791267e-03 1.304236e-05   Fold02\n424   VF   VF 9.179435e-01 7.810563e-02 3.947459e-03 3.452704e-06   Fold02\n425   VF   VF 6.713405e-01 3.228358e-01 5.806723e-03 1.697331e-05   Fold02\n426   VF   VF 8.681524e-01 1.234825e-01 8.337278e-03 2.778107e-05   Fold02\n427   VF   VF 6.727988e-01 3.216458e-01 5.544556e-03 1.076954e-05   Fold02\n428   VF   VF 8.744381e-01 1.203201e-01 5.219914e-03 2.193793e-05   Fold02\n429   VF   VF 9.148481e-01 8.201837e-02 3.128398e-03 5.180481e-06   Fold02\n430   VF   VF 8.598002e-01 1.376134e-01 2.549184e-03 3.720961e-05   Fold02\n431   VF   VF 9.133000e-01 8.342110e-02 3.271903e-03 7.007512e-06   Fold02\n432   VF   VF 8.672375e-01 1.241379e-01 8.593192e-03 3.137136e-05   Fold02\n433   VF   VF 8.917659e-01 1.033957e-01 4.825536e-03 1.285237e-05   Fold02\n434   VF   VF 9.176491e-01 7.830028e-02 4.047207e-03 3.382761e-06   Fold02\n435   VF   VF 8.885116e-01 1.064167e-01 5.058754e-03 1.303120e-05   Fold02\n436   VF   VF 8.561191e-01 1.337242e-01 1.012019e-02 3.653272e-05   Fold02\n437   VF   VF 8.979300e-01 9.603680e-02 6.023683e-03 9.522402e-06   Fold02\n438   VF   VF 9.059634e-01 9.036612e-02 3.664604e-03 5.857743e-06   Fold02\n439   VF   VF 9.026699e-01 9.336250e-02 3.958805e-03 8.783136e-06   Fold02\n440   VF   VF 9.011111e-01 9.300817e-02 5.870732e-03 1.004196e-05   Fold02\n441   VF   VF 9.088555e-01 8.747825e-02 3.657275e-03 8.978887e-06   Fold02\n442   VF   VF 6.399458e-01 3.523235e-01 7.689636e-03 4.107191e-05   Fold02\n443   VF   VF 8.722396e-01 1.221261e-01 5.606501e-03 2.775968e-05   Fold02\n444   VF   VF 8.658305e-01 1.280394e-01 6.094239e-03 3.586795e-05   Fold02\n445   VF   VF 9.109901e-01 8.542539e-02 3.576151e-03 8.379436e-06   Fold02\n446   VF   VF 8.677419e-01 1.296362e-01 2.561238e-03 6.065251e-05   Fold02\n447   VF   VF 6.366535e-01 3.554292e-01 7.874250e-03 4.305485e-05   Fold02\n448   VF   VF 9.135297e-01 8.301764e-02 3.444346e-03 8.288715e-06   Fold02\n449   VF   VF 8.852686e-01 1.091012e-01 5.610555e-03 1.965348e-05   Fold02\n450   VF   VF 8.759383e-01 1.178098e-01 6.231620e-03 2.025520e-05   Fold02\n451   VF   VF 9.039180e-01 9.201638e-02 4.055159e-03 1.041931e-05   Fold02\n452   VF   VF 8.750187e-01 1.193526e-01 5.606590e-03 2.203195e-05   Fold02\n453   VF   VF 8.774378e-01 1.158965e-01 6.641870e-03 2.378966e-05   Fold02\n454   VF   VF 8.384292e-01 1.493110e-01 1.220381e-02 5.606950e-05   Fold02\n455   VF   VF 8.568954e-01 1.360430e-01 7.016126e-03 4.548441e-05   Fold02\n456   VF   VF 8.908956e-01 1.019468e-01 7.143038e-03 1.458013e-05   Fold02\n457   VF   VF 6.123497e-01 3.782664e-01 9.323538e-03 6.032761e-05   Fold02\n458   VF   VF 5.533978e-01 3.475408e-01 7.824624e-02 2.081507e-02   Fold02\n459   VF   VF 8.997931e-01 9.613297e-02 4.065071e-03 8.845146e-06   Fold02\n460   VF   VF 6.516807e-01 2.868333e-01 5.567374e-02 5.812239e-03   Fold02\n461   VF   VF 5.978975e-01 3.923591e-01 9.692574e-03 5.085671e-05   Fold02\n462   VF   VF 8.969996e-01 9.871829e-02 4.272277e-03 9.829512e-06   Fold02\n463   VF   VF 5.811334e-01 4.079691e-01 1.083375e-02 6.379053e-05   Fold02\n464   VF   VF 8.657865e-01 1.263913e-01 7.786435e-03 3.579123e-05   Fold02\n465   VF   VF 5.875310e-01 4.013740e-01 1.101067e-02 8.428505e-05   Fold02\n466   VF   VF 5.859721e-01 4.028244e-01 1.111751e-02 8.597736e-05   Fold02\n467   VF   VF 5.841853e-01 4.043794e-01 1.134229e-02 9.304117e-05   Fold02\n468   VF   VF 5.793755e-01 4.089237e-01 1.160700e-02 9.375931e-05   Fold02\n469   VF   VF 8.349337e-01 1.555209e-01 9.475753e-03 6.957572e-05   Fold02\n470   VF   VF 8.652677e-01 1.237528e-01 1.094377e-02 3.567355e-05   Fold02\n471   VF   VF 8.938884e-01 1.015392e-01 4.559910e-03 1.252024e-05   Fold02\n472   VF   VF 9.227006e-01 7.457426e-02 2.719913e-03 5.239942e-06   Fold02\n473   VF   VF 9.098991e-01 8.523642e-02 4.857212e-03 7.252739e-06   Fold02\n474   VF   VF 8.597071e-01 1.314605e-01 8.808218e-03 2.422368e-05   Fold02\n475   VF   VF 8.802138e-01 1.148186e-01 4.941526e-03 2.611767e-05   Fold02\n476   VF   VF 8.748615e-01 1.178193e-01 7.299947e-03 1.926647e-05   Fold02\n477   VF   VF 8.762783e-01 1.165069e-01 7.195688e-03 1.912051e-05   Fold02\n478   VF   VF 8.366678e-01 1.523969e-01 1.090046e-02 3.477376e-05   Fold02\n479   VF   VF 8.619630e-01 1.293888e-01 8.624808e-03 2.339865e-05   Fold02\n480   VF   VF 9.206736e-01 7.646214e-02 2.858676e-03 5.633997e-06   Fold02\n481   VF   VF 8.640669e-01 1.274155e-01 8.494637e-03 2.292996e-05   Fold02\n482   VF   VF 9.120775e-01 8.454403e-02 3.371626e-03 6.877527e-06   Fold02\n483   VF   VF 8.638588e-01 1.275796e-01 8.538173e-03 2.335009e-05   Fold02\n484   VF   VF 9.646843e-01 3.406696e-02 1.247917e-03 8.381428e-07   Fold02\n485   VF   VF 9.577756e-01 4.059647e-02 1.626338e-03 1.575429e-06   Fold02\n486   VF   VF 9.014167e-01 9.132900e-02 7.229224e-03 2.512618e-05   Fold02\n487   VF   VF 9.410553e-01 5.632757e-02 2.613679e-03 3.502706e-06   Fold02\n488   VF   VF 9.613750e-01 3.717781e-02 1.446094e-03 1.091766e-06   Fold02\n489   VF   VF 9.461803e-01 5.067612e-02 3.140445e-03 3.180952e-06   Fold02\n490   VF   VF 9.676560e-01 3.126576e-02 1.077432e-03 7.787704e-07   Fold02\n491   VF   VF 9.721129e-01 2.702262e-02 8.640418e-04 4.494552e-07   Fold02\n492   VF   VF 9.772282e-01 2.217017e-02 6.014444e-04 2.282313e-07   Fold02\n493   VF   VF 9.738293e-01 2.485784e-02 1.312342e-03 5.555556e-07   Fold02\n494   VF   VF 8.854273e-01 1.044543e-01 1.009270e-02 2.568889e-05   Fold02\n495   VF   VF 9.735859e-01 2.502162e-02 1.391760e-03 7.012472e-07   Fold02\n496   VF   VF 9.225397e-01 7.314578e-02 4.298455e-03 1.605168e-05   Fold02\n497   VF    F 9.891665e-02 7.923023e-01 1.083374e-01 4.436428e-04   Fold02\n498   VF    F 9.615077e-02 7.944174e-01 1.090301e-01 4.017447e-04   Fold02\n499   VF    F 9.071409e-02 7.652603e-01 1.436731e-01 3.524666e-04   Fold02\n500   VF    F 9.362804e-02 7.940470e-01 1.118918e-01 4.331388e-04   Fold02\n501   VF    F 1.077593e-01 7.541877e-01 1.376739e-01 3.790530e-04   Fold02\n502   VF   VF 7.966944e-01 1.866678e-01 1.662655e-02 1.127912e-05   Fold02\n503   VF   VF 8.052172e-01 1.787577e-01 1.601725e-02 7.862689e-06   Fold02\n504   VF    F 3.808753e-01 5.974494e-01 2.163396e-02 4.135462e-05   Fold02\n505   VF   VF 8.075835e-01 1.773012e-01 1.510052e-02 1.479613e-05   Fold02\n506   VF   VF 6.050811e-01 3.390456e-01 5.573077e-02 1.425297e-04   Fold02\n507   VF   VF 7.933887e-01 1.872798e-01 1.931592e-02 1.564763e-05   Fold02\n508   VF   VF 6.966068e-01 2.778978e-01 2.544573e-02 4.966381e-05   Fold02\n509   VF   VF 5.870442e-01 3.462513e-01 6.647342e-02 2.310908e-04   Fold02\n510   VF   VF 6.814658e-01 2.812545e-01 3.720395e-02 7.575593e-05   Fold02\n511   VF   VF 7.194497e-01 2.612959e-01 1.920661e-02 4.783959e-05   Fold02\n512   VF   VF 6.385601e-01 3.127748e-01 4.853846e-02 1.266552e-04   Fold02\n513   VF   VF 6.778155e-01 2.896317e-01 3.249082e-02 6.199087e-05   Fold02\n514   VF   VF 6.578773e-01 3.092501e-01 3.273109e-02 1.414856e-04   Fold02\n515   VF   VF 7.778379e-01 2.068688e-01 1.527608e-02 1.729233e-05   Fold02\n516   VF   VF 6.669539e-01 2.972571e-01 3.570671e-02 8.229915e-05   Fold02\n517   VF   VF 7.440361e-01 2.317735e-01 2.414695e-02 4.342562e-05   Fold02\n518   VF   VF 5.876107e-01 3.464737e-01 6.566927e-02 2.462677e-04   Fold02\n519   VF   VF 5.788706e-01 3.515087e-01 6.934658e-02 2.741279e-04   Fold02\n520   VF   VF 8.063869e-01 1.765658e-01 1.703379e-02 1.347681e-05   Fold02\n521   VF   VF 8.087230e-01 1.794730e-01 1.179160e-02 1.241174e-05   Fold02\n522   VF   VF 8.028081e-01 1.797376e-01 1.744134e-02 1.291490e-05   Fold02\n523   VF    F 9.088469e-02 7.500595e-01 1.567474e-01 2.308367e-03   Fold02\n524   VF    F 8.879143e-02 7.480867e-01 1.593707e-01 3.751221e-03   Fold02\n525    F    F 4.064392e-01 5.192439e-01 7.300694e-02 1.309958e-03   Fold02\n526    F   VF 6.236294e-01 3.438814e-01 3.217634e-02 3.128020e-04   Fold02\n527    F   VF 7.207876e-01 2.569065e-01 2.219231e-02 1.135647e-04   Fold02\n528    F    F 3.959107e-01 4.820750e-01 1.189352e-01 3.079148e-03   Fold02\n529    F   VF 7.656745e-01 1.957957e-01 3.846919e-02 6.061340e-05   Fold02\n530    F   VF 8.341053e-01 1.488405e-01 1.703052e-02 2.362207e-05   Fold02\n531    F   VF 8.504958e-01 1.379948e-01 1.148853e-02 2.088447e-05   Fold02\n532    F    M 2.264767e-02 1.286831e-01 5.025028e-01 3.461664e-01   Fold02\n533    F   VF 7.560049e-01 2.141113e-01 2.977884e-02 1.049495e-04   Fold02\n534    F   VF 8.951685e-01 9.893494e-02 5.890789e-03 5.806999e-06   Fold02\n535    F    F 1.406516e-01 6.517979e-01 2.051326e-01 2.417945e-03   Fold02\n536    F    F 1.798838e-01 6.321490e-01 1.859631e-01 2.004085e-03   Fold02\n537    F    F 1.342906e-01 5.977997e-01 2.655968e-01 2.312893e-03   Fold02\n538    F    F 1.356226e-01 5.965868e-01 2.653479e-01 2.442699e-03   Fold02\n539    F    F 1.222717e-01 6.295693e-01 2.444526e-01 3.706433e-03   Fold02\n540    F    F 1.005331e-01 6.671408e-01 2.269418e-01 5.384362e-03   Fold02\n541    F    F 1.176856e-01 6.377132e-01 2.402652e-01 4.335941e-03   Fold02\n542    F    F 8.373046e-02 5.910335e-01 3.189586e-01 6.277438e-03   Fold02\n543    F    F 1.146523e-01 6.434949e-01 2.380388e-01 3.813946e-03   Fold02\n544    F    F 1.090444e-01 5.967802e-01 2.901686e-01 4.006822e-03   Fold02\n545    F    F 1.488714e-01 6.533644e-01 1.956426e-01 2.121612e-03   Fold02\n546    F    F 1.908803e-01 6.467016e-01 1.611782e-01 1.239797e-03   Fold02\n547    F    F 5.784697e-02 5.362481e-01 3.939492e-01 1.195568e-02   Fold02\n548    F    F 7.166662e-02 6.151653e-01 2.982563e-01 1.491182e-02   Fold02\n549    F    F 1.657426e-01 3.991192e-01 3.225297e-01 1.126085e-01   Fold02\n550    F   VF 7.258770e-01 2.362982e-01 3.739437e-02 4.304325e-04   Fold02\n551    F   VF 8.005867e-01 1.820171e-01 1.731205e-02 8.411125e-05   Fold02\n552    F   VF 8.767429e-01 1.154079e-01 7.832883e-03 1.635613e-05   Fold02\n553    F   VF 8.327172e-01 1.502375e-01 1.699956e-02 4.574769e-05   Fold02\n554    F   VF 8.453146e-01 1.436240e-01 1.101829e-02 4.314354e-05   Fold02\n555    F   VF 8.624319e-01 1.238483e-01 1.369450e-02 2.521888e-05   Fold02\n556    F   VF 3.992397e-01 3.917702e-01 1.957093e-01 1.328071e-02   Fold02\n557    F    L 3.127208e-03 1.294986e-02 1.202146e-02 9.719015e-01   Fold02\n558    F   VF 8.872386e-01 1.052646e-01 7.480712e-03 1.602307e-05   Fold02\n559    F    L 1.120897e-02 3.230391e-02 2.087358e-02 9.356135e-01   Fold02\n560    F    L 8.762418e-03 2.956494e-02 2.954238e-02 9.321303e-01   Fold02\n561    F    L 1.093375e-02 3.138264e-02 2.031907e-02 9.373645e-01   Fold02\n562    F    L 1.043267e-02 3.038563e-02 1.995677e-02 9.392249e-01   Fold02\n563    F   VF 5.746233e-01 4.140088e-01 1.129850e-02 6.941587e-05   Fold02\n564    F   VF 5.620471e-01 4.256348e-01 1.223649e-02 8.157160e-05   Fold02\n565    F   VF 8.610198e-01 1.276057e-01 1.134059e-02 3.400389e-05   Fold02\n566    F   VF 5.381490e-01 4.475273e-01 1.421287e-02 1.108366e-04   Fold02\n567    F   VF 5.111945e-01 4.719165e-01 1.673382e-02 1.551305e-04   Fold02\n568    F   VF 7.699202e-01 2.001013e-01 2.970838e-02 2.701353e-04   Fold02\n569    F   VF 7.948845e-01 1.866775e-01 1.826235e-02 1.756498e-04   Fold02\n570    F   VF 9.795826e-01 1.874203e-02 1.671375e-03 3.992563e-06   Fold02\n571    F   VF 7.874256e-01 1.915075e-01 2.099317e-02 7.374726e-05   Fold02\n572    F   VF 7.769866e-01 1.907255e-01 3.205191e-02 2.359688e-04   Fold02\n573    F    F 8.942033e-02 7.909088e-01 1.190577e-01 6.131789e-04   Fold02\n574    F    F 7.474766e-02 8.101168e-01 1.143002e-01 8.353492e-04   Fold02\n575    F    F 8.114063e-02 7.922197e-01 1.259186e-01 7.211102e-04   Fold02\n576    F    F 9.173913e-02 7.956632e-01 1.121554e-01 4.422693e-04   Fold02\n577    F    F 1.062660e-01 7.889385e-01 1.044723e-01 3.232096e-04   Fold02\n578    F    F 9.156847e-02 7.970780e-01 1.109151e-01 4.384080e-04   Fold02\n579    F    F 6.668764e-02 7.431637e-01 1.893753e-01 7.732824e-04   Fold02\n580    F    F 6.697180e-02 7.407331e-01 1.914532e-01 8.419209e-04   Fold02\n581    F    F 6.486086e-02 7.980260e-01 1.357559e-01 1.357252e-03   Fold02\n582    F    F 8.143671e-02 7.444362e-01 1.735469e-01 5.801566e-04   Fold02\n583    F    F 8.048734e-02 7.826013e-01 1.362195e-01 6.918490e-04   Fold02\n584    F    F 1.414639e-02 8.936077e-01 9.115827e-02 1.087643e-03   Fold02\n585    F    F 2.086121e-02 9.216700e-01 5.721956e-02 2.492490e-04   Fold02\n586    F    F 2.091490e-02 9.216902e-01 5.714598e-02 2.488968e-04   Fold02\n587    F    F 6.406898e-02 7.729428e-01 1.624209e-01 5.673279e-04   Fold02\n588    F    F 7.854926e-02 7.978535e-01 1.229202e-01 6.770825e-04   Fold02\n589    F    F 4.876235e-02 7.798743e-01 1.688005e-01 2.562847e-03   Fold02\n590    F    F 1.131987e-01 7.890271e-01 9.743116e-02 3.429883e-04   Fold02\n591    F    F 3.671448e-02 7.458882e-01 2.140881e-01 3.309201e-03   Fold02\n592    F   VF 7.388899e-01 2.409270e-01 2.015112e-02 3.202613e-05   Fold02\n593    F   VF 7.114176e-01 2.521971e-01 3.635056e-02 3.470208e-05   Fold02\n594    F   VF 7.585858e-01 2.190040e-01 2.238740e-02 2.281452e-05   Fold02\n595    F   VF 6.139739e-01 3.363592e-01 4.953831e-02 1.285798e-04   Fold02\n596    F   VF 5.818455e-01 3.629288e-01 5.509122e-02 1.345015e-04   Fold02\n597    F    F 2.998387e-01 4.837705e-01 1.900607e-01 2.633013e-02   Fold02\n598    F   VF 7.241694e-01 2.440489e-01 3.175242e-02 2.925671e-05   Fold02\n599    F   VF 7.286605e-01 2.421100e-01 2.920051e-02 2.900695e-05   Fold02\n600    F    F 3.890649e-01 4.437612e-01 1.556593e-01 1.151457e-02   Fold02\n601    F    F 3.433363e-01 5.068771e-01 1.264373e-01 2.334919e-02   Fold02\n602    F   VF 7.167028e-01 2.484577e-01 3.478820e-02 5.130090e-05   Fold02\n603    F   VF 6.881027e-01 2.694130e-01 4.241398e-02 7.037093e-05   Fold02\n604    F   VF 6.158479e-01 3.339392e-01 5.003054e-02 1.823443e-04   Fold02\n605    F   VF 5.974018e-01 3.392405e-01 6.313442e-02 2.232891e-04   Fold02\n606    F   VF 5.696431e-01 3.651180e-01 6.489017e-02 3.486786e-04   Fold02\n607    F    F 7.915629e-02 7.828172e-01 1.321479e-01 5.878643e-03   Fold02\n608    F    F 4.727071e-02 7.281883e-01 2.111359e-01 1.340515e-02   Fold02\n609    F    F 6.755609e-02 7.261621e-01 1.992753e-01 7.006518e-03   Fold02\n610    F    F 5.450717e-02 7.794905e-01 1.505651e-01 1.543720e-02   Fold02\n611    F    F 6.798731e-02 7.292482e-01 1.953709e-01 7.393614e-03   Fold02\n612    F    F 6.108301e-02 7.689275e-01 1.603186e-01 9.670894e-03   Fold02\n613    F    F 4.008462e-02 7.209531e-01 2.004599e-01 3.850235e-02   Fold02\n614    F    F 5.847910e-02 7.691995e-01 1.641074e-01 8.214028e-03   Fold02\n615    F    F 6.657739e-02 7.280552e-01 1.978198e-01 7.547641e-03   Fold02\n616    F    F 5.917923e-02 7.698187e-01 1.619595e-01 9.042552e-03   Fold02\n617    F    F 5.798133e-02 7.705209e-01 1.629620e-01 8.535820e-03   Fold02\n618    F    F 5.740832e-02 7.644713e-01 1.686606e-01 9.459866e-03   Fold02\n619    F    F 5.816218e-02 7.657588e-01 1.658153e-01 1.026369e-02   Fold02\n620    F    F 5.539678e-02 7.644403e-01 1.708569e-01 9.306100e-03   Fold02\n621    F    F 3.765972e-02 7.097985e-01 2.083064e-01 4.423536e-02   Fold02\n622    F    F 2.950286e-02 6.338495e-01 2.881120e-01 4.853561e-02   Fold02\n623    F    F 3.748774e-02 7.071996e-01 2.092828e-01 4.602983e-02   Fold02\n624    F    F 5.685145e-02 7.659570e-01 1.672912e-01 9.900321e-03   Fold02\n625    F    F 6.375265e-02 7.231553e-01 2.046883e-01 8.403682e-03   Fold02\n626    F    F 5.325097e-02 7.296654e-01 2.095650e-01 7.518588e-03   Fold02\n627    F    F 6.276755e-02 7.233756e-01 2.070655e-01 6.791439e-03   Fold02\n628    F    F 9.893980e-02 7.853264e-01 1.129401e-01 2.793640e-03   Fold02\n629    F    F 7.081369e-02 7.657014e-01 1.556605e-01 7.824371e-03   Fold02\n630    F    F 8.384278e-02 7.856998e-01 1.254083e-01 5.049120e-03   Fold02\n631    F    F 8.892719e-02 7.497157e-01 1.576898e-01 3.667363e-03   Fold02\n632    F    F 5.632333e-02 7.503343e-01 1.853607e-01 7.981672e-03   Fold02\n633    M    F 3.983214e-01 5.256473e-01 7.465441e-02 1.376903e-03   Fold02\n634    M    M 7.651372e-02 1.325920e-01 5.020904e-01 2.888039e-01   Fold02\n635    M   VF 6.497912e-01 3.161153e-01 3.383116e-02 2.623379e-04   Fold02\n636    M   VF 5.809087e-01 3.588429e-01 5.984878e-02 3.995841e-04   Fold02\n637    M   VF 6.075920e-01 3.220632e-01 7.003494e-02 3.098231e-04   Fold02\n638    M    F 2.452603e-01 5.401834e-01 2.090279e-01 5.528439e-03   Fold02\n639    M    L 3.007043e-03 1.573121e-02 2.274223e-02 9.585195e-01   Fold02\n640    M    F 1.265774e-01 6.213291e-01 2.498779e-01 2.215535e-03   Fold02\n641    M    F 1.196205e-01 6.155522e-01 2.620966e-01 2.730691e-03   Fold02\n642    M    F 1.136184e-01 6.142928e-01 2.681346e-01 3.954289e-03   Fold02\n643    M    M 1.098018e-04 4.347828e-03 9.948806e-01 6.617344e-04   Fold02\n644    M   VF 8.586051e-01 1.156624e-01 2.571338e-02 1.913549e-05   Fold02\n645    M    L 6.024651e-03 1.987037e-02 1.574310e-02 9.583619e-01   Fold02\n646    M   VF 8.296687e-01 1.567095e-01 1.354236e-02 7.949161e-05   Fold02\n647    M    L 2.393823e-03 1.174486e-02 9.204439e-03 9.766569e-01   Fold02\n648    M    F 1.291783e-01 6.812891e-01 1.791814e-01 1.035116e-02   Fold02\n649    M    F 2.398213e-01 4.456009e-01 2.867785e-01 2.779924e-02   Fold02\n650    M    F 6.128864e-02 6.096166e-01 2.855401e-01 4.355460e-02   Fold02\n651    M    M 6.777802e-04 2.739801e-02 9.718307e-01 9.353896e-05   Fold02\n652    M    F 1.212227e-01 7.801304e-01 9.824369e-02 4.032484e-04   Fold02\n653    M    F 3.327596e-02 7.272549e-01 2.309759e-01 8.493172e-03   Fold02\n654    M    F 8.983263e-02 7.959676e-01 1.137239e-01 4.759090e-04   Fold02\n655    M    F 7.515527e-02 7.835037e-01 1.406318e-01 7.092342e-04   Fold02\n656    M    F 7.444434e-02 7.717680e-01 1.527219e-01 1.065738e-03   Fold02\n657    M    F 4.868711e-02 7.806771e-01 1.681730e-01 2.462760e-03   Fold02\n658    M    F 2.695811e-02 7.171030e-01 2.512020e-01 4.736846e-03   Fold02\n659    M    F 2.734568e-02 7.220950e-01 2.457406e-01 4.818753e-03   Fold02\n660    M    F 3.224273e-02 7.365918e-01 2.253597e-01 5.805701e-03   Fold02\n661    M    M 3.066435e-04 1.917751e-02 9.801098e-01 4.060053e-04   Fold02\n662    M    F 1.146995e-01 7.880227e-01 9.696402e-02 3.137385e-04   Fold02\n663    M    F 1.977585e-01 4.592753e-01 3.157922e-01 2.717404e-02   Fold02\n664    M    L 1.373166e-03 1.394217e-02 2.195217e-02 9.627325e-01   Fold02\n665    M    M 1.134322e-01 3.900928e-01 4.636648e-01 3.281022e-02   Fold02\n666    M    L 2.613576e-03 2.173369e-02 2.983233e-02 9.458204e-01   Fold02\n667    M    M 3.256553e-02 2.118359e-01 6.638327e-01 9.176585e-02   Fold02\n668    M    L 4.471984e-03 2.322972e-01 1.508328e-01 6.123980e-01   Fold02\n669    M    L 6.726248e-03 3.069938e-01 1.735241e-01 5.127559e-01   Fold02\n670    M    F 3.870958e-02 6.973682e-01 2.436004e-01 2.032188e-02   Fold02\n671    M    F 5.304247e-02 6.963184e-01 2.413112e-01 9.327967e-03   Fold02\n672    M    F 7.314746e-02 7.692408e-01 1.504190e-01 7.192694e-03   Fold02\n673    M    F 5.368413e-02 7.407970e-01 1.927339e-01 1.278498e-02   Fold02\n674    L    L 2.682881e-05 1.005804e-03 2.256919e-03 9.967104e-01   Fold02\n675    L    L 6.578103e-03 1.230803e-01 3.174556e-01 5.528861e-01   Fold02\n676    L    M 1.709187e-02 6.866608e-02 5.390337e-01 3.752083e-01   Fold02\n677    L    F 1.080682e-01 4.319136e-01 3.945128e-01 6.550538e-02   Fold02\n678    L    L 5.960813e-03 1.359096e-01 3.569841e-01 5.011455e-01   Fold02\n679    L    M 8.795239e-03 3.634550e-02 9.544583e-01 4.009878e-04   Fold02\n680    L    F 1.675067e-01 6.073871e-01 2.241777e-01 9.284058e-04   Fold02\n681    L    L 1.774776e-04 2.521971e-02 2.183373e-01 7.562655e-01   Fold02\n682    L    L 1.121212e-15 3.075492e-11 9.445284e-07 9.999991e-01   Fold02\n683    L   VF 4.776179e-01 3.752606e-01 1.385000e-01 8.621542e-03   Fold02\n684    L    F 2.965333e-01 3.990771e-01 2.707408e-01 3.364881e-02   Fold02\n685    L    F 8.237883e-02 8.070884e-01 1.100057e-01 5.270311e-04   Fold02\n686    L    M 4.145261e-10 4.866262e-06 9.713251e-01 2.867001e-02   Fold02\n687    L    F 4.476781e-02 7.478842e-01 2.049752e-01 2.372757e-03   Fold02\n688    L    M 1.655237e-02 2.070121e-01 5.156354e-01 2.608002e-01   Fold02\n689    L    F 1.002525e-02 4.207830e-01 3.838384e-01 1.853533e-01   Fold02\n690    L    L 4.428292e-08 3.819962e-05 4.199812e-04 9.995418e-01   Fold02\n691    L    L 6.079371e-07 2.164161e-04 7.289791e-04 9.990540e-01   Fold02\n692    L    L 1.155710e-06 3.388861e-04 1.001724e-03 9.986582e-01   Fold02\n693    L    L 5.217781e-07 1.980299e-04 7.044606e-04 9.990970e-01   Fold02\n694    L    L 2.422451e-08 2.457347e-05 6.508882e-05 9.999103e-01   Fold02\n695   VF   VF 9.392230e-01 5.564279e-02 5.126282e-03 7.903643e-06   Fold03\n696   VF   VF 9.279738e-01 6.424454e-02 7.766806e-03 1.482258e-05   Fold03\n697   VF   VF 9.268542e-01 6.526833e-02 7.862435e-03 1.500361e-05   Fold03\n698   VF   VF 9.558025e-01 4.096856e-02 3.225398e-03 3.588761e-06   Fold03\n699   VF    F 4.358237e-01 5.060691e-01 5.730369e-02 8.035155e-04   Fold03\n700   VF    F 4.020163e-01 5.120798e-01 8.475018e-02 1.153708e-03   Fold03\n701   VF   VF 9.613405e-01 3.296303e-02 5.693844e-03 2.644232e-06   Fold03\n702   VF   VF 8.472555e-01 1.182683e-01 3.439174e-02 8.450579e-05   Fold03\n703   VF   VF 8.402251e-01 1.288317e-01 3.082307e-02 1.200905e-04   Fold03\n704   VF   VF 9.581481e-01 3.669287e-02 5.156099e-03 2.896971e-06   Fold03\n705   VF    M 1.267927e-01 1.553620e-01 4.108299e-01 3.070154e-01   Fold03\n706   VF   VF 9.501685e-01 4.116818e-02 8.657078e-03 6.200444e-06   Fold03\n707   VF   VF 9.285738e-01 5.852711e-02 1.288159e-02 1.749436e-05   Fold03\n708   VF   VF 8.848870e-01 9.343222e-02 2.158779e-02 9.295748e-05   Fold03\n709   VF   VF 9.669487e-01 2.854314e-02 4.506335e-03 1.801954e-06   Fold03\n710   VF   VF 9.496143e-01 4.337365e-02 7.007050e-03 5.019080e-06   Fold03\n711   VF   VF 9.884603e-01 1.101876e-02 5.206515e-04 3.087186e-07   Fold03\n712   VF   VF 9.801738e-01 1.840550e-02 1.419072e-03 1.616664e-06   Fold03\n713   VF   VF 9.868503e-01 1.247554e-02 6.736414e-04 5.179917e-07   Fold03\n714   VF   VF 9.802129e-01 1.875819e-02 1.026683e-03 2.275079e-06   Fold03\n715   VF   VF 9.869620e-01 1.225224e-02 7.852858e-04 4.335800e-07   Fold03\n716   VF   VF 9.883945e-01 1.106485e-02 5.403483e-04 3.386952e-07   Fold03\n717   VF   VF 9.807026e-01 1.804614e-02 1.249204e-03 2.007531e-06   Fold03\n718   VF   VF 8.875990e-01 8.568285e-02 2.469079e-02 2.027392e-03   Fold03\n719   VF   VF 9.867302e-01 1.245872e-02 8.106536e-04 4.607080e-07   Fold03\n720   VF   VF 9.882153e-01 1.121579e-02 5.685413e-04 3.803888e-07   Fold03\n721   VF   VF 9.796699e-01 1.881504e-02 1.513193e-03 1.858814e-06   Fold03\n722   VF   VF 9.820615e-01 1.710486e-02 8.321449e-04 1.469724e-06   Fold03\n723   VF   VF 9.826882e-01 1.651403e-02 7.964481e-04 1.370977e-06   Fold03\n724   VF   VF 6.594747e-01 2.949310e-01 4.533126e-02 2.630411e-04   Fold03\n725   VF   VF 6.592468e-01 2.965471e-01 4.394435e-02 2.617817e-04   Fold03\n726   VF   VF 6.608818e-01 2.953026e-01 4.355896e-02 2.566478e-04   Fold03\n727   VF   VF 7.452897e-01 2.274627e-01 2.716565e-02 8.190427e-05   Fold03\n728   VF   VF 7.457022e-01 2.265311e-01 2.768051e-02 8.618131e-05   Fold03\n729   VF   VF 7.491082e-01 2.201834e-01 3.066233e-02 4.606636e-05   Fold03\n730   VF   VF 6.987303e-01 2.627552e-01 3.842949e-02 8.501460e-05   Fold03\n731   VF   VF 7.989117e-01 1.876049e-01 1.346623e-02 1.709659e-05   Fold03\n732   VF   VF 6.763675e-01 2.733648e-01 5.012965e-02 1.379981e-04   Fold03\n733   VF   VF 7.640487e-01 2.092697e-01 2.665498e-02 2.666085e-05   Fold03\n734   VF   VF 8.374506e-01 1.489598e-01 1.358379e-02 5.780933e-06   Fold03\n735   VF   VF 8.004533e-01 1.813132e-01 1.821620e-02 1.726164e-05   Fold03\n736   VF   VF 7.432825e-01 2.258068e-01 3.087039e-02 4.033248e-05   Fold03\n737   VF   VF 8.388702e-01 1.486197e-01 1.250147e-02 8.548363e-06   Fold03\n738   VF   VF 8.176418e-01 1.710762e-01 1.126872e-02 1.328076e-05   Fold03\n739   VF   VF 7.614249e-01 2.159714e-01 2.257310e-02 3.066625e-05   Fold03\n740   VF   VF 8.146284e-01 1.739305e-01 1.142757e-02 1.348164e-05   Fold03\n741   VF   VF 9.113213e-01 8.171942e-02 6.939740e-03 1.954126e-05   Fold03\n742   VF   VF 9.643057e-01 3.420873e-02 1.482391e-03 3.153675e-06   Fold03\n743   VF   VF 9.811015e-01 1.815922e-02 7.387606e-04 5.109052e-07   Fold03\n744   VF   VF 9.847973e-01 1.479319e-02 4.091296e-04 4.196520e-07   Fold03\n745   VF   VF 9.846547e-01 1.482882e-02 5.160670e-04 3.795033e-07   Fold03\n746   VF   VF 9.900181e-01 9.714996e-03 2.667659e-04 1.294148e-07   Fold03\n747   VF   VF 9.546495e-01 4.207893e-02 3.267929e-03 3.593721e-06   Fold03\n748   VF   VF 5.911263e-01 2.741069e-01 1.278836e-01 6.883176e-03   Fold03\n749   VF   VF 9.282367e-01 6.518282e-02 6.563286e-03 1.715652e-05   Fold03\n750   VF   VF 9.760305e-01 2.294782e-02 1.020833e-03 8.600715e-07   Fold03\n751   VF   VF 9.691215e-01 2.935466e-02 1.521685e-03 2.144241e-06   Fold03\n752   VF   VF 9.929682e-01 6.873015e-03 1.587575e-04 4.405447e-08   Fold03\n753   VF   VF 9.877471e-01 1.184856e-02 4.041479e-04 2.304148e-07   Fold03\n754   VF   VF 9.873311e-01 1.212555e-02 5.430330e-04 2.804092e-07   Fold03\n755   VF   VF 9.501511e-01 4.625279e-02 3.590335e-03 5.815424e-06   Fold03\n756   VF   VF 9.921400e-01 7.629045e-03 2.308779e-04 5.669937e-08   Fold03\n757   VF   VF 9.883805e-01 1.117214e-02 4.472339e-04 1.530048e-07   Fold03\n758   VF   VF 9.287307e-01 6.579097e-02 5.460968e-03 1.739852e-05   Fold03\n759   VF   VF 9.801709e-01 1.900260e-02 8.257498e-04 7.754086e-07   Fold03\n760   VF   VF 9.755256e-01 2.327062e-02 1.201810e-03 1.953329e-06   Fold03\n761   VF   VF 9.798449e-01 1.930250e-02 8.517956e-04 8.222916e-07   Fold03\n762   VF   VF 9.300129e-01 6.391093e-02 6.067488e-03 8.718683e-06   Fold03\n763   VF   VF 9.805046e-01 1.873738e-02 7.574714e-04 5.046502e-07   Fold03\n764   VF   VF 9.795468e-01 1.981689e-02 6.354304e-04 8.368963e-07   Fold03\n765   VF   VF 9.805469e-01 1.869470e-02 7.578769e-04 5.075936e-07   Fold03\n766   VF   VF 9.265481e-01 6.695094e-02 6.491344e-03 9.623311e-06   Fold03\n767   VF   VF 9.869379e-01 1.256675e-02 4.950177e-04 2.895791e-07   Fold03\n768   VF   VF 9.121322e-01 8.369869e-02 4.164247e-03 4.834862e-06   Fold03\n769   VF   VF 8.979620e-01 9.570816e-02 6.322900e-03 6.957500e-06   Fold03\n770   VF   VF 8.715645e-01 1.199055e-01 8.517494e-03 1.248711e-05   Fold03\n771   VF   VF 9.104444e-01 8.528634e-02 4.263968e-03 5.325609e-06   Fold03\n772   VF   VF 7.419438e-01 2.227862e-01 3.482566e-02 4.442902e-04   Fold03\n773   VF   VF 8.800780e-01 1.123520e-01 7.555535e-03 1.444826e-05   Fold03\n774   VF   VF 8.853884e-01 1.086927e-01 5.907520e-03 1.140008e-05   Fold03\n775   VF   VF 8.667098e-01 1.270105e-01 6.257848e-03 2.184434e-05   Fold03\n776   VF   VF 8.803371e-01 1.129420e-01 6.705961e-03 1.494930e-05   Fold03\n777   VF   VF 8.492308e-01 1.394479e-01 1.129388e-02 2.742322e-05   Fold03\n778   VF   VF 9.066880e-01 8.882851e-02 4.477689e-03 5.816374e-06   Fold03\n779   VF   VF 8.848504e-01 1.087866e-01 6.349113e-03 1.386669e-05   Fold03\n780   VF   VF 8.699768e-01 1.268590e-01 3.096513e-03 6.772825e-05   Fold03\n781   VF   VF 8.998407e-01 9.485890e-02 5.292233e-03 8.214716e-06   Fold03\n782   VF   VF 8.509021e-01 1.413763e-01 7.689166e-03 3.242639e-05   Fold03\n783   VF   VF 8.796325e-01 1.151424e-01 5.210034e-03 1.502907e-05   Fold03\n784   VF   VF 8.769491e-01 1.157187e-01 7.313540e-03 1.871126e-05   Fold03\n785   VF   VF 8.894992e-01 1.029685e-01 7.521891e-03 1.036536e-05   Fold03\n786   VF   VF 8.977795e-01 9.720317e-02 5.008882e-03 8.412984e-06   Fold03\n787   VF   VF 8.929136e-01 1.011669e-01 5.909464e-03 9.994019e-06   Fold03\n788   VF   VF 8.711450e-01 1.208872e-01 7.946911e-03 2.091696e-05   Fold03\n789   VF   VF 8.843023e-01 1.074765e-01 8.208851e-03 1.230764e-05   Fold03\n790   VF   VF 6.659727e-01 3.244629e-01 9.523925e-03 4.047995e-05   Fold03\n791   VF   VF 8.971974e-01 9.781915e-02 4.975277e-03 8.205929e-06   Fold03\n792   VF   VF 5.032426e-01 3.814974e-01 1.015945e-01 1.366545e-02   Fold03\n793   VF   VF 6.645134e-01 3.258245e-01 9.620714e-03 4.139244e-05   Fold03\n794   VF   VF 9.064028e-01 8.822294e-02 5.369146e-03 5.091921e-06   Fold03\n795   VF   VF 8.750559e-01 1.154066e-01 9.520975e-03 1.658778e-05   Fold03\n796   VF   VF 8.654768e-01 1.260426e-01 8.455876e-03 2.468839e-05   Fold03\n797   VF   VF 8.616184e-01 1.296156e-01 8.740117e-03 2.587981e-05   Fold03\n798   VF   VF 8.371978e-01 1.533981e-01 9.353928e-03 5.024660e-05   Fold03\n799   VF   VF 6.496066e-01 3.396914e-01 1.065022e-02 5.183943e-05   Fold03\n800   VF   VF 8.740537e-01 1.161750e-01 9.753580e-03 1.776014e-05   Fold03\n801   VF   VF 8.716347e-01 1.184471e-01 9.900273e-03 1.792111e-05   Fold03\n802   VF   VF 8.330411e-01 1.570524e-01 9.850802e-03 5.575315e-05   Fold03\n803   VF   VF 6.429843e-01 3.458306e-01 1.112802e-02 5.711923e-05   Fold03\n804   VF   VF 8.609694e-01 1.291667e-01 9.843335e-03 2.063052e-05   Fold03\n805   VF   VF 8.290460e-01 1.604630e-01 1.042711e-02 6.389821e-05   Fold03\n806   VF   VF 7.964457e-01 1.822667e-01 2.119342e-02 9.418904e-05   Fold03\n807   VF   VF 8.957732e-01 9.916889e-02 5.049535e-03 8.406956e-06   Fold03\n808   VF   VF 9.079104e-01 8.691073e-02 5.174128e-03 4.749185e-06   Fold03\n809   VF   VF 8.500978e-01 1.362361e-01 1.363081e-02 3.537794e-05   Fold03\n810   VF   VF 8.612644e-01 1.288733e-01 9.841424e-03 2.083000e-05   Fold03\n811   VF   VF 8.583328e-01 1.315883e-01 1.005757e-02 2.130047e-05   Fold03\n812   VF   VF 9.131736e-01 8.277826e-02 4.043321e-03 4.769412e-06   Fold03\n813   VF   VF 8.552671e-01 1.343370e-01 1.037345e-02 2.241287e-05   Fold03\n814   VF   VF 8.556431e-01 1.339977e-01 1.033695e-02 2.228085e-05   Fold03\n815   VF   VF 8.211794e-01 1.610783e-01 1.765680e-02 8.553195e-05   Fold03\n816   VF   VF 8.755137e-01 1.187721e-01 5.695689e-03 1.855613e-05   Fold03\n817   VF   VF 7.885334e-01 1.961051e-01 1.512463e-02 2.368031e-04   Fold03\n818   VF    M 1.263451e-01 1.140569e-01 7.586469e-01 9.510511e-04   Fold03\n819   VF   VF 7.249015e-01 2.691187e-01 5.968456e-03 1.129729e-05   Fold03\n820   VF   VF 8.842447e-01 1.084543e-01 7.291335e-03 9.678451e-06   Fold03\n821   VF   VF 9.529146e-01 4.474163e-02 2.342346e-03 1.458571e-06   Fold03\n822   VF   VF 9.422102e-01 5.289932e-02 4.886851e-03 3.642372e-06   Fold03\n823   VF   VF 9.417975e-01 5.324827e-02 4.950514e-03 3.739225e-06   Fold03\n824   VF   VF 9.519758e-01 4.558440e-02 2.438183e-03 1.592066e-06   Fold03\n825   VF   VF 9.248098e-01 7.139790e-02 3.787163e-03 5.116255e-06   Fold03\n826   VF   VF 9.301914e-01 6.349037e-02 6.307700e-03 1.057405e-05   Fold03\n827   VF   VF 9.597167e-01 3.878048e-02 1.501562e-03 1.290955e-06   Fold03\n828   VF   VF 9.172161e-01 7.713098e-02 5.646671e-03 6.279466e-06   Fold03\n829   VF   VF 9.746732e-01 2.442757e-02 8.989900e-04 2.350919e-07   Fold03\n830   VF   VF 9.700690e-01 2.881139e-02 1.119140e-03 4.266565e-07   Fold03\n831   VF   VF 9.709701e-01 2.793094e-02 1.098677e-03 3.180557e-07   Fold03\n832   VF   VF 6.839242e-01 2.453762e-01 6.737754e-02 3.322102e-03   Fold03\n833   VF   VF 6.819018e-01 2.465202e-01 6.816854e-02 3.409529e-03   Fold03\n834   VF   VF 9.253815e-01 6.958580e-02 5.027041e-03 5.681560e-06   Fold03\n835   VF   VF 9.728140e-01 2.614983e-02 1.035834e-03 3.072756e-07   Fold03\n836   VF   VF 9.445171e-01 5.309288e-02 2.387568e-03 2.411309e-06   Fold03\n837   VF   VF 9.372981e-01 5.868858e-02 4.008756e-03 4.548621e-06   Fold03\n838   VF   VF 9.183214e-01 7.660856e-02 5.058464e-03 1.160730e-05   Fold03\n839   VF   VF 9.831354e-01 1.635161e-02 5.128911e-04 1.091153e-07   Fold03\n840   VF    F 6.881919e-02 8.006391e-01 1.294338e-01 1.107907e-03   Fold03\n841   VF    F 1.051248e-01 7.560201e-01 1.384562e-01 3.988133e-04   Fold03\n842   VF    F 1.101269e-01 7.626259e-01 1.268403e-01 4.069053e-04   Fold03\n843   VF    F 9.869068e-02 7.736366e-01 1.271642e-01 5.084899e-04   Fold03\n844   VF   VF 7.462537e-01 2.323087e-01 2.141621e-02 2.140225e-05   Fold03\n845   VF   VF 6.251812e-01 3.124039e-01 6.231557e-02 9.931975e-05   Fold03\n846   VF   VF 6.229012e-01 3.129475e-01 6.405106e-02 1.002310e-04   Fold03\n847   VF   VF 7.323538e-01 2.480646e-01 1.954906e-02 3.252233e-05   Fold03\n848   VF   VF 7.886699e-01 1.912924e-01 2.002699e-02 1.064792e-05   Fold03\n849   VF   VF 8.035513e-01 1.815389e-01 1.490032e-02 9.475853e-06   Fold03\n850   VF   VF 8.237516e-01 1.640444e-01 1.219765e-02 6.307038e-06   Fold03\n851   VF   VF 7.668469e-01 2.084251e-01 2.471426e-02 1.369435e-05   Fold03\n852   VF   VF 7.022987e-01 2.729257e-01 2.472595e-02 4.958135e-05   Fold03\n853   VF   VF 6.988402e-01 2.755559e-01 2.555175e-02 5.210822e-05   Fold03\n854   VF   VF 6.394964e-01 3.106537e-01 4.977321e-02 7.667232e-05   Fold03\n855   VF   VF 7.756700e-01 2.067451e-01 1.756997e-02 1.496507e-05   Fold03\n856   VF   VF 7.797257e-01 2.036967e-01 1.656449e-02 1.318375e-05   Fold03\n857   VF   VF 7.465390e-01 2.303978e-01 2.304222e-02 2.089943e-05   Fold03\n858   VF   VF 7.215241e-01 2.504493e-01 2.798982e-02 3.676387e-05   Fold03\n859   VF   VF 7.399997e-01 2.288784e-01 3.109535e-02 2.653663e-05   Fold03\n860   VF   VF 7.034967e-01 2.632680e-01 3.320005e-02 3.532754e-05   Fold03\n861   VF   VF 6.445971e-01 3.194818e-01 3.580639e-02 1.148051e-04   Fold03\n862   VF   VF 7.320085e-01 2.398031e-01 2.815567e-02 3.279742e-05   Fold03\n863   VF   VF 7.331425e-01 2.438912e-01 2.294322e-02 2.310897e-05   Fold03\n864   VF   VF 7.700787e-01 2.115268e-01 1.837815e-02 1.635769e-05   Fold03\n865   VF   VF 8.223500e-01 1.649237e-01 1.271923e-02 7.058193e-06   Fold03\n866   VF   VF 7.316958e-01 2.445975e-01 2.368166e-02 2.498649e-05   Fold03\n867   VF   VF 5.702618e-01 3.586759e-01 7.087108e-02 1.912234e-04   Fold03\n868   VF   VF 7.998659e-01 1.821500e-01 1.797559e-02 8.492663e-06   Fold03\n869   VF   VF 7.075103e-01 2.815187e-01 1.086288e-02 1.081776e-04   Fold03\n870   VF    F 8.159334e-02 7.737307e-01 1.400741e-01 4.601857e-03   Fold03\n871   VF    F 1.046355e-01 7.644900e-01 1.280983e-01 2.776131e-03   Fold03\n872    F    F 3.540986e-01 5.308143e-01 1.136629e-01 1.424174e-03   Fold03\n873    F    F 3.533564e-01 5.312092e-01 1.139999e-01 1.434525e-03   Fold03\n874    F    F 2.353658e-01 6.112217e-01 1.450075e-01 8.404941e-03   Fold03\n875    F    L 4.372793e-03 1.201951e-02 4.561737e-02 9.379903e-01   Fold03\n876    F   VF 5.759077e-01 3.696989e-01 5.371752e-02 6.758832e-04   Fold03\n877    F    F 1.883441e-01 4.755853e-01 2.101694e-01 1.259011e-01   Fold03\n878    F   VF 6.685251e-01 2.831099e-01 4.806674e-02 2.981937e-04   Fold03\n879    F   VF 4.728610e-01 4.187487e-01 1.074421e-01 9.481711e-04   Fold03\n880    F   VF 7.043760e-01 2.438260e-01 5.173929e-02 5.875339e-05   Fold03\n881    F   VF 7.422347e-01 2.263846e-01 3.134546e-02 3.525053e-05   Fold03\n882    F    F 1.473783e-01 6.235839e-01 2.259827e-01 3.055166e-03   Fold03\n883    F    F 1.770840e-01 6.298784e-01 1.915425e-01 1.495065e-03   Fold03\n884    F    F 1.864512e-01 6.220353e-01 1.900090e-01 1.504523e-03   Fold03\n885    F    F 1.259547e-01 6.467871e-01 2.219376e-01 5.320594e-03   Fold03\n886    F    F 1.553489e-01 6.110715e-01 2.309690e-01 2.610576e-03   Fold03\n887    F    F 1.487979e-01 6.155093e-01 2.330665e-01 2.626335e-03   Fold03\n888    F    F 2.089614e-01 6.112888e-01 1.785086e-01 1.241261e-03   Fold03\n889    F    F 1.182466e-01 5.987673e-01 2.793565e-01 3.629574e-03   Fold03\n890    F    F 1.661052e-01 6.295469e-01 2.021633e-01 2.184594e-03   Fold03\n891    F    F 1.648194e-01 6.245039e-01 2.082812e-01 2.395464e-03   Fold03\n892    F    F 1.636482e-01 6.295576e-01 2.045294e-01 2.264787e-03   Fold03\n893    F    F 1.649903e-01 6.255494e-01 2.071084e-01 2.351929e-03   Fold03\n894    F    F 1.229100e-01 6.005473e-01 2.719365e-01 4.606283e-03   Fold03\n895    F    F 8.629696e-02 5.900552e-01 3.148407e-01 8.807080e-03   Fold03\n896    F    F 1.758394e-01 5.948332e-01 2.279126e-01 1.414840e-03   Fold03\n897    F   VF 4.420686e-01 3.508475e-01 1.766232e-01 3.046063e-02   Fold03\n898    F    M 1.029488e-02 2.819522e-01 5.202463e-01 1.875066e-01   Fold03\n899    F   VF 8.353851e-01 1.502623e-01 1.429772e-02 5.492664e-05   Fold03\n900    F   VF 7.396070e-01 2.275676e-01 3.261378e-02 2.115947e-04   Fold03\n901    F   VF 8.860341e-01 1.067541e-01 7.203563e-03 8.268316e-06   Fold03\n902    F   VF 5.458471e-01 3.640810e-01 7.816135e-02 1.191057e-02   Fold03\n903    F   VF 8.703835e-01 1.219420e-01 7.655743e-03 1.873776e-05   Fold03\n904    F   VF 8.618427e-01 1.277091e-01 1.042621e-02 2.191307e-05   Fold03\n905    F   VF 6.387919e-01 3.502259e-01 1.092870e-02 5.351545e-05   Fold03\n906    F    F 2.839755e-01 3.836801e-01 1.546675e-01 1.776769e-01   Fold03\n907    F   VF 6.248190e-01 3.631537e-01 1.196186e-02 6.546686e-05   Fold03\n908    F   VF 6.181529e-01 3.692961e-01 1.247898e-02 7.197495e-05   Fold03\n909    F   VF 8.368846e-01 1.510865e-01 1.198071e-02 4.815147e-05   Fold03\n910    F   VF 6.067329e-01 3.797811e-01 1.340145e-02 8.448944e-05   Fold03\n911    F   VF 8.055987e-01 1.803959e-01 1.390314e-02 1.022703e-04   Fold03\n912    F   VF 5.996193e-01 3.862708e-01 1.401635e-02 9.358922e-05   Fold03\n913    F    L 8.524922e-05 2.234945e-03 2.408019e-02 9.735996e-01   Fold03\n914    F   VF 5.742063e-01 4.093547e-01 1.630701e-02 1.319582e-04   Fold03\n915    F   VF 9.717652e-01 2.694631e-02 1.287533e-03 9.666119e-07   Fold03\n916    F   VF 8.363827e-01 1.471572e-01 1.642426e-02 3.579539e-05   Fold03\n917    F    F 1.158874e-01 7.625865e-01 1.211718e-01 3.543477e-04   Fold03\n918    F    F 1.114302e-01 7.507726e-01 1.375780e-01 2.191449e-04   Fold03\n919    F    F 8.669760e-02 7.401071e-01 1.727261e-01 4.692867e-04   Fold03\n920    F    F 9.113469e-02 7.668267e-01 1.415153e-01 5.232718e-04   Fold03\n921    F    F 3.530895e-02 7.564321e-01 1.997931e-01 8.465890e-03   Fold03\n922    F    F 6.166866e-02 7.389124e-01 1.982256e-01 1.193335e-03   Fold03\n923    F    F 7.371191e-02 7.655725e-01 1.598148e-01 9.008125e-04   Fold03\n924    F    F 5.088020e-02 7.295245e-01 2.179852e-01 1.610087e-03   Fold03\n925    F    F 5.061282e-02 7.165887e-01 2.307206e-01 2.077958e-03   Fold03\n926    F    F 7.380273e-02 7.286040e-01 1.967118e-01 8.815344e-04   Fold03\n927    F    F 5.512098e-02 7.506676e-01 1.923198e-01 1.891618e-03   Fold03\n928    F    F 1.174967e-01 7.713990e-01 1.108324e-01 2.718980e-04   Fold03\n929    F    F 5.924173e-02 7.422609e-01 1.973511e-01 1.146199e-03   Fold03\n930    F   VF 7.468682e-01 2.231821e-01 2.993311e-02 1.658765e-05   Fold03\n931    F   VF 7.946475e-01 1.895168e-01 1.582539e-02 1.027144e-05   Fold03\n932    F   VF 7.449534e-01 2.289364e-01 2.609149e-02 1.871477e-05   Fold03\n933    F   VF 7.247479e-01 2.390013e-01 3.622860e-02 2.222573e-05   Fold03\n934    F   VF 6.797407e-01 2.926026e-01 2.759423e-02 6.248056e-05   Fold03\n935    F   VF 5.648404e-01 3.686881e-01 6.633937e-02 1.321243e-04   Fold03\n936    F    F 4.132978e-01 4.567719e-01 1.253573e-01 4.573032e-03   Fold03\n937    F   VF 6.443989e-01 3.090027e-01 4.651065e-02 8.772436e-05   Fold03\n938    F   VF 6.922013e-01 2.807384e-01 2.699894e-02 6.140870e-05   Fold03\n939    F   VF 7.315882e-01 2.401492e-01 2.822952e-02 3.307213e-05   Fold03\n940    F   VF 7.213794e-01 2.490669e-01 2.951560e-02 3.805647e-05   Fold03\n941    F   VF 6.108565e-01 3.459362e-01 4.303741e-02 1.699160e-04   Fold03\n942    F    F 9.637341e-02 7.756533e-01 1.253351e-01 2.638176e-03   Fold03\n943    F    F 7.783564e-02 7.716949e-01 1.452918e-01 5.177637e-03   Fold03\n944    F    F 8.504192e-02 7.509727e-01 1.609365e-01 3.048814e-03   Fold03\n945    F    F 7.026779e-02 7.614137e-01 1.618097e-01 6.508742e-03   Fold03\n946    F    F 8.429041e-02 7.589022e-01 1.527088e-01 4.098520e-03   Fold03\n947    F    F 4.815380e-02 7.198692e-01 2.203600e-01 1.161697e-02   Fold03\n948    F    F 8.389576e-02 7.612911e-01 1.502971e-01 4.515984e-03   Fold03\n949    F    F 8.380880e-02 7.610330e-01 1.506374e-01 4.520850e-03   Fold03\n950    F    F 8.252936e-02 7.587367e-01 1.540052e-01 4.728691e-03   Fold03\n951    F    F 7.028264e-02 7.276529e-01 1.966984e-01 5.366100e-03   Fold03\n952    F    F 1.239095e-02 4.351246e-01 2.436475e-01 3.088370e-01   Fold03\n953    F    F 1.229227e-02 4.331899e-01 2.434650e-01 3.110529e-01   Fold03\n954    F    F 1.234697e-02 4.342977e-01 2.435751e-01 3.097802e-01   Fold03\n955    F    F 5.743368e-02 7.336986e-01 2.016725e-01 7.195292e-03   Fold03\n956    F    F 6.290046e-02 7.568948e-01 1.715253e-01 8.679464e-03   Fold03\n957    F    F 6.246914e-02 7.589677e-01 1.702362e-01 8.326922e-03   Fold03\n958    F    F 9.829972e-03 3.927726e-01 2.844917e-01 3.129057e-01   Fold03\n959    F    L 4.024299e-03 2.397707e-01 2.193841e-01 5.368209e-01   Fold03\n960    F    F 6.778540e-02 7.272993e-01 1.990189e-01 5.896387e-03   Fold03\n961    F    F 6.743404e-02 7.262747e-01 2.003532e-01 5.938047e-03   Fold03\n962    F    F 6.124635e-02 7.582358e-01 1.717287e-01 8.789146e-03   Fold03\n963    F    F 6.771751e-02 7.270321e-01 1.993249e-01 5.925492e-03   Fold03\n964    F    F 7.073383e-02 7.246412e-01 1.995330e-01 5.092033e-03   Fold03\n965    F    F 3.401149e-02 6.549582e-01 2.693798e-01 4.165057e-02   Fold03\n966    F    F 7.521999e-02 7.538390e-01 1.652866e-01 5.654493e-03   Fold03\n967    F    F 7.307687e-02 7.476104e-01 1.734450e-01 5.867740e-03   Fold03\n968    F    F 3.734269e-02 6.795467e-01 2.404207e-01 4.268994e-02   Fold03\n969    F    F 4.697655e-02 7.671940e-01 1.685934e-01 1.723608e-02   Fold03\n970    F    F 4.020351e-02 6.875439e-01 2.562089e-01 1.604369e-02   Fold03\n971    F    F 5.240731e-02 6.992286e-01 2.404752e-01 7.888891e-03   Fold03\n972    F    F 2.836140e-02 6.468776e-01 2.741945e-01 5.056644e-02   Fold03\n973    F    F 6.131289e-02 7.181065e-01 2.129501e-01 7.630520e-03   Fold03\n974    F    F 8.564945e-02 7.759095e-01 1.344031e-01 4.037910e-03   Fold03\n975    F    F 8.619696e-02 7.761259e-01 1.337077e-01 3.969458e-03   Fold03\n976    F    F 5.553452e-02 7.006927e-01 2.342750e-01 9.497798e-03   Fold03\n977    F    F 5.147259e-02 7.404170e-01 1.947041e-01 1.340626e-02   Fold03\n978    F    F 4.369032e-02 6.890095e-01 2.454337e-01 2.186646e-02   Fold03\n979    F    F 6.041874e-02 7.397875e-01 1.924210e-01 7.372818e-03   Fold03\n980    M    L 5.444004e-05 2.420574e-03 6.672767e-03 9.908522e-01   Fold03\n981    M    F 2.930861e-01 5.570176e-01 1.470793e-01 2.816969e-03   Fold03\n982    M    M 6.188884e-02 1.060684e-01 5.156488e-01 3.163940e-01   Fold03\n983    M   VF 5.745374e-01 3.567988e-01 6.802719e-02 6.366150e-04   Fold03\n984    M    M 1.503479e-01 2.091982e-01 6.389696e-01 1.484298e-03   Fold03\n985    M    L 4.039152e-03 3.599219e-02 6.135541e-02 8.986132e-01   Fold03\n986    M    F 8.957530e-02 5.740522e-01 3.299908e-01 6.381768e-03   Fold03\n987    M    M 2.004250e-02 3.377952e-01 4.574460e-01 1.847162e-01   Fold03\n988    M    L 6.484979e-05 9.793014e-03 1.057020e-01 8.844401e-01   Fold03\n989    M    F 6.876398e-02 5.584170e-01 3.612436e-01 1.157542e-02   Fold03\n990    M    F 4.933345e-02 5.324810e-01 3.968646e-01 2.132091e-02   Fold03\n991    M    L 1.151238e-16 3.571048e-12 2.363631e-08 1.000000e+00   Fold03\n992    M   VF 4.437889e-01 3.772830e-01 1.691550e-01 9.773100e-03   Fold03\n993    M   VF 8.098501e-01 1.776638e-01 1.240127e-02 8.487338e-05   Fold03\n994    M    F 5.035868e-02 7.292495e-01 2.187718e-01 1.620040e-03   Fold03\n995    M    F 1.591372e-02 5.559532e-01 3.744950e-01 5.363810e-02   Fold03\n996    M    F 1.034300e-02 5.053350e-01 4.073158e-01 7.700626e-02   Fold03\n997    M    F 7.343431e-02 7.142307e-01 2.115413e-01 7.936731e-04   Fold03\n998    M    M 1.560828e-03 8.709903e-02 9.090435e-01 2.296656e-03   Fold03\n999    M    F 4.385307e-02 7.585045e-01 1.935751e-01 4.067374e-03   Fold03\n1000   M    F 4.028271e-02 7.499593e-01 2.045453e-01 5.212762e-03   Fold03\n1001   M    M 2.509783e-04 2.213174e-02 9.753955e-01 2.221836e-03   Fold03\n1002   M    M 3.576303e-04 2.692344e-02 9.705451e-01 2.173871e-03   Fold03\n1003   M   VF 5.728841e-01 3.519188e-01 7.493481e-02 2.622893e-04   Fold03\n1004   M    M 3.174502e-01 2.621071e-01 4.202760e-01 1.667150e-04   Fold03\n1005   M    M 4.934287e-02 2.949898e-01 4.985497e-01 1.571176e-01   Fold03\n1006   M    M 2.727811e-02 2.487362e-01 3.989441e-01 3.250416e-01   Fold03\n1007   M    L 1.729866e-02 2.006728e-01 3.762784e-01 4.057501e-01   Fold03\n1008   M    F 1.076474e-02 4.369761e-01 3.612924e-01 1.909668e-01   Fold03\n1009   M    F 7.220459e-02 7.681644e-01 1.535758e-01 6.055216e-03   Fold03\n1010   M    F 4.449073e-02 7.164882e-01 2.265876e-01 1.243342e-02   Fold03\n1011   M    F 6.735155e-02 7.496803e-01 1.757775e-01 7.190654e-03   Fold03\n1012   M    M 1.281622e-02 2.943056e-01 6.825489e-01 1.032935e-02   Fold03\n1013   M    L 4.096351e-03 2.493710e-01 2.225870e-01 5.239457e-01   Fold03\n1014   M    F 2.130507e-02 6.302240e-01 2.457806e-01 1.026904e-01   Fold03\n1015   M    F 1.231547e-02 4.448805e-01 4.365100e-01 1.062940e-01   Fold03\n1016   M    F 3.491930e-02 6.578087e-01 2.634041e-01 4.386792e-02   Fold03\n1017   M    F 6.698115e-02 7.222661e-01 2.042124e-01 6.540262e-03   Fold03\n1018   M    L 7.415832e-06 2.999487e-03 2.120026e-02 9.757928e-01   Fold03\n1019   M    F 5.390159e-02 6.903476e-01 2.475205e-01 8.230259e-03   Fold03\n1020   M    M 1.909148e-04 1.859897e-02 9.661282e-01 1.508196e-02   Fold03\n1021   L    L 3.312804e-03 8.827622e-02 2.169937e-01 6.914173e-01   Fold03\n1022   L    L 5.637959e-03 1.180896e-01 3.247385e-01 5.515339e-01   Fold03\n1023   L    L 8.317836e-03 1.414187e-01 2.934367e-01 5.568268e-01   Fold03\n1024   L    L 4.329726e-03 3.187172e-02 2.231624e-01 7.406361e-01   Fold03\n1025   L    M 9.178392e-02 3.661547e-01 4.202717e-01 1.217896e-01   Fold03\n1026   L   VF 6.697401e-01 2.747438e-01 5.535763e-02 1.584428e-04   Fold03\n1027   L   VF 5.005163e-01 3.724797e-01 1.219454e-01 5.058590e-03   Fold03\n1028   L    L 1.939620e-06 7.135014e-04 3.137242e-03 9.961473e-01   Fold03\n1029   L    L 1.020992e-06 4.292575e-04 1.656421e-03 9.979133e-01   Fold03\n1030   L    L 1.986119e-06 7.325902e-04 3.188006e-03 9.960774e-01   Fold03\n1031   L    L 6.371432e-07 3.204722e-04 1.039003e-03 9.986399e-01   Fold03\n1032   L    L 1.896363e-06 6.550454e-04 2.380164e-03 9.969629e-01   Fold03\n1033   L    L 3.996971e-07 2.352830e-04 1.332700e-03 9.984316e-01   Fold03\n1034   L    L 2.625446e-07 1.755800e-04 1.148032e-03 9.986761e-01   Fold03\n1035   L    F 4.135671e-02 6.867167e-01 2.466560e-01 2.527066e-02   Fold03\n1036   L    F 3.468346e-02 6.462154e-01 2.939892e-01 2.511203e-02   Fold03\n1037   L    F 3.449856e-02 6.450260e-01 2.950125e-01 2.546292e-02   Fold03\n1038   L    L 1.572751e-07 1.186895e-04 9.338258e-04 9.989473e-01   Fold03\n1039   L    F 2.615199e-02 6.814479e-01 2.313285e-01 6.107164e-02   Fold03\n1040   L    L 1.415139e-07 1.105443e-04 8.941587e-04 9.989952e-01   Fold03\n1041   L    L 1.415870e-07 1.110914e-04 8.964077e-04 9.989924e-01   Fold03\n1042  VF   VF 9.490835e-01 4.692744e-02 3.979715e-03 9.354902e-06   Fold04\n1043  VF   VF 9.368736e-01 5.837632e-02 4.726538e-03 2.356136e-05   Fold04\n1044  VF   VF 9.419571e-01 5.289479e-02 5.133313e-03 1.479434e-05   Fold04\n1045  VF   VF 9.495949e-01 4.647279e-02 3.924226e-03 8.130057e-06   Fold04\n1046  VF   VF 9.560343e-01 4.094228e-02 3.018159e-03 5.219952e-06   Fold04\n1047  VF    F 4.024636e-01 5.201454e-01 7.346159e-02 3.929330e-03   Fold04\n1048  VF    F 3.983631e-01 5.229582e-01 7.419519e-02 4.483520e-03   Fold04\n1049  VF    F 4.485242e-01 4.566480e-01 9.241303e-02 2.414742e-03   Fold04\n1050  VF   VF 8.542799e-01 1.120422e-01 3.356851e-02 1.094160e-04   Fold04\n1051  VF   VF 9.428220e-01 4.867167e-02 8.493972e-03 1.236620e-05   Fold04\n1052  VF   VF 9.579077e-01 3.549387e-02 6.592321e-03 6.134470e-06   Fold04\n1053  VF   VF 9.538248e-01 4.084357e-02 5.318264e-03 1.336774e-05   Fold04\n1054  VF   VF 9.648597e-01 3.017644e-02 4.958754e-03 5.105878e-06   Fold04\n1055  VF   VF 8.463139e-01 1.204034e-01 3.316480e-02 1.178593e-04   Fold04\n1056  VF   VF 8.367207e-01 1.300700e-01 3.308588e-02 1.234324e-04   Fold04\n1057  VF   VF 8.169434e-01 1.431333e-01 3.976275e-02 1.605350e-04   Fold04\n1058  VF   VF 9.866742e-01 1.271735e-02 6.077234e-04 7.127221e-07   Fold04\n1059  VF   VF 9.841909e-01 1.496642e-02 8.414767e-04 1.242338e-06   Fold04\n1060  VF   VF 9.839288e-01 1.517245e-02 8.978547e-04 9.413513e-07   Fold04\n1061  VF   VF 9.208087e-01 6.300970e-02 1.591867e-02 2.629672e-04   Fold04\n1062  VF   VF 9.882166e-01 1.130100e-02 4.819190e-04 5.097991e-07   Fold04\n1063  VF   VF 9.876504e-01 1.167864e-02 6.703187e-04 6.703820e-07   Fold04\n1064  VF   VF 9.797472e-01 1.892384e-02 1.326329e-03 2.648737e-06   Fold04\n1065  VF   VF 9.797240e-01 1.894494e-02 1.328423e-03 2.631827e-06   Fold04\n1066  VF   VF 9.799453e-01 1.872219e-02 1.330063e-03 2.411200e-06   Fold04\n1067  VF   VF 9.706538e-01 2.665036e-02 2.686635e-03 9.161298e-06   Fold04\n1068  VF   VF 9.773411e-01 2.097939e-02 1.674717e-03 4.789429e-06   Fold04\n1069  VF   VF 9.587211e-01 3.616167e-02 5.090470e-03 2.679516e-05   Fold04\n1070  VF   VF 9.714356e-01 2.613455e-02 2.421085e-03 8.722617e-06   Fold04\n1071  VF   VF 9.798756e-01 1.878844e-02 1.333292e-03 2.623697e-06   Fold04\n1072  VF   VF 9.534074e-01 4.034023e-02 6.211159e-03 4.123400e-05   Fold04\n1073  VF   VF 9.554464e-01 3.868621e-02 5.830376e-03 3.698824e-05   Fold04\n1074  VF   VF 7.058981e-01 2.655423e-01 2.842160e-02 1.380323e-04   Fold04\n1075  VF   VF 6.730294e-01 2.907375e-01 3.599533e-02 2.378348e-04   Fold04\n1076  VF   VF 5.900136e-01 3.573006e-01 5.231706e-02 3.687319e-04   Fold04\n1077  VF   VF 7.519954e-01 2.129942e-01 3.493095e-02 7.944730e-05   Fold04\n1078  VF   VF 7.275729e-01 2.375997e-01 3.472494e-02 1.024028e-04   Fold04\n1079  VF   VF 8.120016e-01 1.724700e-01 1.550669e-02 2.169241e-05   Fold04\n1080  VF   VF 8.297790e-01 1.539733e-01 1.622910e-02 1.857475e-05   Fold04\n1081  VF   VF 8.656607e-01 1.262700e-01 8.062868e-03 6.487603e-06   Fold04\n1082  VF    F 1.390873e-01 6.779899e-01 1.809609e-01 1.961909e-03   Fold04\n1083  VF    F 1.742615e-01 6.797750e-01 1.446255e-01 1.337943e-03   Fold04\n1084  VF    F 7.885064e-02 6.346601e-01 2.791928e-01 7.296469e-03   Fold04\n1085  VF    F 1.061842e-01 6.632031e-01 2.271859e-01 3.426801e-03   Fold04\n1086  VF   VF 9.312361e-01 6.472675e-02 4.023824e-03 1.333134e-05   Fold04\n1087  VF   VF 9.625568e-01 3.600943e-02 1.428071e-03 5.655049e-06   Fold04\n1088  VF   VF 9.856584e-01 1.387037e-02 4.707727e-04 4.580338e-07   Fold04\n1089  VF   VF 9.567771e-01 4.065650e-02 2.561280e-03 5.070758e-06   Fold04\n1090  VF   VF 9.918604e-01 7.878278e-03 2.611400e-04 1.413577e-07   Fold04\n1091  VF   VF 9.305116e-01 6.441802e-02 5.051035e-03 1.933191e-05   Fold04\n1092  VF   VF 9.754840e-01 2.346389e-02 1.050510e-03 1.554109e-06   Fold04\n1093  VF   VF 9.463898e-01 5.081244e-02 2.789981e-03 7.811681e-06   Fold04\n1094  VF   VF 9.675186e-01 3.059948e-02 1.877954e-03 4.004145e-06   Fold04\n1095  VF   VF 9.845204e-01 1.479363e-02 6.852900e-04 6.432270e-07   Fold04\n1096  VF   VF 9.884730e-01 1.113541e-02 3.913484e-04 2.413772e-07   Fold04\n1097  VF   VF 9.596102e-01 3.819908e-02 2.185512e-03 5.239654e-06   Fold04\n1098  VF   VF 9.931033e-01 6.705364e-03 1.912957e-04 8.175935e-08   Fold04\n1099  VF   VF 9.573024e-01 3.979194e-02 2.896562e-03 9.098018e-06   Fold04\n1100  VF   VF 9.602619e-01 3.781867e-02 1.910162e-03 9.228143e-06   Fold04\n1101  VF   VF 9.803101e-01 1.887708e-02 8.116109e-04 1.251663e-06   Fold04\n1102  VF   VF 9.850779e-01 1.428703e-02 6.341889e-04 8.878616e-07   Fold04\n1103  VF   VF 9.887432e-01 1.096658e-02 2.900573e-04 2.059739e-07   Fold04\n1104  VF   VF 9.748119e-01 2.348315e-02 1.700756e-03 4.236806e-06   Fold04\n1105  VF   VF 9.053934e-01 9.010106e-02 4.495938e-03 9.644080e-06   Fold04\n1106  VF   VF 7.248399e-01 2.702064e-01 4.940653e-03 1.300953e-05   Fold04\n1107  VF   VF 8.916412e-01 1.037191e-01 4.620059e-03 1.965660e-05   Fold04\n1108  VF   VF 9.198160e-01 7.693374e-02 3.244697e-03 5.561639e-06   Fold04\n1109  VF   VF 8.871361e-01 1.065072e-01 6.344281e-03 1.238284e-05   Fold04\n1110  VF   VF 8.971669e-01 9.751790e-02 5.302857e-03 1.233415e-05   Fold04\n1111  VF   VF 9.048962e-01 9.054162e-02 4.551425e-03 1.077577e-05   Fold04\n1112  VF   VF 9.161118e-01 8.034624e-02 3.535332e-03 6.601958e-06   Fold04\n1113  VF   VF 9.138958e-01 8.222272e-02 3.874517e-03 6.918695e-06   Fold04\n1114  VF   VF 8.562803e-01 1.366468e-01 7.032004e-03 4.092568e-05   Fold04\n1115  VF   VF 9.197429e-01 7.611668e-02 4.132493e-03 7.925842e-06   Fold04\n1116  VF   VF 9.171247e-01 7.945476e-02 3.413403e-03 7.123946e-06   Fold04\n1117  VF   VF 9.114867e-01 8.345696e-02 5.047097e-03 9.204820e-06   Fold04\n1118  VF   VF 8.866229e-01 1.083170e-01 5.035278e-03 2.482225e-05   Fold04\n1119  VF   VF 6.690188e-01 3.231913e-01 7.754009e-03 3.597151e-05   Fold04\n1120  VF   VF 9.031394e-01 9.225776e-02 4.593474e-03 9.368788e-06   Fold04\n1121  VF   VF 8.645700e-01 1.260869e-01 9.310860e-03 3.215975e-05   Fold04\n1122  VF   VF 8.611848e-01 1.293649e-01 9.415456e-03 3.480975e-05   Fold04\n1123  VF   VF 8.923356e-01 1.018925e-01 5.756862e-03 1.503035e-05   Fold04\n1124  VF   VF 9.071108e-01 8.868490e-02 4.194812e-03 9.498352e-06   Fold04\n1125  VF   VF 8.920809e-01 1.020695e-01 5.833948e-03 1.564449e-05   Fold04\n1126  VF   VF 8.939562e-01 1.003313e-01 5.697618e-03 1.487891e-05   Fold04\n1127  VF   VF 9.077422e-01 8.797567e-02 4.274301e-03 7.870545e-06   Fold04\n1128  VF   VF 8.721365e-01 1.214859e-01 6.341910e-03 3.575367e-05   Fold04\n1129  VF   VF 9.070680e-01 8.859348e-02 4.330629e-03 7.906743e-06   Fold04\n1130  VF   VF 8.983386e-01 9.502278e-02 6.624341e-03 1.423832e-05   Fold04\n1131  VF   VF 5.622481e-01 3.430227e-01 8.326714e-02 1.146206e-02   Fold04\n1132  VF   VF 6.444029e-01 3.463090e-01 9.236766e-03 5.140102e-05   Fold04\n1133  VF   VF 8.763533e-01 1.159404e-01 7.682867e-03 2.345142e-05   Fold04\n1134  VF   VF 8.767258e-01 1.155498e-01 7.701141e-03 2.326043e-05   Fold04\n1135  VF   VF 8.764988e-01 1.157472e-01 7.731106e-03 2.289460e-05   Fold04\n1136  VF   VF 8.817641e-01 1.111410e-01 7.073198e-03 2.178251e-05   Fold04\n1137  VF   VF 8.785576e-01 1.139547e-01 7.464918e-03 2.280785e-05   Fold04\n1138  VF   VF 8.940590e-01 9.835130e-02 7.570600e-03 1.905690e-05   Fold04\n1139  VF   VF 6.183442e-01 3.705538e-01 1.102792e-02 7.404197e-05   Fold04\n1140  VF   VF 6.561446e-01 2.909416e-01 4.866451e-02 4.249312e-03   Fold04\n1141  VF   VF 6.138994e-01 3.746599e-01 1.136196e-02 7.880931e-05   Fold04\n1142  VF   VF 8.501724e-01 1.410172e-01 8.743783e-03 6.668391e-05   Fold04\n1143  VF   VF 6.123078e-01 3.761399e-01 1.147193e-02 8.039090e-05   Fold04\n1144  VF   VF 9.062039e-01 8.942879e-02 4.358655e-03 8.668332e-06   Fold04\n1145  VF   VF 9.101912e-01 8.573065e-02 4.070183e-03 7.917182e-06   Fold04\n1146  VF   VF 8.786620e-01 1.139258e-01 7.392613e-03 1.954940e-05   Fold04\n1147  VF   VF 8.981703e-01 9.778552e-02 4.027871e-03 1.635622e-05   Fold04\n1148  VF   VF 8.279667e-01 1.585132e-01 1.339939e-02 1.207447e-04   Fold04\n1149  VF   VF 8.551195e-01 1.341901e-01 1.064409e-02 4.634080e-05   Fold04\n1150  VF   VF 8.862436e-01 1.089078e-01 4.827013e-03 2.156779e-05   Fold04\n1151  VF   VF 9.199269e-01 7.686812e-02 3.199522e-03 5.431105e-06   Fold04\n1152  VF   VF 8.766466e-01 1.156226e-01 7.709056e-03 2.169074e-05   Fold04\n1153  VF   VF 8.725806e-01 1.192799e-01 8.116388e-03 2.309909e-05   Fold04\n1154  VF   VF 9.106451e-01 8.537347e-02 3.976342e-03 5.043963e-06   Fold04\n1155  VF   VF 8.582263e-01 1.324620e-01 9.234065e-03 7.763589e-05   Fold04\n1156  VF   VF 9.639168e-01 3.463475e-02 1.447392e-03 1.067201e-06   Fold04\n1157  VF   VF 9.519173e-01 4.583625e-02 2.244499e-03 1.972510e-06   Fold04\n1158  VF   VF 9.308866e-01 6.347862e-02 5.621150e-03 1.363726e-05   Fold04\n1159  VF   VF 9.411366e-01 5.585819e-02 3.002042e-03 3.192009e-06   Fold04\n1160  VF   VF 9.217358e-01 7.306731e-02 5.189266e-03 7.617206e-06   Fold04\n1161  VF   VF 9.621897e-01 3.622772e-02 1.581343e-03 1.256631e-06   Fold04\n1162  VF   VF 9.805010e-01 1.874137e-02 7.573463e-04 3.280279e-07   Fold04\n1163  VF   VF 9.159188e-01 7.988790e-02 4.181394e-03 1.188657e-05   Fold04\n1164  VF   VF 8.902166e-01 1.012664e-01 8.498562e-03 1.839318e-05   Fold04\n1165  VF   VF 9.773114e-01 2.201317e-02 6.751738e-04 2.919907e-07   Fold04\n1166  VF   VF 8.557715e-01 1.224370e-01 2.122121e-02 5.703154e-04   Fold04\n1167  VF   VF 7.793771e-01 1.804575e-01 3.877004e-02 1.395403e-03   Fold04\n1168  VF   VF 9.671867e-01 3.162426e-02 1.187242e-03 1.807285e-06   Fold04\n1169  VF   VF 8.370633e-01 1.494222e-01 1.347841e-02 3.613001e-05   Fold04\n1170  VF   VF 8.785756e-01 1.105459e-01 1.084748e-02 3.099234e-05   Fold04\n1171  VF   VF 9.765783e-01 2.239927e-02 1.021927e-03 4.527373e-07   Fold04\n1172  VF   VF 9.204351e-01 7.511103e-02 4.439415e-03 1.444528e-05   Fold04\n1173  VF   VF 7.490162e-01 2.291696e-01 2.173362e-02 8.063904e-05   Fold04\n1174  VF   VF 9.657924e-01 3.288728e-02 1.319455e-03 9.029590e-07   Fold04\n1175  VF   VF 9.252672e-01 6.895248e-02 5.768639e-03 1.172918e-05   Fold04\n1176  VF   VF 9.702317e-01 2.883237e-02 9.349599e-04 9.713495e-07   Fold04\n1177  VF    F 1.073563e-01 7.584374e-01 1.336799e-01 5.264300e-04   Fold04\n1178  VF    F 1.194957e-01 7.609080e-01 1.192022e-01 3.940765e-04   Fold04\n1179  VF    F 5.607753e-02 7.304695e-01 2.121608e-01 1.292225e-03   Fold04\n1180  VF    F 9.477878e-02 7.565455e-01 1.480428e-01 6.328549e-04   Fold04\n1181  VF    F 7.761712e-02 7.418849e-01 1.795383e-01 9.596590e-04   Fold04\n1182  VF   VF 7.281280e-01 2.540134e-01 1.781699e-02 4.161407e-05   Fold04\n1183  VF   VF 6.329862e-01 3.124071e-01 5.448141e-02 1.252252e-04   Fold04\n1184  VF   VF 6.058137e-01 3.409729e-01 5.305457e-02 1.588545e-04   Fold04\n1185  VF   VF 6.013546e-01 3.420630e-01 5.641810e-02 1.642219e-04   Fold04\n1186  VF   VF 7.540775e-01 2.240403e-01 2.186204e-02 2.016545e-05   Fold04\n1187  VF   VF 7.313800e-01 2.507736e-01 1.780594e-02 4.039136e-05   Fold04\n1188  VF   VF 6.341736e-01 3.371841e-01 2.855141e-02 9.096713e-05   Fold04\n1189  VF   VF 7.316192e-01 2.429638e-01 2.539223e-02 2.474358e-05   Fold04\n1190  VF   VF 6.605708e-01 3.032062e-01 3.616202e-02 6.093159e-05   Fold04\n1191  VF   VF 6.743991e-01 3.001131e-01 2.541639e-02 7.140347e-05   Fold04\n1192  VF   VF 6.722327e-01 3.022636e-01 2.542983e-02 7.385872e-05   Fold04\n1193  VF   VF 7.138924e-01 2.674151e-01 1.865088e-02 4.169724e-05   Fold04\n1194  VF   VF 7.953357e-01 1.923854e-01 1.226872e-02 1.018115e-05   Fold04\n1195  VF   VF 7.949116e-01 1.925073e-01 1.257137e-02 9.812698e-06   Fold04\n1196  VF   VF 7.656067e-01 2.177809e-01 1.659596e-02 1.645208e-05   Fold04\n1197  VF   VF 6.757429e-01 2.928918e-01 3.131422e-02 5.111241e-05   Fold04\n1198  VF   VF 7.023233e-01 2.645291e-01 3.310419e-02 4.338658e-05   Fold04\n1199  VF   VF 7.296353e-01 2.418644e-01 2.846242e-02 3.784495e-05   Fold04\n1200  VF   VF 5.759153e-01 3.666233e-01 5.731145e-02 1.499210e-04   Fold04\n1201  VF   VF 7.635380e-01 2.163151e-01 2.013032e-02 1.662757e-05   Fold04\n1202  VF   VF 7.351508e-01 2.466174e-01 1.821288e-02 1.897018e-05   Fold04\n1203  VF   VF 6.807021e-01 2.872875e-01 3.194549e-02 6.497676e-05   Fold04\n1204  VF   VF 6.242511e-01 3.399584e-01 3.564544e-02 1.450064e-04   Fold04\n1205  VF   VF 7.956550e-01 1.920022e-01 1.233245e-02 1.038600e-05   Fold04\n1206  VF   VF 6.796393e-01 2.802785e-01 4.001345e-02 6.875847e-05   Fold04\n1207  VF   VF 6.815216e-01 2.794469e-01 3.896348e-02 6.808197e-05   Fold04\n1208  VF   VF 5.850906e-01 3.567973e-01 5.792817e-02 1.840026e-04   Fold04\n1209  VF   VF 6.180350e-01 3.432023e-01 3.858186e-02 1.808121e-04   Fold04\n1210  VF   VF 7.776949e-01 2.038980e-01 1.839236e-02 1.468974e-05   Fold04\n1211  VF   VF 6.843413e-01 2.852120e-01 3.037272e-02 7.394554e-05   Fold04\n1212  VF   VF 6.820901e-01 2.870054e-01 3.082830e-02 7.616619e-05   Fold04\n1213  VF   VF 6.058583e-01 3.443860e-01 4.960420e-02 1.515114e-04   Fold04\n1214  VF   VF 7.703290e-01 2.145159e-01 1.514152e-02 1.358349e-05   Fold04\n1215  VF   VF 8.149603e-01 1.717983e-01 1.323496e-02 6.456374e-06   Fold04\n1216  VF   VF 7.154432e-01 2.588317e-01 2.569180e-02 3.329383e-05   Fold04\n1217  VF    F 9.024441e-02 7.936440e-01 1.123278e-01 3.783804e-03   Fold04\n1218  VF    F 8.113302e-02 7.881386e-01 1.269021e-01 3.826363e-03   Fold04\n1219   F   VF 9.128025e-01 7.843247e-02 8.680453e-03 8.456227e-05   Fold04\n1220   F    M 1.996093e-02 5.161912e-02 9.247519e-01 3.668002e-03   Fold04\n1221   F    F 4.319464e-01 4.922443e-01 7.259712e-02 3.212093e-03   Fold04\n1222   F    F 4.262223e-01 4.829283e-01 8.879164e-02 2.057863e-03   Fold04\n1223   F    F 3.532215e-01 5.284608e-01 1.140785e-01 4.239223e-03   Fold04\n1224   F    F 3.046457e-01 5.467679e-01 1.411733e-01 7.413074e-03   Fold04\n1225   F    M 1.498314e-01 1.450312e-01 4.496056e-01 2.555318e-01   Fold04\n1226   F    M 1.007316e-01 1.174164e-01 5.562805e-01 2.255715e-01   Fold04\n1227   F   VF 6.200583e-01 3.263891e-01 5.322428e-02 3.283264e-04   Fold04\n1228   F   VF 6.633522e-01 3.152356e-01 1.986624e-02 1.545986e-03   Fold04\n1229   F   VF 7.996571e-01 1.766726e-01 2.364665e-02 2.364052e-05   Fold04\n1230   F    F 9.292083e-02 6.561271e-01 2.466127e-01 4.339438e-03   Fold04\n1231   F    F 8.273533e-02 6.464751e-01 2.656857e-01 5.103827e-03   Fold04\n1232   F    F 1.213971e-01 6.817647e-01 1.939767e-01 2.861511e-03   Fold04\n1233   F    F 6.879513e-02 6.301348e-01 2.940248e-01 7.045216e-03   Fold04\n1234   F    F 1.300063e-01 6.766513e-01 1.911978e-01 2.144647e-03   Fold04\n1235   F    F 1.347806e-01 6.780367e-01 1.850224e-01 2.160433e-03   Fold04\n1236   F    F 1.304248e-01 6.769431e-01 1.904636e-01 2.168423e-03   Fold04\n1237   F    F 1.555538e-01 6.819247e-01 1.609369e-01 1.584638e-03   Fold04\n1238   F   VF 8.527355e-01 1.332586e-01 1.394458e-02 6.133981e-05   Fold04\n1239   F   VF 8.867293e-01 1.070325e-01 6.214211e-03 2.403136e-05   Fold04\n1240   F   VF 7.086915e-01 2.465422e-01 4.419842e-02 5.678569e-04   Fold04\n1241   F   VF 8.096465e-01 1.714833e-01 1.872457e-02 1.457144e-04   Fold04\n1242   F   VF 8.891818e-01 9.783132e-02 1.297105e-02 1.580939e-05   Fold04\n1243   F   VF 5.879235e-01 3.304124e-01 7.263709e-02 9.026970e-03   Fold04\n1244   F   VF 8.882438e-01 1.057035e-01 6.037820e-03 1.487386e-05   Fold04\n1245   F   VF 9.018274e-01 9.133489e-02 6.825806e-03 1.191152e-05   Fold04\n1246   F   VF 8.767679e-01 1.160621e-01 7.152208e-03 1.783365e-05   Fold04\n1247   F   VF 8.650968e-01 1.259030e-01 8.968789e-03 3.148272e-05   Fold04\n1248   F   VF 8.787157e-01 1.156548e-01 5.600874e-03 2.869510e-05   Fold04\n1249   F   VF 8.687502e-01 1.249336e-01 6.281697e-03 3.451903e-05   Fold04\n1250   F   VF 9.010662e-01 9.289632e-02 6.026014e-03 1.146598e-05   Fold04\n1251   F    L 1.936798e-02 6.230156e-02 7.461608e-02 8.437144e-01   Fold04\n1252   F    L 1.922193e-02 6.198429e-02 7.437094e-02 8.444228e-01   Fold04\n1253   F   VF 8.743062e-01 1.178153e-01 7.854142e-03 2.428535e-05   Fold04\n1254   F   VF 8.740579e-01 1.196471e-01 6.246719e-03 4.822180e-05   Fold04\n1255   F   VF 6.046067e-01 3.832935e-01 1.199832e-02 1.014281e-04   Fold04\n1256   F   VF 6.030975e-01 3.846830e-01 1.211591e-02 1.035081e-04   Fold04\n1257   F   VF 6.028416e-01 3.849261e-01 1.212855e-02 1.037016e-04   Fold04\n1258   F   VF 6.012674e-01 3.863643e-01 1.226212e-02 1.061686e-04   Fold04\n1259   F   VF 5.697516e-01 4.151412e-01 1.494652e-02 1.607459e-04   Fold04\n1260   F   VF 8.140425e-01 1.718580e-01 1.393191e-02 1.675634e-04   Fold04\n1261   F   VF 7.965992e-01 1.868631e-01 1.630376e-02 2.340099e-04   Fold04\n1262   F   VF 8.024273e-01 1.770249e-01 2.042866e-02 1.191161e-04   Fold04\n1263   F   VF 8.870843e-01 1.053146e-01 7.562421e-03 3.865051e-05   Fold04\n1264   F   VF 7.908417e-01 1.840245e-01 2.494983e-02 1.840392e-04   Fold04\n1265   F    F 1.066402e-01 7.508638e-01 1.419875e-01 5.085222e-04   Fold04\n1266   F    F 5.949727e-02 7.283098e-01 2.107849e-01 1.408053e-03   Fold04\n1267   F    F 1.042536e-01 7.575108e-01 1.377011e-01 5.344896e-04   Fold04\n1268   F    F 7.940113e-02 7.529101e-01 1.668303e-01 8.584882e-04   Fold04\n1269   F    F 3.529485e-02 7.179872e-01 2.354686e-01 1.124929e-02   Fold04\n1270   F    F 8.124147e-02 7.281058e-01 1.898149e-01 8.378103e-04   Fold04\n1271   F    F 4.286197e-02 6.679111e-01 2.870335e-01 2.193442e-03   Fold04\n1272   F    F 6.849180e-02 7.450171e-01 1.853938e-01 1.097342e-03   Fold04\n1273   F    F 7.843523e-02 7.546153e-01 1.660482e-01 9.012796e-04   Fold04\n1274   F    F 8.077909e-02 7.588665e-01 1.594701e-01 8.843804e-04   Fold04\n1275   F    F 4.592777e-02 7.098572e-01 2.420466e-01 2.168378e-03   Fold04\n1276   F    F 4.576100e-02 7.082984e-01 2.437354e-01 2.205175e-03   Fold04\n1277   F    F 5.686209e-02 7.335308e-01 2.081897e-01 1.417442e-03   Fold04\n1278   F    F 7.115522e-02 7.102867e-01 2.174243e-01 1.133868e-03   Fold04\n1279   F    F 8.113173e-02 7.127768e-01 2.049846e-01 1.106934e-03   Fold04\n1280   F    F 4.149896e-02 6.936841e-01 2.619102e-01 2.906704e-03   Fold04\n1281   F    F 3.970360e-02 6.881539e-01 2.692277e-01 2.914819e-03   Fold04\n1282   F    F 9.306375e-02 7.849439e-01 1.208512e-01 1.141115e-03   Fold04\n1283   F    F 6.768462e-02 7.411896e-01 1.896946e-01 1.431158e-03   Fold04\n1284   F    F 7.320005e-02 7.396520e-01 1.858772e-01 1.270728e-03   Fold04\n1285   F    F 9.966099e-02 7.595644e-01 1.401269e-01 6.476749e-04   Fold04\n1286   F    F 1.160321e-01 7.579404e-01 1.256277e-01 3.998270e-04   Fold04\n1287   F    F 6.308465e-02 6.946210e-01 2.406122e-01 1.682162e-03   Fold04\n1288   F    F 3.881023e-02 6.778948e-01 2.794784e-01 3.816520e-03   Fold04\n1289   F    F 1.092200e-01 7.295964e-01 1.606726e-01 5.110417e-04   Fold04\n1290   F   VF 7.505512e-01 2.306015e-01 1.882633e-02 2.092400e-05   Fold04\n1291   F   VF 6.238656e-01 3.336202e-01 4.243229e-02 8.186488e-05   Fold04\n1292   F   VF 6.208683e-01 3.352387e-01 4.380218e-02 9.075712e-05   Fold04\n1293   F   VF 6.611326e-01 2.936307e-01 4.518340e-02 5.336803e-05   Fold04\n1294   F    F 3.962879e-01 4.783813e-01 1.200847e-01 5.246080e-03   Fold04\n1295   F    F 3.966108e-01 4.783915e-01 1.197843e-01 5.213474e-03   Fold04\n1296   F    F 3.918280e-01 4.538552e-01 1.484856e-01 5.831220e-03   Fold04\n1297   F   VF 4.940496e-01 4.294582e-01 7.619582e-02 2.964472e-04   Fold04\n1298   F   VF 6.381803e-01 3.214420e-01 4.028412e-02 9.355571e-05   Fold04\n1299   F   VF 6.400545e-01 3.191639e-01 4.064742e-02 1.341889e-04   Fold04\n1300   F   VF 4.964986e-01 4.145982e-01 8.835640e-02 5.468109e-04   Fold04\n1301   F   VF 5.728023e-01 3.668794e-01 6.005250e-02 2.657819e-04   Fold04\n1302   F    F 7.098955e-02 7.842913e-01 1.388010e-01 5.918162e-03   Fold04\n1303   F    F 8.976533e-02 7.914803e-01 1.155898e-01 3.164590e-03   Fold04\n1304   F    F 6.999833e-02 7.576769e-01 1.651579e-01 7.166889e-03   Fold04\n1305   F    F 7.343974e-02 7.891014e-01 1.313486e-01 6.110205e-03   Fold04\n1306   F    F 7.379105e-02 7.893433e-01 1.308136e-01 6.052038e-03   Fold04\n1307   F    F 6.072570e-02 7.755125e-01 1.548759e-01 8.885965e-03   Fold04\n1308   F    F 5.855020e-02 7.737522e-01 1.585161e-01 9.181537e-03   Fold04\n1309   F    F 6.496296e-02 7.519173e-01 1.754434e-01 7.676244e-03   Fold04\n1310   F    F 6.006373e-02 7.758442e-01 1.551280e-01 8.964078e-03   Fold04\n1311   F    F 1.864899e-02 5.979236e-01 2.831926e-01 1.002348e-01   Fold04\n1312   F    F 5.756167e-02 7.723675e-01 1.606955e-01 9.375405e-03   Fold04\n1313   F    F 1.877998e-02 6.003741e-01 2.812219e-01 9.962403e-02   Fold04\n1314   F    F 6.321221e-02 7.495536e-01 1.789664e-01 8.267785e-03   Fold04\n1315   F    F 5.740976e-02 7.682985e-01 1.640102e-01 1.028158e-02   Fold04\n1316   F    F 3.386126e-02 6.998210e-01 2.170429e-01 4.927482e-02   Fold04\n1317   F    F 2.912383e-02 6.449474e-01 2.696263e-01 5.630243e-02   Fold04\n1318   F    F 2.912701e-02 6.464683e-01 2.676084e-01 5.679624e-02   Fold04\n1319   F    F 4.727081e-02 7.822910e-01 1.489883e-01 2.144985e-02   Fold04\n1320   F    F 8.769177e-02 7.988799e-01 1.097380e-01 3.690307e-03   Fold04\n1321   F    F 7.911210e-02 7.904392e-01 1.255010e-01 4.947685e-03   Fold04\n1322   F    F 3.654909e-02 7.630191e-01 1.692556e-01 3.117621e-02   Fold04\n1323   F    F 5.910244e-02 7.376326e-01 1.921041e-01 1.116087e-02   Fold04\n1324   F    F 8.089051e-02 7.900485e-01 1.242170e-01 4.843980e-03   Fold04\n1325   F    F 3.340599e-02 7.272090e-01 1.930365e-01 4.634851e-02   Fold04\n1326   F    M 4.307330e-04 2.495340e-02 9.558944e-01 1.872149e-02   Fold04\n1327   M    F 2.956260e-01 5.552362e-01 1.414121e-01 7.725724e-03   Fold04\n1328   M   VF 4.829384e-01 4.357774e-01 7.991814e-02 1.366030e-03   Fold04\n1329   M    M 5.066402e-02 8.067164e-02 5.054706e-01 3.631937e-01   Fold04\n1330   M    F 1.567516e-01 4.628297e-01 2.707526e-01 1.096661e-01   Fold04\n1331   M    F 2.742101e-01 5.420220e-01 1.778732e-01 5.894696e-03   Fold04\n1332   M    M 9.153815e-04 1.480698e-02 9.825682e-01 1.709436e-03   Fold04\n1333   M    F 7.595158e-02 6.399507e-01 2.779878e-01 6.109943e-03   Fold04\n1334   M    M 3.694707e-02 3.524006e-01 6.053621e-01 5.290215e-03   Fold04\n1335   M    L 3.303967e-04 3.629088e-02 2.830420e-01 6.803367e-01   Fold04\n1336   M    L 1.252217e-04 2.033098e-02 1.818637e-01 7.976801e-01   Fold04\n1337   M    F 1.074441e-01 6.272058e-01 2.613709e-01 3.979226e-03   Fold04\n1338   M    F 5.146101e-02 6.016517e-01 3.346702e-01 1.221705e-02   Fold04\n1339   M    M 1.782747e-04 6.840549e-03 9.884244e-01 4.556801e-03   Fold04\n1340   M    F 1.111652e-01 7.037419e-01 1.806681e-01 4.424861e-03   Fold04\n1341   M   VF 8.702941e-01 1.211282e-01 8.550503e-03 2.722259e-05   Fold04\n1342   M   VF 8.801721e-01 1.130406e-01 6.756414e-03 3.093120e-05   Fold04\n1343   M   VF 8.259951e-01 1.552294e-01 1.866082e-02 1.146301e-04   Fold04\n1344   M    M 1.954360e-02 3.009956e-01 4.330054e-01 2.464554e-01   Fold04\n1345   M    F 2.403299e-01 4.031951e-01 2.735298e-01 8.294520e-02   Fold04\n1346   M    F 1.541818e-02 4.999937e-01 4.166928e-01 6.789537e-02   Fold04\n1347   M    F 4.299353e-02 6.919680e-01 2.621376e-01 2.900884e-03   Fold04\n1348   M    F 9.182718e-02 7.846332e-01 1.223728e-01 1.166806e-03   Fold04\n1349   M    F 4.369252e-02 7.214107e-01 2.300306e-01 4.866203e-03   Fold04\n1350   M    F 2.481928e-02 6.570951e-01 3.122107e-01 5.874905e-03   Fold04\n1351   M    M 5.208726e-02 5.183565e-02 8.959241e-01 1.530123e-04   Fold04\n1352   M   VF 8.169950e-01 1.699907e-01 1.300712e-02 7.178258e-06   Fold04\n1353   M    F 1.164671e-01 4.417846e-01 4.149423e-01 2.680597e-02   Fold04\n1354   M    M 3.716724e-02 2.866859e-01 4.420779e-01 2.340689e-01   Fold04\n1355   M    M 4.509053e-02 2.663740e-01 5.661692e-01 1.223663e-01   Fold04\n1356   M    F 4.580473e-01 4.684776e-01 7.320374e-02 2.713666e-04   Fold04\n1357   M   VF 5.910763e-01 3.629874e-01 4.569391e-02 2.424085e-04   Fold04\n1358   M    F 7.344938e-02 7.956500e-01 1.260234e-01 4.877253e-03   Fold04\n1359   M    F 4.672467e-02 7.521511e-01 1.877590e-01 1.336524e-02   Fold04\n1360   M    F 4.376704e-02 7.495884e-01 1.922285e-01 1.441604e-02   Fold04\n1361   M    F 4.222282e-02 7.487576e-01 1.941984e-01 1.482117e-02   Fold04\n1362   M    F 2.548889e-02 5.553673e-01 3.971220e-01 2.202181e-02   Fold04\n1363   M    F 1.688410e-02 5.794174e-01 2.941242e-01 1.095743e-01   Fold04\n1364   M    F 2.742781e-02 6.141310e-01 3.037828e-01 5.465836e-02   Fold04\n1365   M    F 2.751481e-02 6.324279e-01 2.872637e-01 5.279361e-02   Fold04\n1366   M    F 3.762316e-02 7.211632e-01 2.193064e-01 2.190717e-02   Fold04\n1367   M    F 3.847080e-02 7.240698e-01 2.162147e-01 2.124472e-02   Fold04\n1368   L    L 3.080997e-03 3.035189e-02 2.378548e-01 7.287123e-01   Fold04\n1369   L    M 5.348338e-02 3.140373e-01 4.609642e-01 1.715151e-01   Fold04\n1370   L    M 4.170915e-02 2.936725e-01 4.669960e-01 1.976224e-01   Fold04\n1371   L    L 4.087309e-06 1.050384e-03 8.557372e-03 9.903882e-01   Fold04\n1372   L    F 9.033795e-02 6.617906e-01 2.434321e-01 4.439302e-03   Fold04\n1373   L    L 1.143324e-04 1.939195e-02 1.764739e-01 8.040198e-01   Fold04\n1374   L    L 1.782566e-04 2.729675e-02 2.060178e-01 7.665072e-01   Fold04\n1375   L   VF 5.084804e-01 3.587676e-01 1.242818e-01 8.470145e-03   Fold04\n1376   L    F 3.407465e-01 4.018003e-01 2.322666e-01 2.518663e-02   Fold04\n1377   L   VF 8.734669e-01 1.178920e-01 8.618906e-03 2.215615e-05   Fold04\n1378   L    M 2.970181e-02 2.641481e-01 5.369723e-01 1.691778e-01   Fold04\n1379   L    M 4.232681e-02 2.601169e-01 5.768431e-01 1.207132e-01   Fold04\n1380   L    F 2.443368e-02 6.095882e-01 3.122888e-01 5.368937e-02   Fold04\n1381   L    L 2.096729e-06 7.775258e-04 3.644602e-03 9.955758e-01   Fold04\n1382   L    L 7.758935e-07 3.751443e-04 1.330338e-03 9.982937e-01   Fold04\n1383   L    L 1.507153e-06 6.759062e-04 2.949761e-03 9.963728e-01   Fold04\n1384   L    L 1.490250e-06 6.705522e-04 2.934903e-03 9.963931e-01   Fold04\n1385   L    L 2.243900e-06 8.651784e-04 3.243278e-03 9.958893e-01   Fold04\n1386   L    L 2.241632e-06 8.646623e-04 3.241633e-03 9.958915e-01   Fold04\n1387   L    L 2.274129e-06 8.736200e-04 3.263240e-03 9.958609e-01   Fold04\n1388   L    L 2.244359e-06 8.614024e-04 3.212429e-03 9.959239e-01   Fold04\n1389  VF   VF 9.605942e-01 3.705498e-02 2.348191e-03 2.609714e-06   Fold05\n1390  VF   VF 9.587613e-01 3.752252e-02 3.711380e-03 4.783782e-06   Fold05\n1391  VF   VF 9.408962e-01 5.456370e-02 4.532461e-03 7.638048e-06   Fold05\n1392  VF   VF 9.387705e-01 5.394102e-02 7.270201e-03 1.825200e-05   Fold05\n1393  VF   VF 9.497857e-01 4.652623e-02 3.680317e-03 7.777788e-06   Fold05\n1394  VF   VF 9.496156e-01 4.668136e-02 3.695167e-03 7.834083e-06   Fold05\n1395  VF   VF 9.576951e-01 3.935446e-02 2.945176e-03 5.261404e-06   Fold05\n1396  VF    F 3.771941e-01 5.415848e-01 8.001618e-02 1.204926e-03   Fold05\n1397  VF   VF 8.173887e-01 1.520716e-01 3.041367e-02 1.260751e-04   Fold05\n1398  VF   VF 8.481571e-01 1.248791e-01 2.681460e-02 1.492027e-04   Fold05\n1399  VF   VF 8.709345e-01 1.049746e-01 2.388228e-02 2.086012e-04   Fold05\n1400  VF   VF 8.971459e-01 8.702092e-02 1.576983e-02 6.330088e-05   Fold05\n1401  VF   VF 8.993503e-01 8.397465e-02 1.655055e-02 1.244949e-04   Fold05\n1402  VF   VF 9.345176e-01 5.528516e-02 1.017261e-02 2.463040e-05   Fold05\n1403  VF   VF 9.799064e-01 1.855346e-02 1.538123e-03 2.055565e-06   Fold05\n1404  VF   VF 9.837202e-01 1.549661e-02 7.820243e-04 1.174037e-06   Fold05\n1405  VF   VF 9.828406e-01 1.631922e-02 8.391734e-04 1.028805e-06   Fold05\n1406  VF   VF 9.847565e-01 1.426197e-02 9.806897e-04 8.364112e-07   Fold05\n1407  VF   VF 9.868735e-01 1.240058e-02 7.254995e-04 4.679031e-07   Fold05\n1408  VF   VF 9.876059e-01 1.192559e-02 4.681797e-04 3.012926e-07   Fold05\n1409  VF   VF 9.874974e-01 1.202447e-02 4.778429e-04 3.134558e-07   Fold05\n1410  VF   VF 9.803766e-01 1.842529e-02 1.196068e-03 2.094174e-06   Fold05\n1411  VF   VF 9.024823e-01 7.342144e-02 2.201643e-02 2.079794e-03   Fold05\n1412  VF   VF 9.847317e-01 1.456726e-02 7.003237e-04 7.386442e-07   Fold05\n1413  VF   VF 9.673771e-01 2.962935e-02 2.981781e-03 1.177279e-05   Fold05\n1414  VF   VF 9.821668e-01 1.682260e-02 1.009938e-03 6.822031e-07   Fold05\n1415  VF   VF 6.385471e-01 3.203244e-01 4.092348e-02 2.049940e-04   Fold05\n1416  VF   VF 6.793888e-01 2.814869e-01 3.900031e-02 1.239612e-04   Fold05\n1417  VF   VF 5.405701e-01 4.002918e-01 5.874069e-02 3.975035e-04   Fold05\n1418  VF   VF 6.695289e-01 2.981114e-01 3.225483e-02 1.049423e-04   Fold05\n1419  VF   VF 6.819446e-01 2.852407e-01 3.270377e-02 1.108895e-04   Fold05\n1420  VF   VF 7.052342e-01 2.572191e-01 3.743872e-02 1.080039e-04   Fold05\n1421  VF   VF 6.014200e-01 3.338460e-01 6.438894e-02 3.450689e-04   Fold05\n1422  VF   VF 7.082463e-01 2.637462e-01 2.791801e-02 8.941041e-05   Fold05\n1423  VF   VF 4.609017e-01 4.301743e-01 1.064810e-01 2.442903e-03   Fold05\n1424  VF   VF 7.262305e-01 2.440273e-01 2.968968e-02 5.252876e-05   Fold05\n1425  VF   VF 6.730200e-01 2.890384e-01 3.784563e-02 9.596018e-05   Fold05\n1426  VF   VF 7.863537e-01 1.971076e-01 1.651541e-02 2.328541e-05   Fold05\n1427  VF   VF 6.461757e-01 2.854516e-01 6.818657e-02 1.861247e-04   Fold05\n1428  VF   VF 7.704952e-01 2.102571e-01 1.922144e-02 2.633121e-05   Fold05\n1429  VF   VF 7.882058e-01 1.968578e-01 1.492084e-02 1.553402e-05   Fold05\n1430  VF   VF 7.622867e-01 2.108317e-01 2.684758e-02 3.400789e-05   Fold05\n1431  VF   VF 8.172007e-01 1.650924e-01 1.769252e-02 1.440867e-05   Fold05\n1432  VF   VF 7.408474e-01 2.285996e-01 3.050875e-02 4.420517e-05   Fold05\n1433  VF   VF 8.479173e-01 1.440116e-01 8.066713e-03 4.375903e-06   Fold05\n1434  VF   VF 7.151165e-01 2.473422e-01 3.747496e-02 6.628851e-05   Fold05\n1435  VF   VF 8.092656e-01 1.773600e-01 1.335666e-02 1.782436e-05   Fold05\n1436  VF   VF 7.330243e-01 2.422536e-01 2.467551e-02 4.664131e-05   Fold05\n1437  VF    F 1.894177e-01 6.485925e-01 1.613006e-01 6.891717e-04   Fold05\n1438  VF    F 1.661467e-01 6.172565e-01 2.157185e-01 8.783162e-04   Fold05\n1439  VF    F 1.674387e-01 6.105224e-01 2.211068e-01 9.321038e-04   Fold05\n1440  VF   VF 9.541234e-01 4.137451e-02 4.484258e-03 1.780230e-05   Fold05\n1441  VF   VF 9.197979e-01 7.410014e-02 6.091978e-03 1.001248e-05   Fold05\n1442  VF   VF 9.793899e-01 1.993492e-02 6.743044e-04 8.563704e-07   Fold05\n1443  VF   VF 9.883019e-01 1.140376e-02 2.941569e-04 1.500502e-07   Fold05\n1444  VF   VF 8.985543e-01 9.162026e-02 9.796020e-03 2.944233e-05   Fold05\n1445  VF   VF 9.816609e-01 1.776049e-02 5.778515e-04 7.188157e-07   Fold05\n1446  VF   VF 9.929870e-01 6.885199e-03 1.277455e-04 3.876489e-08   Fold05\n1447  VF   VF 9.896612e-01 1.011202e-02 2.267202e-04 9.082600e-08   Fold05\n1448  VF   VF 9.493790e-01 4.814020e-02 2.474535e-03 6.273977e-06   Fold05\n1449  VF   VF 9.714688e-01 2.738950e-02 1.140221e-03 1.520983e-06   Fold05\n1450  VF   VF 9.447115e-01 5.105211e-02 4.227618e-03 8.821712e-06   Fold05\n1451  VF   VF 9.727513e-01 2.621851e-02 1.029171e-03 1.022096e-06   Fold05\n1452  VF   VF 9.860962e-01 1.352065e-02 3.828922e-04 2.368695e-07   Fold05\n1453  VF   VF 9.384028e-01 5.811573e-02 3.474913e-03 6.543775e-06   Fold05\n1454  VF   VF 9.555687e-01 3.966139e-02 4.639170e-03 1.306932e-04   Fold05\n1455  VF   VF 9.730387e-01 2.537844e-02 1.581120e-03 1.759652e-06   Fold05\n1456  VF   VF 9.882780e-01 1.127634e-02 4.453514e-04 2.667153e-07   Fold05\n1457  VF   VF 9.746762e-01 2.403484e-02 1.287701e-03 1.300389e-06   Fold05\n1458  VF   VF 7.964857e-01 1.782295e-01 2.517377e-02 1.110753e-04   Fold05\n1459  VF   VF 9.828241e-01 1.662624e-02 5.491622e-04 4.646083e-07   Fold05\n1460  VF   VF 9.921165e-01 7.664906e-03 2.185630e-04 7.096868e-08   Fold05\n1461  VF   VF 9.206628e-01 7.783154e-02 1.502193e-03 3.437319e-06   Fold05\n1462  VF   VF 9.818161e-01 1.754799e-02 6.352958e-04 6.234136e-07   Fold05\n1463  VF   VF 9.599543e-01 3.780587e-02 2.232317e-03 7.524170e-06   Fold05\n1464  VF   VF 9.766459e-01 2.260257e-02 7.509603e-04 5.494110e-07   Fold05\n1465  VF   VF 9.924279e-01 7.371807e-03 2.002420e-04 4.448679e-08   Fold05\n1466  VF   VF 9.059503e-01 8.839929e-02 5.636463e-03 1.398439e-05   Fold05\n1467  VF   VF 9.140985e-01 8.222634e-02 3.670766e-03 4.380510e-06   Fold05\n1468  VF   VF 8.762112e-01 1.150376e-01 8.733080e-03 1.813536e-05   Fold05\n1469  VF   VF 7.112422e-01 2.838292e-01 4.920770e-03 7.833260e-06   Fold05\n1470  VF   VF 7.654011e-01 2.072108e-01 2.724605e-02 1.420676e-04   Fold05\n1471  VF   VF 9.159304e-01 8.072922e-02 3.336338e-03 4.093723e-06   Fold05\n1472  VF   VF 9.257768e-01 7.046938e-02 3.751027e-03 2.748602e-06   Fold05\n1473  VF   VF 8.847080e-01 1.100048e-01 5.276939e-03 1.033452e-05   Fold05\n1474  VF   VF 8.841003e-01 1.084993e-01 7.390424e-03 9.910193e-06   Fold05\n1475  VF   VF 8.993905e-01 9.502287e-02 5.579824e-03 6.809443e-06   Fold05\n1476  VF   VF 9.109510e-01 8.545602e-02 3.588383e-03 4.638360e-06   Fold05\n1477  VF   VF 8.745916e-01 1.188221e-01 6.565409e-03 2.091870e-05   Fold05\n1478  VF   VF 8.809759e-01 1.140582e-01 4.958681e-03 7.218492e-06   Fold05\n1479  VF   VF 8.587343e-01 1.348737e-01 6.381421e-03 1.060105e-05   Fold05\n1480  VF   VF 6.925381e-01 3.000082e-01 7.428510e-03 2.517814e-05   Fold05\n1481  VF   VF 8.808677e-01 1.128306e-01 6.281471e-03 2.020287e-05   Fold05\n1482  VF   VF 8.754112e-01 1.184832e-01 6.087326e-03 1.821200e-05   Fold05\n1483  VF   VF 9.186737e-01 7.840727e-02 2.915949e-03 3.029723e-06   Fold05\n1484  VF   VF 8.524990e-01 1.364599e-01 1.101363e-02 2.744678e-05   Fold05\n1485  VF   VF 8.932771e-01 1.001445e-01 6.568757e-03 9.666910e-06   Fold05\n1486  VF   VF 8.818495e-01 1.113195e-01 6.817174e-03 1.389294e-05   Fold05\n1487  VF   VF 8.574410e-01 1.309165e-01 1.161023e-02 3.229571e-05   Fold05\n1488  VF   VF 8.765250e-01 1.165806e-01 6.869813e-03 2.452059e-05   Fold05\n1489  VF   VF 8.964766e-01 9.871244e-02 4.803151e-03 7.790342e-06   Fold05\n1490  VF   VF 8.767053e-01 1.134252e-01 9.858026e-03 1.155452e-05   Fold05\n1491  VF   VF 8.932061e-01 9.996749e-02 6.815724e-03 1.068617e-05   Fold05\n1492  VF   VF 6.709907e-01 3.202160e-01 8.757620e-03 3.571767e-05   Fold05\n1493  VF   VF 8.838187e-01 1.086327e-01 7.536174e-03 1.233523e-05   Fold05\n1494  VF   VF 6.621993e-01 2.743818e-01 5.853253e-02 4.886445e-03   Fold05\n1495  VF   VF 6.671190e-01 3.238258e-01 9.017184e-03 3.803642e-05   Fold05\n1496  VF   VF 8.842738e-01 1.077120e-01 7.999956e-03 1.424337e-05   Fold05\n1497  VF   VF 6.505171e-01 3.406680e-01 8.779914e-03 3.495373e-05   Fold05\n1498  VF   VF 9.118095e-01 8.360515e-02 4.580562e-03 4.776186e-06   Fold05\n1499  VF   VF 8.846058e-01 1.099580e-01 5.426397e-03 9.792412e-06   Fold05\n1500  VF   VF 8.609364e-01 1.308779e-01 8.152180e-03 3.352484e-05   Fold05\n1501  VF   VF 8.668486e-01 1.190399e-01 1.409095e-02 2.055350e-05   Fold05\n1502  VF   VF 6.673770e-01 2.812433e-01 4.674926e-02 4.630426e-03   Fold05\n1503  VF   VF 8.487269e-01 1.428918e-01 8.355656e-03 2.566329e-05   Fold05\n1504  VF   VF 8.425638e-01 1.476431e-01 9.746158e-03 4.696462e-05   Fold05\n1505  VF   VF 8.461867e-01 1.438029e-01 9.959979e-03 5.040364e-05   Fold05\n1506  VF   VF 8.759731e-01 1.170434e-01 6.967176e-03 1.634395e-05   Fold05\n1507  VF   VF 8.203597e-01 1.651854e-01 1.432286e-02 1.320663e-04   Fold05\n1508  VF   VF 9.198485e-01 7.705282e-02 3.095103e-03 3.545456e-06   Fold05\n1509  VF   VF 8.505175e-01 1.393315e-01 1.010949e-02 4.152188e-05   Fold05\n1510  VF   VF 9.208736e-01 7.603275e-02 3.090102e-03 3.567667e-06   Fold05\n1511  VF   VF 8.808526e-01 1.109139e-01 8.217377e-03 1.611968e-05   Fold05\n1512  VF   VF 9.098114e-01 8.532819e-02 4.854899e-03 5.468754e-06   Fold05\n1513  VF   VF 9.105394e-01 8.467918e-02 4.776144e-03 5.293215e-06   Fold05\n1514  VF   VF 8.625989e-01 1.285242e-01 8.859729e-03 1.718487e-05   Fold05\n1515  VF   VF 8.409200e-01 1.475410e-01 1.151286e-02 2.615026e-05   Fold05\n1516  VF   VF 9.119388e-01 8.611081e-02 1.920241e-03 3.016098e-05   Fold05\n1517  VF   VF 9.349984e-01 6.160214e-02 3.395645e-03 3.859036e-06   Fold05\n1518  VF   VF 9.139978e-01 8.090270e-02 5.092014e-03 7.497918e-06   Fold05\n1519  VF   VF 9.571141e-01 4.094096e-02 1.943632e-03 1.358213e-06   Fold05\n1520  VF   VF 9.075757e-01 8.652979e-02 5.887427e-03 7.063046e-06   Fold05\n1521  VF   VF 9.664118e-01 3.224440e-02 1.343141e-03 6.945932e-07   Fold05\n1522  VF   VF 8.907552e-01 9.791341e-02 1.131392e-02 1.748595e-05   Fold05\n1523  VF   VF 9.777296e-01 2.115665e-02 1.113329e-03 4.294257e-07   Fold05\n1524  VF   VF 9.231362e-01 6.981719e-02 7.037551e-03 9.062683e-06   Fold05\n1525  VF   VF 9.692676e-01 2.950482e-02 1.227063e-03 5.656424e-07   Fold05\n1526  VF   VF 9.302485e-01 6.610490e-02 3.643594e-03 2.980760e-06   Fold05\n1527  VF   VF 9.619674e-01 3.641353e-02 1.618187e-03 9.250590e-07   Fold05\n1528  VF   VF 9.552903e-01 4.265160e-02 2.056736e-03 1.333769e-06   Fold05\n1529  VF   VF 9.401260e-01 5.487620e-02 4.993309e-03 4.480448e-06   Fold05\n1530  VF   VF 9.446230e-01 5.195971e-02 3.411605e-03 5.689995e-06   Fold05\n1531  VF    F 1.012987e-01 7.785035e-01 1.198562e-01 3.415275e-04   Fold05\n1532  VF    F 6.314205e-02 7.845379e-01 1.515943e-01 7.257322e-04   Fold05\n1533  VF    F 6.726518e-02 7.698154e-01 1.617159e-01 1.203597e-03   Fold05\n1534  VF    F 1.078163e-01 7.862606e-01 1.056810e-01 2.421271e-04   Fold05\n1535  VF    F 1.127634e-01 7.842242e-01 1.027853e-01 2.271625e-04   Fold05\n1536  VF    F 8.362395e-02 7.520255e-01 1.638767e-01 4.738205e-04   Fold05\n1537  VF    F 1.107242e-01 7.782782e-01 1.107301e-01 2.675059e-04   Fold05\n1538  VF    F 7.180732e-02 7.425995e-01 1.848704e-01 7.228332e-04   Fold05\n1539  VF   VF 5.734712e-01 3.611507e-01 6.512092e-02 2.571826e-04   Fold05\n1540  VF   VF 5.972176e-01 3.472696e-01 5.537290e-02 1.399310e-04   Fold05\n1541  VF   VF 7.294933e-01 2.469908e-01 2.348102e-02 3.487842e-05   Fold05\n1542  VF   VF 5.698759e-01 3.594972e-01 7.041183e-02 2.150581e-04   Fold05\n1543  VF   VF 7.191930e-01 2.511945e-01 2.959074e-02 2.177645e-05   Fold05\n1544  VF   VF 8.040446e-01 1.841389e-01 1.181051e-02 5.998141e-06   Fold05\n1545  VF   VF 7.859920e-01 1.973514e-01 1.664469e-02 1.186477e-05   Fold05\n1546  VF   VF 7.079861e-01 2.628349e-01 2.912415e-02 5.490571e-05   Fold05\n1547  VF   VF 7.350729e-01 2.432742e-01 2.163413e-02 1.869924e-05   Fold05\n1548  VF   VF 7.966792e-01 1.900302e-01 1.328312e-02 7.420709e-06   Fold05\n1549  VF   VF 7.654159e-01 2.162333e-01 1.833607e-02 1.469189e-05   Fold05\n1550  VF   VF 7.722094e-01 2.095471e-01 1.822692e-02 1.657193e-05   Fold05\n1551  VF   VF 7.229130e-01 2.437891e-01 3.326433e-02 3.347647e-05   Fold05\n1552  VF   VF 7.714716e-01 2.102506e-01 1.826126e-02 1.657638e-05   Fold05\n1553  VF   VF 8.073547e-01 1.795085e-01 1.312922e-02 7.586973e-06   Fold05\n1554  VF   VF 6.864838e-01 2.561640e-01 5.730740e-02 4.487916e-05   Fold05\n1555  VF   VF 6.976394e-01 2.665111e-01 3.581256e-02 3.699206e-05   Fold05\n1556  VF   VF 7.308202e-01 2.429737e-01 2.617505e-02 3.107200e-05   Fold05\n1557  VF   VF 6.546905e-01 3.035712e-01 4.162224e-02 1.160153e-04   Fold05\n1558  VF   VF 6.553723e-01 3.032729e-01 4.124070e-02 1.141285e-04   Fold05\n1559  VF    F 3.522372e-01 6.119658e-01 3.568902e-02 1.080606e-04   Fold05\n1560  VF   VF 7.068983e-01 2.613462e-01 3.170934e-02 4.620307e-05   Fold05\n1561  VF   VF 6.369311e-01 3.184579e-01 4.447505e-02 1.359441e-04   Fold05\n1562  VF   VF 7.950711e-01 1.911030e-01 1.381776e-02 8.185663e-06   Fold05\n1563  VF   VF 4.679806e-01 4.207190e-01 1.108468e-01 4.535335e-04   Fold05\n1564  VF    F 9.311536e-02 7.848674e-01 1.205133e-01 1.504015e-03   Fold05\n1565  VF    F 6.920304e-02 8.091009e-01 1.176356e-01 4.060434e-03   Fold05\n1566   F   VF 8.779963e-01 9.700638e-02 2.479006e-02 2.072530e-04   Fold05\n1567   F   VF 8.822723e-01 9.389438e-02 2.364007e-02 1.932779e-04   Fold05\n1568   F    F 3.349775e-01 5.518220e-01 1.117687e-01 1.431762e-03   Fold05\n1569   F    L 1.770539e-03 3.534356e-03 1.314412e-02 9.815510e-01   Fold05\n1570   F    M 1.558408e-01 1.416183e-01 3.566438e-01 3.458970e-01   Fold05\n1571   F    F 1.877419e-01 4.428844e-01 2.314316e-01 1.379421e-01   Fold05\n1572   F   VF 5.616965e-01 3.670861e-01 7.081897e-02 3.984761e-04   Fold05\n1573   F   VF 4.534056e-01 4.385674e-01 1.071877e-01 8.393142e-04   Fold05\n1574   F   VF 6.210877e-01 3.340838e-01 4.449429e-02 3.341445e-04   Fold05\n1575   F    M 9.953713e-02 3.680775e-01 4.343582e-01 9.802712e-02   Fold05\n1576   F   VF 7.583330e-01 2.175034e-01 2.413800e-02 2.562687e-05   Fold05\n1577   F    F 1.246760e-01 6.010279e-01 2.725006e-01 1.795506e-03   Fold05\n1578   F    F 1.246343e-01 6.577669e-01 2.152157e-01 2.383074e-03   Fold05\n1579   F    F 1.008245e-01 5.834940e-01 3.128634e-01 2.818108e-03   Fold05\n1580   F    F 1.149662e-01 6.511118e-01 2.314467e-01 2.475242e-03   Fold05\n1581   F    F 1.440804e-01 6.104853e-01 2.443659e-01 1.068515e-03   Fold05\n1582   F    F 1.334188e-01 6.562808e-01 2.086359e-01 1.664502e-03   Fold05\n1583   F    F 1.465886e-01 6.445159e-01 2.076616e-01 1.233909e-03   Fold05\n1584   F    F 1.338323e-01 6.552021e-01 2.095341e-01 1.431474e-03   Fold05\n1585   F    F 1.847491e-01 6.487191e-01 1.657901e-01 7.416517e-04   Fold05\n1586   F    F 9.398133e-02 6.331141e-01 2.680145e-01 4.890067e-03   Fold05\n1587   F    F 1.635504e-01 6.164416e-01 2.190912e-01 9.168671e-04   Fold05\n1588   F    F 1.526410e-01 6.548950e-01 1.911813e-01 1.282621e-03   Fold05\n1589   F    F 1.316935e-01 6.648956e-01 2.014148e-01 1.996142e-03   Fold05\n1590   F    F 1.855697e-01 6.349933e-01 1.785543e-01 8.826132e-04   Fold05\n1591   F   VF 8.174628e-01 1.680666e-01 1.426866e-02 2.019485e-04   Fold05\n1592   F   VF 8.260969e-01 1.571368e-01 1.671550e-02 5.071842e-05   Fold05\n1593   F   VF 8.360326e-01 1.352326e-01 2.869783e-02 3.697617e-05   Fold05\n1594   F   VF 7.197431e-01 2.304134e-01 4.931421e-02 5.292593e-04   Fold05\n1595   F   VF 6.754309e-01 3.166712e-01 7.870652e-03 2.723348e-05   Fold05\n1596   F   VF 9.067097e-01 8.885914e-02 4.424036e-03 7.111620e-06   Fold05\n1597   F   VF 8.710230e-01 1.160843e-01 1.287231e-02 2.044354e-05   Fold05\n1598   F   VF 4.133653e-01 3.616799e-01 1.250005e-01 9.995428e-02   Fold05\n1599   F    L 1.468911e-02 3.901727e-02 3.723399e-02 9.090596e-01   Fold05\n1600   F    L 1.018560e-02 3.048121e-02 3.987403e-02 9.194592e-01   Fold05\n1601   F    F 3.171634e-01 3.386933e-01 1.345308e-01 2.096125e-01   Fold05\n1602   F   VF 6.267474e-01 3.628272e-01 1.037527e-02 5.013621e-05   Fold05\n1603   F   VF 8.288978e-01 1.549894e-01 1.605241e-02 6.036716e-05   Fold05\n1604   F   VF 6.200655e-01 3.690275e-01 1.085180e-02 5.525317e-05   Fold05\n1605   F   VF 6.206883e-01 3.684453e-01 1.081157e-02 5.483680e-05   Fold05\n1606   F   VF 8.646338e-01 1.265766e-01 8.758182e-03 3.134086e-05   Fold05\n1607   F   VF 6.049627e-01 3.829513e-01 1.201691e-02 6.907052e-05   Fold05\n1608   F   VF 7.685133e-01 2.061294e-01 2.495439e-02 4.029254e-04   Fold05\n1609   F   VF 8.235027e-01 1.658261e-01 1.061747e-02 5.379122e-05   Fold05\n1610   F   VF 5.915934e-01 3.952074e-01 1.311557e-02 8.367057e-05   Fold05\n1611   F   VF 7.983961e-01 1.786770e-01 2.280099e-02 1.259827e-04   Fold05\n1612   F   VF 7.868177e-01 1.823876e-01 3.060452e-02 1.901593e-04   Fold05\n1613   F   VF 8.631637e-01 1.282759e-01 8.539297e-03 2.114877e-05   Fold05\n1614   F   VF 8.409006e-01 1.426511e-01 1.642867e-02 1.956473e-05   Fold05\n1615   F   VF 5.652505e-01 2.393185e-01 1.768850e-01 1.854594e-02   Fold05\n1616   F    F 5.049370e-02 7.347832e-01 2.131579e-01 1.565155e-03   Fold05\n1617   F    F 5.054420e-02 7.760569e-01 1.720089e-01 1.389992e-03   Fold05\n1618   F    F 8.123199e-02 7.396645e-01 1.785592e-01 5.442942e-04   Fold05\n1619   F    F 3.397748e-02 7.219589e-01 2.356993e-01 8.364312e-03   Fold05\n1620   F    F 6.317629e-02 7.424550e-01 1.934305e-01 9.382704e-04   Fold05\n1621   F    F 5.764809e-02 7.191858e-01 2.219465e-01 1.219625e-03   Fold05\n1622   F    F 2.836298e-02 6.320360e-01 3.300490e-01 9.551997e-03   Fold05\n1623   F    F 3.396303e-02 7.106124e-01 2.460400e-01 9.384649e-03   Fold05\n1624   F    F 6.363450e-02 7.216877e-01 2.134723e-01 1.205464e-03   Fold05\n1625   F    F 6.647639e-02 7.706031e-01 1.620905e-01 8.299138e-04   Fold05\n1626   F    F 6.424375e-02 7.690508e-01 1.657323e-01 9.731962e-04   Fold05\n1627   F    F 4.788195e-02 7.756635e-01 1.751516e-01 1.302984e-03   Fold05\n1628   F    F 5.672918e-02 7.707794e-01 1.712847e-01 1.206747e-03   Fold05\n1629   F    F 5.104638e-02 7.437473e-01 2.027850e-01 2.421392e-03   Fold05\n1630   F    F 7.942600e-02 8.029075e-01 1.172633e-01 4.031983e-04   Fold05\n1631   F    F 9.594039e-02 8.040699e-01 9.976795e-02 2.217549e-04   Fold05\n1632   F    F 9.540081e-02 8.038557e-01 1.005171e-01 2.263237e-04   Fold05\n1633   F    F 1.028486e-01 7.857105e-01 1.111625e-01 2.783580e-04   Fold05\n1634   F    F 6.266008e-02 7.274483e-01 2.088855e-01 1.006104e-03   Fold05\n1635   F   VF 7.510918e-01 2.267271e-01 2.215864e-02 2.250136e-05   Fold05\n1636   F    F 3.383968e-01 4.366463e-01 2.225480e-01 2.408917e-03   Fold05\n1637   F    F 1.859150e-01 4.396696e-01 3.484466e-01 2.596874e-02   Fold05\n1638   F    F 4.130268e-01 4.540142e-01 1.270592e-01 5.899723e-03   Fold05\n1639   F    F 2.840690e-01 4.464797e-01 2.483417e-01 2.110960e-02   Fold05\n1640   F    F 3.748295e-01 4.401604e-01 1.770357e-01 7.974329e-03   Fold05\n1641   F    F 3.004964e-01 4.682547e-01 2.189565e-01 1.229251e-02   Fold05\n1642   F    F 3.382979e-01 4.705791e-01 1.722135e-01 1.890948e-02   Fold05\n1643   F   VF 7.548890e-01 2.259111e-01 1.918252e-02 1.739962e-05   Fold05\n1644   F   VF 7.210248e-01 2.428838e-01 3.605779e-02 3.356811e-05   Fold05\n1645   F    M 1.317491e-02 1.261485e-01 4.701503e-01 3.905263e-01   Fold05\n1646   F   VF 6.095174e-01 3.258095e-01 6.453949e-02 1.335568e-04   Fold05\n1647   F   VF 5.691745e-01 3.372208e-01 9.328464e-02 3.201022e-04   Fold05\n1648   F   VF 4.447318e-01 4.299763e-01 1.235605e-01 1.731419e-03   Fold05\n1649   F   VF 6.225645e-01 3.483224e-01 2.823452e-02 8.785499e-04   Fold05\n1650   F   VF 5.163841e-01 4.122781e-01 7.096960e-02 3.681891e-04   Fold05\n1651   F    F 6.850735e-02 8.035476e-01 1.221296e-01 5.815446e-03   Fold05\n1652   F    F 2.422913e-02 6.662403e-01 2.572168e-01 5.231376e-02   Fold05\n1653   F    F 1.075923e-01 7.991762e-01 9.194429e-02 1.287157e-03   Fold05\n1654   F    F 1.074270e-02 4.813058e-01 3.744453e-01 1.335062e-01   Fold05\n1655   F    F 6.498924e-02 7.928579e-01 1.362823e-01 5.870560e-03   Fold05\n1656   F    F 6.445382e-02 7.526662e-01 1.776394e-01 5.240609e-03   Fold05\n1657   F    F 6.551861e-02 7.590536e-01 1.702371e-01 5.190616e-03   Fold05\n1658   F    F 1.999015e-02 6.058792e-01 2.934148e-01 8.071592e-02   Fold05\n1659   F    F 5.826735e-02 7.923251e-01 1.425226e-01 6.885009e-03   Fold05\n1660   F    F 3.602227e-02 7.477384e-01 1.871580e-01 2.908128e-02   Fold05\n1661   F    F 7.114039e-02 8.010845e-01 1.239436e-01 3.831502e-03   Fold05\n1662   F    F 6.262384e-02 7.533561e-01 1.780860e-01 5.934002e-03   Fold05\n1663   F    F 3.115391e-02 6.617010e-01 2.618226e-01 4.532251e-02   Fold05\n1664   F    F 3.515671e-02 7.435688e-01 1.894242e-01 3.185034e-02   Fold05\n1665   F    F 2.958857e-02 6.817040e-01 2.530264e-01 3.568098e-02   Fold05\n1666   F    F 4.138581e-02 7.237451e-01 2.214463e-01 1.342278e-02   Fold05\n1667   F    F 5.860813e-02 7.665664e-01 1.693925e-01 5.432895e-03   Fold05\n1668   F    F 6.520672e-02 7.993621e-01 1.311664e-01 4.264746e-03   Fold05\n1669   F    F 6.810463e-02 8.012922e-01 1.263974e-01 4.205813e-03   Fold05\n1670   F    F 6.207966e-02 7.636693e-01 1.688134e-01 5.437611e-03   Fold05\n1671   F    F 3.171922e-02 7.122879e-01 2.386610e-01 1.733185e-02   Fold05\n1672   F    F 6.017198e-02 7.844193e-01 1.485567e-01 6.852022e-03   Fold05\n1673   F    F 2.702431e-02 6.243094e-01 3.141848e-01 3.448142e-02   Fold05\n1674   M   VF 9.050056e-01 8.246011e-02 1.245171e-02 8.263412e-05   Fold05\n1675   M    F 8.639698e-02 4.678800e-01 2.372195e-01 2.085035e-01   Fold05\n1676   M    L 1.304969e-01 1.352742e-01 3.567109e-01 3.775180e-01   Fold05\n1677   M    M 7.818056e-02 1.074127e-01 4.358633e-01 3.785435e-01   Fold05\n1678   M    L 7.516617e-02 1.136954e-01 3.467324e-01 4.644060e-01   Fold05\n1679   M    M 8.531959e-02 1.092673e-01 4.371871e-01 3.682260e-01   Fold05\n1680   M    M 8.613018e-02 1.108924e-01 4.376726e-01 3.653049e-01   Fold05\n1681   M    F 3.080992e-01 5.268263e-01 1.618664e-01 3.208062e-03   Fold05\n1682   M    F 2.128235e-01 4.990041e-01 2.817435e-01 6.428928e-03   Fold05\n1683   M    M 6.290807e-03 1.912821e-02 9.729794e-01 1.601585e-03   Fold05\n1684   M    M 3.903279e-02 2.256897e-01 5.460525e-01 1.892251e-01   Fold05\n1685   M    F 1.089408e-01 6.382318e-01 2.499321e-01 2.895241e-03   Fold05\n1686   M    M 1.489406e-02 3.040466e-01 3.785813e-01 3.024781e-01   Fold05\n1687   M    F 1.598039e-01 6.076506e-01 2.314867e-01 1.058775e-03   Fold05\n1688   M    F 1.795941e-01 6.501958e-01 1.694236e-01 7.865273e-04   Fold05\n1689   M   VF 8.674291e-01 1.095975e-01 2.295505e-02 1.840823e-05   Fold05\n1690   M   VF 8.663556e-01 1.207501e-01 1.287254e-02 2.183045e-05   Fold05\n1691   M    L 7.672887e-03 2.311847e-02 2.672252e-02 9.424861e-01   Fold05\n1692   M   VF 8.085719e-01 1.708141e-01 2.051824e-02 9.575320e-05   Fold05\n1693   M    F 7.086462e-02 7.807406e-01 1.477046e-01 6.901104e-04   Fold05\n1694   M    M 4.507493e-03 3.134386e-01 5.462480e-01 1.358059e-01   Fold05\n1695   M    F 3.896799e-02 7.053549e-01 2.538445e-01 1.832576e-03   Fold05\n1696   M    F 6.636427e-02 7.352486e-01 1.974683e-01 9.188587e-04   Fold05\n1697   M    F 6.086238e-02 7.146157e-01 2.235384e-01 9.834668e-04   Fold05\n1698   M    F 3.064479e-02 6.139930e-01 3.507351e-01 4.627154e-03   Fold05\n1699   M    M 3.634140e-04 2.122794e-02 9.766844e-01 1.724265e-03   Fold05\n1700   M    M 3.734893e-02 2.345582e-01 5.899057e-01 1.381872e-01   Fold05\n1701   M   VF 5.994612e-01 3.462342e-01 5.417020e-02 1.343753e-04   Fold05\n1702   M    F 7.608613e-02 8.030746e-01 1.173924e-01 3.446860e-03   Fold05\n1703   M    L 1.234719e-03 1.590845e-01 2.694979e-01 5.701829e-01   Fold05\n1704   M    L 7.534281e-03 3.563874e-01 1.892889e-01 4.467894e-01   Fold05\n1705   M    F 3.501065e-02 7.137776e-01 2.144977e-01 3.671407e-02   Fold05\n1706   M    F 3.301731e-02 7.231956e-01 2.100018e-01 3.378528e-02   Fold05\n1707   M    F 1.835874e-02 5.923488e-01 3.000215e-01 8.927095e-02   Fold05\n1708   M    F 2.749251e-02 6.254271e-01 3.060333e-01 4.104707e-02   Fold05\n1709   M    M 1.204371e-02 4.290766e-01 4.617113e-01 9.716839e-02   Fold05\n1710   M    F 3.588184e-02 7.226528e-01 2.037675e-01 3.769790e-02   Fold05\n1711   M    F 3.445833e-02 7.085605e-01 2.192341e-01 3.774715e-02   Fold05\n1712   M    L 1.968510e-04 3.729261e-02 1.104504e-01 8.520601e-01   Fold05\n1713   M    F 5.443022e-02 7.475930e-01 1.904780e-01 7.498781e-03   Fold05\n1714   M    F 3.064097e-02 7.204325e-01 2.245449e-01 2.438162e-02   Fold05\n1715   L    L 1.208849e-03 4.844665e-02 1.564660e-01 7.938785e-01   Fold05\n1716   L    L 9.572098e-03 1.669699e-01 3.223378e-01 5.011202e-01   Fold05\n1717   L    F 1.088122e-01 4.212986e-01 3.938672e-01 7.602200e-02   Fold05\n1718   L    M 9.074337e-02 4.075430e-01 4.124275e-01 8.928620e-02   Fold05\n1719   L    F 8.732260e-02 5.785409e-01 3.305767e-01 3.559738e-03   Fold05\n1720   L    F 1.091862e-01 6.365194e-01 2.506532e-01 3.641157e-03   Fold05\n1721   L    F 1.100983e-01 6.360801e-01 2.501768e-01 3.644869e-03   Fold05\n1722   L    F 1.026831e-01 6.583385e-01 2.357615e-01 3.216904e-03   Fold05\n1723   L    L 5.026882e-04 6.099789e-02 4.297158e-01 5.087836e-01   Fold05\n1724   L   VF 4.952782e-01 3.710892e-01 1.256785e-01 7.954096e-03   Fold05\n1725   L    F 2.438566e-01 3.886296e-01 3.064821e-01 6.103166e-02   Fold05\n1726   L    L 8.226731e-07 2.775420e-04 1.116063e-03 9.986056e-01   Fold05\n1727   L    L 4.108996e-07 1.877894e-04 1.078646e-03 9.987332e-01   Fold05\n1728   L    L 4.245366e-07 1.924611e-04 1.096053e-03 9.987111e-01   Fold05\n1729   L    L 4.445772e-03 3.250005e-01 3.231022e-01 3.474515e-01   Fold05\n1730   L    L 1.514420e-06 4.492441e-04 1.591140e-03 9.979581e-01   Fold05\n1731   L    F 3.835691e-02 7.336162e-01 2.152697e-01 1.275717e-02   Fold05\n1732   L    F 3.297060e-02 6.933894e-01 2.527752e-01 2.086479e-02   Fold05\n1733   L    F 2.516036e-02 7.015664e-01 2.275272e-01 4.574608e-02   Fold05\n1734   L    L 7.309981e-08 5.706510e-05 3.741870e-04 9.995687e-01   Fold05\n1735   L    F 2.489177e-02 6.998654e-01 2.286205e-01 4.662232e-02   Fold05\n1736  VF   VF 9.481529e-01 4.796182e-02 3.877481e-03 7.835153e-06   Fold06\n1737  VF   VF 9.448080e-01 5.106752e-02 4.116365e-03 8.132075e-06   Fold06\n1738  VF   VF 9.590757e-01 3.825165e-02 2.668882e-03 3.748604e-06   Fold06\n1739  VF    F 3.576745e-01 5.619005e-01 7.780405e-02 2.621006e-03   Fold06\n1740  VF    F 3.376900e-01 5.728823e-01 8.609712e-02 3.330586e-03   Fold06\n1741  VF   VF 9.075015e-01 7.444029e-02 1.801527e-02 4.294840e-05   Fold06\n1742  VF   VF 9.518569e-01 4.080840e-02 7.328765e-03 5.975556e-06   Fold06\n1743  VF   VF 8.693147e-01 9.789957e-02 3.266607e-02 1.196805e-04   Fold06\n1744  VF   VF 7.899832e-01 1.664066e-01 4.347725e-02 1.329462e-04   Fold06\n1745  VF   VF 4.401304e-01 3.459282e-01 2.121231e-01 1.818232e-03   Fold06\n1746  VF   VF 9.203043e-01 6.818378e-02 1.149568e-02 1.623378e-05   Fold06\n1747  VF   VF 7.541605e-01 1.797632e-01 6.581414e-02 2.621602e-04   Fold06\n1748  VF   VF 9.007753e-01 7.637549e-02 2.280582e-02 4.334319e-05   Fold06\n1749  VF   VF 9.354796e-01 5.510662e-02 9.399633e-03 1.410360e-05   Fold06\n1750  VF   VF 8.778988e-01 9.868870e-02 2.334634e-02 6.612766e-05   Fold06\n1751  VF   VF 9.276222e-01 6.156340e-02 1.080417e-02 1.018523e-05   Fold06\n1752  VF   VF 9.833211e-01 1.573354e-02 9.432487e-04 2.125782e-06   Fold06\n1753  VF   VF 9.891466e-01 1.040691e-02 4.461249e-04 3.545177e-07   Fold06\n1754  VF   VF 9.842204e-01 1.499044e-02 7.877921e-04 1.385357e-06   Fold06\n1755  VF   VF 9.808329e-01 1.796214e-02 1.201697e-03 3.255589e-06   Fold06\n1756  VF   VF 9.892565e-01 1.028202e-02 4.610510e-04 4.017010e-07   Fold06\n1757  VF   VF 9.895652e-01 9.971173e-03 4.631957e-04 4.361618e-07   Fold06\n1758  VF   VF 9.892313e-01 1.030719e-02 4.611165e-04 3.996987e-07   Fold06\n1759  VF   VF 9.377635e-01 5.027996e-02 1.131974e-02 6.367742e-04   Fold06\n1760  VF   VF 9.162990e-01 6.821857e-02 1.352725e-02 1.955219e-03   Fold06\n1761  VF   VF 9.875658e-01 1.179748e-02 6.358267e-04 8.780049e-07   Fold06\n1762  VF   VF 6.635110e-01 2.961887e-01 4.001875e-02 2.814975e-04   Fold06\n1763  VF   VF 7.146196e-01 2.477186e-01 3.754990e-02 1.119552e-04   Fold06\n1764  VF   VF 6.044835e-01 3.273338e-01 6.773814e-02 4.446332e-04   Fold06\n1765  VF   VF 6.615336e-01 2.817863e-01 5.640878e-02 2.713127e-04   Fold06\n1766  VF   VF 6.964234e-01 2.688744e-01 3.446404e-02 2.382142e-04   Fold06\n1767  VF   VF 6.872279e-01 2.755876e-01 3.690842e-02 2.760237e-04   Fold06\n1768  VF   VF 7.145499e-01 2.560798e-01 2.922677e-02 1.435055e-04   Fold06\n1769  VF   VF 8.743074e-01 1.172914e-01 8.394449e-03 6.696377e-06   Fold06\n1770  VF   VF 7.309179e-01 2.337452e-01 3.525349e-02 8.339776e-05   Fold06\n1771  VF   VF 5.536063e-01 3.339641e-01 1.116986e-01 7.309183e-04   Fold06\n1772  VF   VF 8.661055e-01 1.248517e-01 9.035637e-03 7.237582e-06   Fold06\n1773  VF   VF 8.416816e-01 1.451707e-01 1.312985e-02 1.781522e-05   Fold06\n1774  VF   VF 7.907311e-01 1.909931e-01 1.824345e-02 3.232365e-05   Fold06\n1775  VF   VF 8.411102e-01 1.416790e-01 1.719428e-02 1.658723e-05   Fold06\n1776  VF   VF 8.232336e-01 1.573991e-01 1.934869e-02 1.861452e-05   Fold06\n1777  VF    F 1.451247e-01 6.651651e-01 1.879631e-01 1.747129e-03   Fold06\n1778  VF    F 1.573200e-01 6.498501e-01 1.912912e-01 1.538742e-03   Fold06\n1779  VF    F 1.020449e-01 6.937596e-01 2.005671e-01 3.628350e-03   Fold06\n1780  VF    F 1.350379e-01 6.768905e-01 1.861787e-01 1.892896e-03   Fold06\n1781  VF    F 1.130849e-01 7.158657e-01 1.682551e-01 2.794361e-03   Fold06\n1782  VF    F 1.002441e-01 6.798685e-01 2.167177e-01 3.169751e-03   Fold06\n1783  VF   VF 9.855040e-01 1.399584e-02 4.995595e-04 5.560702e-07   Fold06\n1784  VF   VF 9.921464e-01 7.584435e-03 2.690365e-04 1.343705e-07   Fold06\n1785  VF   VF 9.878948e-01 1.176593e-02 3.388773e-04 3.601588e-07   Fold06\n1786  VF   VF 9.901912e-01 9.557738e-03 2.508648e-04 2.320116e-07   Fold06\n1787  VF   VF 9.813600e-01 1.770928e-02 9.298854e-04 8.385177e-07   Fold06\n1788  VF   VF 9.915922e-01 8.208544e-03 1.991139e-04 1.022930e-07   Fold06\n1789  VF   VF 7.565397e-01 2.139713e-01 2.914858e-02 3.404227e-04   Fold06\n1790  VF   VF 9.876147e-01 1.189986e-02 4.849141e-04 4.971787e-07   Fold06\n1791  VF   VF 9.741288e-01 2.448174e-02 1.387133e-03 2.334462e-06   Fold06\n1792  VF   VF 9.731873e-01 2.538073e-02 1.429598e-03 2.357061e-06   Fold06\n1793  VF   VF 9.773321e-01 2.144375e-02 1.222946e-03 1.240774e-06   Fold06\n1794  VF   VF 9.735851e-01 2.540351e-02 1.010046e-03 1.313254e-06   Fold06\n1795  VF   VF 9.800784e-01 1.912289e-02 7.978102e-04 8.626944e-07   Fold06\n1796  VF   VF 5.524029e-01 2.570955e-01 1.717060e-01 1.879566e-02   Fold06\n1797  VF   VF 9.878201e-01 1.180605e-02 3.735587e-04 2.976039e-07   Fold06\n1798  VF   VF 9.937644e-01 6.106797e-03 1.287131e-04 5.333882e-08   Fold06\n1799  VF   VF 8.713804e-01 9.594342e-02 3.124623e-02 1.429963e-03   Fold06\n1800  VF   VF 9.819019e-01 1.734857e-02 7.483469e-04 1.148858e-06   Fold06\n1801  VF   VF 9.517352e-01 4.538139e-02 2.872642e-03 1.073919e-05   Fold06\n1802  VF   VF 9.570318e-01 4.050872e-02 2.450860e-03 8.591131e-06   Fold06\n1803  VF   VF 9.661994e-01 3.165245e-02 2.143346e-03 4.837247e-06   Fold06\n1804  VF   VF 9.926792e-01 7.157168e-03 1.635280e-04 7.682395e-08   Fold06\n1805  VF   VF 9.442760e-01 5.122902e-02 4.447635e-03 4.730790e-05   Fold06\n1806  VF   VF 9.830949e-01 1.606690e-02 8.369317e-04 1.297162e-06   Fold06\n1807  VF   VF 9.792120e-01 1.974658e-02 1.040470e-03 9.261090e-07   Fold06\n1808  VF   VF 9.790921e-01 1.985730e-02 1.049692e-03 9.389690e-07   Fold06\n1809  VF   VF 9.899709e-01 9.607747e-03 4.210935e-04 2.994062e-07   Fold06\n1810  VF   VF 9.804736e-01 1.866741e-02 8.569612e-04 2.051302e-06   Fold06\n1811  VF   VF 9.740051e-01 2.480439e-02 1.188688e-03 1.816755e-06   Fold06\n1812  VF   VF 9.867432e-01 1.282424e-02 4.320961e-04 4.381659e-07   Fold06\n1813  VF   VF 9.583174e-01 3.943390e-02 2.243678e-03 5.015317e-06   Fold06\n1814  VF   VF 4.289678e-01 4.267278e-01 1.408089e-01 3.495467e-03   Fold06\n1815  VF   VF 8.898984e-01 1.021493e-01 7.915514e-03 3.686527e-05   Fold06\n1816  VF   VF 9.806036e-01 1.868934e-02 7.063581e-04 7.292967e-07   Fold06\n1817  VF   VF 9.750581e-01 2.356758e-02 1.369389e-03 4.905517e-06   Fold06\n1818  VF   VF 9.448776e-01 4.950164e-02 5.590022e-03 3.073106e-05   Fold06\n1819  VF   VF 9.028022e-01 9.243803e-02 4.748849e-03 1.096026e-05   Fold06\n1820  VF   VF 8.546347e-01 1.360610e-01 9.279841e-03 2.447180e-05   Fold06\n1821  VF   VF 9.037512e-01 9.178721e-02 4.451966e-03 9.576656e-06   Fold06\n1822  VF   VF 9.410227e-01 5.772217e-02 1.220412e-03 3.475804e-05   Fold06\n1823  VF   VF 8.913582e-01 1.034148e-01 5.215224e-03 1.175665e-05   Fold06\n1824  VF   VF 9.165970e-01 7.968642e-02 3.709920e-03 6.708335e-06   Fold06\n1825  VF   VF 6.779764e-01 3.150269e-01 6.971130e-03 2.553931e-05   Fold06\n1826  VF   VF 8.607008e-01 1.287924e-01 1.048191e-02 2.485776e-05   Fold06\n1827  VF   VF 8.961282e-01 9.858545e-02 5.275053e-03 1.133257e-05   Fold06\n1828  VF   VF 8.387929e-01 1.434649e-01 1.769869e-02 4.348993e-05   Fold06\n1829  VF   VF 8.989738e-01 9.667400e-02 4.345184e-03 6.969368e-06   Fold06\n1830  VF   VF 8.779102e-01 1.163426e-01 5.736668e-03 1.056950e-05   Fold06\n1831  VF   VF 9.016371e-01 9.248507e-02 5.869316e-03 8.471043e-06   Fold06\n1832  VF   VF 6.658492e-01 3.264935e-01 7.626563e-03 3.067566e-05   Fold06\n1833  VF   VF 8.617917e-01 1.302524e-01 7.921527e-03 3.442291e-05   Fold06\n1834  VF   VF 8.616280e-01 1.305534e-01 7.786043e-03 3.249461e-05   Fold06\n1835  VF   VF 6.683051e-01 3.233120e-01 8.341914e-03 4.096757e-05   Fold06\n1836  VF   VF 8.576352e-01 1.348486e-01 7.489291e-03 2.688023e-05   Fold06\n1837  VF   VF 8.899907e-01 1.042544e-01 5.739572e-03 1.529668e-05   Fold06\n1838  VF   VF 8.995224e-01 9.368757e-02 6.777447e-03 1.256856e-05   Fold06\n1839  VF   VF 5.296500e-01 3.659425e-01 8.751790e-02 1.688961e-02   Fold06\n1840  VF   VF 9.079653e-01 8.810224e-02 3.925249e-03 7.223259e-06   Fold06\n1841  VF   VF 9.191108e-01 7.780421e-02 3.080949e-03 4.073647e-06   Fold06\n1842  VF   VF 8.948413e-01 9.988477e-02 5.262453e-03 1.144259e-05   Fold06\n1843  VF   VF 8.975728e-01 9.530613e-02 7.107195e-03 1.392776e-05   Fold06\n1844  VF   VF 9.089281e-01 8.693397e-02 4.129239e-03 8.657682e-06   Fold06\n1845  VF   VF 9.007909e-01 9.422190e-02 4.976247e-03 1.093369e-05   Fold06\n1846  VF   VF 9.001866e-01 9.492280e-02 4.880390e-03 1.022695e-05   Fold06\n1847  VF   VF 6.396499e-01 3.510411e-01 9.262934e-03 4.608693e-05   Fold06\n1848  VF   VF 6.417912e-01 3.480894e-01 1.005936e-02 6.005751e-05   Fold06\n1849  VF   VF 8.809002e-01 1.118434e-01 7.230409e-03 2.594512e-05   Fold06\n1850  VF   VF 8.815500e-01 1.113654e-01 7.060253e-03 2.440883e-05   Fold06\n1851  VF   VF 8.830176e-01 1.083556e-01 8.608283e-03 1.851945e-05   Fold06\n1852  VF   VF 8.299554e-01 1.555504e-01 1.442726e-02 6.690479e-05   Fold06\n1853  VF   VF 8.593218e-01 1.317006e-01 8.955766e-03 2.188067e-05   Fold06\n1854  VF   VF 9.121136e-01 8.287203e-02 5.007573e-03 6.798681e-06   Fold06\n1855  VF   VF 8.630362e-01 1.286095e-01 8.325514e-03 2.879220e-05   Fold06\n1856  VF   VF 8.345698e-01 1.538556e-01 1.150317e-02 7.140202e-05   Fold06\n1857  VF   VF 8.328869e-01 1.559106e-01 1.113964e-02 6.286958e-05   Fold06\n1858  VF   VF 9.173047e-01 7.947514e-02 3.215714e-03 4.411899e-06   Fold06\n1859  VF   VF 7.640434e-01 2.113202e-01 2.430709e-02 3.293702e-04   Fold06\n1860  VF   VF 8.274651e-01 1.597511e-01 1.264515e-02 1.386567e-04   Fold06\n1861  VF   VF 9.148626e-01 8.175402e-02 3.378604e-03 4.797044e-06   Fold06\n1862  VF   VF 8.676402e-01 1.235298e-01 8.804006e-03 2.600499e-05   Fold06\n1863  VF   VF 8.679574e-01 1.234785e-01 8.540291e-03 2.376325e-05   Fold06\n1864  VF   VF 9.060788e-01 8.866262e-02 5.251867e-03 6.730641e-06   Fold06\n1865  VF   VF 8.789465e-01 1.147720e-01 6.258834e-03 2.271146e-05   Fold06\n1866  VF   VF 8.614847e-01 1.298415e-01 8.651805e-03 2.204099e-05   Fold06\n1867  VF   VF 8.506990e-01 1.396147e-01 9.660220e-03 2.603789e-05   Fold06\n1868  VF   VF 8.580774e-01 1.324519e-01 9.443290e-03 2.738887e-05   Fold06\n1869  VF   VF 9.468715e-01 5.222600e-02 8.895661e-04 1.295499e-05   Fold06\n1870  VF   VF 9.466099e-01 5.248291e-02 8.941449e-04 1.301786e-05   Fold06\n1871  VF   VF 8.585453e-01 1.319565e-01 9.426262e-03 7.192301e-05   Fold06\n1872  VF   VF 8.595084e-01 1.264458e-01 1.403319e-02 1.264456e-05   Fold06\n1873  VF   VF 9.652400e-01 3.332101e-02 1.437765e-03 1.254335e-06   Fold06\n1874  VF   VF 8.926621e-01 9.736516e-02 9.954501e-03 1.822149e-05   Fold06\n1875  VF   VF 8.716804e-01 1.152723e-01 1.301937e-02 2.784384e-05   Fold06\n1876  VF   VF 9.680933e-01 3.051582e-02 1.389846e-03 9.813743e-07   Fold06\n1877  VF   VF 8.273250e-01 1.543033e-01 1.833112e-02 4.055559e-05   Fold06\n1878  VF   VF 8.805077e-01 1.066896e-01 1.277167e-02 3.098999e-05   Fold06\n1879  VF   VF 9.667713e-01 3.156795e-02 1.659383e-03 1.334365e-06   Fold06\n1880  VF   VF 6.394375e-01 3.162265e-01 4.411316e-02 2.228714e-04   Fold06\n1881  VF    F 6.743769e-02 7.987536e-01 1.330086e-01 8.000583e-04   Fold06\n1882  VF    F 7.074850e-02 7.909554e-01 1.375086e-01 7.874587e-04   Fold06\n1883  VF    F 7.575013e-02 7.672482e-01 1.564578e-01 5.437979e-04   Fold06\n1884  VF    F 1.098636e-01 7.793424e-01 1.104575e-01 3.365639e-04   Fold06\n1885  VF    F 6.132819e-02 7.836033e-01 1.538157e-01 1.252823e-03   Fold06\n1886  VF   VF 6.889604e-01 2.654937e-01 4.549414e-02 5.170616e-05   Fold06\n1887  VF   VF 8.026263e-01 1.783146e-01 1.904545e-02 1.366007e-05   Fold06\n1888  VF   VF 8.234330e-01 1.616963e-01 1.486449e-02 6.206126e-06   Fold06\n1889  VF   VF 7.538511e-01 2.241086e-01 2.200851e-02 3.175399e-05   Fold06\n1890  VF   VF 7.598692e-01 2.225286e-01 1.757876e-02 2.342971e-05   Fold06\n1891  VF   VF 7.099197e-01 2.583800e-01 3.164372e-02 5.660453e-05   Fold06\n1892  VF   VF 7.926709e-01 1.916968e-01 1.561799e-02 1.433013e-05   Fold06\n1893  VF   VF 6.513471e-01 3.068053e-01 4.177610e-02 7.156318e-05   Fold06\n1894  VF   VF 8.258463e-01 1.632361e-01 1.091057e-02 7.102564e-06   Fold06\n1895  VF   VF 7.560523e-01 2.235759e-01 2.034975e-02 2.203660e-05   Fold06\n1896  VF   VF 8.234436e-01 1.654913e-01 1.105799e-02 7.169803e-06   Fold06\n1897  VF   VF 6.884956e-01 2.803163e-01 3.111300e-02 7.512549e-05   Fold06\n1898  VF   VF 8.139702e-01 1.699240e-01 1.609654e-02 9.323408e-06   Fold06\n1899  VF   VF 7.725257e-01 2.005717e-01 2.687340e-02 2.919755e-05   Fold06\n1900  VF   VF 6.614387e-01 2.926961e-01 4.576144e-02 1.037656e-04   Fold06\n1901  VF   VF 7.296192e-01 2.342785e-01 3.605872e-02 4.349209e-05   Fold06\n1902  VF   VF 7.360413e-01 2.380346e-01 2.588604e-02 3.813121e-05   Fold06\n1903  VF   VF 7.350930e-01 2.375296e-01 2.733129e-02 4.607592e-05   Fold06\n1904  VF   VF 5.904813e-01 3.452851e-01 6.404299e-02 1.906608e-04   Fold06\n1905  VF   VF 7.097344e-01 2.575520e-01 3.264005e-02 7.359499e-05   Fold06\n1906  VF   VF 8.198374e-01 1.684356e-01 1.171879e-02 8.184719e-06   Fold06\n1907  VF   VF 7.196850e-01 2.491914e-01 3.106342e-02 6.016400e-05   Fold06\n1908  VF   VF 8.426459e-01 1.454554e-01 1.189453e-02 4.106692e-06   Fold06\n1909  VF   VF 5.607278e-01 3.807010e-01 5.833744e-02 2.337648e-04   Fold06\n1910  VF   VF 7.414191e-01 2.340899e-01 2.446381e-02 2.724107e-05   Fold06\n1911  VF    F 7.488348e-02 7.800171e-01 1.405313e-01 4.568088e-03   Fold06\n1912  VF    F 6.375455e-02 7.989088e-01 1.297824e-01 7.554252e-03   Fold06\n1913   F    F 3.865793e-01 5.264143e-01 8.444438e-02 2.562023e-03   Fold06\n1914   F    F 3.068312e-01 5.763985e-01 1.105396e-01 6.230663e-03   Fold06\n1915   F   VF 8.570977e-01 9.730973e-02 4.361082e-02 1.981738e-03   Fold06\n1916   F   VF 6.068394e-01 3.089905e-01 8.375223e-02 4.179282e-04   Fold06\n1917   F    F 1.454957e-01 4.014841e-01 2.687609e-01 1.842593e-01   Fold06\n1918   F   VF 6.626491e-01 2.971070e-01 3.995041e-02 2.935551e-04   Fold06\n1919   F   VF 4.660898e-01 4.079171e-01 1.226141e-01 3.379072e-03   Fold06\n1920   F    M 4.934709e-03 1.554180e-02 9.773344e-01 2.189132e-03   Fold06\n1921   F   VF 6.293654e-01 2.865389e-01 8.373821e-02 3.575256e-04   Fold06\n1922   F   VF 7.713775e-01 2.029408e-01 2.563939e-02 4.232900e-05   Fold06\n1923   F   VF 7.280220e-01 2.356156e-01 3.627836e-02 8.408423e-05   Fold06\n1924   F    F 1.301303e-01 6.847478e-01 1.822660e-01 2.855865e-03   Fold06\n1925   F    F 1.530099e-01 6.854580e-01 1.600044e-01 1.527689e-03   Fold06\n1926   F    F 1.109330e-01 6.733099e-01 2.117493e-01 4.007811e-03   Fold06\n1927   F    F 1.221011e-01 6.830610e-01 1.925907e-01 2.247168e-03   Fold06\n1928   F    F 7.762115e-02 5.393908e-01 3.791155e-01 3.872550e-03   Fold06\n1929   F    F 6.914893e-02 6.772171e-01 2.450543e-01 8.579590e-03   Fold06\n1930   F    F 9.629563e-02 6.682203e-01 2.315110e-01 3.973088e-03   Fold06\n1931   F    F 2.414172e-02 8.320713e-01 1.376095e-01 6.177474e-03   Fold06\n1932   F    F 1.609587e-01 6.950640e-01 1.427448e-01 1.232519e-03   Fold06\n1933   F    F 1.102148e-01 7.122780e-01 1.743803e-01 3.126896e-03   Fold06\n1934   F   VF 9.855673e-01 1.398579e-02 4.465530e-04 3.506497e-07   Fold06\n1935   F   VF 8.163907e-01 1.492695e-01 2.961270e-02 4.727094e-03   Fold06\n1936   F    F 3.132730e-01 3.773021e-01 2.024259e-01 1.069991e-01   Fold06\n1937   F   VF 7.275933e-01 1.549427e-01 1.018345e-01 1.562951e-02   Fold06\n1938   F   VF 9.013458e-01 9.395726e-02 4.687571e-03 9.346345e-06   Fold06\n1939   F   VF 8.916595e-01 1.017953e-01 6.535724e-03 9.488443e-06   Fold06\n1940   F   VF 7.795235e-01 1.906202e-01 2.970146e-02 1.548231e-04   Fold06\n1941   F   VF 8.094014e-01 1.791635e-01 1.138540e-02 4.972626e-05   Fold06\n1942   F   VF 9.360062e-01 6.244334e-02 1.496855e-03 5.360733e-05   Fold06\n1943   F   VF 9.127472e-01 8.473364e-02 2.378068e-03 1.411117e-04   Fold06\n1944   F   VF 8.432445e-01 1.458577e-01 1.086441e-02 3.339378e-05   Fold06\n1945   F   VF 8.886252e-01 1.053961e-01 5.961991e-03 1.677347e-05   Fold06\n1946   F   VF 8.226462e-01 1.641507e-01 1.315771e-02 4.531432e-05   Fold06\n1947   F    L 4.834570e-03 1.985175e-02 2.019537e-02 9.551183e-01   Fold06\n1948   F   VF 8.947519e-01 9.844536e-02 6.791494e-03 1.124385e-05   Fold06\n1949   F   VF 8.783394e-01 1.141768e-01 7.456624e-03 2.716780e-05   Fold06\n1950   F   VF 7.839762e-01 1.958902e-01 1.980414e-02 3.295092e-04   Fold06\n1951   F   VF 5.936021e-01 3.937611e-01 1.255031e-02 8.647146e-05   Fold06\n1952   F   VF 5.907867e-01 3.963534e-01 1.277026e-02 8.963316e-05   Fold06\n1953   F   VF 8.057184e-01 1.789414e-01 1.522825e-02 1.119327e-04   Fold06\n1954   F   VF 5.638589e-01 4.209317e-01 1.508220e-02 1.271296e-04   Fold06\n1955   F   VF 6.581617e-01 2.872532e-01 5.433514e-02 2.499770e-04   Fold06\n1956   F    L 4.912433e-02 5.997767e-02 5.597512e-02 8.349229e-01   Fold06\n1957   F   VF 8.728420e-01 1.136285e-01 1.346586e-02 6.362722e-05   Fold06\n1958   F    F 5.358526e-02 7.760694e-01 1.697171e-01 6.281674e-04   Fold06\n1959   F    F 1.121279e-01 7.742860e-01 1.132159e-01 3.701789e-04   Fold06\n1960   F    F 6.479566e-02 7.963428e-01 1.379715e-01 8.900555e-04   Fold06\n1961   F    F 7.357506e-02 6.756566e-01 2.501718e-01 5.966082e-04   Fold06\n1962   F    F 1.132792e-01 7.742765e-01 1.121323e-01 3.120937e-04   Fold06\n1963   F    F 6.996199e-02 7.892399e-01 1.401932e-01 6.048640e-04   Fold06\n1964   F    F 8.240377e-02 7.860567e-01 1.309569e-01 5.826596e-04   Fold06\n1965   F    F 8.038491e-02 7.403070e-01 1.787378e-01 5.702877e-04   Fold06\n1966   F    F 6.174431e-02 7.935064e-01 1.437501e-01 9.991851e-04   Fold06\n1967   F    F 5.807901e-02 7.876129e-01 1.531209e-01 1.187266e-03   Fold06\n1968   F    F 7.219253e-02 7.973134e-01 1.297345e-01 7.595616e-04   Fold06\n1969   F    F 6.293129e-02 7.612058e-01 1.747561e-01 1.106738e-03   Fold06\n1970   F    F 1.024870e-01 7.794629e-01 1.175947e-01 4.554351e-04   Fold06\n1971   F    F 7.473554e-02 7.295464e-01 1.948528e-01 8.652697e-04   Fold06\n1972   F    F 6.392931e-02 7.709611e-01 1.643244e-01 7.852369e-04   Fold06\n1973   F    F 8.696028e-02 7.922070e-01 1.204313e-01 4.014872e-04   Fold06\n1974   F    F 1.004191e-01 7.776373e-01 1.214419e-01 5.017373e-04   Fold06\n1975   F    F 1.126553e-01 7.823921e-01 1.046657e-01 2.869248e-04   Fold06\n1976   F    F 6.322658e-02 7.163710e-01 2.191675e-01 1.234898e-03   Fold06\n1977   F    F 5.624339e-02 7.601632e-01 1.820658e-01 1.527681e-03   Fold06\n1978   F    F 9.997751e-02 7.687299e-01 1.310368e-01 2.557964e-04   Fold06\n1979   F    F 9.553950e-02 7.955512e-01 1.085720e-01 3.373443e-04   Fold06\n1980   F    F 1.139461e-01 7.826056e-01 1.031731e-01 2.752617e-04   Fold06\n1981   F    F 1.045719e-01 7.503654e-01 1.446960e-01 3.666245e-04   Fold06\n1982   F   VF 6.408849e-01 3.142005e-01 4.486607e-02 4.856880e-05   Fold06\n1983   F   VF 7.549713e-01 2.067590e-01 3.824449e-02 2.516763e-05   Fold06\n1984   F   VF 7.784107e-01 1.982209e-01 2.334863e-02 1.980623e-05   Fold06\n1985   F   VF 8.180672e-01 1.689929e-01 1.293049e-02 9.454018e-06   Fold06\n1986   F   VF 6.478791e-01 3.044374e-01 4.760280e-02 8.067403e-05   Fold06\n1987   F   VF 7.012751e-01 2.520588e-01 4.660522e-02 6.092000e-05   Fold06\n1988   F   VF 7.359519e-01 2.418329e-01 2.217776e-02 3.747273e-05   Fold06\n1989   F    F 4.388022e-01 4.483900e-01 1.076454e-01 5.162450e-03   Fold06\n1990   F   VF 7.175530e-01 2.542021e-01 2.819526e-02 4.960269e-05   Fold06\n1991   F    F 2.896151e-01 4.977747e-01 1.953167e-01 1.729352e-02   Fold06\n1992   F   VF 7.450695e-01 2.307415e-01 2.415068e-02 3.838146e-05   Fold06\n1993   F   VF 6.205951e-01 3.246765e-01 5.459065e-02 1.377008e-04   Fold06\n1994   F   VF 6.450437e-01 3.124337e-01 4.238608e-02 1.365103e-04   Fold06\n1995   F   VF 5.465188e-01 3.489741e-01 1.041202e-01 3.868805e-04   Fold06\n1996   F    F 4.232980e-01 4.241936e-01 1.506824e-01 1.825971e-03   Fold06\n1997   F   VF 5.283343e-01 4.023946e-01 6.893659e-02 3.345401e-04   Fold06\n1998   F   VF 5.940040e-01 3.485171e-01 5.728668e-02 1.922202e-04   Fold06\n1999   F   VF 4.825848e-01 4.190410e-01 9.795699e-02 4.171541e-04   Fold06\n2000   F    F 1.887320e-02 6.216605e-01 3.011684e-01 5.829795e-02   Fold06\n2001   F    F 6.315932e-02 7.799922e-01 1.521340e-01 4.714522e-03   Fold06\n2002   F    F 5.508999e-02 7.949090e-01 1.415556e-01 8.445440e-03   Fold06\n2003   F    F 5.423780e-02 7.900209e-01 1.453067e-01 1.043467e-02   Fold06\n2004   F    F 4.607422e-02 7.572616e-01 1.855054e-01 1.115878e-02   Fold06\n2005   F    F 5.220192e-02 7.996738e-01 1.392674e-01 8.856880e-03   Fold06\n2006   F    F 6.606254e-02 7.843375e-01 1.430201e-01 6.579902e-03   Fold06\n2007   F    F 6.358760e-02 7.874888e-01 1.417747e-01 7.148955e-03   Fold06\n2008   F    F 6.117648e-02 7.797291e-01 1.504567e-01 8.637662e-03   Fold06\n2009   F    F 1.433520e-02 5.235861e-01 3.643542e-01 9.772449e-02   Fold06\n2010   F    F 9.362462e-03 3.955070e-01 2.419715e-01 3.531591e-01   Fold06\n2011   F    F 6.019828e-02 7.775143e-01 1.537074e-01 8.579931e-03   Fold06\n2012   F    F 6.871213e-02 7.847288e-01 1.403156e-01 6.243420e-03   Fold06\n2013   F    F 6.737718e-02 7.903625e-01 1.366518e-01 5.608545e-03   Fold06\n2014   F    F 6.700322e-02 7.894779e-01 1.378250e-01 5.693898e-03   Fold06\n2015   F    F 3.365520e-02 7.228860e-01 2.016500e-01 4.180887e-02   Fold06\n2016   F    F 2.043907e-02 6.597777e-01 2.286092e-01 9.117401e-02   Fold06\n2017   F    F 6.771897e-02 7.874741e-01 1.388271e-01 5.979787e-03   Fold06\n2018   F    F 5.708289e-02 7.362646e-01 1.980042e-01 8.648345e-03   Fold06\n2019   F    F 3.773113e-02 7.627472e-01 1.799942e-01 1.952754e-02   Fold06\n2020   F    F 9.239168e-02 7.958982e-01 1.086682e-01 3.041913e-03   Fold06\n2021   M    F 2.571930e-01 6.033523e-01 1.343059e-01 5.148850e-03   Fold06\n2022   M    M 1.235445e-01 1.587574e-01 5.195034e-01 1.981947e-01   Fold06\n2023   M    M 6.195636e-02 1.234604e-01 5.963385e-01 2.182448e-01   Fold06\n2024   M    F 4.185533e-01 4.715301e-01 1.080550e-01 1.861557e-03   Fold06\n2025   M   VF 4.479141e-01 4.283795e-01 1.214537e-01 2.252728e-03   Fold06\n2026   M    F 7.313612e-02 6.567018e-01 2.644220e-01 5.740117e-03   Fold06\n2027   M    F 9.939780e-02 6.405206e-01 2.558962e-01 4.185340e-03   Fold06\n2028   M    L 8.345076e-03 2.858394e-01 3.329729e-01 3.728426e-01   Fold06\n2029   M    M 1.759871e-02 3.653900e-01 3.865012e-01 2.305101e-01   Fold06\n2030   M    M 1.765324e-02 3.794453e-01 3.881333e-01 2.147681e-01   Fold06\n2031   M    L 1.066578e-02 3.148382e-01 2.810476e-01 3.934484e-01   Fold06\n2032   M    L 2.398019e-04 3.116502e-02 2.012398e-01 7.673554e-01   Fold06\n2033   M    L 8.482343e-05 1.806427e-02 1.680794e-01 8.137715e-01   Fold06\n2034   M    F 3.956028e-02 5.678232e-01 3.740120e-01 1.860449e-02   Fold06\n2035   M    F 3.536910e-02 5.258985e-01 4.200716e-01 1.866086e-02   Fold06\n2036   M   VF 7.658834e-01 2.080928e-01 2.582937e-02 1.944968e-04   Fold06\n2037   M    M 9.832688e-03 4.631839e-01 4.794979e-01 4.748547e-02   Fold06\n2038   M    M 1.113436e-02 4.718605e-01 4.721213e-01 4.488384e-02   Fold06\n2039   M    F 3.576906e-02 7.365249e-01 2.257424e-01 1.963640e-03   Fold06\n2040   M    F 8.835108e-02 7.431351e-01 1.679542e-01 5.595843e-04   Fold06\n2041   M    F 9.888216e-02 7.781679e-01 1.225069e-01 4.430315e-04   Fold06\n2042   M    F 9.131547e-03 4.833095e-01 3.711861e-01 1.363729e-01   Fold06\n2043   M    M 6.759294e-04 1.258582e-01 4.875033e-01 3.859626e-01   Fold06\n2044   M    F 7.928941e-02 7.378276e-01 1.821796e-01 7.033415e-04   Fold06\n2045   M    F 6.922647e-02 7.299378e-01 1.999256e-01 9.101868e-04   Fold06\n2046   M    F 6.678837e-02 7.655240e-01 1.668457e-01 8.419744e-04   Fold06\n2047   M    F 2.239219e-02 6.205152e-01 3.522271e-01 4.865508e-03   Fold06\n2048   M   VF 7.304196e-01 2.261707e-01 4.336766e-02 4.205913e-05   Fold06\n2049   M   VF 6.287329e-01 2.722335e-01 9.894518e-02 8.842739e-05   Fold06\n2050   M    M 1.449089e-02 1.803635e-01 5.796469e-01 2.254987e-01   Fold06\n2051   M   VF 5.139329e-01 4.019478e-01 8.374271e-02 3.765704e-04   Fold06\n2052   M   VF 6.297759e-01 3.153266e-01 5.471086e-02 1.865672e-04   Fold06\n2053   M    L 7.684780e-03 3.562277e-01 1.814900e-01 4.545976e-01   Fold06\n2054   M    L 3.723315e-03 2.553863e-01 1.818909e-01 5.589995e-01   Fold06\n2055   M    L 3.693620e-03 2.539638e-01 1.814596e-01 5.608830e-01   Fold06\n2056   M    F 3.398050e-02 7.104917e-01 2.078574e-01 4.767045e-02   Fold06\n2057   M    F 1.699957e-02 5.935308e-01 2.772192e-01 1.122504e-01   Fold06\n2058   M    F 2.802277e-02 6.285678e-01 2.926800e-01 5.072946e-02   Fold06\n2059   M    F 2.665217e-02 6.063211e-01 3.154876e-01 5.153917e-02   Fold06\n2060   M    F 5.397073e-02 7.733061e-01 1.621036e-01 1.061951e-02   Fold06\n2061   M    F 2.214798e-02 6.400538e-01 2.879029e-01 4.989537e-02   Fold06\n2062   L    M 5.085915e-02 1.279589e-01 5.346177e-01 2.865643e-01   Fold06\n2063   L    L 1.511928e-03 2.881694e-02 3.045803e-01 6.650909e-01   Fold06\n2064   L    M 9.271214e-02 1.420267e-01 5.228025e-01 2.424587e-01   Fold06\n2065   L    F 8.507716e-02 7.026335e-01 2.072259e-01 5.063476e-03   Fold06\n2066   L    L 6.999926e-05 1.618126e-02 1.579062e-01 8.258426e-01   Fold06\n2067   L    L 2.441309e-04 3.271910e-02 2.745594e-01 6.924774e-01   Fold06\n2068   L    F 8.133032e-02 6.648205e-01 2.471010e-01 6.748124e-03   Fold06\n2069   L    L 1.447982e-16 7.030129e-12 1.412199e-07 9.999999e-01   Fold06\n2070   L    L 9.787038e-06 3.231026e-03 1.072371e-02 9.860355e-01   Fold06\n2071   L    M 4.523283e-04 1.389815e-01 5.896570e-01 2.709091e-01   Fold06\n2072   L    F 4.216652e-02 7.234277e-01 2.321265e-01 2.279252e-03   Fold06\n2073   L    F 4.155053e-02 7.235652e-01 2.326002e-01 2.284010e-03   Fold06\n2074   L    M 1.924973e-02 2.230522e-01 5.260555e-01 2.316426e-01   Fold06\n2075   L    L 4.397054e-07 2.614061e-04 1.096824e-03 9.986413e-01   Fold06\n2076   L    L 9.336702e-07 3.656366e-04 1.172643e-03 9.984608e-01   Fold06\n2077   L    L 1.500937e-06 5.391212e-04 1.574478e-03 9.978849e-01   Fold06\n2078   L    L 1.442868e-06 5.238754e-04 1.546203e-03 9.979285e-01   Fold06\n2079   L    L 4.190614e-05 1.598216e-02 8.168084e-02 9.022951e-01   Fold06\n2080   L    F 3.904202e-02 7.398036e-01 2.026970e-01 1.845733e-02   Fold06\n2081   L    F 2.287801e-02 6.853287e-01 2.402746e-01 5.151863e-02   Fold06\n2082   L    F 2.311580e-02 6.874308e-01 2.389720e-01 5.048140e-02   Fold06\n2083  VF   VF 9.534535e-01 4.237568e-02 4.163943e-03 6.843660e-06   Fold07\n2084  VF   VF 9.616171e-01 3.551236e-02 2.867298e-03 3.288119e-06   Fold07\n2085  VF   VF 9.693959e-01 2.845582e-02 2.146341e-03 1.973602e-06   Fold07\n2086  VF   VF 9.529592e-01 4.352765e-02 3.505685e-03 7.504453e-06   Fold07\n2087  VF   VF 9.700753e-01 2.769779e-02 2.224777e-03 2.138509e-06   Fold07\n2088  VF   VF 8.917249e-01 8.251027e-02 2.559267e-02 1.721312e-04   Fold07\n2089  VF    F 3.394970e-01 5.540628e-01 1.049883e-01 1.451882e-03   Fold07\n2090  VF    F 3.560519e-01 5.488736e-01 9.357257e-02 1.501916e-03   Fold07\n2091  VF    F 2.975172e-01 5.803266e-01 1.178941e-01 4.262080e-03   Fold07\n2092  VF   VF 9.568821e-01 3.771361e-02 5.401962e-03 2.316976e-06   Fold07\n2093  VF   VF 8.580494e-01 1.214592e-01 2.044815e-02 4.325762e-05   Fold07\n2094  VF   VF 8.573182e-01 1.223916e-01 2.024792e-02 4.224684e-05   Fold07\n2095  VF   VF 9.527470e-01 4.286796e-02 4.381072e-03 3.967520e-06   Fold07\n2096  VF   VF 8.612858e-01 1.083148e-01 3.035813e-02 4.125327e-05   Fold07\n2097  VF   VF 8.264198e-01 1.388856e-01 3.462888e-02 6.577056e-05   Fold07\n2098  VF   VF 9.454045e-01 4.718392e-02 7.407541e-03 4.013281e-06   Fold07\n2099  VF    M 9.864111e-02 1.180734e-01 4.291796e-01 3.541059e-01   Fold07\n2100  VF   VF 8.466955e-01 1.150186e-01 3.820955e-02 7.627524e-05   Fold07\n2101  VF   VF 9.525558e-01 4.265266e-02 4.789138e-03 2.394556e-06   Fold07\n2102  VF   VF 5.614100e-01 3.216417e-01 1.166651e-01 2.831326e-04   Fold07\n2103  VF   VF 9.842582e-01 1.487010e-02 8.711071e-04 5.733196e-07   Fold07\n2104  VF   VF 9.889453e-01 1.061109e-02 4.433320e-04 2.508709e-07   Fold07\n2105  VF   VF 9.296270e-01 5.837150e-02 1.149564e-02 5.058429e-04   Fold07\n2106  VF   VF 9.752478e-01 2.317931e-02 1.568451e-03 4.458081e-06   Fold07\n2107  VF   VF 8.684573e-01 1.146737e-01 1.676231e-02 1.067452e-04   Fold07\n2108  VF   VF 9.853637e-01 1.399944e-02 6.364340e-04 4.759609e-07   Fold07\n2109  VF   VF 9.863799e-01 1.291096e-02 7.087394e-04 3.878930e-07   Fold07\n2110  VF   VF 9.839176e-01 1.518122e-02 9.007060e-04 4.703050e-07   Fold07\n2111  VF   VF 7.085333e-01 2.513129e-01 3.998474e-02 1.690102e-04   Fold07\n2112  VF   VF 7.502161e-01 2.216706e-01 2.801770e-02 9.554160e-05   Fold07\n2113  VF   VF 6.931023e-01 2.729810e-01 3.378069e-02 1.359898e-04   Fold07\n2114  VF   VF 5.748839e-01 3.539078e-01 7.037739e-02 8.308952e-04   Fold07\n2115  VF   VF 4.365163e-01 3.822112e-01 1.775288e-01 3.743670e-03   Fold07\n2116  VF   VF 7.133479e-01 2.560825e-01 3.050556e-02 6.408563e-05   Fold07\n2117  VF   VF 7.922640e-01 1.919029e-01 1.581701e-02 1.608612e-05   Fold07\n2118  VF   VF 8.041380e-01 1.840202e-01 1.182778e-02 1.396583e-05   Fold07\n2119  VF   VF 8.435367e-01 1.486958e-01 7.761367e-03 6.133112e-06   Fold07\n2120  VF   VF 8.136323e-01 1.717163e-01 1.463417e-02 1.721634e-05   Fold07\n2121  VF    F 9.608062e-02 7.136576e-01 1.880363e-01 2.225425e-03   Fold07\n2122  VF    F 9.527628e-02 7.136636e-01 1.888184e-01 2.241646e-03   Fold07\n2123  VF    F 1.157205e-01 6.996244e-01 1.834804e-01 1.174706e-03   Fold07\n2124  VF    F 1.424086e-01 6.663262e-01 1.905679e-01 6.973455e-04   Fold07\n2125  VF    F 1.364615e-01 6.878136e-01 1.747904e-01 9.345910e-04   Fold07\n2126  VF    F 1.387216e-01 6.688690e-01 1.916914e-01 7.181081e-04   Fold07\n2127  VF    F 1.384738e-01 6.767472e-01 1.841202e-01 6.587669e-04   Fold07\n2128  VF    F 1.396212e-01 6.724650e-01 1.872345e-01 6.792673e-04   Fold07\n2129  VF   VF 9.874039e-01 1.225537e-02 3.405547e-04 1.657815e-07   Fold07\n2130  VF   VF 9.746147e-01 2.436470e-02 1.020217e-03 4.160279e-07   Fold07\n2131  VF   VF 9.939357e-01 5.926148e-03 1.380801e-04 4.122459e-08   Fold07\n2132  VF   VF 9.922723e-01 7.573278e-03 1.543614e-04 5.585060e-08   Fold07\n2133  VF   VF 9.878851e-01 1.169552e-02 4.192385e-04 1.630668e-07   Fold07\n2134  VF   VF 9.869574e-01 1.264688e-02 3.953381e-04 3.848384e-07   Fold07\n2135  VF   VF 9.369669e-01 5.413620e-02 8.828239e-03 6.864540e-05   Fold07\n2136  VF   VF 9.561630e-01 4.128327e-02 2.551094e-03 2.620662e-06   Fold07\n2137  VF   VF 9.876393e-01 1.188301e-02 4.772843e-04 4.306855e-07   Fold07\n2138  VF   VF 9.775238e-01 2.162021e-02 8.547104e-04 1.317222e-06   Fold07\n2139  VF   VF 9.558417e-01 4.179105e-02 2.363509e-03 3.709587e-06   Fold07\n2140  VF   VF 9.549471e-01 4.297451e-02 2.073495e-03 4.923440e-06   Fold07\n2141  VF   VF 9.583073e-01 3.963157e-02 2.058220e-03 2.945566e-06   Fold07\n2142  VF   VF 9.909740e-01 8.803821e-03 2.221027e-04 7.694042e-08   Fold07\n2143  VF   VF 9.870960e-01 1.229148e-02 6.120870e-04 4.123293e-07   Fold07\n2144  VF   VF 9.806142e-01 1.861698e-02 7.681177e-04 7.018003e-07   Fold07\n2145  VF   VF 9.900497e-01 9.589610e-03 3.604984e-04 1.540707e-07   Fold07\n2146  VF   VF 9.778475e-01 2.122596e-02 9.260818e-04 4.981956e-07   Fold07\n2147  VF   VF 9.901065e-01 9.516702e-03 3.766527e-04 1.749394e-07   Fold07\n2148  VF   VF 9.924225e-01 7.380885e-03 1.965153e-04 6.071993e-08   Fold07\n2149  VF   VF 9.582274e-01 3.913765e-02 2.631897e-03 3.012068e-06   Fold07\n2150  VF   VF 8.853128e-01 1.064641e-01 8.203704e-03 1.943839e-05   Fold07\n2151  VF   VF 9.837603e-01 1.556490e-02 6.741459e-04 6.234511e-07   Fold07\n2152  VF   VF 9.868565e-01 1.277079e-02 3.723751e-04 3.077440e-07   Fold07\n2153  VF   VF 9.883765e-01 1.132849e-02 2.948557e-04 1.279585e-07   Fold07\n2154  VF   VF 9.820419e-01 1.740559e-02 5.519137e-04 5.723541e-07   Fold07\n2155  VF   VF 9.718381e-01 2.686257e-02 1.295930e-03 3.349106e-06   Fold07\n2156  VF   VF 9.851933e-01 1.417026e-02 6.360579e-04 3.562744e-07   Fold07\n2157  VF   VF 9.849377e-01 1.435769e-02 7.041460e-04 4.300119e-07   Fold07\n2158  VF   VF 9.895968e-01 1.008570e-02 3.173636e-04 1.456078e-07   Fold07\n2159  VF   VF 9.928862e-01 6.929173e-03 1.845527e-04 4.413808e-08   Fold07\n2160  VF   VF 9.312515e-01 6.427153e-02 4.471306e-03 5.671270e-06   Fold07\n2161  VF   VF 9.198092e-01 7.570647e-02 4.476049e-03 8.240480e-06   Fold07\n2162  VF   VF 9.895271e-01 1.004529e-02 4.272487e-04 3.244662e-07   Fold07\n2163  VF   VF 9.757796e-01 2.303265e-02 1.186802e-03 9.082533e-07   Fold07\n2164  VF   VF 9.930878e-01 6.746554e-03 1.655969e-04 5.351954e-08   Fold07\n2165  VF   VF 8.582291e-01 1.321783e-01 9.575545e-03 1.711020e-05   Fold07\n2166  VF   VF 8.969707e-01 9.764559e-02 5.378718e-03 5.012970e-06   Fold07\n2167  VF   VF 9.052681e-01 9.045760e-02 4.270573e-03 3.696863e-06   Fold07\n2168  VF   VF 8.948614e-01 1.003686e-01 4.765395e-03 4.613323e-06   Fold07\n2169  VF   VF 8.942553e-01 1.009680e-01 4.769914e-03 6.791877e-06   Fold07\n2170  VF   VF 8.932471e-01 1.016086e-01 5.135892e-03 8.393978e-06   Fold07\n2171  VF   VF 8.693164e-01 1.217390e-01 8.928571e-03 1.609297e-05   Fold07\n2172  VF   VF 9.090522e-01 8.760374e-02 3.340188e-03 3.825499e-06   Fold07\n2173  VF    M 3.129911e-01 1.151795e-01 5.717299e-01 9.950676e-05   Fold07\n2174  VF   VF 9.082746e-01 8.789480e-02 3.825665e-03 4.939182e-06   Fold07\n2175  VF   VF 8.979164e-01 9.673320e-02 5.344284e-03 6.130130e-06   Fold07\n2176  VF   VF 9.074820e-01 8.832460e-02 4.187546e-03 5.886735e-06   Fold07\n2177  VF   VF 9.138143e-01 8.243132e-02 3.749450e-03 4.921895e-06   Fold07\n2178  VF   VF 8.502431e-01 1.402114e-01 9.527871e-03 1.767849e-05   Fold07\n2179  VF   VF 8.481888e-01 1.421600e-01 9.633297e-03 1.792063e-05   Fold07\n2180  VF   VF 8.553585e-01 1.382639e-01 6.357677e-03 1.999272e-05   Fold07\n2181  VF   VF 9.041158e-01 9.196834e-02 3.912750e-03 3.122034e-06   Fold07\n2182  VF   VF 6.954998e-01 2.976096e-01 6.871765e-03 1.881780e-05   Fold07\n2183  VF   VF 8.887095e-01 1.052144e-01 6.068333e-03 7.800299e-06   Fold07\n2184  VF   VF 8.023725e-01 1.820103e-01 1.546156e-02 1.555817e-04   Fold07\n2185  VF   VF 8.473131e-01 1.409514e-01 1.170818e-02 2.731898e-05   Fold07\n2186  VF   VF 8.585674e-01 1.307415e-01 1.066752e-02 2.364632e-05   Fold07\n2187  VF   VF 8.751336e-01 1.137950e-01 1.106006e-02 1.142566e-05   Fold07\n2188  VF   VF 8.896849e-01 1.040448e-01 6.261810e-03 8.471645e-06   Fold07\n2189  VF   VF 8.585063e-01 1.346258e-01 6.843705e-03 2.419104e-05   Fold07\n2190  VF   VF 6.382628e-01 3.091657e-01 4.921889e-02 3.352632e-03   Fold07\n2191  VF   VF 6.778470e-01 3.142508e-01 7.876663e-03 2.558238e-05   Fold07\n2192  VF   VF 8.805941e-01 1.126848e-01 6.712047e-03 9.017484e-06   Fold07\n2193  VF   VF 5.775746e-01 3.557902e-01 5.560476e-02 1.103049e-02   Fold07\n2194  VF   VF 9.063012e-01 8.943409e-02 4.260946e-03 3.753001e-06   Fold07\n2195  VF   VF 8.826291e-01 1.100190e-01 7.340144e-03 1.181814e-05   Fold07\n2196  VF   VF 8.800868e-01 1.124301e-01 7.470923e-03 1.218257e-05   Fold07\n2197  VF   VF 9.135571e-01 8.313740e-02 3.302034e-03 3.499793e-06   Fold07\n2198  VF   VF 6.656638e-01 3.241953e-01 1.009713e-02 4.374884e-05   Fold07\n2199  VF   VF 8.868464e-01 1.065637e-01 6.584814e-03 5.112427e-06   Fold07\n2200  VF   VF 6.647898e-01 3.250083e-01 1.015763e-02 4.432734e-05   Fold07\n2201  VF   VF 8.862434e-01 1.072045e-01 6.538031e-03 1.409642e-05   Fold07\n2202  VF   VF 6.589537e-01 3.303792e-01 1.061820e-02 4.888503e-05   Fold07\n2203  VF   VF 9.018466e-01 9.419617e-02 3.952154e-03 5.028731e-06   Fold07\n2204  VF   VF 9.242625e-01 7.272751e-02 3.006823e-03 3.159516e-06   Fold07\n2205  VF   VF 8.276147e-01 1.611150e-01 1.119260e-02 7.770332e-05   Fold07\n2206  VF   VF 8.749043e-01 1.199100e-01 5.172926e-03 1.275924e-05   Fold07\n2207  VF   VF 8.424772e-01 1.454285e-01 1.206966e-02 2.465187e-05   Fold07\n2208  VF   VF 9.222081e-01 7.483270e-02 2.956148e-03 3.018625e-06   Fold07\n2209  VF   VF 8.529068e-01 1.356149e-01 1.144923e-02 2.910903e-05   Fold07\n2210  VF   VF 8.718236e-01 1.207921e-01 7.373638e-03 1.065008e-05   Fold07\n2211  VF   VF 9.059310e-01 8.989062e-02 4.174690e-03 3.694560e-06   Fold07\n2212  VF   VF 9.217143e-01 7.511933e-02 3.162863e-03 3.502875e-06   Fold07\n2213  VF   VF 8.733309e-01 1.193211e-01 7.337122e-03 1.082614e-05   Fold07\n2214  VF   VF 8.952588e-01 9.987005e-02 4.863626e-03 7.488473e-06   Fold07\n2215  VF   VF 9.730933e-01 2.597099e-02 9.354167e-04 3.217169e-07   Fold07\n2216  VF   VF 9.509212e-01 4.696995e-02 2.107647e-03 1.203490e-06   Fold07\n2217  VF   VF 9.610850e-01 3.763571e-02 1.278370e-03 9.238715e-07   Fold07\n2218  VF   VF 9.122870e-01 8.308868e-02 4.620225e-03 4.066526e-06   Fold07\n2219  VF   VF 9.708751e-01 2.813041e-02 9.941381e-04 3.609594e-07   Fold07\n2220  VF   VF 9.309798e-01 6.403500e-02 4.981260e-03 3.936048e-06   Fold07\n2221  VF   VF 9.519267e-01 4.628195e-02 1.789738e-03 1.637597e-06   Fold07\n2222  VF   VF 9.128224e-01 8.278647e-02 4.384371e-03 6.754654e-06   Fold07\n2223  VF   VF 9.630377e-01 3.566861e-02 1.293221e-03 4.783504e-07   Fold07\n2224  VF   VF 8.517998e-01 1.357989e-01 1.238741e-02 1.390796e-05   Fold07\n2225  VF   VF 4.802893e-01 4.311897e-01 8.814824e-02 3.728280e-04   Fold07\n2226  VF   VF 9.797277e-01 1.949633e-02 7.755850e-04 3.377212e-07   Fold07\n2227  VF   VF 9.686247e-01 3.038095e-02 9.940140e-04 3.524198e-07   Fold07\n2228  VF   VF 9.683558e-01 3.064852e-02 9.953497e-04 3.495497e-07   Fold07\n2229  VF   VF 9.147545e-01 7.989480e-02 5.343605e-03 7.122217e-06   Fold07\n2230  VF   VF 9.620878e-01 3.640145e-02 1.510093e-03 6.591975e-07   Fold07\n2231  VF    F 8.833487e-02 7.772245e-01 1.340706e-01 3.700076e-04   Fold07\n2232  VF    F 8.227526e-02 7.706643e-01 1.466248e-01 4.356182e-04   Fold07\n2233  VF    F 1.066043e-01 7.503329e-01 1.428250e-01 2.377198e-04   Fold07\n2234  VF    F 1.262514e-01 7.613052e-01 1.122384e-01 2.050759e-04   Fold07\n2235  VF    F 9.588809e-02 7.798339e-01 1.239829e-01 2.950987e-04   Fold07\n2236  VF   VF 8.113921e-01 1.756566e-01 1.294458e-02 6.772152e-06   Fold07\n2237  VF   VF 8.146732e-01 1.723142e-01 1.300579e-02 6.777062e-06   Fold07\n2238  VF   VF 7.903209e-01 1.943378e-01 1.533450e-02 6.779010e-06   Fold07\n2239  VF   VF 7.163324e-01 2.608264e-01 2.281878e-02 2.239427e-05   Fold07\n2240  VF   VF 7.795908e-01 2.052897e-01 1.511294e-02 6.563314e-06   Fold07\n2241  VF   VF 7.789987e-01 2.057838e-01 1.521098e-02 6.561891e-06   Fold07\n2242  VF   VF 8.070862e-01 1.803906e-01 1.251647e-02 6.799602e-06   Fold07\n2243  VF   VF 7.612034e-01 2.234031e-01 1.538315e-02 1.033860e-05   Fold07\n2244  VF   VF 8.244354e-01 1.659519e-01 9.608662e-03 4.078993e-06   Fold07\n2245  VF   VF 8.257835e-01 1.645644e-01 9.647904e-03 4.126310e-06   Fold07\n2246  VF   VF 7.273848e-01 2.496881e-01 2.290638e-02 2.065453e-05   Fold07\n2247  VF   VF 7.245380e-01 2.493035e-01 2.614098e-02 1.755674e-05   Fold07\n2248  VF   VF 6.915077e-01 2.871757e-01 2.128504e-02 3.158193e-05   Fold07\n2249  VF   VF 6.945551e-01 2.780277e-01 2.738574e-02 3.150471e-05   Fold07\n2250  VF   VF 6.758065e-01 2.900987e-01 3.406646e-02 2.829956e-05   Fold07\n2251  VF   VF 7.703472e-01 2.157602e-01 1.388448e-02 8.088824e-06   Fold07\n2252  VF   VF 7.795412e-01 2.075271e-01 1.292453e-02 7.179371e-06   Fold07\n2253  VF   VF 7.773885e-01 2.094070e-01 1.319712e-02 7.479921e-06   Fold07\n2254  VF   VF 6.924973e-01 2.736977e-01 3.376969e-02 3.533574e-05   Fold07\n2255  VF   VF 8.046750e-01 1.841350e-01 1.118460e-02 5.331669e-06   Fold07\n2256  VF   VF 7.952259e-01 1.906119e-01 1.415372e-02 8.435924e-06   Fold07\n2257  VF    F 8.949814e-02 7.754776e-01 1.326077e-01 2.416601e-03   Fold07\n2258  VF    F 8.127567e-02 7.823114e-01 1.344283e-01 1.984583e-03   Fold07\n2259   F   VF 9.539072e-01 4.167564e-02 4.409392e-03 7.758133e-06   Fold07\n2260   F   VF 9.504847e-01 4.448049e-02 5.024765e-03 9.995157e-06   Fold07\n2261   F   VF 9.502466e-01 4.478075e-02 4.962907e-03 9.724210e-06   Fold07\n2262   F   VF 8.877380e-01 8.516486e-02 2.691044e-02 1.867428e-04   Fold07\n2263   F    F 3.102963e-01 5.478796e-01 1.399038e-01 1.920231e-03   Fold07\n2264   F    F 3.479192e-01 5.546338e-01 9.587374e-02 1.573221e-03   Fold07\n2265   F    L 6.434805e-03 1.358419e-02 4.495763e-02 9.350234e-01   Fold07\n2266   F    F 2.001517e-01 3.941323e-01 3.765893e-01 2.912659e-02   Fold07\n2267   F   VF 5.850412e-01 3.534134e-01 6.079488e-02 7.505179e-04   Fold07\n2268   F    F 1.491164e-01 3.965946e-01 3.120777e-01 1.422112e-01   Fold07\n2269   F    F 2.442840e-01 4.511395e-01 2.439287e-01 6.064785e-02   Fold07\n2270   F   VF 6.571374e-01 3.011723e-01 4.149363e-02 1.966206e-04   Fold07\n2271   F   VF 5.968741e-01 3.241775e-01 7.802543e-02 9.229070e-04   Fold07\n2272   F   VF 7.869832e-01 1.876034e-01 2.539465e-02 1.867778e-05   Fold07\n2273   F    F 1.219498e-01 6.846127e-01 1.922040e-01 1.233450e-03   Fold07\n2274   F    F 1.253544e-01 6.631100e-01 2.105996e-01 9.360219e-04   Fold07\n2275   F    F 1.172559e-01 6.685628e-01 2.125938e-01 1.587478e-03   Fold07\n2276   F    F 6.805486e-02 6.027293e-01 3.256267e-01 3.589092e-03   Fold07\n2277   F    F 1.143110e-01 6.749192e-01 2.092496e-01 1.520126e-03   Fold07\n2278   F    F 1.145108e-01 6.635106e-01 2.202440e-01 1.734546e-03   Fold07\n2279   F    F 5.540221e-02 5.972923e-01 3.428214e-01 4.484109e-03   Fold07\n2280   F    F 9.009470e-02 6.499208e-01 2.569727e-01 3.011760e-03   Fold07\n2281   F    F 5.978621e-02 6.483774e-01 2.837815e-01 8.054807e-03   Fold07\n2282   F    F 6.050331e-02 6.536883e-01 2.781795e-01 7.628923e-03   Fold07\n2283   F    L 3.222640e-16 6.374744e-13 1.361531e-08 1.000000e+00   Fold07\n2284   F   VF 4.810723e-01 4.352002e-01 8.261963e-02 1.107803e-03   Fold07\n2285   F   VF 6.049618e-01 1.730641e-01 1.634772e-01 5.849687e-02   Fold07\n2286   F    L 3.109786e-01 1.240764e-01 2.404486e-01 3.244964e-01   Fold07\n2287   F   VF 6.130802e-01 2.458268e-01 1.093083e-01 3.178469e-02   Fold07\n2288   F   VF 8.812641e-01 1.103590e-01 8.368089e-03 8.786008e-06   Fold07\n2289   F   VF 8.162403e-01 1.667268e-01 1.697911e-02 5.374443e-05   Fold07\n2290   F   VF 7.906233e-01 1.869563e-01 2.226153e-02 1.589052e-04   Fold07\n2291   F   VF 7.900642e-01 1.875959e-01 2.218437e-02 1.556082e-04   Fold07\n2292   F   VF 8.671803e-01 1.262107e-01 6.587387e-03 2.168785e-05   Fold07\n2293   F   VF 8.839279e-01 1.105213e-01 5.541111e-03 9.728738e-06   Fold07\n2294   F   VF 8.408449e-01 1.473267e-01 1.180229e-02 2.612568e-05   Fold07\n2295   F    L 1.293028e-02 4.131919e-02 3.519344e-02 9.105571e-01   Fold07\n2296   F    L 1.313102e-02 4.139627e-02 3.541970e-02 9.100530e-01   Fold07\n2297   F    M 1.020339e-01 2.348420e-01 3.336162e-01 3.295079e-01   Fold07\n2298   F   VF 6.682954e-01 3.231796e-01 8.494774e-03 3.018457e-05   Fold07\n2299   F   VF 6.559748e-01 3.346703e-01 9.317949e-03 3.697065e-05   Fold07\n2300   F   VF 7.975499e-01 1.895410e-01 1.283192e-02 7.723827e-05   Fold07\n2301   F   VF 6.379876e-01 3.513524e-01 1.061072e-02 4.919989e-05   Fold07\n2302   F   VF 6.344235e-01 3.546306e-01 1.089370e-02 5.218966e-05   Fold07\n2303   F   VF 6.261234e-01 3.622705e-01 1.154673e-02 5.934163e-05   Fold07\n2304   F   VF 6.211074e-01 3.668696e-01 1.195880e-02 6.415550e-05   Fold07\n2305   F   VF 6.036477e-01 3.827981e-01 1.347049e-02 8.364984e-05   Fold07\n2306   F   VF 5.961134e-01 3.896297e-01 1.416331e-02 9.358182e-05   Fold07\n2307   F   VF 8.369144e-01 1.504011e-01 1.265520e-02 2.931517e-05   Fold07\n2308   F    M 2.316165e-01 3.351302e-01 3.849293e-01 4.832393e-02   Fold07\n2309   F    L 6.377337e-02 7.353100e-02 6.678145e-02 7.959142e-01   Fold07\n2310   F   VF 7.256981e-01 2.467866e-01 2.743554e-02 7.975653e-05   Fold07\n2311   F    F 1.148678e-01 7.442885e-01 1.404979e-01 3.458427e-04   Fold07\n2312   F    F 8.024232e-02 7.448362e-01 1.745104e-01 4.111646e-04   Fold07\n2313   F    F 8.646726e-02 7.654451e-01 1.476311e-01 4.565385e-04   Fold07\n2314   F    F 8.049794e-02 7.406780e-01 1.784002e-01 4.238846e-04   Fold07\n2315   F    F 1.096257e-01 7.496030e-01 1.405947e-01 1.765991e-04   Fold07\n2316   F    F 8.418816e-02 7.750503e-01 1.403372e-01 4.242539e-04   Fold07\n2317   F    F 6.220570e-02 7.112362e-01 2.256689e-01 8.891655e-04   Fold07\n2318   F    F 8.066484e-02 7.923519e-01 1.264335e-01 5.497890e-04   Fold07\n2319   F    F 5.100372e-02 7.194244e-01 2.285639e-01 1.008013e-03   Fold07\n2320   F    F 6.651854e-02 7.471796e-01 1.855720e-01 7.299439e-04   Fold07\n2321   F    F 1.181118e-01 7.645950e-01 1.170649e-01 2.282005e-04   Fold07\n2322   F    F 8.073453e-02 7.550415e-01 1.636440e-01 5.799911e-04   Fold07\n2323   F    F 5.598527e-02 7.609156e-01 1.815828e-01 1.516313e-03   Fold07\n2324   F    F 5.882860e-02 6.625339e-01 2.779752e-01 6.623148e-04   Fold07\n2325   F    F 6.412035e-02 7.392904e-01 1.959916e-01 5.976428e-04   Fold07\n2326   F    F 4.451864e-02 6.981239e-01 2.558665e-01 1.490960e-03   Fold07\n2327   F    F 7.645327e-02 7.468806e-01 1.758830e-01 7.831595e-04   Fold07\n2328   F    F 7.144733e-02 7.393692e-01 1.882207e-01 9.627264e-04   Fold07\n2329   F    F 3.924645e-02 6.520903e-01 3.064453e-01 2.217974e-03   Fold07\n2330   F    F 7.195726e-02 7.148826e-01 2.124569e-01 7.032834e-04   Fold07\n2331   F   VF 7.536093e-01 2.268794e-01 1.949823e-02 1.312777e-05   Fold07\n2332   F   VF 6.782604e-01 2.700223e-01 5.168180e-02 3.547657e-05   Fold07\n2333   F   VF 6.987017e-01 2.672746e-01 3.399381e-02 2.990204e-05   Fold07\n2334   F   VF 7.226936e-01 2.477086e-01 2.957248e-02 2.531464e-05   Fold07\n2335   F   VF 5.980833e-01 3.499872e-01 5.186252e-02 6.704995e-05   Fold07\n2336   F   VF 6.358401e-01 3.090627e-01 5.504803e-02 4.915266e-05   Fold07\n2337   F    F 2.998885e-01 4.993787e-01 1.897101e-01 1.102271e-02   Fold07\n2338   F   VF 6.510877e-01 3.128747e-01 3.595529e-02 8.227535e-05   Fold07\n2339   F   VF 5.835896e-01 3.724464e-01 4.382406e-02 1.399082e-04   Fold07\n2340   F   VF 6.258918e-01 3.319179e-01 4.209554e-02 9.473695e-05   Fold07\n2341   F    F 8.829615e-02 7.761960e-01 1.332747e-01 2.233175e-03   Fold07\n2342   F    F 7.016484e-02 7.671005e-01 1.601796e-01 2.555015e-03   Fold07\n2343   F    F 7.006903e-02 7.708640e-01 1.564316e-01 2.635375e-03   Fold07\n2344   F    F 4.871224e-02 7.459778e-01 1.986797e-01 6.630256e-03   Fold07\n2345   F    F 5.384421e-02 8.136337e-01 1.263525e-01 6.169546e-03   Fold07\n2346   F    F 4.749898e-02 7.352208e-01 2.096744e-01 7.605825e-03   Fold07\n2347   F    F 7.714459e-02 7.828405e-01 1.364693e-01 3.545570e-03   Fold07\n2348   F    F 6.447928e-02 7.651083e-01 1.665841e-01 3.828361e-03   Fold07\n2349   F    F 6.441770e-02 7.653339e-01 1.664236e-01 3.824741e-03   Fold07\n2350   F    F 5.973253e-02 7.973743e-01 1.385880e-01 4.305129e-03   Fold07\n2351   F    F 1.696226e-02 5.444799e-01 3.763487e-01 6.220919e-02   Fold07\n2352   F    F 5.784111e-02 7.945485e-01 1.428867e-01 4.723627e-03   Fold07\n2353   F    F 5.205369e-02 7.957382e-01 1.436564e-01 8.551767e-03   Fold07\n2354   F    F 6.088764e-02 7.622135e-01 1.725633e-01 4.335566e-03   Fold07\n2355   F    F 2.237478e-02 6.741745e-01 2.223332e-01 8.111753e-02   Fold07\n2356   F    F 2.205889e-02 6.922369e-01 2.129548e-01 7.274939e-02   Fold07\n2357   F    F 7.228489e-02 7.801132e-01 1.434333e-01 4.168562e-03   Fold07\n2358   F    F 6.011680e-02 7.665304e-01 1.694227e-01 3.930165e-03   Fold07\n2359   F    F 9.700475e-02 7.968634e-01 1.044756e-01 1.656264e-03   Fold07\n2360   F    F 9.738602e-02 7.971013e-01 1.038868e-01 1.625950e-03   Fold07\n2361   F    F 6.124676e-02 7.720605e-01 1.630582e-01 3.634548e-03   Fold07\n2362   F    F 8.376521e-02 7.843976e-01 1.300024e-01 1.834766e-03   Fold07\n2363   F    F 1.033400e-01 7.881767e-01 1.067837e-01 1.699663e-03   Fold07\n2364   F    F 9.657030e-02 8.001466e-01 1.016904e-01 1.592706e-03   Fold07\n2365   F    F 1.026250e-01 7.806206e-01 1.147670e-01 1.987353e-03   Fold07\n2366   M    M 4.947858e-02 8.721043e-02 5.304680e-01 3.328430e-01   Fold07\n2367   M    M 3.731210e-02 7.237551e-02 5.109462e-01 3.793662e-01   Fold07\n2368   M   VF 4.811100e-01 3.901067e-01 1.272622e-01 1.521138e-03   Fold07\n2369   M    L 5.652663e-03 3.275032e-02 4.914992e-02 9.124471e-01   Fold07\n2370   M    L 2.373169e-03 1.384210e-01 4.149017e-01 4.443041e-01   Fold07\n2371   M    M 1.730942e-02 4.196160e-01 4.293397e-01 1.337349e-01   Fold07\n2372   M    F 1.602580e-02 4.192334e-01 3.859298e-01 1.788109e-01   Fold07\n2373   M    L 1.197859e-04 1.766814e-02 1.929450e-01 7.892670e-01   Fold07\n2374   M    L 1.343216e-04 1.965695e-02 2.019965e-01 7.782122e-01   Fold07\n2375   M    F 1.316831e-01 6.777698e-01 1.898217e-01 7.254534e-04   Fold07\n2376   M    F 8.391164e-02 6.225796e-01 2.891008e-01 4.408012e-03   Fold07\n2377   M    M 2.229758e-04 9.621402e-03 9.876429e-01 2.512744e-03   Fold07\n2378   M   VF 7.499963e-01 2.237314e-01 2.601711e-02 2.551905e-04   Fold07\n2379   M    L 8.612143e-03 3.311301e-02 3.643511e-02 9.218397e-01   Fold07\n2380   M   VF 8.664274e-01 1.146302e-01 1.893198e-02 1.040481e-05   Fold07\n2381   M    L 4.118856e-18 1.122439e-13 3.959066e-09 1.000000e+00   Fold07\n2382   M   VF 6.612540e-01 2.755539e-01 6.144490e-02 1.747157e-03   Fold07\n2383   M   VF 7.690233e-01 2.041924e-01 2.666338e-02 1.208733e-04   Fold07\n2384   M    F 2.387948e-01 4.004246e-01 2.713438e-01 8.943683e-02   Fold07\n2385   M    F 4.411397e-02 5.578830e-01 3.718878e-01 2.611521e-02   Fold07\n2386   M    F 5.715773e-02 7.176410e-01 2.242590e-01 9.423217e-04   Fold07\n2387   M    F 5.812844e-02 7.101204e-01 2.307619e-01 9.892770e-04   Fold07\n2388   M    F 8.569340e-03 4.437723e-01 4.363735e-01 1.112849e-01   Fold07\n2389   M    F 1.045136e-02 5.091815e-01 3.389608e-01 1.414063e-01   Fold07\n2390   M   VF 6.986111e-01 2.602014e-01 4.116240e-02 2.511790e-05   Fold07\n2391   M    F 2.092228e-01 4.598480e-01 3.156068e-01 1.532240e-02   Fold07\n2392   M   VF 6.722226e-01 3.011366e-01 2.658807e-02 5.274851e-05   Fold07\n2393   M   VF 7.125964e-01 2.505140e-01 3.686689e-02 2.264840e-05   Fold07\n2394   M   VF 7.079481e-01 2.630313e-01 2.899547e-02 2.513262e-05   Fold07\n2395   M   VF 4.831740e-01 4.312469e-01 8.531646e-02 2.626397e-04   Fold07\n2396   M    L 5.700488e-03 3.261028e-01 2.035418e-01 4.646549e-01   Fold07\n2397   M    F 4.950208e-02 7.434711e-01 2.003498e-01 6.677071e-03   Fold07\n2398   M    F 6.092352e-02 7.576835e-01 1.771881e-01 4.204805e-03   Fold07\n2399   M    L 4.056701e-03 2.587853e-01 2.336759e-01 5.034821e-01   Fold07\n2400   M    F 3.138623e-02 6.703100e-01 2.645800e-01 3.372368e-02   Fold07\n2401   M    F 3.571051e-02 6.985828e-01 2.285926e-01 3.711406e-02   Fold07\n2402   M    F 1.416546e-02 4.834330e-01 4.276390e-01 7.476250e-02   Fold07\n2403   M    F 1.946294e-02 4.830286e-01 4.570116e-01 4.049685e-02   Fold07\n2404   M    F 3.431808e-02 6.825518e-01 2.696606e-01 1.346952e-02   Fold07\n2405   M    F 5.969176e-02 7.640684e-01 1.717148e-01 4.525055e-03   Fold07\n2406   M    F 7.078801e-02 7.705030e-01 1.561012e-01 2.607800e-03   Fold07\n2407   L    L 1.485886e-03 1.880705e-02 1.843891e-01 7.953180e-01   Fold07\n2408   L    L 2.787186e-03 2.314553e-02 3.063527e-01 6.677146e-01   Fold07\n2409   L    M 1.122223e-02 3.425308e-01 3.926504e-01 2.535966e-01   Fold07\n2410   L    F 7.425046e-02 6.304922e-01 2.925769e-01 2.680361e-03   Fold07\n2411   L    F 7.466270e-02 6.159487e-01 3.064356e-01 2.952931e-03   Fold07\n2412   L    L 3.960424e-06 7.522564e-04 5.326332e-03 9.939175e-01   Fold07\n2413   L    F 4.048561e-02 5.864220e-01 3.523854e-01 2.070700e-02   Fold07\n2414   L    L 9.134281e-05 1.509516e-02 1.426840e-01 8.421295e-01   Fold07\n2415   L    F 5.635604e-02 5.786854e-01 3.596354e-01 5.323202e-03   Fold07\n2416   L   VF 4.354015e-01 3.751938e-01 1.707597e-01 1.864495e-02   Fold07\n2417   L    L 9.416483e-09 1.141630e-05 1.182831e-04 9.998703e-01   Fold07\n2418   L    L 1.375752e-06 5.380026e-04 2.157067e-03 9.973036e-01   Fold07\n2419   L    L 1.382686e-06 5.401087e-04 2.163045e-03 9.972955e-01   Fold07\n2420   L    L 8.843666e-07 3.635608e-04 1.390413e-03 9.982451e-01   Fold07\n2421   L    L 1.430946e-06 5.670438e-04 2.205321e-03 9.972262e-01   Fold07\n2422   L    L 2.726872e-03 2.090869e-01 3.038397e-01 4.843465e-01   Fold07\n2423   L    L 8.530185e-04 9.775303e-02 2.597648e-01 6.416291e-01   Fold07\n2424   L    L 1.266099e-06 4.299550e-04 1.600291e-03 9.979685e-01   Fold07\n2425   L    F 3.095648e-02 6.832924e-01 2.655135e-01 2.023768e-02   Fold07\n2426   L    F 3.106732e-02 6.831885e-01 2.655647e-01 2.017945e-02   Fold07\n2427   L    F 2.339090e-02 6.918453e-01 2.362037e-01 4.856011e-02   Fold07\n2428  VF   VF 9.464211e-01 4.965031e-02 3.920335e-03 8.270067e-06   Fold08\n2429  VF   VF 9.384909e-01 5.752868e-02 3.969261e-03 1.114173e-05   Fold08\n2430  VF   VF 9.398544e-01 5.633315e-02 3.802108e-03 1.037496e-05   Fold08\n2431  VF   VF 9.407888e-01 5.542072e-02 3.780092e-03 1.037273e-05   Fold08\n2432  VF   VF 9.596045e-01 3.845659e-02 1.936151e-03 2.726087e-06   Fold08\n2433  VF   VF 9.583385e-01 3.942703e-02 2.230920e-03 3.563415e-06   Fold08\n2434  VF   VF 9.282725e-01 6.548653e-02 6.226149e-03 1.478026e-05   Fold08\n2435  VF   VF 9.579877e-01 3.984326e-02 2.165635e-03 3.362603e-06   Fold08\n2436  VF   VF 9.308508e-01 6.363363e-02 5.503888e-03 1.172822e-05   Fold08\n2437  VF    F 3.870040e-01 5.428815e-01 6.746913e-02 2.645336e-03   Fold08\n2438  VF   VF 5.521346e-01 4.104413e-01 3.690669e-02 5.174924e-04   Fold08\n2439  VF   VF 9.404565e-01 5.337128e-02 6.157699e-03 1.449705e-05   Fold08\n2440  VF   VF 8.807248e-01 8.773486e-02 3.139826e-02 1.421217e-04   Fold08\n2441  VF   VF 9.656542e-01 3.056963e-02 3.772752e-03 3.388958e-06   Fold08\n2442  VF   VF 9.144314e-01 6.870744e-02 1.683243e-02 2.868842e-05   Fold08\n2443  VF   VF 9.612352e-01 3.500190e-02 3.755409e-03 7.517966e-06   Fold08\n2444  VF   VF 4.542364e-01 3.624780e-01 1.820954e-01 1.190228e-03   Fold08\n2445  VF   VF 8.565258e-01 1.079602e-01 3.540835e-02 1.056805e-04   Fold08\n2446  VF   VF 8.701212e-01 9.742648e-02 3.235405e-02 9.822600e-05   Fold08\n2447  VF   VF 9.379075e-01 5.408941e-02 7.993537e-03 9.577261e-06   Fold08\n2448  VF    L 1.857902e-16 1.572671e-12 6.750471e-08 9.999999e-01   Fold08\n2449  VF   VF 9.880199e-01 1.151083e-02 4.688541e-04 4.550914e-07   Fold08\n2450  VF   VF 9.232908e-01 6.385753e-02 1.209450e-02 7.571435e-04   Fold08\n2451  VF   VF 9.890781e-01 1.044588e-02 4.756761e-04 3.166468e-07   Fold08\n2452  VF   VF 9.708034e-01 2.743842e-02 1.748532e-03 9.631984e-06   Fold08\n2453  VF   VF 9.473787e-01 4.448419e-02 8.081817e-03 5.527133e-05   Fold08\n2454  VF   VF 6.330782e-01 3.291702e-01 3.751868e-02 2.329616e-04   Fold08\n2455  VF   VF 6.902408e-01 2.778896e-01 3.171462e-02 1.549816e-04   Fold08\n2456  VF   VF 6.222140e-01 3.472903e-01 3.020872e-02 2.869738e-04   Fold08\n2457  VF   VF 7.335811e-01 2.461098e-01 2.025672e-02 5.228693e-05   Fold08\n2458  VF   VF 6.635018e-01 3.013940e-01 3.495641e-02 1.477825e-04   Fold08\n2459  VF   VF 6.031086e-01 3.531331e-01 4.345997e-02 2.983738e-04   Fold08\n2460  VF   VF 6.958211e-01 2.791499e-01 2.493556e-02 9.350210e-05   Fold08\n2461  VF   VF 7.187334e-01 2.589741e-01 2.221493e-02 7.756467e-05   Fold08\n2462  VF   VF 4.978214e-01 4.323640e-01 6.937398e-02 4.406174e-04   Fold08\n2463  VF   VF 4.707561e-01 4.315212e-01 9.669841e-02 1.024310e-03   Fold08\n2464  VF   VF 6.558641e-01 3.135869e-01 3.039349e-02 1.555330e-04   Fold08\n2465  VF   VF 7.845923e-01 1.873052e-01 2.804881e-02 5.366398e-05   Fold08\n2466  VF   VF 7.501821e-01 2.178179e-01 3.192530e-02 7.467604e-05   Fold08\n2467  VF   VF 8.581105e-01 1.292965e-01 1.258168e-02 1.128687e-05   Fold08\n2468  VF   VF 7.126949e-01 2.382726e-01 4.886978e-02 1.627152e-04   Fold08\n2469  VF   VF 5.617923e-01 3.214772e-01 1.161340e-01 5.965130e-04   Fold08\n2470  VF   VF 8.538675e-01 1.319450e-01 1.417327e-02 1.426565e-05   Fold08\n2471  VF   VF 7.612506e-01 2.063313e-01 3.236679e-02 5.129648e-05   Fold08\n2472  VF   VF 8.748255e-01 1.153783e-01 9.789142e-03 7.075220e-06   Fold08\n2473  VF   VF 8.507782e-01 1.361152e-01 1.309902e-02 7.634335e-06   Fold08\n2474  VF    F 1.351200e-01 6.687794e-01 1.936007e-01 2.499958e-03   Fold08\n2475  VF    F 1.523523e-01 6.663412e-01 1.793708e-01 1.935772e-03   Fold08\n2476  VF    F 1.004216e-01 7.048185e-01 1.891357e-01 5.624185e-03   Fold08\n2477  VF    F 1.540101e-01 6.627115e-01 1.810885e-01 2.189860e-03   Fold08\n2478  VF    F 1.072474e-01 6.275188e-01 2.608445e-01 4.389299e-03   Fold08\n2479  VF   VF 9.682057e-01 3.050938e-02 1.283435e-03 1.515116e-06   Fold08\n2480  VF   VF 9.636711e-01 3.460814e-02 1.718350e-03 2.429687e-06   Fold08\n2481  VF   VF 9.827490e-01 1.682149e-02 4.287207e-04 7.698783e-07   Fold08\n2482  VF   VF 9.898653e-01 9.826797e-03 3.077257e-04 2.100531e-07   Fold08\n2483  VF   VF 9.418406e-01 4.992581e-02 8.138044e-03 9.552620e-05   Fold08\n2484  VF   VF 9.858282e-01 1.381183e-02 3.594164e-04 5.808029e-07   Fold08\n2485  VF   VF 9.151603e-01 7.840883e-02 6.412669e-03 1.819286e-05   Fold08\n2486  VF   VF 6.352623e-01 2.536438e-01 1.034465e-01 7.647369e-03   Fold08\n2487  VF   VF 9.582421e-01 3.899924e-02 2.752667e-03 5.973524e-06   Fold08\n2488  VF   VF 7.652764e-01 2.125785e-01 2.204696e-02 9.812877e-05   Fold08\n2489  VF   VF 9.536445e-01 4.306011e-02 3.287213e-03 8.217476e-06   Fold08\n2490  VF   VF 9.798147e-01 1.943687e-02 7.471999e-04 1.199009e-06   Fold08\n2491  VF    L 1.316128e-01 9.664238e-02 2.957417e-01 4.760031e-01   Fold08\n2492  VF   VF 7.811060e-01 1.994010e-01 1.934303e-02 1.500087e-04   Fold08\n2493  VF   VF 9.620283e-01 3.613062e-02 1.836357e-03 4.710996e-06   Fold08\n2494  VF   VF 9.509917e-01 4.635185e-02 2.648341e-03 8.075769e-06   Fold08\n2495  VF   VF 8.426357e-01 1.500463e-01 7.274674e-03 4.326453e-05   Fold08\n2496  VF   VF 9.885086e-01 1.112539e-02 3.657581e-04 2.524446e-07   Fold08\n2497  VF   VF 9.860797e-01 1.332694e-02 5.924195e-04 9.049387e-07   Fold08\n2498  VF   VF 9.581556e-01 4.040256e-02 1.437532e-03 4.327609e-06   Fold08\n2499  VF   VF 9.754574e-01 2.257061e-02 1.966292e-03 5.712408e-06   Fold08\n2500  VF   VF 8.998058e-01 9.603259e-02 4.153167e-03 8.426934e-06   Fold08\n2501  VF   VF 8.684951e-01 1.239867e-01 7.503178e-03 1.503617e-05   Fold08\n2502  VF   VF 7.809808e-01 2.045051e-01 1.437474e-02 1.393658e-04   Fold08\n2503  VF   VF 9.167471e-01 7.993800e-02 3.311685e-03 3.245135e-06   Fold08\n2504  VF   VF 8.595220e-01 1.306921e-01 9.763996e-03 2.185036e-05   Fold08\n2505  VF   VF 8.575199e-01 1.337000e-01 8.760517e-03 1.959747e-05   Fold08\n2506  VF   VF 8.559786e-01 1.350706e-01 8.930618e-03 2.018390e-05   Fold08\n2507  VF   VF 8.567906e-01 1.340973e-01 9.091169e-03 2.088960e-05   Fold08\n2508  VF   VF 8.710216e-01 1.239108e-01 5.048472e-03 1.907525e-05   Fold08\n2509  VF   VF 8.769993e-01 1.185246e-01 4.458515e-03 1.752818e-05   Fold08\n2510  VF   VF 9.141059e-01 8.247798e-02 3.410616e-03 5.519023e-06   Fold08\n2511  VF   VF 9.090455e-01 8.693441e-02 4.012852e-03 7.278900e-06   Fold08\n2512  VF   VF 9.081740e-01 8.854048e-02 3.280616e-03 4.918781e-06   Fold08\n2513  VF   VF 9.010726e-01 9.480974e-02 4.108988e-03 8.634462e-06   Fold08\n2514  VF   VF 8.745052e-01 1.203637e-01 5.108296e-03 2.275439e-05   Fold08\n2515  VF   VF 9.130060e-01 8.327500e-02 3.712500e-03 6.492317e-06   Fold08\n2516  VF   VF 8.864245e-01 1.086063e-01 4.957722e-03 1.149787e-05   Fold08\n2517  VF   VF 8.736871e-01 1.214422e-01 4.850586e-03 2.009986e-05   Fold08\n2518  VF   VF 8.738337e-01 1.207044e-01 5.436315e-03 2.560080e-05   Fold08\n2519  VF   VF 8.999299e-01 9.497368e-02 5.086959e-03 9.442424e-06   Fold08\n2520  VF   VF 8.551480e-01 1.359369e-01 8.894166e-03 2.090158e-05   Fold08\n2521  VF   VF 8.673070e-01 1.269150e-01 5.749787e-03 2.814212e-05   Fold08\n2522  VF   VF 8.900124e-01 1.045577e-01 5.415829e-03 1.411947e-05   Fold08\n2523  VF   VF 8.451219e-01 1.437651e-01 1.108426e-02 2.870052e-05   Fold08\n2524  VF   VF 8.943550e-01 1.004979e-01 5.134023e-03 1.307509e-05   Fold08\n2525  VF   VF 9.007799e-01 9.372645e-02 5.482390e-03 1.123050e-05   Fold08\n2526  VF   VF 8.905027e-01 1.043573e-01 5.126895e-03 1.304029e-05   Fold08\n2527  VF   VF 8.992877e-01 9.491224e-02 5.787631e-03 1.241127e-05   Fold08\n2528  VF   VF 9.253117e-01 7.206411e-02 2.620834e-03 3.357339e-06   Fold08\n2529  VF   VF 8.987987e-01 9.639917e-02 4.791989e-03 1.017992e-05   Fold08\n2530  VF   VF 9.154717e-01 8.075981e-02 3.763994e-03 4.499903e-06   Fold08\n2531  VF   VF 9.049901e-01 9.084416e-02 4.157690e-03 8.070192e-06   Fold08\n2532  VF   VF 6.423797e-01 3.489389e-01 8.635670e-03 4.571275e-05   Fold08\n2533  VF   VF 8.967351e-01 9.718888e-02 6.062497e-03 1.355697e-05   Fold08\n2534  VF   VF 6.213650e-01 3.216455e-01 5.264193e-02 4.347639e-03   Fold08\n2535  VF   VF 6.351105e-01 3.557488e-01 9.089928e-03 5.079786e-05   Fold08\n2536  VF   VF 6.301796e-01 3.615691e-01 8.209643e-03 4.170393e-05   Fold08\n2537  VF   VF 6.329617e-01 3.577687e-01 9.217345e-03 5.223922e-05   Fold08\n2538  VF   VF 6.279284e-01 3.636868e-01 8.341742e-03 4.311783e-05   Fold08\n2539  VF   VF 6.287398e-01 3.617067e-01 9.497969e-03 5.559674e-05   Fold08\n2540  VF   VF 8.858352e-01 1.066798e-01 7.466287e-03 1.872084e-05   Fold08\n2541  VF   VF 6.213903e-01 3.698410e-01 8.721523e-03 4.724566e-05   Fold08\n2542  VF   VF 6.188948e-01 3.721830e-01 8.873286e-03 4.895810e-05   Fold08\n2543  VF   VF 8.870742e-01 1.062772e-01 6.632733e-03 1.590648e-05   Fold08\n2544  VF   VF 6.200476e-01 3.698028e-01 1.008675e-02 6.285897e-05   Fold08\n2545  VF   VF 6.146058e-01 3.762171e-01 9.125214e-03 5.186494e-05   Fold08\n2546  VF   VF 8.202193e-01 1.656647e-01 1.406959e-02 4.635769e-05   Fold08\n2547  VF   VF 8.287830e-01 1.568658e-01 1.429870e-02 5.248194e-05   Fold08\n2548  VF   VF 8.890127e-01 1.072369e-01 3.737991e-03 1.245555e-05   Fold08\n2549  VF   VF 5.996023e-01 3.887882e-01 1.152658e-02 8.291685e-05   Fold08\n2550  VF   VF 8.759582e-01 1.160361e-01 7.983072e-03 2.269992e-05   Fold08\n2551  VF   VF 9.105342e-01 8.622281e-02 3.237588e-03 5.428413e-06   Fold08\n2552  VF   VF 9.098795e-01 8.650170e-02 3.612119e-03 6.681297e-06   Fold08\n2553  VF   VF 8.824764e-01 1.112731e-01 6.233065e-03 1.741472e-05   Fold08\n2554  VF   VF 8.274524e-01 1.611432e-01 1.130739e-02 9.705482e-05   Fold08\n2555  VF   VF 8.185697e-01 1.702948e-01 1.103667e-02 9.880865e-05   Fold08\n2556  VF   VF 6.877098e-01 3.069599e-01 5.313091e-03 1.725419e-05   Fold08\n2557  VF   VF 8.685677e-01 1.240032e-01 7.414666e-03 1.440287e-05   Fold08\n2558  VF   VF 9.145652e-01 8.158372e-02 3.845542e-03 5.494572e-06   Fold08\n2559  VF   VF 9.164947e-01 7.962530e-02 3.874146e-03 5.813275e-06   Fold08\n2560  VF   VF 9.225256e-01 7.435624e-02 3.113476e-03 4.655701e-06   Fold08\n2561  VF   VF 8.718024e-01 1.202473e-01 7.933580e-03 1.673247e-05   Fold08\n2562  VF   VF 8.675408e-01 1.257205e-01 6.726730e-03 1.205170e-05   Fold08\n2563  VF   VF 9.226112e-01 7.437230e-02 3.012092e-03 4.430631e-06   Fold08\n2564  VF   VF 8.538043e-01 1.362593e-01 9.912592e-03 2.385642e-05   Fold08\n2565  VF   VF 8.516601e-01 1.381835e-01 1.013158e-02 2.480046e-05   Fold08\n2566  VF   VF 9.376818e-01 6.061430e-02 1.701352e-03 2.555131e-06   Fold08\n2567  VF   VF 9.078716e-01 8.530474e-02 6.783112e-03 4.053456e-05   Fold08\n2568  VF   VF 9.506970e-01 4.621402e-02 3.084161e-03 4.816088e-06   Fold08\n2569  VF   VF 9.305917e-01 6.303975e-02 6.352700e-03 1.581090e-05   Fold08\n2570  VF   VF 8.290180e-01 1.580290e-01 1.293101e-02 2.198861e-05   Fold08\n2571  VF   VF 9.276231e-01 6.783700e-02 4.535228e-03 4.693039e-06   Fold08\n2572  VF   VF 9.834236e-01 1.604649e-02 5.296829e-04 2.390808e-07   Fold08\n2573  VF   VF 9.130185e-01 8.205119e-02 4.922985e-03 7.332405e-06   Fold08\n2574  VF   VF 9.405419e-01 5.726386e-02 2.190327e-03 3.941521e-06   Fold08\n2575  VF   VF 9.101667e-01 8.605721e-02 3.767447e-03 8.618745e-06   Fold08\n2576  VF   VF 9.316075e-01 6.468867e-02 3.698918e-03 4.941339e-06   Fold08\n2577  VF   VF 9.784382e-01 2.082581e-02 7.354901e-04 4.591377e-07   Fold08\n2578  VF   VF 6.710028e-01 3.065477e-01 2.233660e-02 1.129001e-04   Fold08\n2579  VF   VF 6.466660e-01 2.863919e-01 6.337801e-02 3.564072e-03   Fold08\n2580  VF   VF 7.596428e-01 2.099118e-01 2.801992e-02 2.425428e-03   Fold08\n2581  VF   VF 9.761905e-01 2.272483e-02 1.083939e-03 7.083897e-07   Fold08\n2582  VF   VF 9.655274e-01 3.228023e-02 2.190291e-03 2.045764e-06   Fold08\n2583  VF   VF 9.383242e-01 5.857688e-02 3.090110e-03 8.780077e-06   Fold08\n2584  VF    F 1.161383e-01 7.549975e-01 1.284717e-01 3.925106e-04   Fold08\n2585  VF    F 1.262755e-01 7.661281e-01 1.073366e-01 2.597991e-04   Fold08\n2586  VF   VF 7.870977e-01 1.998596e-01 1.302796e-02 1.471002e-05   Fold08\n2587  VF   VF 7.422348e-01 2.426083e-01 1.512488e-02 3.200631e-05   Fold08\n2588  VF   VF 7.333497e-01 2.515513e-01 1.506469e-02 3.426794e-05   Fold08\n2589  VF   VF 7.467415e-01 2.392917e-01 1.393593e-02 3.078972e-05   Fold08\n2590  VF    F 4.166434e-01 4.300336e-01 1.522080e-01 1.114963e-03   Fold08\n2591  VF   VF 6.957452e-01 2.718627e-01 3.234377e-02 4.824151e-05   Fold08\n2592  VF   VF 8.012719e-01 1.850939e-01 1.362193e-02 1.223679e-05   Fold08\n2593  VF   VF 7.728932e-01 2.122612e-01 1.483008e-02 1.554237e-05   Fold08\n2594  VF   VF 6.767692e-01 2.896275e-01 3.355076e-02 5.258816e-05   Fold08\n2595  VF   VF 7.503774e-01 2.298206e-01 1.976924e-02 3.274250e-05   Fold08\n2596  VF   VF 8.167018e-01 1.715246e-01 1.176452e-02 9.104123e-06   Fold08\n2597  VF   VF 8.213311e-01 1.669157e-01 1.174402e-02 9.186360e-06   Fold08\n2598  VF   VF 6.782854e-01 2.962403e-01 2.538611e-02 8.820992e-05   Fold08\n2599  VF   VF 7.476575e-01 2.335589e-01 1.876494e-02 1.857857e-05   Fold08\n2600  VF   VF 5.799732e-01 3.597366e-01 6.011582e-02 1.743471e-04   Fold08\n2601  VF   VF 8.448738e-01 1.463398e-01 8.782022e-03 4.380442e-06   Fold08\n2602  VF   VF 6.843066e-01 2.835443e-01 3.210345e-02 4.566619e-05   Fold08\n2603  VF    F 1.003621e-01 7.780904e-01 1.193424e-01 2.205155e-03   Fold08\n2604  VF    F 8.764778e-02 7.701021e-01 1.382607e-01 3.989436e-03   Fold08\n2605   F    F 3.250082e-01 5.590534e-01 1.122984e-01 3.640026e-03   Fold08\n2606   F    L 8.507209e-05 4.687913e-03 3.731225e-02 9.579148e-01   Fold08\n2607   F    F 3.523256e-01 5.273191e-01 1.157233e-01 4.631937e-03   Fold08\n2608   F   VF 8.805140e-01 8.811000e-02 3.123539e-02 1.405696e-04   Fold08\n2609   F   VF 8.224122e-01 1.449463e-01 3.252723e-02 1.142278e-04   Fold08\n2610   F   VF 5.522690e-01 3.738508e-01 7.348627e-02 3.939795e-04   Fold08\n2611   F   VF 5.916296e-01 3.530030e-01 5.499628e-02 3.710817e-04   Fold08\n2612   F   VF 7.523770e-01 2.160056e-01 3.154231e-02 7.504425e-05   Fold08\n2613   F   VF 7.937710e-01 1.821460e-01 2.405374e-02 2.925247e-05   Fold08\n2614   F   VF 8.747611e-01 1.172364e-01 7.996353e-03 6.228099e-06   Fold08\n2615   F   VF 8.405183e-01 1.434256e-01 1.603813e-02 1.793352e-05   Fold08\n2616   F   VF 7.943076e-01 1.814937e-01 2.417419e-02 2.443803e-05   Fold08\n2617   F   VF 6.863566e-01 2.719951e-01 4.138829e-02 2.599985e-04   Fold08\n2618   F    F 1.360282e-01 6.612616e-01 1.998552e-01 2.855006e-03   Fold08\n2619   F    F 1.589140e-01 6.487743e-01 1.902022e-01 2.109402e-03   Fold08\n2620   F    F 1.061059e-01 7.085737e-01 1.804061e-01 4.914328e-03   Fold08\n2621   F    F 1.265759e-01 6.395208e-01 2.309788e-01 2.924408e-03   Fold08\n2622   F    F 7.465000e-02 5.971354e-01 3.226819e-01 5.532728e-03   Fold08\n2623   F    F 7.060374e-02 6.063278e-01 3.168210e-01 6.247443e-03   Fold08\n2624   F    F 1.088271e-01 6.642587e-01 2.228864e-01 4.027843e-03   Fold08\n2625   F    F 8.403929e-02 6.873558e-01 2.200600e-01 8.544986e-03   Fold08\n2626   F    F 8.761766e-02 6.147331e-01 2.919970e-01 5.652214e-03   Fold08\n2627   F    F 1.718148e-01 6.666662e-01 1.600826e-01 1.436504e-03   Fold08\n2628   F    F 9.178771e-02 6.127666e-01 2.909328e-01 4.512851e-03   Fold08\n2629   F    F 6.172077e-02 6.124382e-01 3.136522e-01 1.218881e-02   Fold08\n2630   F    F 1.107339e-01 7.361541e-01 1.492824e-01 3.829579e-03   Fold08\n2631   F   VF 8.450624e-01 1.430081e-01 1.187996e-02 4.950756e-05   Fold08\n2632   F   VF 9.156991e-01 8.084351e-02 3.452013e-03 5.405146e-06   Fold08\n2633   F   VF 9.007293e-01 9.277723e-02 6.483399e-03 1.003246e-05   Fold08\n2634   F   VF 8.663952e-01 1.286561e-01 4.928186e-03 2.057033e-05   Fold08\n2635   F   VF 8.990423e-01 9.563191e-02 5.315594e-03 1.020574e-05   Fold08\n2636   F   VF 8.943739e-01 1.004676e-01 5.149003e-03 9.476535e-06   Fold08\n2637   F   VF 8.797975e-01 1.140752e-01 6.110057e-03 1.721249e-05   Fold08\n2638   F   VF 7.498647e-01 1.403134e-01 1.097416e-01 8.027007e-05   Fold08\n2639   F   VF 6.236792e-01 3.661805e-01 1.007771e-02 6.256275e-05   Fold08\n2640   F   VF 8.126656e-01 1.777189e-01 9.549139e-03 6.637456e-05   Fold08\n2641   F   VF 8.343728e-01 1.574899e-01 8.085328e-03 5.196641e-05   Fold08\n2642   F   VF 8.314291e-01 1.601688e-01 8.347461e-03 5.470830e-05   Fold08\n2643   F   VF 6.091494e-01 3.813231e-01 9.471538e-03 5.602545e-05   Fold08\n2644   F   VF 8.164560e-01 1.679410e-01 1.554267e-02 6.034933e-05   Fold08\n2645   F   VF 5.935120e-01 3.959380e-01 1.048089e-02 6.909003e-05   Fold08\n2646   F    L 3.586914e-03 3.418134e-02 1.751913e-01 7.870405e-01   Fold08\n2647   F   VF 5.909461e-01 3.983153e-01 1.066687e-02 7.169727e-05   Fold08\n2648   F   VF 5.860730e-01 4.028407e-01 1.100968e-02 7.660049e-05   Fold08\n2649   F   VF 5.765586e-01 4.116644e-01 1.169015e-02 8.680269e-05   Fold08\n2650   F   VF 5.724106e-01 4.155051e-01 1.199269e-02 9.152591e-05   Fold08\n2651   F   VF 8.012856e-01 1.866249e-01 1.188036e-02 2.091206e-04   Fold08\n2652   F    M 4.886930e-02 2.940904e-02 9.213231e-01 3.985408e-04   Fold08\n2653   F   VF 6.659941e-01 2.759124e-01 5.410291e-02 3.990632e-03   Fold08\n2654   F   VF 8.241101e-01 1.513135e-01 2.447398e-02 1.024197e-04   Fold08\n2655   F    F 2.181860e-02 9.040511e-01 7.355658e-02 5.736893e-04   Fold08\n2656   F    F 5.262948e-02 7.026159e-01 2.426917e-01 2.062935e-03   Fold08\n2657   F    F 9.425196e-02 7.623862e-01 1.427524e-01 6.094979e-04   Fold08\n2658   F    F 8.706585e-02 7.724806e-01 1.398508e-01 6.026930e-04   Fold08\n2659   F    F 1.161495e-01 7.617840e-01 1.217055e-01 3.608728e-04   Fold08\n2660   F    F 5.361275e-02 7.438978e-01 1.995585e-01 2.930964e-03   Fold08\n2661   F    F 1.250860e-01 7.648775e-01 1.097648e-01 2.717181e-04   Fold08\n2662   F    F 6.896988e-02 7.864800e-01 1.432019e-01 1.348195e-03   Fold08\n2663   F    F 8.326201e-02 7.495285e-01 1.666383e-01 5.711996e-04   Fold08\n2664   F    F 1.055506e-01 7.601293e-01 1.339615e-01 3.586289e-04   Fold08\n2665   F    F 9.146757e-02 7.653919e-01 1.424952e-01 6.453368e-04   Fold08\n2666   F    F 1.229772e-01 7.666286e-01 1.101191e-01 2.750996e-04   Fold08\n2667   F    F 8.053200e-02 7.578089e-01 1.609289e-01 7.301241e-04   Fold08\n2668   F    F 6.031305e-02 7.533959e-01 1.850446e-01 1.246409e-03   Fold08\n2669   F    F 6.732523e-02 7.396644e-01 1.922360e-01 7.743872e-04   Fold08\n2670   F    F 9.222759e-02 7.274411e-01 1.797196e-01 6.116840e-04   Fold08\n2671   F    F 1.014580e-01 7.640908e-01 1.339445e-01 5.066443e-04   Fold08\n2672   F    F 6.307255e-02 7.126795e-01 2.227648e-01 1.483085e-03   Fold08\n2673   F    F 3.876439e-02 7.635017e-01 1.942433e-01 3.490600e-03   Fold08\n2674   F    F 5.726244e-02 7.629264e-01 1.792824e-01 5.287861e-04   Fold08\n2675   F   VF 7.793030e-01 2.048440e-01 1.583339e-02 1.960862e-05   Fold08\n2676   F   VF 7.623587e-01 2.207815e-01 1.683868e-02 2.117391e-05   Fold08\n2677   F   VF 7.596410e-01 2.212210e-01 1.911820e-02 1.976660e-05   Fold08\n2678   F   VF 7.337134e-01 2.404480e-01 2.580636e-02 3.224453e-05   Fold08\n2679   F    F 4.290561e-01 4.700914e-01 9.661384e-02 4.238666e-03   Fold08\n2680   F    F 2.788358e-01 5.248024e-01 1.841721e-01 1.218964e-02   Fold08\n2681   F    M 1.607171e-02 1.864848e-01 5.573972e-01 2.400462e-01   Fold08\n2682   F    M 3.507008e-02 2.366652e-01 5.355466e-01 1.927181e-01   Fold08\n2683   F    F 4.432864e-01 4.451734e-01 1.096922e-01 1.848083e-03   Fold08\n2684   F    M 1.706024e-02 2.668827e-02 9.559252e-01 3.263129e-04   Fold08\n2685   F   VF 6.330742e-01 3.276522e-01 3.915346e-02 1.201020e-04   Fold08\n2686   F    F 8.024038e-02 7.600028e-01 1.551336e-01 4.623294e-03   Fold08\n2687   F    F 8.553289e-02 7.903652e-01 1.203270e-01 3.774849e-03   Fold08\n2688   F    F 1.009230e-01 7.795518e-01 1.173311e-01 2.194135e-03   Fold08\n2689   F    F 7.699228e-02 7.803934e-01 1.372860e-01 5.328300e-03   Fold08\n2690   F    F 7.680915e-02 7.867290e-01 1.315724e-01 4.889441e-03   Fold08\n2691   F    F 7.603893e-02 7.844266e-01 1.344760e-01 5.058491e-03   Fold08\n2692   F    F 7.570570e-02 7.866988e-01 1.325751e-01 5.020384e-03   Fold08\n2693   F    F 6.644972e-02 7.526290e-01 1.732783e-01 7.642984e-03   Fold08\n2694   F    F 6.566348e-02 7.516886e-01 1.748006e-01 7.847289e-03   Fold08\n2695   F    F 6.554340e-02 7.516678e-01 1.749228e-01 7.866062e-03   Fold08\n2696   F    F 3.641556e-02 7.263727e-01 2.009963e-01 3.621548e-02   Fold08\n2697   F    F 1.751391e-02 6.008033e-01 2.977512e-01 8.393158e-02   Fold08\n2698   F    F 6.330608e-02 7.480190e-01 1.801664e-01 8.508482e-03   Fold08\n2699   F    F 6.291272e-02 7.456913e-01 1.825756e-01 8.820454e-03   Fold08\n2700   F    F 3.395615e-02 7.072309e-01 2.169551e-01 4.185786e-02   Fold08\n2701   F    F 3.757737e-02 7.097126e-01 2.364426e-01 1.626741e-02   Fold08\n2702   F    F 6.674457e-02 7.628363e-01 1.638643e-01 6.554889e-03   Fold08\n2703   F    F 6.442880e-02 7.454198e-01 1.833699e-01 6.781497e-03   Fold08\n2704   F    F 1.845003e-02 6.266042e-01 2.478760e-01 1.070698e-01   Fold08\n2705   F    F 5.455932e-02 7.553647e-01 1.827189e-01 7.357102e-03   Fold08\n2706   F    F 5.108267e-02 7.586407e-01 1.827517e-01 7.524971e-03   Fold08\n2707   F    F 1.013793e-01 7.836772e-01 1.119423e-01 3.001211e-03   Fold08\n2708   F    F 1.012784e-01 7.923164e-01 1.038128e-01 2.592430e-03   Fold08\n2709   F    F 2.518465e-02 6.670126e-01 2.593829e-01 4.841988e-02   Fold08\n2710   F    F 7.830673e-02 7.909167e-01 1.259722e-01 4.804391e-03   Fold08\n2711   F    F 9.718902e-02 7.859468e-01 1.137200e-01 3.144133e-03   Fold08\n2712   F    F 8.346097e-02 7.680352e-01 1.440326e-01 4.471196e-03   Fold08\n2713   M   VF 8.859705e-01 1.011508e-01 1.278193e-02 9.683605e-05   Fold08\n2714   M    L 1.208134e-03 3.372610e-02 1.188927e-01 8.461731e-01   Fold08\n2715   M    F 2.681581e-01 5.805866e-01 1.444426e-01 6.812643e-03   Fold08\n2716   M    M 7.248691e-02 1.205176e-01 4.350604e-01 3.719351e-01   Fold08\n2717   M    F 1.882425e-01 5.106265e-01 2.362756e-01 6.485547e-02   Fold08\n2718   M    F 2.239372e-01 5.836353e-01 1.867755e-01 5.652079e-03   Fold08\n2719   M    M 3.410486e-03 1.251394e-02 9.825428e-01 1.532726e-03   Fold08\n2720   M    F 7.337486e-02 6.073815e-01 3.132312e-01 6.012451e-03   Fold08\n2721   M    F 1.179747e-01 6.513740e-01 2.271022e-01 3.549062e-03   Fold08\n2722   M    M 8.331369e-03 2.812742e-01 4.068442e-01 3.035503e-01   Fold08\n2723   M    M 1.034221e-03 2.085531e-02 9.743591e-01 3.751411e-03   Fold08\n2724   M    F 1.724035e-01 6.570945e-01 1.689055e-01 1.596473e-03   Fold08\n2725   M    F 7.491715e-02 6.019019e-01 3.087728e-01 1.440815e-02   Fold08\n2726   M    L 1.199697e-02 5.063268e-02 5.213600e-02 8.852343e-01   Fold08\n2727   M   VF 8.711648e-01 1.212750e-01 7.542633e-03 1.757058e-05   Fold08\n2728   M    L 6.151887e-03 3.199974e-02 3.338070e-02 9.284677e-01   Fold08\n2729   M   VF 8.003195e-01 1.836458e-01 1.589650e-02 1.381095e-04   Fold08\n2730   M    M 2.754143e-04 3.474477e-01 4.662660e-01 1.860109e-01   Fold08\n2731   M    F 2.903561e-02 6.384803e-01 3.130641e-01 1.942000e-02   Fold08\n2732   M    M 1.086122e-02 1.760632e-01 8.120871e-01 9.884129e-04   Fold08\n2733   M    F 6.057773e-02 7.320116e-01 2.062453e-01 1.165319e-03   Fold08\n2734   M    F 4.130536e-02 6.889771e-01 2.675756e-01 2.141940e-03   Fold08\n2735   M    F 5.839518e-02 7.221628e-01 2.184960e-01 9.460417e-04   Fold08\n2736   M    F 5.232476e-02 7.593163e-01 1.857747e-01 2.584271e-03   Fold08\n2737   M   VF 5.204042e-01 3.834363e-01 9.555165e-02 6.078308e-04   Fold08\n2738   M   VF 7.945106e-01 1.923384e-01 1.313965e-02 1.137938e-05   Fold08\n2739   M    F 1.920589e-01 4.777972e-01 3.146857e-01 1.545810e-02   Fold08\n2740   M    M 2.900700e-02 2.170816e-01 5.514709e-01 2.024405e-01   Fold08\n2741   M    M 2.757061e-02 2.254249e-01 5.075507e-01 2.394538e-01   Fold08\n2742   M    M 2.848641e-02 2.231324e-01 5.492218e-01 1.991594e-01   Fold08\n2743   M    M 2.898929e-02 2.236860e-01 5.492416e-01 1.980832e-01   Fold08\n2744   M   VF 5.715329e-01 3.554954e-01 7.265088e-02 3.207543e-04   Fold08\n2745   M   VF 5.885191e-01 3.497360e-01 6.150534e-02 2.395542e-04   Fold08\n2746   M    F 8.009929e-02 7.631393e-01 1.516457e-01 5.115643e-03   Fold08\n2747   M    F 3.977086e-02 7.488638e-01 2.006777e-01 1.068759e-02   Fold08\n2748   M    F 6.203872e-02 7.740385e-01 1.551355e-01 8.787300e-03   Fold08\n2749   M    F 2.085917e-02 6.783664e-01 2.004760e-01 1.002984e-01   Fold08\n2750   M    F 2.014213e-02 6.492808e-01 2.136626e-01 1.169145e-01   Fold08\n2751   M    F 3.413977e-02 7.136108e-01 2.110328e-01 4.121664e-02   Fold08\n2752   M    F 3.082031e-02 6.538528e-01 2.641475e-01 5.117937e-02   Fold08\n2753   M    F 5.493372e-02 7.495924e-01 1.854522e-01 1.002167e-02   Fold08\n2754   M    F 3.760863e-02 7.123490e-01 2.267199e-01 2.332244e-02   Fold08\n2755   L    L 1.864357e-03 2.609525e-02 3.537901e-01 6.182503e-01   Fold08\n2756   L    F 1.922481e-01 6.603861e-01 1.450378e-01 2.328029e-03   Fold08\n2757   L    F 1.783137e-01 6.558467e-01 1.617209e-01 4.118721e-03   Fold08\n2758   L    L 2.881471e-06 7.132784e-04 5.181816e-03 9.941020e-01   Fold08\n2759   L    L 7.262086e-12 1.558945e-08 1.249030e-05 9.999875e-01   Fold08\n2760   L    F 2.714319e-01 4.121605e-01 2.829717e-01 3.343592e-02   Fold08\n2761   L   VF 4.350734e-01 3.915295e-01 1.605647e-01 1.283237e-02   Fold08\n2762   L    M 4.771883e-02 2.142368e-01 4.216452e-01 3.163992e-01   Fold08\n2763   L    L 9.258804e-08 1.527864e-03 3.074160e-02 9.677304e-01   Fold08\n2764   L    M 1.077460e-02 4.646060e-01 5.021146e-01 2.250481e-02   Fold08\n2765   L    M 5.207665e-03 4.099134e-01 4.527588e-01 1.321202e-01   Fold08\n2766   L    M 1.923176e-02 2.015502e-01 5.017830e-01 2.774350e-01   Fold08\n2767   L    L 4.619928e-08 6.088433e-05 4.691444e-04 9.994699e-01   Fold08\n2768   L    L 1.678422e-08 3.106861e-05 1.198070e-04 9.998491e-01   Fold08\n2769   L    L 1.631124e-06 7.190900e-04 2.762572e-03 9.965167e-01   Fold08\n2770   L    L 5.819943e-07 4.044906e-04 2.032719e-03 9.975622e-01   Fold08\n2771   L    L 5.994774e-07 3.745515e-04 1.124534e-03 9.985003e-01   Fold08\n2772   L    L 2.274412e-03 2.039117e-01 3.087879e-01 4.850260e-01   Fold08\n2773   L    F 3.899267e-02 7.210286e-01 2.176173e-01 2.236147e-02   Fold08\n2774   L    L 4.629758e-08 6.564526e-05 1.892774e-04 9.997450e-01   Fold08\n2775   L    L 9.709405e-08 1.051000e-04 6.678651e-04 9.992269e-01   Fold08\n2776  VF   VF 9.399712e-01 5.579105e-02 4.228538e-03 9.180935e-06   Fold09\n2777  VF   VF 9.532388e-01 4.384571e-02 2.910178e-03 5.354773e-06   Fold09\n2778  VF   VF 9.403719e-01 5.496889e-02 4.648928e-03 1.031191e-05   Fold09\n2779  VF   VF 9.600075e-01 3.804551e-02 1.944769e-03 2.194442e-06   Fold09\n2780  VF   VF 9.389143e-01 5.620018e-02 4.874313e-03 1.124373e-05   Fold09\n2781  VF   VF 9.426079e-01 5.209919e-02 5.283423e-03 9.463407e-06   Fold09\n2782  VF   VF 9.428437e-01 5.317823e-02 3.970011e-03 8.027906e-06   Fold09\n2783  VF   VF 9.430623e-01 5.297092e-02 3.958811e-03 7.966000e-06   Fold09\n2784  VF   VF 9.235479e-01 6.899301e-02 7.439464e-03 1.964565e-05   Fold09\n2785  VF   VF 9.220405e-01 7.032157e-02 7.617273e-03 2.066097e-05   Fold09\n2786  VF   VF 4.829568e-01 4.547236e-01 6.197821e-02 3.414327e-04   Fold09\n2787  VF    F 3.630383e-01 5.471667e-01 8.714509e-02 2.649884e-03   Fold09\n2788  VF    F 3.000263e-01 5.622204e-01 1.351650e-01 2.588281e-03   Fold09\n2789  VF   VF 9.308594e-01 6.142122e-02 7.689851e-03 2.949034e-05   Fold09\n2790  VF   VF 9.525404e-01 4.312138e-02 4.326048e-03 1.221606e-05   Fold09\n2791  VF   VF 9.534790e-01 4.103169e-02 5.480941e-03 8.377839e-06   Fold09\n2792  VF   VF 7.774346e-01 1.647265e-01 5.749932e-02 3.396343e-04   Fold09\n2793  VF   VF 8.404618e-01 1.316186e-01 2.778885e-02 1.308125e-04   Fold09\n2794  VF   VF 7.982905e-01 1.516923e-01 4.971233e-02 3.049493e-04   Fold09\n2795  VF   VF 9.327288e-01 5.680698e-02 1.044576e-02 1.844916e-05   Fold09\n2796  VF   VF 9.125367e-01 7.218018e-02 1.524741e-02 3.570882e-05   Fold09\n2797  VF   VF 9.873897e-01 1.206176e-02 5.478255e-04 7.152366e-07   Fold09\n2798  VF   VF 9.855222e-01 1.359803e-02 8.787373e-04 1.023682e-06   Fold09\n2799  VF   VF 9.811921e-01 1.775422e-02 1.049777e-03 3.917675e-06   Fold09\n2800  VF   VF 7.703857e-01 1.269287e-01 9.507454e-02 7.611080e-03   Fold09\n2801  VF   VF 9.887503e-01 1.084450e-02 4.047838e-04 4.275620e-07   Fold09\n2802  VF   VF 9.857636e-01 1.354796e-02 6.875189e-04 8.832654e-07   Fold09\n2803  VF   VF 9.832546e-01 1.581048e-02 9.329710e-04 1.973676e-06   Fold09\n2804  VF   VF 9.853480e-01 1.397197e-02 6.791101e-04 8.911330e-07   Fold09\n2805  VF   VF 9.764766e-01 2.132656e-02 2.192104e-03 4.770989e-06   Fold09\n2806  VF   VF 5.430722e-01 3.345343e-01 1.215615e-01 8.319772e-04   Fold09\n2807  VF   VF 6.624602e-01 2.948105e-01 4.244099e-02 2.882905e-04   Fold09\n2808  VF   VF 6.739023e-01 2.799942e-01 4.581675e-02 2.867590e-04   Fold09\n2809  VF    M 9.058835e-03 2.006131e-02 9.684046e-01 2.475251e-03   Fold09\n2810  VF   VF 7.546264e-01 2.239087e-01 2.139129e-02 7.361055e-05   Fold09\n2811  VF   VF 7.557299e-01 2.225452e-01 2.165047e-02 7.437970e-05   Fold09\n2812  VF   VF 6.194039e-01 3.061031e-01 7.392671e-02 5.663276e-04   Fold09\n2813  VF   VF 6.202304e-01 3.070868e-01 7.214173e-02 5.411342e-04   Fold09\n2814  VF   VF 7.406857e-01 2.341292e-01 2.506113e-02 1.238924e-04   Fold09\n2815  VF   VF 8.755458e-01 1.185185e-01 5.929153e-03 6.611115e-06   Fold09\n2816  VF   VF 8.318636e-01 1.597779e-01 8.330465e-03 2.810742e-05   Fold09\n2817  VF   VF 8.644707e-01 1.271290e-01 8.392338e-03 7.957968e-06   Fold09\n2818  VF   VF 8.535356e-01 1.381632e-01 8.292889e-03 8.300649e-06   Fold09\n2819  VF   VF 8.294102e-01 1.578899e-01 1.267916e-02 2.082008e-05   Fold09\n2820  VF    F 1.344154e-01 6.344969e-01 2.284739e-01 2.613842e-03   Fold09\n2821  VF    F 1.406737e-01 6.851633e-01 1.720029e-01 2.160120e-03   Fold09\n2822  VF    F 1.090653e-01 6.543515e-01 2.311143e-01 5.468880e-03   Fold09\n2823  VF    F 1.501612e-01 6.352904e-01 2.125133e-01 2.035132e-03   Fold09\n2824  VF    F 9.832909e-02 6.419956e-01 2.552636e-01 4.411748e-03   Fold09\n2825  VF   VF 9.871426e-01 1.248286e-02 3.741562e-04 3.392093e-07   Fold09\n2826  VF   VF 9.841030e-01 1.531829e-02 5.780385e-04 6.527838e-07   Fold09\n2827  VF   VF 9.840025e-01 1.541621e-02 5.806535e-04 6.585718e-07   Fold09\n2828  VF   VF 9.680927e-01 3.030092e-02 1.603171e-03 3.255607e-06   Fold09\n2829  VF   VF 9.894022e-01 1.034505e-02 2.523630e-04 3.919746e-07   Fold09\n2830  VF   VF 7.063664e-01 2.138699e-01 7.683478e-02 2.928880e-03   Fold09\n2831  VF   VF 9.861855e-01 1.338979e-02 4.242474e-04 4.410498e-07   Fold09\n2832  VF   VF 9.275705e-01 6.775844e-02 4.652351e-03 1.874030e-05   Fold09\n2833  VF   VF 9.914029e-01 8.411158e-03 1.857931e-04 1.170619e-07   Fold09\n2834  VF   VF 9.911137e-01 8.693131e-03 1.930838e-04 1.241724e-07   Fold09\n2835  VF   VF 9.592793e-01 3.855461e-02 2.154802e-03 1.132067e-05   Fold09\n2836  VF   VF 7.235043e-01 2.295029e-01 4.628390e-02 7.088859e-04   Fold09\n2837  VF   VF 4.819877e-01 3.876804e-01 1.261031e-01 4.228810e-03   Fold09\n2838  VF   VF 9.314052e-01 6.290051e-02 5.670685e-03 2.364281e-05   Fold09\n2839  VF   VF 9.853615e-01 1.403231e-02 6.054781e-04 6.951604e-07   Fold09\n2840  VF   VF 5.879786e-01 2.259903e-01 1.662901e-01 1.974098e-02   Fold09\n2841  VF   VF 9.918074e-01 8.027100e-03 1.653444e-04 1.904058e-07   Fold09\n2842  VF   VF 9.668384e-01 3.089886e-02 2.257288e-03 5.483149e-06   Fold09\n2843  VF   VF 9.913144e-01 8.465285e-03 2.201643e-04 1.751871e-07   Fold09\n2844  VF   VF 9.558171e-01 4.140778e-02 2.766602e-03 8.536429e-06   Fold09\n2845  VF   VF 9.877346e-01 1.189147e-02 3.731062e-04 7.852504e-07   Fold09\n2846  VF   VF 9.803187e-01 1.898742e-02 6.929841e-04 8.564028e-07   Fold09\n2847  VF   VF 9.520709e-01 4.479764e-02 3.123106e-03 8.320581e-06   Fold09\n2848  VF   VF 9.913482e-01 8.347339e-03 3.042833e-04 1.491134e-07   Fold09\n2849  VF   VF 9.822918e-01 1.677014e-02 9.368596e-04 1.235131e-06   Fold09\n2850  VF   VF 9.878453e-01 1.182565e-02 3.285406e-04 5.388699e-07   Fold09\n2851  VF   VF 9.876484e-01 1.200616e-02 3.451267e-04 2.925847e-07   Fold09\n2852  VF   VF 9.684467e-01 3.010743e-02 1.443174e-03 2.705556e-06   Fold09\n2853  VF   VF 9.941327e-01 5.749537e-03 1.176949e-04 5.638522e-08   Fold09\n2854  VF   VF 9.848868e-01 1.444785e-02 6.642892e-04 1.055045e-06   Fold09\n2855  VF   VF 8.215162e-01 1.564869e-01 2.180548e-02 1.914092e-04   Fold09\n2856  VF   VF 7.077506e-01 2.874213e-01 4.815941e-03 1.213407e-05   Fold09\n2857  VF   VF 9.088823e-01 8.908039e-02 1.992047e-03 4.523638e-05   Fold09\n2858  VF   VF 9.187098e-01 7.802986e-02 3.254344e-03 5.976959e-06   Fold09\n2859  VF   VF 9.143897e-01 8.262926e-02 2.975429e-03 5.576676e-06   Fold09\n2860  VF   VF 8.856369e-01 1.092528e-01 5.085780e-03 2.454318e-05   Fold09\n2861  VF   VF 8.814312e-01 1.134277e-01 5.116076e-03 2.496190e-05   Fold09\n2862  VF   VF 8.835107e-01 1.111326e-01 5.343491e-03 1.315414e-05   Fold09\n2863  VF   VF 8.616752e-01 1.281041e-01 1.018969e-02 3.101163e-05   Fold09\n2864  VF   VF 8.615328e-01 1.285993e-01 9.837773e-03 3.015743e-05   Fold09\n2865  VF   VF 8.866767e-01 1.074443e-01 5.864284e-03 1.470297e-05   Fold09\n2866  VF   VF 9.103583e-01 8.654287e-02 3.092639e-03 6.239275e-06   Fold09\n2867  VF   VF 8.941165e-01 1.017440e-01 4.122747e-03 1.678934e-05   Fold09\n2868  VF   VF 8.763451e-01 1.170439e-01 6.573895e-03 3.702208e-05   Fold09\n2869  VF   VF 8.736682e-01 1.200789e-01 6.218002e-03 3.482837e-05   Fold09\n2870  VF   VF 8.785298e-01 1.155400e-01 5.898063e-03 3.215202e-05   Fold09\n2871  VF   VF 8.780642e-01 1.158876e-01 6.014944e-03 3.320224e-05   Fold09\n2872  VF   VF 8.838802e-01 1.105541e-01 5.551158e-03 1.449253e-05   Fold09\n2873  VF   VF 8.769480e-01 1.151662e-01 7.867303e-03 1.851316e-05   Fold09\n2874  VF   VF 8.888467e-01 1.067125e-01 4.425692e-03 1.511304e-05   Fold09\n2875  VF   VF 9.036399e-01 9.200399e-02 4.347702e-03 8.414703e-06   Fold09\n2876  VF   VF 8.938372e-01 9.930980e-02 6.838680e-03 1.431049e-05   Fold09\n2877  VF   VF 6.395005e-01 3.523341e-01 8.121586e-03 4.378398e-05   Fold09\n2878  VF   VF 6.373864e-01 3.543151e-01 8.253270e-03 4.523981e-05   Fold09\n2879  VF   VF 8.732006e-01 1.193572e-01 7.418186e-03 2.403728e-05   Fold09\n2880  VF   VF 8.965082e-01 9.889279e-02 4.586561e-03 1.246423e-05   Fold09\n2881  VF   VF 6.357948e-01 3.543305e-01 9.816502e-03 5.822883e-05   Fold09\n2882  VF   VF 8.723874e-01 1.196293e-01 7.956882e-03 2.637935e-05   Fold09\n2883  VF   VF 8.616162e-01 1.307188e-01 7.611716e-03 5.324806e-05   Fold09\n2884  VF   VF 6.214246e-01 3.692494e-01 9.268620e-03 5.731012e-05   Fold09\n2885  VF   VF 8.540560e-01 1.372831e-01 8.594522e-03 6.630485e-05   Fold09\n2886  VF   VF 8.558186e-01 1.356968e-01 8.420595e-03 6.410276e-05   Fold09\n2887  VF   VF 6.227186e-01 3.664648e-01 1.074645e-02 7.013741e-05   Fold09\n2888  VF   VF 6.200962e-01 3.688503e-01 1.098034e-02 7.313748e-05   Fold09\n2889  VF   VF 8.477632e-01 1.423027e-01 9.851621e-03 8.251125e-05   Fold09\n2890  VF   VF 8.844712e-01 1.103076e-01 5.197047e-03 2.411265e-05   Fold09\n2891  VF   VF 9.105457e-01 8.473580e-02 4.711332e-03 7.153698e-06   Fold09\n2892  VF   VF 9.217091e-01 7.524578e-02 3.039840e-03 5.246440e-06   Fold09\n2893  VF   VF 8.682759e-01 1.230327e-01 8.667577e-03 2.378368e-05   Fold09\n2894  VF   VF 7.893979e-01 1.866619e-01 2.380131e-02 1.388227e-04   Fold09\n2895  VF   VF 8.643746e-01 1.271464e-01 8.455495e-03 2.351059e-05   Fold09\n2896  VF   VF 9.045543e-01 9.037927e-02 5.058340e-03 8.112333e-06   Fold09\n2897  VF   VF 8.989809e-01 9.608209e-02 4.921690e-03 1.532538e-05   Fold09\n2898  VF   VF 8.613687e-01 1.291847e-01 9.419203e-03 2.735526e-05   Fold09\n2899  VF   VF 8.684634e-01 1.223566e-01 9.154499e-03 2.554584e-05   Fold09\n2900  VF   VF 8.623994e-01 1.283042e-01 9.220208e-03 7.621788e-05   Fold09\n2901  VF   VF 9.754492e-01 2.376012e-02 7.903064e-04 4.144074e-07   Fold09\n2902  VF   VF 9.522711e-01 4.479755e-02 2.927377e-03 3.929022e-06   Fold09\n2903  VF   VF 9.709685e-01 2.790683e-02 1.123316e-03 1.379053e-06   Fold09\n2904  VF   VF 9.761278e-01 2.322108e-02 6.508601e-04 3.076348e-07   Fold09\n2905  VF   VF 9.848568e-01 1.480209e-02 3.410460e-04 1.096326e-07   Fold09\n2906  VF   VF 9.794627e-01 1.997210e-02 5.649578e-04 2.392984e-07   Fold09\n2907  VF   VF 9.759938e-01 2.289365e-02 1.112006e-03 5.641352e-07   Fold09\n2908  VF    F 4.415518e-01 4.454314e-01 1.120335e-01 9.833464e-04   Fold09\n2909  VF   VF 9.267095e-01 6.693902e-02 6.340378e-03 1.107729e-05   Fold09\n2910  VF   VF 9.463202e-01 5.117456e-02 2.502921e-03 2.354355e-06   Fold09\n2911  VF   VF 9.848905e-01 1.457271e-02 5.366206e-04 1.449820e-07   Fold09\n2912  VF   VF 9.650739e-01 3.361417e-02 1.311005e-03 9.030409e-07   Fold09\n2913  VF   VF 9.510005e-01 4.440124e-02 4.590734e-03 7.517473e-06   Fold09\n2914  VF    F 7.624403e-02 7.988438e-01 1.240423e-01 8.698090e-04   Fold09\n2915  VF    F 1.046580e-01 7.958902e-01 9.913979e-02 3.120450e-04   Fold09\n2916  VF    F 7.956870e-02 7.578948e-01 1.620325e-01 5.040092e-04   Fold09\n2917  VF    F 8.779238e-02 7.734261e-01 1.383836e-01 3.978778e-04   Fold09\n2918  VF    F 9.120986e-02 8.004179e-01 1.080341e-01 3.380852e-04   Fold09\n2919  VF    F 1.145305e-01 7.993631e-01 8.587987e-02 2.265080e-04   Fold09\n2920  VF    F 1.053964e-01 7.733958e-01 1.209428e-01 2.649839e-04   Fold09\n2921  VF   VF 6.013366e-01 3.453226e-01 5.319490e-02 1.458558e-04   Fold09\n2922  VF   VF 7.546991e-01 2.208620e-01 2.441792e-02 2.098478e-05   Fold09\n2923  VF   VF 7.867115e-01 1.985194e-01 1.475477e-02 1.432008e-05   Fold09\n2924  VF   VF 7.919319e-01 1.930260e-01 1.502807e-02 1.402517e-05   Fold09\n2925  VF   VF 7.106629e-01 2.616628e-01 2.760309e-02 7.117905e-05   Fold09\n2926  VF   VF 6.887443e-01 2.855626e-01 2.561660e-02 7.646848e-05   Fold09\n2927  VF   VF 6.378371e-01 3.147318e-01 4.734378e-02 8.727547e-05   Fold09\n2928  VF   VF 7.784815e-01 2.032877e-01 1.821800e-02 1.283809e-05   Fold09\n2929  VF   VF 6.272711e-01 3.221065e-01 5.052543e-02 9.690799e-05   Fold09\n2930  VF   VF 7.523572e-01 2.276107e-01 2.000735e-02 2.477693e-05   Fold09\n2931  VF   VF 7.511132e-01 2.309382e-01 1.791267e-02 3.596958e-05   Fold09\n2932  VF   VF 7.725046e-01 2.115755e-01 1.590535e-02 1.456118e-05   Fold09\n2933  VF   VF 7.412103e-01 2.378194e-01 2.094257e-02 2.772570e-05   Fold09\n2934  VF   VF 6.241941e-01 3.222248e-01 5.347373e-02 1.073997e-04   Fold09\n2935  VF   VF 6.849625e-01 2.841291e-01 3.080817e-02 1.002921e-04   Fold09\n2936  VF   VF 7.998608e-01 1.840215e-01 1.610728e-02 1.039932e-05   Fold09\n2937  VF   VF 6.619187e-01 2.997966e-01 3.821589e-02 6.875610e-05   Fold09\n2938  VF   VF 7.218078e-01 2.428006e-01 3.534976e-02 4.185808e-05   Fold09\n2939  VF   VF 6.763860e-01 2.932945e-01 3.021036e-02 1.091686e-04   Fold09\n2940  VF   VF 6.051013e-01 3.351005e-01 5.965356e-02 1.446380e-04   Fold09\n2941  VF   VF 6.584390e-01 3.039954e-01 3.741236e-02 1.532331e-04   Fold09\n2942  VF   VF 8.109067e-01 1.772818e-01 1.180200e-02 9.439481e-06   Fold09\n2943  VF   VF 7.664865e-01 2.150995e-01 1.839611e-02 1.786094e-05   Fold09\n2944  VF   VF 6.424773e-01 3.180827e-01 3.925878e-02 1.812777e-04   Fold09\n2945  VF   VF 7.938679e-01 1.880558e-01 1.806373e-02 1.258982e-05   Fold09\n2946  VF   VF 6.023094e-01 3.427356e-01 5.461322e-02 3.418138e-04   Fold09\n2947  VF   VF 6.872860e-01 2.793948e-01 3.327355e-02 4.570549e-05   Fold09\n2948  VF    F 6.842116e-02 8.244206e-01 1.033474e-01 3.810834e-03   Fold09\n2949  VF    F 8.754319e-02 8.050499e-01 1.056067e-01 1.800261e-03   Fold09\n2950  VF    F 8.190155e-02 8.250275e-01 9.007223e-02 2.998753e-03   Fold09\n2951  VF    F 8.364137e-02 8.185516e-01 9.458825e-02 3.218798e-03   Fold09\n2952  VF    F 6.655034e-02 8.221456e-01 1.074843e-01 3.819758e-03   Fold09\n2953   F    F 3.044691e-01 5.581997e-01 1.348026e-01 2.528551e-03   Fold09\n2954   F    F 3.052249e-01 5.579878e-01 1.342846e-01 2.502756e-03   Fold09\n2955   F   VF 5.847529e-01 2.093094e-01 1.825095e-01 2.342816e-02   Fold09\n2956   F    F 3.222262e-01 4.108290e-01 2.585258e-01 8.419025e-03   Fold09\n2957   F   VF 5.966737e-01 2.974332e-01 1.054318e-01 4.612830e-04   Fold09\n2958   F   VF 6.799034e-01 2.724607e-01 4.741473e-02 2.211843e-04   Fold09\n2959   F    M 7.597496e-02 2.670726e-01 4.195205e-01 2.374319e-01   Fold09\n2960   F   VF 6.999918e-01 2.528384e-01 4.695877e-02 2.110410e-04   Fold09\n2961   F   VF 5.689378e-01 3.500870e-01 8.025611e-02 7.190736e-04   Fold09\n2962   F   VF 8.555515e-01 1.287587e-01 1.567813e-02 1.163448e-05   Fold09\n2963   F   VF 7.915449e-01 1.855987e-01 2.281935e-02 3.704979e-05   Fold09\n2964   F   VF 8.276730e-01 1.589785e-01 1.332628e-02 2.220177e-05   Fold09\n2965   F   VF 8.441143e-01 1.435692e-01 1.229810e-02 1.840404e-05   Fold09\n2966   F   VF 7.886291e-01 1.914958e-01 1.977815e-02 9.694337e-05   Fold09\n2967   F    F 1.383968e-01 6.372290e-01 2.220364e-01 2.337848e-03   Fold09\n2968   F    F 1.154427e-01 6.890654e-01 1.894525e-01 6.039400e-03   Fold09\n2969   F    F 1.591286e-01 6.577436e-01 1.818840e-01 1.243744e-03   Fold09\n2970   F    L 2.705969e-04 2.689725e-02 2.395774e-01 7.332547e-01   Fold09\n2971   F    F 8.919176e-02 6.532580e-01 2.454080e-01 1.214226e-02   Fold09\n2972   F    F 1.000640e-01 6.439074e-01 2.518136e-01 4.215017e-03   Fold09\n2973   F    L 2.526308e-05 4.800168e-03 8.822822e-02 9.069464e-01   Fold09\n2974   F    F 8.593627e-02 5.810340e-01 3.271995e-01 5.830240e-03   Fold09\n2975   F    F 1.362640e-01 6.850510e-01 1.763681e-01 2.316876e-03   Fold09\n2976   F    F 1.738410e-01 6.779041e-01 1.466508e-01 1.604121e-03   Fold09\n2977   F    F 7.113471e-02 6.392423e-01 2.722608e-01 1.736218e-02   Fold09\n2978   F    L 7.483836e-02 3.945731e-02 5.966672e-02 8.260376e-01   Fold09\n2979   F   VF 8.998556e-01 7.965692e-02 1.904474e-02 1.442778e-03   Fold09\n2980   F   VF 6.011922e-01 2.052879e-01 1.403892e-01 5.313067e-02   Fold09\n2981   F   VF 4.451629e-01 1.943156e-01 2.678622e-01 9.265926e-02   Fold09\n2982   F    L 1.914928e-04 1.952795e-03 3.420599e-02 9.636497e-01   Fold09\n2983   F    L 1.045781e-06 1.006890e-04 5.387912e-03 9.945104e-01   Fold09\n2984   F   VF 8.190935e-01 1.665585e-01 1.405517e-02 2.928626e-04   Fold09\n2985   F   VF 8.468522e-01 1.418790e-01 1.121383e-02 5.493804e-05   Fold09\n2986   F   VF 7.959108e-01 1.865314e-01 1.734087e-02 2.169018e-04   Fold09\n2987   F    M 2.226956e-01 7.161382e-02 7.054752e-01 2.154047e-04   Fold09\n2988   F   VF 9.085166e-01 8.737595e-02 4.098746e-03 8.651379e-06   Fold09\n2989   F   VF 8.874718e-01 1.066538e-01 5.859655e-03 1.467617e-05   Fold09\n2990   F   VF 8.544969e-01 1.366725e-01 8.802046e-03 2.855871e-05   Fold09\n2991   F   VF 8.984035e-01 9.719930e-02 4.386331e-03 1.089752e-05   Fold09\n2992   F   VF 3.850398e-01 3.810905e-01 2.145042e-01 1.936547e-02   Fold09\n2993   F    L 1.390652e-02 3.773591e-02 6.520270e-02 8.831549e-01   Fold09\n2994   F    L 1.400706e-02 3.795384e-02 6.540925e-02 8.826298e-01   Fold09\n2995   F    L 1.833694e-02 4.388057e-02 5.638027e-02 8.814022e-01   Fold09\n2996   F   VF 7.752829e-01 1.536197e-01 7.101324e-02 8.412220e-05   Fold09\n2997   F    L 1.810954e-02 4.403015e-02 5.563104e-02 8.822293e-01   Fold09\n2998   F   VF 5.937616e-01 3.949387e-01 1.121482e-02 8.484735e-05   Fold09\n2999   F   VF 5.805688e-01 4.070782e-01 1.225119e-02 1.018118e-04   Fold09\n3000   F   VF 5.520425e-01 4.330748e-01 1.473303e-02 1.496229e-04   Fold09\n3001   F   VF 5.487098e-01 4.360771e-01 1.505653e-02 1.566354e-04   Fold09\n3002   F   VF 8.435713e-01 1.421370e-01 1.425740e-02 3.435744e-05   Fold09\n3003   F   VF 7.581732e-01 2.085254e-01 3.315584e-02 1.455763e-04   Fold09\n3004   F    F 3.767735e-01 4.951596e-01 1.241692e-01 3.897653e-03   Fold09\n3005   F   VF 4.681197e-01 4.138585e-01 1.166559e-01 1.365887e-03   Fold09\n3006   F    F 2.917125e-01 3.866736e-01 2.907292e-01 3.088477e-02   Fold09\n3007   F    L 6.031987e-02 6.055909e-02 1.026041e-01 7.765169e-01   Fold09\n3008   F    F 7.530469e-02 8.039068e-01 1.198638e-01 9.247096e-04   Fold09\n3009   F    F 7.941536e-02 7.771105e-01 1.430458e-01 4.283598e-04   Fold09\n3010   F    F 1.132479e-02 5.072392e-01 4.629534e-01 1.848264e-02   Fold09\n3011   F    F 5.232829e-02 7.376475e-01 2.089571e-01 1.067134e-03   Fold09\n3012   F    F 9.211559e-02 7.926633e-01 1.148097e-01 4.113320e-04   Fold09\n3013   F    F 6.799611e-02 7.929433e-01 1.377818e-01 1.278794e-03   Fold09\n3014   F    F 4.549973e-02 7.447722e-01 2.083748e-01 1.353298e-03   Fold09\n3015   F    F 6.475090e-02 7.890007e-01 1.455265e-01 7.219219e-04   Fold09\n3016   F    F 9.027554e-02 7.907297e-01 1.186176e-01 3.772080e-04   Fold09\n3017   F    F 6.159713e-02 7.809023e-01 1.559276e-01 1.572978e-03   Fold09\n3018   F    F 6.393756e-02 7.403661e-01 1.950326e-01 6.636970e-04   Fold09\n3019   F    F 5.891374e-02 7.856335e-01 1.537286e-01 1.724093e-03   Fold09\n3020   F    F 5.773214e-02 7.891011e-01 1.513105e-01 1.856283e-03   Fold09\n3021   F    F 5.560235e-02 7.433320e-01 2.001273e-01 9.382598e-04   Fold09\n3022   F    F 7.187327e-02 8.013737e-01 1.262608e-01 4.922656e-04   Fold09\n3023   F    F 8.735172e-02 7.901417e-01 1.220970e-01 4.095883e-04   Fold09\n3024   F    F 7.751850e-02 8.293537e-01 9.247187e-02 6.559563e-04   Fold09\n3025   F    F 9.864444e-02 8.542448e-01 4.555845e-02 1.552291e-03   Fold09\n3026   F    F 5.538209e-02 7.618595e-01 1.814283e-01 1.330193e-03   Fold09\n3027   F   VF 8.002386e-01 1.857817e-01 1.396778e-02 1.185528e-05   Fold09\n3028   F   VF 7.205647e-01 2.535005e-01 2.588191e-02 5.285373e-05   Fold09\n3029   F   VF 6.213937e-01 3.303100e-01 4.822813e-02 6.813194e-05   Fold09\n3030   F   VF 7.061312e-01 2.695257e-01 2.428356e-02 5.953161e-05   Fold09\n3031   F    F 3.381472e-01 4.447021e-01 2.146076e-01 2.543060e-03   Fold09\n3032   F   VF 7.395848e-01 2.379744e-01 2.238833e-02 5.240954e-05   Fold09\n3033   F   VF 4.392063e-01 4.348324e-01 1.205492e-01 5.412172e-03   Fold09\n3034   F   VF 4.386171e-01 4.350806e-01 1.208585e-01 5.443749e-03   Fold09\n3035   F   VF 6.678148e-01 2.837957e-01 4.827624e-02 1.132276e-04   Fold09\n3036   F    F 4.029741e-01 4.191431e-01 1.714238e-01 6.459057e-03   Fold09\n3037   F   VF 7.314332e-01 2.323774e-01 3.615711e-02 3.224852e-05   Fold09\n3038   F    F 3.527995e-01 4.610041e-01 1.673016e-01 1.889494e-02   Fold09\n3039   F    F 3.500007e-01 4.626098e-01 1.682109e-01 1.917858e-02   Fold09\n3040   F   VF 6.774138e-01 2.874825e-01 3.505631e-02 4.742526e-05   Fold09\n3041   F   VF 6.806180e-01 2.862553e-01 3.300437e-02 1.222808e-04   Fold09\n3042   F    F 7.984564e-02 8.083305e-01 1.088649e-01 2.958965e-03   Fold09\n3043   F    F 6.469550e-02 8.103287e-01 1.201127e-01 4.863050e-03   Fold09\n3044   F    F 6.641043e-02 7.635618e-01 1.655879e-01 4.439898e-03   Fold09\n3045   F    F 6.781904e-02 7.937491e-01 1.342058e-01 4.226022e-03   Fold09\n3046   F    F 5.590173e-02 8.237041e-01 1.104268e-01 9.967460e-03   Fold09\n3047   F    F 5.719455e-02 8.122551e-01 1.245114e-01 6.039015e-03   Fold09\n3048   F    F 6.124552e-02 7.880451e-01 1.453497e-01 5.359671e-03   Fold09\n3049   F    F 7.104445e-02 8.181823e-01 1.062562e-01 4.517101e-03   Fold09\n3050   F    F 1.310780e-02 4.982083e-01 3.973056e-01 9.137836e-02   Fold09\n3051   F    F 5.222560e-02 8.133435e-01 1.274668e-01 6.964098e-03   Fold09\n3052   F    F 1.184582e-02 4.610589e-01 2.031852e-01 3.239100e-01   Fold09\n3053   F    F 5.084929e-02 8.081521e-01 1.333637e-01 7.634949e-03   Fold09\n3054   F    F 3.422652e-02 7.487231e-01 1.775841e-01 3.946638e-02   Fold09\n3055   F    F 3.633304e-02 7.449079e-01 2.032619e-01 1.549714e-02   Fold09\n3056   F    F 3.685369e-02 7.472600e-01 2.008926e-01 1.499374e-02   Fold09\n3057   F    F 3.670330e-02 7.456104e-01 2.024770e-01 1.520933e-02   Fold09\n3058   F    F 6.511812e-02 8.194983e-01 1.100458e-01 5.337790e-03   Fold09\n3059   F    F 3.428996e-02 7.497495e-01 1.769219e-01 3.903856e-02   Fold09\n3060   F    F 4.068773e-02 7.677610e-01 1.653214e-01 2.622981e-02   Fold09\n3061   M    M 7.155176e-02 1.002124e-01 5.431120e-01 2.851238e-01   Fold09\n3062   M    M 1.031897e-01 1.210380e-01 4.139541e-01 3.618182e-01   Fold09\n3063   M   VF 8.033616e-01 1.812899e-01 1.530473e-02 4.369350e-05   Fold09\n3064   M   VF 7.921806e-01 1.904370e-01 1.734455e-02 3.782830e-05   Fold09\n3065   M   VF 7.919321e-01 1.906453e-01 1.738459e-02 3.799163e-05   Fold09\n3066   M   VF 7.881023e-01 1.939767e-01 1.788144e-02 3.952626e-05   Fold09\n3067   M    M 7.652060e-02 2.890910e-01 3.734261e-01 2.609623e-01   Fold09\n3068   M    F 1.937012e-01 6.713061e-01 1.339702e-01 1.022423e-03   Fold09\n3069   M    M 8.169701e-03 2.160225e-01 4.077350e-01 3.680728e-01   Fold09\n3070   M    L 2.293211e-04 2.525961e-02 2.430033e-01 7.315078e-01   Fold09\n3071   M    F 9.146686e-02 6.058880e-01 2.972285e-01 5.416638e-03   Fold09\n3072   M    F 1.474748e-01 6.526049e-01 1.980145e-01 1.905824e-03   Fold09\n3073   M    F 5.564258e-02 5.963860e-01 3.323806e-01 1.559079e-02   Fold09\n3074   M    F 9.291633e-02 6.478259e-01 2.545714e-01 4.686358e-03   Fold09\n3075   M    F 4.888165e-02 4.760127e-01 4.179091e-01 5.719655e-02   Fold09\n3076   M    F 5.163201e-02 7.502890e-01 1.965162e-01 1.562812e-03   Fold09\n3077   M    F 5.315519e-02 6.653500e-01 2.808374e-01 6.574022e-04   Fold09\n3078   M    F 3.308398e-02 7.785636e-01 1.868234e-01 1.529091e-03   Fold09\n3079   M    F 9.865592e-03 4.859068e-01 4.025036e-01 1.017240e-01   Fold09\n3080   M    F 6.468475e-02 7.860637e-01 1.484475e-01 8.040829e-04   Fold09\n3081   M    F 6.278392e-02 7.403023e-01 1.960268e-01 8.869233e-04   Fold09\n3082   M    F 7.380439e-02 7.563929e-01 1.691383e-01 6.643957e-04   Fold09\n3083   M    F 5.892666e-02 7.907055e-01 1.485316e-01 1.836240e-03   Fold09\n3084   M    F 7.382064e-02 7.862605e-01 1.391616e-01 7.572529e-04   Fold09\n3085   M    F 5.046410e-02 7.292536e-01 2.179550e-01 2.327336e-03   Fold09\n3086   M    F 2.044804e-01 4.610804e-01 3.209104e-01 1.352875e-02   Fold09\n3087   M    M 3.556389e-02 2.135077e-01 6.186306e-01 1.322979e-01   Fold09\n3088   M    F 4.876141e-02 7.797634e-01 1.638589e-01 7.616301e-03   Fold09\n3089   M    M 5.229260e-04 2.020596e-02 9.754873e-01 3.783810e-03   Fold09\n3090   M    F 5.691944e-02 8.250694e-01 1.080621e-01 9.949122e-03   Fold09\n3091   M    F 3.990752e-02 7.707606e-01 1.780682e-01 1.126364e-02   Fold09\n3092   M    L 4.927611e-03 2.605232e-01 1.462656e-01 5.882836e-01   Fold09\n3093   M    F 2.225047e-02 6.590935e-01 2.050389e-01 1.136171e-01   Fold09\n3094   M    F 2.839088e-02 6.887525e-01 2.374251e-01 4.543149e-02   Fold09\n3095   M    F 2.444126e-02 6.139674e-01 3.142641e-01 4.732718e-02   Fold09\n3096   M    F 3.067627e-02 6.864103e-01 2.363844e-01 4.652900e-02   Fold09\n3097   M    F 3.349867e-02 7.494133e-01 1.775927e-01 3.949530e-02   Fold09\n3098   M    F 5.768277e-02 7.752278e-01 1.602284e-01 6.861075e-03   Fold09\n3099   M    F 4.355500e-02 7.834942e-01 1.646656e-01 8.285158e-03   Fold09\n3100   M    F 4.762625e-02 7.875491e-01 1.544011e-01 1.042353e-02   Fold09\n3101   M    M 1.997023e-04 1.478912e-02 9.728664e-01 1.214481e-02   Fold09\n3102   L    L 3.599973e-03 8.254870e-02 3.295524e-01 5.842989e-01   Fold09\n3103   L    F 7.578033e-02 6.295435e-01 2.878722e-01 6.803947e-03   Fold09\n3104   L    M 1.627056e-01 3.145099e-01 3.570698e-01 1.657148e-01   Fold09\n3105   L    L 2.338712e-05 5.504847e-03 2.739960e-02 9.670722e-01   Fold09\n3106   L    L 1.189884e-05 3.533848e-03 2.286841e-02 9.735858e-01   Fold09\n3107   L    L 8.963581e-05 2.945136e-02 2.674465e-01 7.030125e-01   Fold09\n3108   L    F 3.355886e-02 6.920330e-01 2.714073e-01 3.000763e-03   Fold09\n3109   L    F 3.219717e-02 6.913808e-01 2.726879e-01 3.734085e-03   Fold09\n3110   L    M 1.374424e-02 1.635275e-01 4.439600e-01 3.787682e-01   Fold09\n3111   L    L 7.360614e-08 5.826236e-05 7.346419e-04 9.992070e-01   Fold09\n3112   L    L 4.412389e-08 3.721722e-05 5.274408e-04 9.994353e-01   Fold09\n3113   L    L 1.941808e-06 6.004316e-04 3.468040e-03 9.959296e-01   Fold09\n3114   L    F 5.120046e-02 8.093935e-01 1.324682e-01 6.937845e-03   Fold09\n3115   L    L 2.025675e-06 6.294111e-04 3.495364e-03 9.958732e-01   Fold09\n3116   L    L 1.343929e-06 4.732826e-04 2.479341e-03 9.970460e-01   Fold09\n3117   L    L 1.038833e-03 1.335763e-01 2.803882e-01 5.849967e-01   Fold09\n3118   L    L 9.922434e-04 1.345295e-01 2.734747e-01 5.910035e-01   Fold09\n3119   L    L 2.261222e-06 6.570032e-04 2.773033e-03 9.965677e-01   Fold09\n3120   L    L 1.620529e-04 3.417920e-02 1.068429e-01 8.588159e-01   Fold09\n3121   L    L 1.555944e-07 1.030788e-04 1.046124e-03 9.988506e-01   Fold09\n3122  VF   VF 9.514221e-01 4.530554e-02 3.265587e-03 6.767586e-06   Fold10\n3123  VF   VF 9.568102e-01 4.074899e-02 2.436014e-03 4.792772e-06   Fold10\n3124  VF   VF 9.291768e-01 6.415580e-02 6.648588e-03 1.878517e-05   Fold10\n3125  VF   VF 9.542097e-01 4.285761e-02 2.928108e-03 4.538548e-06   Fold10\n3126  VF   VF 9.424877e-01 5.253284e-02 4.967264e-03 1.218337e-05   Fold10\n3127  VF   VF 9.539190e-01 4.334142e-02 2.734514e-03 5.096001e-06   Fold10\n3128  VF   VF 9.539798e-01 4.306550e-02 2.949139e-03 5.582539e-06   Fold10\n3129  VF    F 3.051345e-01 5.763936e-01 1.156068e-01 2.865124e-03   Fold10\n3130  VF    F 3.059256e-01 5.771329e-01 1.140598e-01 2.881769e-03   Fold10\n3131  VF    F 3.343705e-01 5.896899e-01 7.224146e-02 3.698135e-03   Fold10\n3132  VF   VF 8.954744e-01 8.867221e-02 1.582015e-02 3.321604e-05   Fold10\n3133  VF   VF 9.617978e-01 3.512873e-02 3.070558e-03 2.939357e-06   Fold10\n3134  VF   VF 8.130249e-01 1.462838e-01 4.055096e-02 1.404274e-04   Fold10\n3135  VF   VF 9.449739e-01 4.944682e-02 5.570691e-03 8.618843e-06   Fold10\n3136  VF   VF 8.654076e-01 1.064289e-01 2.808360e-02 7.987037e-05   Fold10\n3137  VF   VF 9.524250e-01 4.134834e-02 6.189532e-03 3.712844e-05   Fold10\n3138  VF   VF 9.873147e-01 1.221845e-02 4.663728e-04 4.617135e-07   Fold10\n3139  VF   VF 9.883752e-01 1.119965e-02 4.247770e-04 3.717456e-07   Fold10\n3140  VF   VF 9.883205e-01 1.129336e-02 3.857927e-04 3.306408e-07   Fold10\n3141  VF   VF 9.863060e-01 1.307976e-02 6.136382e-04 6.086910e-07   Fold10\n3142  VF   VF 9.875955e-01 1.183610e-02 5.680278e-04 4.106255e-07   Fold10\n3143  VF   VF 9.832349e-01 1.589041e-02 8.735099e-04 1.231191e-06   Fold10\n3144  VF   VF 9.882197e-01 1.137279e-02 4.071665e-04 3.576893e-07   Fold10\n3145  VF   VF 9.881919e-01 1.139885e-02 4.089310e-04 3.608608e-07   Fold10\n3146  VF   VF 9.710775e-01 2.646343e-02 2.452878e-03 6.167748e-06   Fold10\n3147  VF   VF 9.872222e-01 1.213333e-02 6.439404e-04 5.008502e-07   Fold10\n3148  VF   VF 9.688941e-01 2.830724e-02 2.788264e-03 1.039504e-05   Fold10\n3149  VF   VF 9.577471e-01 3.698798e-02 5.238573e-03 2.633376e-05   Fold10\n3150  VF   VF 9.811184e-01 1.781809e-02 1.062277e-03 1.280137e-06   Fold10\n3151  VF   VF 6.281864e-01 3.255770e-01 4.593123e-02 3.054329e-04   Fold10\n3152  VF   VF 6.189651e-01 3.299256e-01 5.078587e-02 3.233464e-04   Fold10\n3153  VF   VF 6.631933e-01 2.992065e-01 3.739432e-02 2.058539e-04   Fold10\n3154  VF   VF 6.584101e-01 3.032385e-01 3.814091e-02 2.104754e-04   Fold10\n3155  VF   VF 6.999611e-01 2.745512e-01 2.536641e-02 1.213124e-04   Fold10\n3156  VF    F 2.572819e-01 4.691768e-01 2.148868e-01 5.865450e-02   Fold10\n3157  VF   VF 5.930160e-01 3.542788e-01 5.234501e-02 3.602053e-04   Fold10\n3158  VF   VF 5.661458e-01 3.687834e-01 6.460525e-02 4.655278e-04   Fold10\n3159  VF   VF 7.531268e-01 2.203777e-01 2.643213e-02 6.340267e-05   Fold10\n3160  VF   VF 6.992592e-01 2.757589e-01 2.481356e-02 1.683653e-04   Fold10\n3161  VF   VF 7.538301e-01 2.212435e-01 2.486730e-02 5.910245e-05   Fold10\n3162  VF   VF 7.681959e-01 2.123296e-01 1.943970e-02 3.481800e-05   Fold10\n3163  VF   VF 8.752764e-01 1.197254e-01 4.993461e-03 4.661045e-06   Fold10\n3164  VF   VF 8.454331e-01 1.468888e-01 7.666968e-03 1.103832e-05   Fold10\n3165  VF   VF 7.217801e-01 2.503299e-01 2.780625e-02 8.378787e-05   Fold10\n3166  VF   VF 8.312228e-01 1.588680e-01 9.900599e-03 8.657324e-06   Fold10\n3167  VF    F 1.412455e-01 7.003857e-01 1.568964e-01 1.472364e-03   Fold10\n3168  VF    F 1.144852e-01 6.905127e-01 1.927344e-01 2.267585e-03   Fold10\n3169  VF    F 1.631928e-01 6.981869e-01 1.375864e-01 1.033909e-03   Fold10\n3170  VF    F 9.871402e-02 6.737052e-01 2.252124e-01 2.368358e-03   Fold10\n3171  VF    F 1.499231e-01 6.735086e-01 1.754776e-01 1.090742e-03   Fold10\n3172  VF    F 1.491029e-01 6.772259e-01 1.725887e-01 1.082497e-03   Fold10\n3173  VF    F 1.011149e-01 6.821096e-01 2.145862e-01 2.189355e-03   Fold10\n3174  VF   VF 9.849743e-01 1.453340e-02 4.918709e-04 4.205055e-07   Fold10\n3175  VF   VF 9.883642e-01 1.132387e-02 3.116954e-04 2.232618e-07   Fold10\n3176  VF   VF 9.680502e-01 2.959425e-02 2.345404e-03 1.010421e-05   Fold10\n3177  VF   VF 9.904326e-01 9.265616e-03 3.016537e-04 1.510847e-07   Fold10\n3178  VF   VF 9.848644e-01 1.469313e-02 4.421188e-04 3.918460e-07   Fold10\n3179  VF   VF 9.281075e-01 6.246247e-02 9.337937e-03 9.212001e-05   Fold10\n3180  VF   VF 9.882386e-01 1.146142e-02 2.997163e-04 2.195609e-07   Fold10\n3181  VF   VF 9.731552e-01 2.547481e-02 1.368123e-03 1.861074e-06   Fold10\n3182  VF   VF 9.783048e-01 2.094747e-02 7.467845e-04 9.470801e-07   Fold10\n3183  VF   VF 9.646973e-01 3.345445e-02 1.845001e-03 3.219963e-06   Fold10\n3184  VF   VF 7.392687e-01 1.919953e-01 6.399283e-02 4.743186e-03   Fold10\n3185  VF   VF 9.237454e-01 7.036131e-02 5.871696e-03 2.157344e-05   Fold10\n3186  VF   VF 9.851032e-01 1.432518e-02 5.710278e-04 5.601634e-07   Fold10\n3187  VF   VF 9.862158e-01 1.331658e-02 4.673414e-04 2.792190e-07   Fold10\n3188  VF   VF 9.784322e-01 2.062670e-02 9.402754e-04 7.929473e-07   Fold10\n3189  VF   VF 9.808739e-01 1.848564e-02 6.398543e-04 6.083373e-07   Fold10\n3190  VF   VF 9.890717e-01 1.056385e-02 3.641483e-04 2.519430e-07   Fold10\n3191  VF   VF 9.930071e-01 6.822660e-03 1.702009e-04 6.105509e-08   Fold10\n3192  VF   VF 9.791389e-01 2.012059e-02 7.397187e-04 7.650041e-07   Fold10\n3193  VF   VF 9.843271e-01 1.516033e-02 5.121061e-04 4.285837e-07   Fold10\n3194  VF   VF 9.574003e-01 4.021529e-02 2.371759e-03 1.261105e-05   Fold10\n3195  VF   VF 9.225513e-01 7.232964e-02 5.103770e-03 1.524394e-05   Fold10\n3196  VF   VF 9.217023e-01 6.941999e-02 8.823514e-03 5.423598e-05   Fold10\n3197  VF   VF 9.200507e-01 7.660454e-02 3.341999e-03 2.787769e-06   Fold10\n3198  VF   VF 9.100671e-01 8.545909e-02 4.467688e-03 6.107734e-06   Fold10\n3199  VF   VF 9.142292e-01 8.259240e-02 3.173201e-03 5.214556e-06   Fold10\n3200  VF   VF 7.451132e-01 2.508837e-01 3.995402e-03 7.755209e-06   Fold10\n3201  VF   VF 8.818977e-01 1.138044e-01 4.281372e-03 1.648486e-05   Fold10\n3202  VF   VF 9.161274e-01 8.101692e-02 2.852641e-03 3.071419e-06   Fold10\n3203  VF   VF 8.879266e-01 1.101127e-01 1.923372e-03 3.735555e-05   Fold10\n3204  VF   VF 9.137139e-01 8.333563e-02 2.947260e-03 3.245870e-06   Fold10\n3205  VF   VF 8.861806e-01 1.081490e-01 5.662578e-03 7.867341e-06   Fold10\n3206  VF   VF 8.969003e-01 9.826763e-02 4.822675e-03 9.446285e-06   Fold10\n3207  VF   VF 8.971352e-01 9.831552e-02 4.540480e-03 8.817279e-06   Fold10\n3208  VF   VF 9.063464e-01 8.888691e-02 4.759782e-03 6.913858e-06   Fold10\n3209  VF   VF 8.766856e-01 1.183169e-01 4.978957e-03 1.853647e-05   Fold10\n3210  VF   VF 9.042690e-01 9.112703e-02 4.597124e-03 6.801036e-06   Fold10\n3211  VF   VF 9.024593e-01 9.365663e-02 3.877260e-03 6.790875e-06   Fold10\n3212  VF   VF 8.920650e-01 1.027028e-01 5.222530e-03 9.654491e-06   Fold10\n3213  VF   VF 8.732340e-01 1.214774e-01 5.265142e-03 2.339197e-05   Fold10\n3214  VF   VF 8.246509e-01 1.672341e-01 8.068366e-03 4.663232e-05   Fold10\n3215  VF   VF 9.050825e-01 9.136082e-02 3.550011e-03 6.716442e-06   Fold10\n3216  VF   VF 8.753873e-01 1.194647e-01 5.125292e-03 2.264083e-05   Fold10\n3217  VF   VF 8.858971e-01 1.097612e-01 4.332684e-03 8.977267e-06   Fold10\n3218  VF   VF 9.033086e-01 9.213333e-02 4.551145e-03 6.897518e-06   Fold10\n3219  VF   VF 8.757775e-01 1.189575e-01 5.241604e-03 2.333741e-05   Fold10\n3220  VF   VF 9.080271e-01 8.837620e-02 3.589947e-03 6.779953e-06   Fold10\n3221  VF   VF 8.743503e-01 1.203844e-01 5.241787e-03 2.358993e-05   Fold10\n3222  VF   VF 7.001047e-01 2.936634e-01 6.208482e-03 2.342360e-05   Fold10\n3223  VF   VF 8.970493e-01 9.753311e-02 5.408550e-03 9.045693e-06   Fold10\n3224  VF   VF 9.012768e-01 9.437902e-02 4.335107e-03 9.040578e-06   Fold10\n3225  VF   VF 8.458709e-01 1.439358e-01 1.016394e-02 2.940540e-05   Fold10\n3226  VF   VF 8.440360e-01 1.455497e-01 1.038377e-02 3.050488e-05   Fold10\n3227  VF   VF 8.986242e-01 9.559348e-02 5.772299e-03 9.996981e-06   Fold10\n3228  VF   VF 9.058249e-01 9.049562e-02 3.673460e-03 5.986375e-06   Fold10\n3229  VF   VF 8.858638e-01 1.083711e-01 5.751013e-03 1.403198e-05   Fold10\n3230  VF   VF 8.639772e-01 1.298856e-01 6.104873e-03 3.232113e-05   Fold10\n3231  VF   VF 6.850308e-01 3.069512e-01 7.982283e-03 3.580066e-05   Fold10\n3232  VF   VF 9.023474e-01 9.314208e-02 4.503708e-03 6.797660e-06   Fold10\n3233  VF   VF 8.935143e-01 1.004101e-01 6.064225e-03 1.142885e-05   Fold10\n3234  VF   VF 6.441512e-01 3.028195e-01 4.738832e-02 5.640970e-03   Fold10\n3235  VF   VF 5.243918e-01 3.706282e-01 9.039268e-02 1.458730e-02   Fold10\n3236  VF   VF 8.845133e-01 1.085741e-01 6.898513e-03 1.411259e-05   Fold10\n3237  VF   VF 8.498040e-01 1.411139e-01 9.041676e-03 4.043085e-05   Fold10\n3238  VF   VF 6.741292e-01 3.181391e-01 7.695431e-03 3.625886e-05   Fold10\n3239  VF   VF 6.612989e-01 3.301978e-01 8.459010e-03 4.422868e-05   Fold10\n3240  VF   VF 8.508736e-01 1.422148e-01 6.869257e-03 4.236146e-05   Fold10\n3241  VF   VF 8.497420e-01 1.428955e-01 7.315485e-03 4.700475e-05   Fold10\n3242  VF   VF 8.674893e-01 1.267562e-01 5.730809e-03 2.374933e-05   Fold10\n3243  VF   VF 9.042352e-01 9.198056e-02 3.777884e-03 6.325411e-06   Fold10\n3244  VF   VF 9.032656e-01 9.253871e-02 4.188472e-03 7.255467e-06   Fold10\n3245  VF    M 9.958636e-04 9.965516e-04 9.980064e-01 1.166570e-06   Fold10\n3246  VF   VF 6.335764e-01 3.560600e-01 1.029661e-02 6.702278e-05   Fold10\n3247  VF   VF 8.246315e-01 1.624608e-01 1.279654e-02 1.111397e-04   Fold10\n3248  VF   VF 9.043795e-01 9.117832e-02 4.434344e-03 7.825596e-06   Fold10\n3249  VF   VF 8.730556e-01 1.201315e-01 6.798761e-03 1.411977e-05   Fold10\n3250  VF   VF 8.713242e-01 1.215762e-01 7.084642e-03 1.498045e-05   Fold10\n3251  VF   VF 8.664885e-01 1.263236e-01 7.172238e-03 1.566667e-05   Fold10\n3252  VF   VF 8.694431e-01 1.231717e-01 7.369189e-03 1.606589e-05   Fold10\n3253  VF   VF 9.137397e-01 8.316989e-02 3.085499e-03 4.925504e-06   Fold10\n3254  VF   VF 8.691694e-01 1.232085e-01 7.605268e-03 1.682985e-05   Fold10\n3255  VF   VF 8.814067e-01 1.140497e-01 4.526358e-03 1.730094e-05   Fold10\n3256  VF   VF 8.556349e-01 1.354395e-01 8.903709e-03 2.185067e-05   Fold10\n3257  VF   VF 9.460621e-01 5.078562e-02 3.149838e-03 2.405500e-06   Fold10\n3258  VF   VF 9.696622e-01 2.926659e-02 1.070694e-03 5.517008e-07   Fold10\n3259  VF   VF 9.687882e-01 3.008486e-02 1.126344e-03 5.940453e-07   Fold10\n3260  VF   VF 9.315239e-01 6.407315e-02 4.398569e-03 4.392856e-06   Fold10\n3261  VF   VF 9.368366e-01 6.003122e-02 3.126373e-03 5.806580e-06   Fold10\n3262  VF   VF 8.837864e-01 1.055924e-01 1.060427e-02 1.688499e-05   Fold10\n3263  VF   VF 9.158996e-01 7.974884e-02 4.341606e-03 9.997086e-06   Fold10\n3264  VF   VF 9.366717e-01 5.944152e-02 3.882792e-03 3.993117e-06   Fold10\n3265  VF   VF 9.748292e-01 2.407932e-02 1.090994e-03 4.682561e-07   Fold10\n3266  VF   VF 7.864407e-01 1.823930e-01 2.856552e-02 2.600798e-03   Fold10\n3267  VF   VF 8.956022e-01 9.542830e-02 8.954392e-03 1.515661e-05   Fold10\n3268  VF   VF 9.298445e-01 6.550212e-02 4.649214e-03 4.153736e-06   Fold10\n3269  VF   VF 9.611417e-01 3.707765e-02 1.779088e-03 1.538885e-06   Fold10\n3270  VF   VF 9.850822e-01 1.450082e-02 4.169373e-04 6.539769e-08   Fold10\n3271  VF   VF 9.663309e-01 3.236095e-02 1.307463e-03 7.101709e-07   Fold10\n3272  VF   VF 9.422418e-01 5.275113e-02 4.998689e-03 8.362538e-06   Fold10\n3273  VF    F 8.255248e-02 7.944479e-01 1.220650e-01 9.346355e-04   Fold10\n3274  VF    F 1.104197e-01 7.709640e-01 1.182243e-01 3.919524e-04   Fold10\n3275  VF    F 1.252981e-01 7.688806e-01 1.055356e-01 2.857870e-04   Fold10\n3276  VF    F 6.854194e-02 7.674827e-01 1.625842e-01 1.391099e-03   Fold10\n3277  VF   VF 7.535396e-01 2.262756e-01 2.016570e-02 1.915448e-05   Fold10\n3278  VF   VF 5.527293e-01 3.925880e-01 5.435950e-02 3.231230e-04   Fold10\n3279  VF   VF 7.170689e-01 2.644267e-01 1.846426e-02 4.015327e-05   Fold10\n3280  VF   VF 7.121354e-01 2.649163e-01 2.292135e-02 2.703703e-05   Fold10\n3281  VF   VF 7.861125e-01 2.004479e-01 1.343114e-02 8.416641e-06   Fold10\n3282  VF   VF 8.055184e-01 1.840793e-01 1.039452e-02 7.744884e-06   Fold10\n3283  VF   VF 8.109529e-01 1.793186e-01 9.721466e-03 7.025266e-06   Fold10\n3284  VF   VF 6.318595e-01 3.262930e-01 4.177794e-02 6.960469e-05   Fold10\n3285  VF   VF 6.612790e-01 3.022457e-01 3.642015e-02 5.520327e-05   Fold10\n3286  VF   VF 7.308872e-01 2.497852e-01 1.930285e-02 2.477368e-05   Fold10\n3287  VF   VF 8.022719e-01 1.867704e-01 1.094931e-02 8.341282e-06   Fold10\n3288  VF   VF 7.970929e-01 1.885540e-01 1.434414e-02 8.941798e-06   Fold10\n3289  VF   VF 6.408531e-01 3.149738e-01 4.408812e-02 8.490988e-05   Fold10\n3290  VF   VF 6.568930e-01 3.118862e-01 3.110534e-02 1.154806e-04   Fold10\n3291  VF   VF 6.481586e-01 3.227658e-01 2.896539e-02 1.101607e-04   Fold10\n3292  VF   VF 6.892240e-01 2.748133e-01 3.590692e-02 5.586125e-05   Fold10\n3293  VF   VF 6.883761e-01 2.761112e-01 3.545829e-02 5.446438e-05   Fold10\n3294  VF   VF 5.709919e-01 3.689732e-01 5.986903e-02 1.659350e-04   Fold10\n3295  VF   VF 7.833825e-01 2.034302e-01 1.317584e-02 1.154022e-05   Fold10\n3296  VF    F 2.845935e-01 6.733221e-01 4.180295e-02 2.814639e-04   Fold10\n3297  VF    F 7.513177e-02 7.578536e-01 1.639884e-01 3.026218e-03   Fold10\n3298  VF    F 8.591125e-02 7.622860e-01 1.489805e-01 2.822241e-03   Fold10\n3299   F   VF 9.396376e-01 5.549891e-02 4.849121e-03 1.439688e-05   Fold10\n3300   F    F 3.989024e-01 5.080768e-01 9.176893e-02 1.251873e-03   Fold10\n3301   F    F 3.897505e-01 5.289050e-01 7.972614e-02 1.618429e-03   Fold10\n3302   F   VF 5.813145e-01 3.384096e-01 7.991871e-02 3.571687e-04   Fold10\n3303   F    F 1.401751e-01 4.434608e-01 2.750020e-01 1.413621e-01   Fold10\n3304   F    F 4.545646e-01 4.581804e-01 8.470999e-02 2.545026e-03   Fold10\n3305   F    F 2.946675e-01 4.540168e-01 2.372589e-01 1.405689e-02   Fold10\n3306   F    F 4.290040e-01 4.378192e-01 1.216478e-01 1.152898e-02   Fold10\n3307   F    F 2.721393e-02 4.858864e-01 4.654333e-01 2.146640e-02   Fold10\n3308   F    F 9.308164e-02 7.135020e-01 1.886244e-01 4.791935e-03   Fold10\n3309   F    F 7.227280e-02 6.661839e-01 2.528087e-01 8.734591e-03   Fold10\n3310   F    F 1.053506e-01 6.920031e-01 2.001612e-01 2.485122e-03   Fold10\n3311   F    F 1.169096e-01 7.018513e-01 1.790969e-01 2.142216e-03   Fold10\n3312   F    F 7.260131e-02 6.929962e-01 2.257434e-01 8.659067e-03   Fold10\n3313   F    F 8.076367e-02 6.268018e-01 2.879678e-01 4.466764e-03   Fold10\n3314   F   VF 8.897631e-01 9.372955e-02 1.533873e-02 1.168577e-03   Fold10\n3315   F   VF 8.918551e-01 9.125487e-02 1.590425e-02 9.857425e-04   Fold10\n3316   F    L 3.133772e-04 3.028098e-03 3.465810e-02 9.620004e-01   Fold10\n3317   F    F 2.832124e-01 4.704341e-01 2.320579e-01 1.429550e-02   Fold10\n3318   F   VF 8.323011e-01 1.538041e-01 1.384326e-02 5.157988e-05   Fold10\n3319   F   VF 7.875113e-01 1.968431e-01 1.547723e-02 1.684445e-04   Fold10\n3320   F   VF 6.215988e-01 3.099467e-01 6.699588e-02 1.458601e-03   Fold10\n3321   F   VF 8.239046e-01 1.680340e-01 8.015460e-03 4.600077e-05   Fold10\n3322   F   VF 8.983125e-01 9.645535e-02 5.223840e-03 8.346597e-06   Fold10\n3323   F   VF 8.481582e-01 1.427856e-01 9.030780e-03 2.537743e-05   Fold10\n3324   F   VF 8.650916e-01 1.293128e-01 5.568448e-03 2.718898e-05   Fold10\n3325   F   VF 8.483372e-01 1.416329e-01 1.000034e-02 2.959235e-05   Fold10\n3326   F   VF 8.901412e-01 1.035146e-01 6.332022e-03 1.218735e-05   Fold10\n3327   F   VF 6.714788e-01 3.204429e-01 8.041069e-03 3.726736e-05   Fold10\n3328   F    L 1.573486e-02 4.740018e-02 3.710253e-02 8.997624e-01   Fold10\n3329   F   VF 8.738234e-01 1.187894e-01 7.365747e-03 2.139840e-05   Fold10\n3330   F   VF 8.828109e-01 1.099026e-01 7.270133e-03 1.631239e-05   Fold10\n3331   F   VF 6.509873e-01 3.398449e-01 9.116016e-03 5.177438e-05   Fold10\n3332   F   VF 6.496901e-01 3.410536e-01 9.203494e-03 5.280067e-05   Fold10\n3333   F   VF 6.464061e-01 3.441252e-01 9.413366e-03 5.541514e-05   Fold10\n3334   F   VF 6.489429e-01 3.417564e-01 9.247336e-03 5.337555e-05   Fold10\n3335   F   VF 8.412956e-01 1.509971e-01 7.653294e-03 5.404386e-05   Fold10\n3336   F   VF 6.226108e-01 3.662135e-01 1.109706e-02 7.859632e-05   Fold10\n3337   F   VF 6.014630e-01 3.856655e-01 1.276538e-02 1.061279e-04   Fold10\n3338   F   VF 6.012050e-01 3.859028e-01 1.278576e-02 1.064837e-04   Fold10\n3339   F   VF 5.939840e-01 3.925008e-01 1.339738e-02 1.178285e-04   Fold10\n3340   F   VF 7.984191e-01 1.842712e-01 1.720019e-02 1.095323e-04   Fold10\n3341   F   VF 7.902885e-01 1.904373e-01 1.918150e-02 9.268433e-05   Fold10\n3342   F   VF 5.242455e-01 3.403977e-01 1.200729e-01 1.528388e-02   Fold10\n3343   F   VF 7.983731e-01 1.644183e-01 3.587742e-02 1.331156e-03   Fold10\n3344   F    M 7.312116e-02 3.483290e-01 3.832883e-01 1.952616e-01   Fold10\n3345   F    M 2.555879e-01 2.341758e-01 3.497437e-01 1.604926e-01   Fold10\n3346   F    F 8.366084e-02 7.128418e-01 2.029940e-01 5.033150e-04   Fold10\n3347   F    F 5.455765e-02 7.257311e-01 2.177578e-01 1.953482e-03   Fold10\n3348   F    F 5.489096e-02 7.249775e-01 2.182330e-01 1.898510e-03   Fold10\n3349   F    F 6.211730e-02 7.508877e-01 1.857944e-01 1.200580e-03   Fold10\n3350   F    F 6.234568e-02 7.294374e-01 2.071938e-01 1.023134e-03   Fold10\n3351   F    F 1.116787e-01 7.742443e-01 1.137007e-01 3.762649e-04   Fold10\n3352   F    F 7.844218e-02 7.760475e-01 1.443064e-01 1.203893e-03   Fold10\n3353   F    F 5.124790e-02 7.260106e-01 2.214398e-01 1.301630e-03   Fold10\n3354   F    F 5.883685e-02 7.378920e-01 2.021681e-01 1.103084e-03   Fold10\n3355   F    F 7.010212e-02 7.266660e-01 2.023327e-01 8.991028e-04   Fold10\n3356   F    F 6.454112e-02 7.901735e-01 1.438295e-01 1.455847e-03   Fold10\n3357   F    F 5.220094e-02 7.268542e-01 2.196148e-01 1.330084e-03   Fold10\n3358   F    F 8.559942e-02 7.756783e-01 1.381299e-01 5.924269e-04   Fold10\n3359   F    F 8.510178e-02 7.892683e-01 1.247705e-01 8.594018e-04   Fold10\n3360   F    F 7.796203e-02 7.459881e-01 1.751820e-01 8.678454e-04   Fold10\n3361   F    F 7.718889e-02 7.447205e-01 1.772070e-01 8.835278e-04   Fold10\n3362   F    F 6.509583e-02 7.797231e-01 1.535227e-01 1.658412e-03   Fold10\n3363   F    F 8.751113e-02 7.658210e-01 1.459440e-01 7.238220e-04   Fold10\n3364   F    F 6.015870e-02 7.268962e-01 2.120989e-01 8.461477e-04   Fold10\n3365   F    F 4.607267e-02 6.996932e-01 2.521956e-01 2.038538e-03   Fold10\n3366   F    F 9.584826e-02 7.685018e-01 1.351899e-01 4.599994e-04   Fold10\n3367   F    F 7.111726e-02 7.729838e-01 1.552725e-01 6.264806e-04   Fold10\n3368   F    F 5.151406e-02 7.562598e-01 1.891719e-01 3.054227e-03   Fold10\n3369   F    F 5.112324e-02 7.560314e-01 1.898088e-01 3.036559e-03   Fold10\n3370   F    F 4.265938e-02 6.449072e-01 3.090580e-01 3.375343e-03   Fold10\n3371   F    F 2.137335e-02 6.098192e-01 3.465878e-01 2.221966e-02   Fold10\n3372   F    F 2.042137e-02 6.018540e-01 3.536670e-01 2.405765e-02   Fold10\n3373   F    F 6.553183e-02 7.506878e-01 1.829557e-01 8.246216e-04   Fold10\n3374   F    F 8.634013e-02 7.615047e-01 1.517715e-01 3.837093e-04   Fold10\n3375   F   VF 6.429703e-01 3.065373e-01 5.043875e-02 5.364224e-05   Fold10\n3376   F   VF 7.322487e-01 2.449792e-01 2.274680e-02 2.529930e-05   Fold10\n3377   F   VF 6.882275e-01 2.907604e-01 2.095664e-02 5.545821e-05   Fold10\n3378   F   VF 5.947168e-01 3.569154e-01 4.829175e-02 7.605327e-05   Fold10\n3379   F    F 3.737139e-01 4.645942e-01 1.539867e-01 7.705155e-03   Fold10\n3380   F   VF 6.000980e-01 3.474251e-01 5.235793e-02 1.189918e-04   Fold10\n3381   F   VF 6.259384e-01 3.348401e-01 3.905451e-02 1.669312e-04   Fold10\n3382   F   VF 5.727101e-01 3.739135e-01 5.317118e-02 2.052810e-04   Fold10\n3383   F    F 7.854496e-02 7.561767e-01 1.619136e-01 3.364775e-03   Fold10\n3384   F    F 7.252697e-02 7.798738e-01 1.434528e-01 4.146412e-03   Fold10\n3385   F    M 3.645518e-04 1.785099e-02 9.810190e-01 7.654082e-04   Fold10\n3386   F    F 4.683787e-02 7.387130e-01 2.060180e-01 8.431142e-03   Fold10\n3387   F    F 6.034584e-02 7.453832e-01 1.889526e-01 5.318335e-03   Fold10\n3388   F    F 6.031108e-02 7.633028e-01 1.700231e-01 6.363034e-03   Fold10\n3389   F    F 4.242350e-02 7.257230e-01 2.217225e-01 1.013092e-02   Fold10\n3390   F    F 6.436310e-02 7.370142e-01 1.930997e-01 5.522972e-03   Fold10\n3391   F    F 6.436861e-02 7.370488e-01 1.930657e-01 5.516947e-03   Fold10\n3392   F    F 1.111912e-02 4.250841e-01 2.242538e-01 3.395430e-01   Fold10\n3393   F    F 7.530117e-02 7.863404e-01 1.340068e-01 4.351563e-03   Fold10\n3394   F    F 3.620114e-02 7.235443e-01 2.051559e-01 3.509871e-02   Fold10\n3395   F    F 3.576042e-02 7.186762e-01 2.106502e-01 3.491323e-02   Fold10\n3396   F    F 3.617975e-02 7.214570e-01 2.078053e-01 3.455796e-02   Fold10\n3397   F    F 5.815600e-02 7.572231e-01 1.776518e-01 6.969088e-03   Fold10\n3398   F    F 5.505858e-02 7.561949e-01 1.809024e-01 7.844189e-03   Fold10\n3399   F    F 5.448253e-02 7.485584e-01 1.888842e-01 8.074848e-03   Fold10\n3400   F    F 3.405383e-02 7.068634e-01 2.184265e-01 4.065631e-02   Fold10\n3401   F    F 6.633425e-02 7.780817e-01 1.498679e-01 5.716156e-03   Fold10\n3402   F    F 6.666098e-02 7.813643e-01 1.464482e-01 5.526542e-03   Fold10\n3403   F    F 7.820025e-02 7.825834e-01 1.358249e-01 3.391397e-03   Fold10\n3404   F    F 3.428402e-02 6.941636e-01 2.557650e-01 1.578734e-02   Fold10\n3405   F    F 3.191901e-02 7.153514e-01 2.156967e-01 3.703295e-02   Fold10\n3406   M   VF 9.000477e-01 8.779851e-02 1.207123e-02 8.255188e-05   Fold10\n3407   M    F 3.200817e-01 6.002819e-01 7.558022e-02 4.056202e-03   Fold10\n3408   M    M 9.023987e-02 1.460182e-01 5.176402e-01 2.461018e-01   Fold10\n3409   M    M 1.235607e-01 1.573909e-01 5.041599e-01 2.148885e-01   Fold10\n3410   M    M 1.004217e-01 1.480523e-01 5.099637e-01 2.415623e-01   Fold10\n3411   M    M 6.487370e-02 1.210072e-01 5.138978e-01 3.002212e-01   Fold10\n3412   M   VF 4.873922e-01 4.318217e-01 8.016319e-02 6.229664e-04   Fold10\n3413   M   VF 7.004299e-01 2.680803e-01 3.132883e-02 1.609393e-04   Fold10\n3414   M   VF 6.692603e-01 2.816374e-01 4.891119e-02 1.911035e-04   Fold10\n3415   M   VF 4.793778e-01 4.151271e-01 9.613941e-02 9.355695e-03   Fold10\n3416   M    M 1.086074e-01 3.642969e-01 3.913196e-01 1.357761e-01   Fold10\n3417   M    F 7.959453e-02 6.559188e-01 2.608542e-01 3.632485e-03   Fold10\n3418   M    F 7.536563e-02 6.651094e-01 2.556567e-01 3.868286e-03   Fold10\n3419   M    F 1.038730e-01 6.854122e-01 2.081433e-01 2.571422e-03   Fold10\n3420   M    F 1.976981e-02 4.167811e-01 3.899671e-01 1.734821e-01   Fold10\n3421   M    L 1.224025e-04 2.202331e-02 1.841624e-01 7.936919e-01   Fold10\n3422   M    L 1.150979e-04 2.102706e-02 1.782157e-01 8.006422e-01   Fold10\n3423   M    L 3.025311e-04 3.593680e-02 2.601299e-01 7.036308e-01   Fold10\n3424   M    F 1.050967e-01 6.561263e-01 2.360976e-01 2.679372e-03   Fold10\n3425   M    F 9.254355e-02 6.833771e-01 2.215459e-01 2.533484e-03   Fold10\n3426   M   VF 7.705587e-01 2.046493e-01 2.451008e-02 2.819165e-04   Fold10\n3427   M   VF 7.892612e-01 1.908725e-01 1.971855e-02 1.477261e-04   Fold10\n3428   M    F 4.032799e-01 4.188101e-01 1.490769e-01 2.883308e-02   Fold10\n3429   M   VF 8.302380e-01 1.588654e-01 1.085402e-02 4.260128e-05   Fold10\n3430   M    F 1.370829e-02 5.120313e-01 4.400808e-01 3.417956e-02   Fold10\n3431   M    F 7.163479e-02 7.676417e-01 1.599732e-01 7.503257e-04   Fold10\n3432   M    F 4.315250e-02 7.166167e-01 2.385972e-01 1.633618e-03   Fold10\n3433   M    F 5.278915e-02 7.725977e-01 1.725827e-01 2.030386e-03   Fold10\n3434   M    F 3.888084e-02 7.229395e-01 2.364466e-01 1.733099e-03   Fold10\n3435   M    F 1.798937e-02 5.486894e-01 3.722467e-01 6.107456e-02   Fold10\n3436   M    F 1.204258e-02 5.164570e-01 3.146866e-01 1.568139e-01   Fold10\n3437   M    F 4.813479e-02 7.499385e-01 1.985993e-01 3.327385e-03   Fold10\n3438   M   VF 6.658218e-01 2.948182e-01 3.930095e-02 5.896477e-05   Fold10\n3439   M    M 1.141483e-01 4.224569e-01 4.342112e-01 2.918353e-02   Fold10\n3440   M    M 4.623456e-02 2.804910e-01 5.394355e-01 1.338390e-01   Fold10\n3441   M    F 1.588206e-01 4.591816e-01 3.213826e-01 6.061522e-02   Fold10\n3442   M    M 3.026016e-03 1.011372e-02 9.868036e-01 5.666579e-05   Fold10\n3443   M    F 7.106207e-02 7.760795e-01 1.486179e-01 4.240589e-03   Fold10\n3444   M    L 1.520639e-03 1.527062e-01 2.205268e-01 6.252464e-01   Fold10\n3445   M    F 2.427786e-02 5.863709e-01 3.507483e-01 3.860300e-02   Fold10\n3446   M    M 8.880259e-03 3.674734e-01 5.514802e-01 7.216619e-02   Fold10\n3447   M    M 1.645507e-02 4.484980e-01 4.953824e-01 3.966449e-02   Fold10\n3448   L    L 2.901247e-02 8.932486e-02 3.335124e-01 5.481503e-01   Fold10\n3449   L    L 2.101474e-02 7.820178e-02 4.494369e-01 4.513466e-01   Fold10\n3450   L    F 7.512525e-02 3.697475e-01 3.192041e-01 2.359232e-01   Fold10\n3451   L    F 1.099139e-01 6.929609e-01 1.957551e-01 1.370081e-03   Fold10\n3452   L    M 1.171274e-02 3.287034e-01 3.652292e-01 2.943546e-01   Fold10\n3453   L    M 2.839876e-03 5.066844e-02 9.457296e-01 7.620754e-04   Fold10\n3454   L    F 5.637572e-02 6.216107e-01 3.148691e-01 7.144522e-03   Fold10\n3455   L    F 5.645592e-02 6.234120e-01 3.130495e-01 7.082616e-03   Fold10\n3456   L    L 1.046709e-01 3.093231e-01 2.616896e-01 3.243165e-01   Fold10\n3457   L    M 3.312802e-03 2.880069e-01 5.832242e-01 1.254561e-01   Fold10\n3458   L    F 4.378049e-02 6.972524e-01 2.568080e-01 2.159094e-03   Fold10\n3459   L    L 1.782569e-06 5.863452e-04 2.701501e-03 9.967104e-01   Fold10\n3460   L    L 1.206733e-03 1.456164e-01 3.141510e-01 5.390258e-01   Fold10\n3461   L    L 4.725859e-07 2.517555e-04 1.462692e-03 9.982851e-01   Fold10\n3462   L    L 1.587719e-06 5.269729e-04 2.000793e-03 9.974706e-01   Fold10\n3463   L    F 2.762672e-02 6.629804e-01 2.549542e-01 5.443863e-02   Fold10\n3464   L    F 3.712051e-02 6.731643e-01 2.640418e-01 2.567334e-02   Fold10\n3465   L    M 8.635688e-11 1.271806e-06 8.259137e-01 1.740851e-01   Fold10\n3466   L    L 6.189979e-08 9.829959e-05 3.335443e-03 9.965662e-01   Fold10\n3467   L    F 3.203979e-02 6.495508e-01 2.938443e-01 2.456504e-02   Fold10"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-11",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-11",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\n\n&gt; # load the data and convert it to a tibble\n&gt; hpc_cv &lt;- modeldata::hpc_cv %&gt;% tibble::tibble()\n&gt; # compute accuracy (same as binary case)\n&gt; yardstick::accuracy(hpc_cv, obs, pred)\n&gt; # compute matthews correlation coefficient (same as binary case)\n&gt; yardstick::mcc(hpc_cv, obs, pred)\n&gt; \n&gt; # apply the sensitivity metrics\n&gt; yardstick::sensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n&gt; yardstick::sensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n&gt; yardstick::sensitivity(hpc_cv, obs, pred, estimator = \"micro\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-12",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-12",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\n\n&gt; # multi-class estimates for probability metrics\n&gt; hpc_cv %&gt;% yardstick::roc_auc(obs, VF, F, M, L)\n&gt; # multi-class estimates with estimator (one of \"hand_till\", \"macro\", or \"macro_weighted\")\n&gt; hpc_cv %&gt;% yardstick::roc_auc(\n+   obs, VF, F, M, L, estimator = \"macro_weighted\"\n+ )\n&gt; \n&gt; # show metrics by groups (re-sampling in this case)\n&gt; hpc_cv %&gt;% \n+   dplyr::group_by(Resample) %&gt;% \n+   yardstick::accuracy(obs, pred)\n&gt; \n&gt; hpc_cv %&gt;% \n+   dplyr::group_by(Resample) %&gt;% \n+   yardstick::roc_curve(obs, VF, F, M, L) %&gt;% \n+   autoplot()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: performance evaluation",
    "text": "Tidymodels: performance evaluation\nRe-substitution: comparison using same training data\n\n&gt; # create  random forest model object\n&gt; rf_model &lt;- \n+   parsnip::rand_forest(trees = 1000) %&gt;% \n+   parsnip::set_engine(\"ranger\") %&gt;% \n+   parsnip::set_mode(\"regression\")\n&gt; \n&gt; # create a workflow using the random forest model\n&gt; rf_wflow &lt;- \n+   workflows::workflow() %&gt;% \n+   workflows::add_formula(\n+     Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n+       Latitude + Longitude) %&gt;% \n+   workflows::add_model(rf_model) \n&gt; \n&gt; # fit the random forest model with the ames training set\n&gt; rf_fit &lt;- rf_wflow %&gt;% parsnip::fit(data = ames_train)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: performance evaluation",
    "text": "Tidymodels: performance evaluation\nA function to compare models\n\n&gt; estimate_perf &lt;- function(model, dat) {\n+   # Capture the names of the `model` and `dat` objects\n+   cl &lt;- match.call()\n+   obj_name &lt;- as.character(cl$model)         # get the model name\n+   data_name &lt;- as.character(cl$dat)          # get the dataset name\n+   data_name &lt;- gsub(\"ames_\", \"\", data_name)  # replace underlines\n+   \n+   # Estimate these metrics:\n+   reg_metrics &lt;- \n+     yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n+   \n+   model %&gt;%\n+     stats::predict(dat) %&gt;%                  # predict\n+     dplyr::bind_cols(dat %&gt;% dplyr::select(Sale_Price)) %&gt;% \n+     reg_metrics(Sale_Price, .pred) %&gt;%\n+     dplyr::select(-.estimator) %&gt;%\n+     dplyr::mutate(object = obj_name, data = data_name)\n+ }"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-2",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-2",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: performance evaluation",
    "text": "Tidymodels: performance evaluation\nUse the function on the random forest and linear models\n\nrand forest - trainlinear -trainlinear - test\n\n\n\n&gt; # get performance of the random forest model (train)\n&gt; estimate_perf(rf_fit, ames_train)\n\n# A tibble: 2 × 4\n  .metric .estimate object data \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1 rmse       0.0366 rf_fit train\n2 rsq        0.960  rf_fit train\n\n\n\n\n\n&gt; # get performance of the linear model (train)\n&gt; estimate_perf(lm_fit, ames_train)\n\n# A tibble: 2 × 4\n  .metric .estimate object data \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1 rmse        0.160 lm_fit train\n2 rsq         0.168 lm_fit train\n\n\n\n\n\n&gt; # get performance of the linear model  (test)\n&gt; estimate_perf(rf_fit, ames_test)\n\n# A tibble: 2 × 4\n  .metric .estimate object data \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1 rmse       0.0702 rf_fit test \n2 rsq        0.852  rf_fit test"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-3",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-3",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: performance evaluation",
    "text": "Tidymodels: performance evaluation\nSummarize and present the performance comparison\n\n\nCode\n&gt; # get performance of the random forest model (train)\n&gt; dplyr::bind_rows(\n+   estimate_perf(rf_fit, ames_train)\n+   , estimate_perf(lm_fit, ames_train)\n+   , estimate_perf(rf_fit, ames_test)\n+   , estimate_perf(lm_fit, ames_test)\n+ ) %&gt;% \n+   dplyr::filter(.metric == 'rmse') %&gt;% \n+   dplyr::select(-.metric) %&gt;% \n+   tidyr::pivot_wider(\n+     names_from = data\n+     , values_from = .estimate\n+   ) %&gt;% \n+   gt::gt() %&gt;% \n+   gt::fmt_number(decimals=4) %&gt;% \n+   gt::tab_header(title = \"Performance statistics\", subtitle = \"metric: rmse\") %&gt;% \n+   gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nPerformance statistics\n\n\nmetric: rmse\n\n\nobject\ntrain\ntest\n\n\n\n\nrf_fit\n0.0366\n0.0702\n\n\nlm_fit\n0.1603\n0.1638"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling",
    "href": "slides/BSMM_8740_lec_04.html#sampling",
    "title": "The Tidymodels Framework",
    "section": "Sampling",
    "text": "Sampling\nThe primary approach for empirical model validation is to split the existing pool of data into two distinct sets, the training set and the test set.\nThe training set is used to develop and optimize the model and is usually the majority of the data.\nThe test set is the remainder of the data, held in reserve to determine the efficacy of the model. It is critical to look at the test set only once; otherwise, it becomes part of the modeling process."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling-1",
    "href": "slides/BSMM_8740_lec_04.html#sampling-1",
    "title": "The Tidymodels Framework",
    "section": "Sampling",
    "text": "Sampling\nThe rsample package has tools for making data splits, as follows\n\n&gt; set.seed(501)\n&gt; \n&gt; # Save the split information for an 80/20 split of the data\n&gt; ames_split &lt;- rsample::initial_split(ames, prop = 0.80)\n&gt; ames_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2344/586/2930&gt;\n\n\nThe functions training and testing extract the corresponding data.\n\n&gt; ames_train &lt;- rsample::training(ames_split)\n&gt; ames_test  &lt;- rsample::testing(ames_split)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling-class-imbalance",
    "href": "slides/BSMM_8740_lec_04.html#sampling-class-imbalance",
    "title": "The Tidymodels Framework",
    "section": "Sampling: class imbalance",
    "text": "Sampling: class imbalance\nSimple random sampling is appropriate in many cases but there are exceptions. When there is a dramatic class imbalance in classification problems, one class occurs much less frequently than another. To avoid this, stratified sampling can be used. For regession problems the outcome data can be binned and then stratified sampling will keep the distributions of the outcome similar between the training and test set.\n\n&gt; set.seed(502)\n&gt; ames_split &lt;- \n+   rsample::initial_split(ames, prop = 0.80, strata = Sale_Price)\n&gt; ames_train &lt;- rsample::training(ames_split)\n&gt; ames_test  &lt;- rsample::testing(ames_split)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling-time-variables",
    "href": "slides/BSMM_8740_lec_04.html#sampling-time-variables",
    "title": "The Tidymodels Framework",
    "section": "Sampling: time variables",
    "text": "Sampling: time variables\nWith time variables, random sampling is not the best choice. The rsample package contains a function called initial_time_split() that is very similar to initial_split() .\nThe prop argument denotes what proportion of the first part of the data should be used as the training set; the function assumes that the data have been pre-sorted by time in an appropriate order."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling-validation-sets",
    "href": "slides/BSMM_8740_lec_04.html#sampling-validation-sets",
    "title": "The Tidymodels Framework",
    "section": "Sampling: validation sets",
    "text": "Sampling: validation sets\nValidation sets are the answer to the question: “How can we tell what is best if we don’t measure performance until the test set?” The validation set is a means to get a rough sense of how well the model performed prior to the test set.\nValidation sets are a special case of resampling methods.\n\n&gt; set.seed(52)\n&gt; # To put 60% into training, 20% in validation, and 20% in testing:\n&gt; ames_val_split &lt;- \n+   rsample::initial_validation_split(ames, prop = c(0.6, 0.2))\n&gt; ames_val_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;1758/586/586/2930&gt;\n\n\nThe functions validation extracts the corresponding data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling",
    "href": "slides/BSMM_8740_lec_04.html#resampling",
    "title": "The Tidymodels Framework",
    "section": "Resampling",
    "text": "Resampling\nWhile the test set is used for obtaining an unbiased estimate of performance, we usually need to understand the performance of a model or even multiple models before using the test set."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-1",
    "href": "slides/BSMM_8740_lec_04.html#resampling-1",
    "title": "The Tidymodels Framework",
    "section": "Resampling",
    "text": "Resampling\nResampling is conducted only on the training set; the test set is not involved."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-cross-validation",
    "href": "slides/BSMM_8740_lec_04.html#resampling-cross-validation",
    "title": "The Tidymodels Framework",
    "section": "Resampling: cross-validation",
    "text": "Resampling: cross-validation\nWhile there are a number of variations, the most common cross-validation method is V-fold cross-validation. The data are randomly partitioned into V sets of roughly equal size (called the folds).\nIn the example of 3-fold cross validation. In each of 3 iterations, one fold is held out for assessment statistics and the remaining folds are used for modeling.\nThe final resampling estimate of performance averages each of the V replicates.\nIn practice, values of V are most often 5 or 10."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-cv-example",
    "href": "slides/BSMM_8740_lec_04.html#resampling-cv-example",
    "title": "The Tidymodels Framework",
    "section": "Resampling: cv example",
    "text": "Resampling: cv example\n\n&gt; set.seed(1001)\n&gt; ames_folds &lt;- rsample::vfold_cv(ames_train, v = 10)\n&gt; ames_folds |&gt; dplyr::slice_head(n=5)\n\n# A tibble: 5 × 2\n  splits             id    \n  &lt;list&gt;             &lt;chr&gt; \n1 &lt;split [2107/235]&gt; Fold01\n2 &lt;split [2107/235]&gt; Fold02\n3 &lt;split [2108/234]&gt; Fold03\n4 &lt;split [2108/234]&gt; Fold04\n5 &lt;split [2108/234]&gt; Fold05\n\n\nThe rsample::analysis() and rsample::assessment() functions return the corresponding data frames:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-repeated-cv",
    "href": "slides/BSMM_8740_lec_04.html#resampling-repeated-cv",
    "title": "The Tidymodels Framework",
    "section": "Resampling: repeated cv",
    "text": "Resampling: repeated cv\n\nThe most important variation on cross-validation is repeated V-fold cross-validation. Repeated V-fold cv, reduce noise in the estimate by using more data, i.e. averaging more V statistics.\nR repetitions of V-fold cross-validation reduces the standard error variance by a factor of \\(1/\\sqrt{\\text{R}}\\).\n\n&gt; rsample::vfold_cv(ames_train, v = 10, repeats = 5) \n\n\n\n#  10-fold cross-validation repeated 5 times \n# A tibble: 50 × 3\n   splits             id      id2   \n   &lt;list&gt;             &lt;chr&gt;   &lt;chr&gt; \n 1 &lt;split [2107/235]&gt; Repeat1 Fold01\n 2 &lt;split [2107/235]&gt; Repeat1 Fold02\n 3 &lt;split [2108/234]&gt; Repeat1 Fold03\n 4 &lt;split [2108/234]&gt; Repeat1 Fold04\n 5 &lt;split [2108/234]&gt; Repeat1 Fold05\n 6 &lt;split [2108/234]&gt; Repeat1 Fold06\n 7 &lt;split [2108/234]&gt; Repeat1 Fold07\n 8 &lt;split [2108/234]&gt; Repeat1 Fold08\n 9 &lt;split [2108/234]&gt; Repeat1 Fold09\n10 &lt;split [2108/234]&gt; Repeat1 Fold10\n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-loo-cv",
    "href": "slides/BSMM_8740_lec_04.html#resampling-loo-cv",
    "title": "The Tidymodels Framework",
    "section": "Resampling: LOO cv",
    "text": "Resampling: LOO cv\nOne variation of cross-validation is leave-one-out (LOO) cross-validation. If there are n training set samples, n models are fit using n−1 rows of the training set.\nEach model predicts the single excluded data point. At the end of resampling, the n predictions are pooled to produce a single performance statistic. LOO cv is created using rsample::loo_cv().\nLeave-one-out methods are deficient compared to almost any other method. For anything but pathologically small samples, LOO is computationally excessive."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-mc-cv",
    "href": "slides/BSMM_8740_lec_04.html#resampling-mc-cv",
    "title": "The Tidymodels Framework",
    "section": "Resampling: MC cv",
    "text": "Resampling: MC cv\nAnother variant of V-fold cross-validation is Monte Carlo cross-validation (MCCV).\nThe difference between MCCV and regular cross-validation is that, for MCCV, the input proportion of the data is randomly selected each time.\nMCCV is performed using rsample::mv_cv()."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-boostrapping",
    "href": "slides/BSMM_8740_lec_04.html#resampling-boostrapping",
    "title": "The Tidymodels Framework",
    "section": "Resampling: Boostrapping",
    "text": "Resampling: Boostrapping\nRe-sampling: bootstrapping:\nA bootstrap sample of the training set is a sample that is the same size as the training set but is drawn with replacement.\nWhen bootstrapping, the assessment set is often called the out-of-bag sample.\n&gt; rsample::bootstraps(ames_train, times = 5)\n\n\n\n# Bootstrap sampling \n# A tibble: 5 × 2\n  splits             id        \n  &lt;list&gt;             &lt;chr&gt;     \n1 &lt;split [2342/882]&gt; Bootstrap1\n2 &lt;split [2342/882]&gt; Bootstrap2\n3 &lt;split [2342/858]&gt; Bootstrap3\n4 &lt;split [2342/877]&gt; Bootstrap4\n5 &lt;split [2342/837]&gt; Bootstrap5"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-boostrapping-1",
    "href": "slides/BSMM_8740_lec_04.html#resampling-boostrapping-1",
    "title": "The Tidymodels Framework",
    "section": "Resampling: Boostrapping",
    "text": "Resampling: Boostrapping\nEach data point has a 63.2% chance of inclusion in the training set at least once.\nThe assessment set contains all of the training set samples that were not selected for the analysis set (on average, with 36.8% of the training set), and so they can vary in size.\nBootstrap samples produce performance estimates that have very low variance (unlike cross-validation) but have significant pessimistic bias. This means that, if the true accuracy of a model is 90%, the bootstrap would tend to estimate the value to be less than 90%."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\nRe-sampling: resampling time:\nWhen the data have a strong time component, a resampling method needs to support modeling to estimate seasonal and other temporal trends within the data. \nFor this type of resampling, the size of the initial analysis and assessment sets are specified and subsequent iterations are shifted in time"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables-1",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables-1",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\nRolling forecast orgin resampling:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables-2",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables-2",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\nTwo different configurations of rolling forecast orgin resampling:\n\nThe analysis set can cumulatively grow (as opposed to remaining the same size). After the first initial analysis set, new samples can accrue without discarding the earlier data.\nThe resamples need not increment by one. For example, for large data sets, the incremental block could be a week or month instead of a day."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables-3",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables-3",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\nFor a year’s worth of data, suppose that six sets of 30-day blocks define the analysis set. For assessment sets of 30 days with a 29-day skip, we can use the rsample package to specify:\n\n&gt; time_slices &lt;- \n+   tibble::tibble(x = 1:365) %&gt;% \n+   rsample::rolling_origin(\n+     initial = 6 * 30\n+     , assess = 30\n+     , skip = 29\n+     , cumulative = FALSE\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables-4",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables-4",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\n\nanalysisassessment\n\n\n\n&gt; # pull out first and last data points in the analysis dataset\n&gt; time_slices$splits %&gt;% \n+   purrr::map_dfr( \n+     .f = ~rsample::analysis(.x) %&gt;% \n+       dplyr::summarize(first = min(.), last = max(.))\n+   )\n\n# A tibble: 6 × 2\n  first  last\n  &lt;int&gt; &lt;int&gt;\n1     1   180\n2    31   210\n3    61   240\n4    91   270\n5   121   300\n6   151   330\n\n\n\n\n\n&gt; # pull out first and last data points in the assessment dataset\n&gt; time_slices$splits %&gt;% \n+   purrr::map_dfr(\n+     .f = ~rsample::assessment(.x) %&gt;% \n+       dplyr::summarize(first = min(.), last = max(.))\n+   )\n\n# A tibble: 6 × 2\n  first  last\n  &lt;int&gt; &lt;int&gt;\n1   181   210\n2   211   240\n3   241   270\n4   271   300\n5   301   330\n6   331   360"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-7",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-7",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\n\n\nDuring resampling, the analysis set is used to preprocess the data, apply the pre-processing to itself, and use these processed data to fit the model.\nThe pre-processing statistics produced by the analysis set are applied to the assessment set. The predictions from the assessment set estimate performance on new data.\n\nThis sequence repeats for every resample."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-8",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-8",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\n\nAny of the resampling methods discussed here can be used to evaluate the modeling process (including preprocessing, model fitting, etc). These methods are effective because different groups of data are used to train the model and assess the model. To reiterate, the process to use resampling is:\n\nDuring resampling, the analysis set is used to preprocess the data, apply the preprocessing to itself, and use these processed data to fit the model.\nThe preprocessing statistics produced by the analysis set are applied to the assessment set. The predictions from the assessment set estimate performance on new data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-9",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-9",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nThe function tune::fit_resamples is like parsnip::fit with a resamples argument instead of a data argument:\n\n&gt; #\n&gt; model_spec %&gt;% tune::fit_resamples(formula,  resamples, ...)\n&gt; \n&gt; model_spec %&gt;% tune::fit_resamples(recipe,   resamples, ...)\n&gt; \n&gt; workflow   %&gt;% tune::fit_resamples(          resamples, ...)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-10",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-10",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nOptional arguments are:\n\nmetrics: A metric set of performance statistics to compute. By default, regression models use RMSE and R2 while classification models compute the area under the ROC curve and overall accuracy.\ncontrol: A list created by tune::control_resamples() with various options."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-11",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-11",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nControl arguments are:\n\nverbose: A logical for printing logging.\nextract: A function for retaining objects from each model iteration (discussed later).\nsave_pred: A logical for saving the assessment set predictions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-12",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-12",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nSave the predictions in order to visualize the model fit and residuals:\n\nCode\n&gt; keep_pred &lt;- \n+   tune::control_resamples(save_pred = TRUE, save_workflow = TRUE)\n&gt; \n&gt; set.seed(1003)\n&gt; rf_res &lt;- \n+   rf_wflow %&gt;% \n+   tune::fit_resamples(resamples = ames_folds, control = keep_pred)\n&gt; rf_res\n\n\n\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits             id     .metrics         .notes           .predictions      \n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n 1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [235 × 4]&gt;\n 2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [235 × 4]&gt;\n 3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n 4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n 5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n 6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n 7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n 8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n 9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-13",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-13",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nThe return value is a tibble similar to the input resamples, along with some extra columns:\n\n.metrics is a list column of tibbles containing the assessment set performance statistics.\n.notes is list column of tibbles cataloging any warnings/errors generated during resampling.\n.predictions is present when save_pred = TRUE. This list column contains tibbles with the out-of-sample predictions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-14",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-14",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nWhile the list columns may look daunting, they can be easily reconfigured using tidyr or with convenience functions that tidymodels provides. For example, to return the performance metrics in a more usable format:\n\n&gt; rf_res %&gt;% tune::collect_metrics()\n&gt; \n&gt; rf_res %&gt;% tune::collect_predictions()\n&gt; \n&gt; val_res %&gt;% tune::collect_metrics()\n\nWhat is collected are the resampling estimates averaged over the individual replicates. To get the metrics for each resample, use the option summarize = FALSE."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-parallelism",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-parallelism",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: parallelism",
    "text": "Tidymodels: parallelism\nThe models created during resampling are independent of one another and can be processed in parallel across processors on the same computer.\nThe code below determines the number of of possible worker processes.\n\n&gt; # The number of physical cores in the hardware:\n&gt; parallel::detectCores(logical = FALSE)\n&gt; \n&gt; # The number of possible independent processes that can \n&gt; # be simultaneously used:  \n&gt; parallel::detectCores(logical = TRUE)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-parallelism-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-parallelism-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: parallelism",
    "text": "Tidymodels: parallelism\nFor fit_resamples() and other functions in tune, parallel processing occurs when the user registers a parallel backend package.\nThese R backend packages define how to execute parallel processing."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#recap",
    "href": "slides/BSMM_8740_lec_04.html#recap",
    "title": "The Tidymodels Framework",
    "section": "Recap",
    "text": "Recap\n\nIn this section we have worked with the tidymodels package to build a workflow that facilitates building and evaluating multiple models.\nCombined with the recipes package we now have a complete data modeling framework."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSMM-8740: Data Analytic Methods & Algorithms",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the Fall 2024 semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\n\nLab\nLab Solution\nExam\nProject\n\n\n\n\n1\nWed, Sep 11\nTidyverse, EDA & Git\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 1\n\n\n\n💻\n✅\n\n\n\n\n2\nWed, Sep 18\nThe Recipes Package\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 2\n\n\n\n💻\n✅\n\n\n\n\n3\nWed, Sep 25\nRegression Methods\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 3\n\n\n\n💻\n✅\n\n\n\n\n4\nWed, Oct 02\nThe TidyModels Framework\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 4\n\n\n\n💻\n✅\n\n\n\n\n5\nWed, Oct 09\nClassification & Clustering Methods\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 5\n\n\n\n💻\n✅\n✅\n\n\n\n6\nWed, Oct 23\nTime Series Methods\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 6\n\n\n\n💻\n✅\n\n\n\n\n7\nWed, Oct 30\nCausality: DAGs\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 7\n\n\n\n💻\n✅\n\n\n\n\n8\nWed, Nov 06\nCausality: Methods\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 8\n\n\n\n💻\n✅\n✅\n\n\n\n9\nWed, Nov 13\nMonte Carlo Methods\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 9\n\n\n\n💻\n✅\n\n\n\n\n10\nWed, Nov 20\nBayesian methods\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 10\n\n\n\n💻\n✅\n✅\n\n\n\n11\nWed, Nov 27\nAdvanced Topics [DRAFT STAGE]\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 11\n\n\n\n⛔\n\n\n\n\n\n12\nWed, Dec 04\nFinal Exam\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 12\n\n\n\n⛔",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, you’ll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance you’ll use for the course."
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#data-sources",
    "href": "project-tips-resources.html#data-sources",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %&gt;%\n  count(manufacturer) %&gt;%\n  mutate(manufacturer = str_to_title(manufacturer)) %&gt;%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html",
    "href": "ae/ae-10-flight-delays.html",
    "title": "AE 10: Flight delays",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-10-flight-delays-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#packages",
    "href": "ae/ae-10-flight-delays.html#packages",
    "title": "AE 10: Flight delays",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#data",
    "href": "ae/ae-10-flight-delays.html#data",
    "title": "AE 10: Flight delays",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of 25,000 randomly sampled flights that departed one of three NYC airports (JFK, LGA, EWR) in 2013.\n\nflight_data &lt;- read_csv(\"data/flight-data.csv\")\n\nRows: 25000 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): origin, dest, carrier, arr_delay\ndbl  (4): dep_time, flight, air_time, distance\ndttm (1): time_hour\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nConvert arr_delay to factor with levels \"late\" (first level) and \"on_time\" (second level). This variable is our outcome and it indicates whether the flight’s arrival was more than 30 minutes.\n\n\nflight_data &lt;- flight_data %&gt;%\n  mutate(arr_delay = as.factor(arr_delay))\n\nlevels(flight_data$arr_delay)\n\n[1] \"late\"    \"on_time\"\n\n\n\nLet’s get started with some data prep: Convert all variables that are character strings to factors.\n\n\n#flight_data &lt;- flight_data %&gt;%\n#  mutate(\n#    origin = as.factor(origin),\n#    carrier = as.factor(carrier),\n#    dest = as.factor(dest)\n#    )\n\nflight_data &lt;- flight_data %&gt;%\n  #go across all columns and convert that are characters to factors\n  #go across all columns and convert if is.character = TRUE to factors\n  #go across all columns and if is.character apply as.factor\n  mutate(across(where(is.character), as.factor))"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#modeling-prep",
    "href": "ae/ae-10-flight-delays.html#modeling-prep",
    "title": "AE 10: Flight delays",
    "section": "Modeling prep",
    "text": "Modeling prep\n\nSplit the data into testing (75%) and training (25%), and save each subset.\n\n\nset.seed(222)\n\nflight_split &lt;- initial_split(flight_data)\n\nflight_train &lt;- training(flight_split)\nflight_test &lt;- testing(flight_split)\n\n\nSpecify a logistic regression model that uses the \"glm\" engine.\n\n\nflight_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nNext, we’ll create two recipes and workflows and compare them to each other."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "href": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "title": "AE 10: Flight delays",
    "section": "Model 1: Everything and the kitchen sink",
    "text": "Model 1: Everything and the kitchen sink\n\nDefine a recipe that predicts arr_delay using all variables except for flight and time_hour, which, in combination, can be used to identify a flight. Also make sure this recipe handles dummy coding as well as issues that can arise due to having categorical variables with some levels apparent in the training set but not in the testing set. Call this recipe flights_rec1.\n\n\nflights_rec1 &lt;- recipe(arr_delay ~ ., data = flight_train) %&gt;%\n  update_role(flight, time_hour, new_role = \"id\") %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nCreate a workflow that uses flights_rec1 and the model you specified.\n\n\nflight_wflow1 &lt;- workflow() %&gt;%\n  add_recipe(flights_rec1) %&gt;%\n  add_model(flight_spec)\n\n\nFit the this model to the training data using your workflow and display a tidy summary of the model fit.\n\n\nflight_fit1 &lt;- flight_wflow1 %&gt;%\n  fit(data = flight_train)\n\ntidy(flight_fit1)\n\n# A tibble: 119 × 5\n   term          estimate   std.error statistic   p.value\n   &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  13.3      287.          0.0464  9.63e-  1\n 2 dep_time     -0.00164    0.0000504 -32.6     1.04e-233\n 3 air_time     -0.0349     0.00179   -19.5     1.75e- 84\n 4 distance      0.00533    0.00523     1.02    3.08e-  1\n 5 date          0.000227   0.000198    1.15    2.51e-  1\n 6 origin_JFK    0.0830     0.102       0.815   4.15e-  1\n 7 origin_LGA   -0.0360     0.0983     -0.366   7.14e-  1\n 8 dest_ACK    -12.4      287.         -0.0434  9.65e-  1\n 9 dest_ALB    -12.4      287.         -0.0433  9.65e-  1\n10 dest_ANC     -3.75     928.         -0.00404 9.97e-  1\n# … with 109 more rows\n\n\n\nPredict arr_delay for the testing data using this model.\n\n\nflight_aug1 &lt;- augment(flight_fit1, flight_test)\n\n\nPlot the ROC curve and find the area under the curve. Comment on how well you think this model has done for predicting arrival delay.\n\n\nflight_aug1 %&gt;%\n  roc_curve(\n    truth = arr_delay,\n    .pred_late\n  ) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nflight_aug1 %&gt;%\n  roc_auc(\n    truth = arr_delay,\n    .pred_late\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.734"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "href": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "title": "AE 10: Flight delays",
    "section": "Model 2: Let’s be a bit more thoughtful",
    "text": "Model 2: Let’s be a bit more thoughtful\n\nDefine a new recipe, flights_rec2, that, in addition to what was done in flights_rec1, adds features for day of week and month based on date and also adds indicators for all US holidays (also based on date). A list of these holidays can be found in timeDate::listHolidays(\"US\"). Once these features are added, date should be removed from the data. Then, create a new workflow, fit the same model (logistic regression) to the training data, and do predictions on the testing data. Finally, draw another ROC curve and find the area under the curve. Compare the predictive performance of this new model to the previous one. Based on the area under the curve statistic, which model does better?"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "href": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "title": "AE 10: Flight delays",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\nCreate an ROC curve that plots both models, in different colors, and adds a legend indicating which model is which."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#acknowledgement",
    "href": "ae/ae-10-flight-delays.html#acknowledgement",
    "title": "AE 10: Flight delays",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://www.tidymodels.org/start/recipes/."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html",
    "href": "ae/ae-3-duke-forest.html",
    "title": "AE 3: Duke Forest houses",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-3-duke-forest-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#packages",
    "href": "ae/ae-3-duke-forest.html#packages",
    "title": "AE 3: Duke Forest houses",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "href": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "title": "AE 3: Duke Forest houses",
    "section": "Predict sale price from area",
    "text": "Predict sale price from area\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#model-conditions",
    "href": "ae/ae-3-duke-forest.html#model-conditions",
    "title": "AE 3: Duke Forest houses",
    "section": "Model conditions",
    "text": "Model conditions\n\nExercise 1\nThe following code produces the residuals vs. fitted values plot for this model. Comment out the layer that defines the y-axis limits and re-create the plot. How does the plot change? Why might we want to define the limits explicitly?\n\ndf_aug &lt;- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n\n\n\n\n\n\n\n\n\n\nExercise 2\nImprove how the values on the axes of the plot are displayed by modifying the code below.\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html",
    "href": "ae/ae-7-exam-2-review.html",
    "title": "AE 7: Exam 2 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-7-exam-2-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#packages",
    "href": "ae/ae-7-exam-2-review.html#packages",
    "title": "AE 7: Exam 2 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\n# fix data!\nloans_full_schema &lt;- droplevels(loans_full_schema)"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#goal",
    "href": "ae/ae-7-exam-2-review.html#goal",
    "title": "AE 7: Exam 2 Review",
    "section": "Goal",
    "text": "Goal\nCreate a model for precicting interest_rate."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#view-data",
    "href": "ae/ae-7-exam-2-review.html#view-data",
    "title": "AE 7: Exam 2 Review",
    "section": "View data",
    "text": "View data\nNote the dimensions of the data and the variable names. Review the data dictionary.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "href": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "title": "AE 7: Exam 2 Review",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#write-the-model",
    "href": "ae/ae-7-exam-2-review.html#write-the-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Write the model",
    "text": "Write the model\nWrite the model for predicting interest rate (interest_rate) from debt to income ratio (debt_to_income), the term of loan (term), the number of inquiries (credit checks) into the applicant’s credit during the last 12 months (inquiries_last_12m), whether there are any bankruptcies listed in the public record for this applicant (bankrupt), and the type of application (application_type). The model should allow for the effect of to income ratio on interest rate to vary by application type.\nAdd model here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#exploration",
    "href": "ae/ae-7-exam-2-review.html#exploration",
    "title": "AE 7: Exam 2 Review",
    "section": "Exploration",
    "text": "Exploration\nExplore characteristics of the variables you’ll use for the model using the training data only.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#specify-model",
    "href": "ae/ae-7-exam-2-review.html#specify-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-recipe",
    "href": "ae/ae-7-exam-2-review.html#create-recipe",
    "title": "AE 7: Exam 2 Review",
    "section": "Create recipe",
    "text": "Create recipe\n\nPredict interest_rate from debt_to_income, term, inquiries_last_12m, public_record_bankrupt, and application_type.\nMean center debt_to_income.\nMake term a factor.\nCreate a new variable: bankrupt that takes on the value “no” if public_record_bankrupt is 0 and the value “yes” if public_record_bankrupt is 1 or higher. Then, remove public_record_bankrupt.\nInteract application_type with debt_to_income.\nCreate dummy variables where needed and drop any zero variance variables.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-workflow",
    "href": "ae/ae-7-exam-2-review.html#create-workflow",
    "title": "AE 7: Exam 2 Review",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#cross-validation",
    "href": "ae/ae-7-exam-2-review.html#cross-validation",
    "title": "AE 7: Exam 2 Review",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "href": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "title": "AE 7: Exam 2 Review",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here\n\nWhy are we focusing on R-squared and RMSE instead of adjusted R-squared, AIC, BIC?\n[Add response here]"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#next-steps",
    "href": "ae/ae-7-exam-2-review.html#next-steps",
    "title": "AE 7: Exam 2 Review",
    "section": "Next steps…",
    "text": "Next steps…\nDepending on time, either\n\nCreate a workflow for another model with a new recipe (omitting the interaction variable), conduct CV, do model selection between these two, and then interpret the coefficients for the selected model.\nOr interpret the coefficients for the one model you fit.\n\nMake sure to interpret the intercept and slope coefficient for at least one numerical, one categorical, and one interaction predictor."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html",
    "href": "ae/ae-12-exam-3-review.html",
    "title": "AE 12: Exam 3 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-12-exam-3-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#packages",
    "href": "ae/ae-12-exam-3-review.html#packages",
    "title": "AE 12: Exam 3 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data)\nlibrary(rms)\nlibrary(nnet)"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#data",
    "href": "ae/ae-12-exam-3-review.html#data",
    "title": "AE 12: Exam 3 Review",
    "section": "Data",
    "text": "Data\nAs part of a study of the effects of predatory intertidal crab species on snail populations, researchers measured the mean closing forces and the propodus heights of the claws on several crabs of three species.\n\n\nclaws &lt;- read_csv(here::here(\"ae\", \"data/claws.csv\"))\n\nWe will use the following variables:\n\nforce: Closing force of claw (newtons)\nheight: Propodus height (mm)\nspecies: Crab species - Cp(Cancer productus), Hn (Hemigrapsus nudus), Lb(Lophopanopeus bellus)\nlb: 1 if Lophopanopeus bellus species, 0 otherwise\nhn: 1 if Hemigrapsus nudus species, 0 otherwise\ncp: 1 if Cancer productus species, 0 otherwise\nforce_cent: mean centered force\nheight_cent: mean centered height\n\nBefore we get started, let’s make the categorical and indicator variables factors.\n\nclaws &lt;- claws %&gt;%\n  mutate(\n    species = as_factor(species),\n    lb = as_factor(lb),\n    hn = as_factor(hn),\n    cp = as_factor(cp)\n  )"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "href": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "title": "AE 12: Exam 3 Review",
    "section": "Probabilities vs. odds vs. log-odds",
    "text": "Probabilities vs. odds vs. log-odds\nWhy we use log-odds as response variable: https://sta210-s22.github.io/website/slides/lec-18.html#/do-teenagers-get-7-hours-of-sleep"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-1",
    "href": "ae/ae-12-exam-3-review.html#exercise-1",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 1",
    "text": "Exercise 1\nFill in the blanks:\n\nUse log-odds to fit the model (outcome)\nUse odds to interpret model results\nUse probabilities to make predictions for individual observations and ultimately to make classification decisions"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-2",
    "href": "ae/ae-12-exam-3-review.html#exercise-2",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 2",
    "text": "Exercise 2\nSuppose we want to use force to determine whether or not a crab is from the Lophopanopeus bellus (Lb) species. Why should we use a logistic regression model for this analysis?\n\nclaws %&gt;%\n  distinct(lb)\n\n# A tibble: 2 × 1\n  lb   \n  &lt;fct&gt;\n1 0    \n2 1"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-3",
    "href": "ae/ae-12-exam-3-review.html#exercise-3",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe will use the mean-centered variables for force in the model. The model output is below. Write the equation of the model produced by R. Don’t forget to fill in the blanks for ….\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.798\n0.358\n-2.233\n0.026\n-1.542\n-0.123\n\n\nforce_cent\n0.043\n0.039\n1.090\n0.276\n-0.034\n0.123\n\n\n\n\n\nLet π\\pi be probability that a crab is from Lb species.\nlog(π̂1−π̂)=−0.798+0.043*force_cent\n\\log\\Big(\\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\Big) = -0.798 + 0.043 * force\\_cent"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-4",
    "href": "ae/ae-12-exam-3-review.html#exercise-4",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 4",
    "text": "Exercise 4\nInterpret the intercept in the context of the data.\n\nmean_force &lt;- round(mean(claws$force), 2)\n\nFor crabs with average closing force (12.13 newtons), we expect odds of the crab being Lophopanopeus bellus is 0.45 (exp(-0.798))."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-5",
    "href": "ae/ae-12-exam-3-review.html#exercise-5",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the effect of force in the context of the data.\nWhen x goes up by 1 unit, we expect y to change by (slope) units.\nFor each additional unit increase in closing force, the odds of crab being from lb species multiplies on average by a factor of 1.0439379."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-6",
    "href": "ae/ae-12-exam-3-review.html#exercise-6",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 6",
    "text": "Exercise 6\nNow let’s consider adding height_cent to the model. Fit the model that includes height_cent. Then use AIC to choose the model that best fits the data.\n\nlb_fit_2 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(lb ~ force_cent + height_cent, data = claws)\n\ntidy(lb_fit_2, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -1.13     0.463      -2.44  0.0146  -2.17      -0.306\n2 force_cent     0.211    0.0925      2.28  0.0227   0.0563     0.424\n3 height_cent   -0.895    0.398      -2.25  0.0245  -1.82      -0.234\n\nglance(lb_fit_1)$AIC\n\n[1] 50.19535\n\nglance(lb_fit_2)$AIC\n\n[1] 44.11812"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-7",
    "href": "ae/ae-12-exam-3-review.html#exercise-7",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 7",
    "text": "Exercise 7\nWhat do the following mean in the context of this data. Explain and calculate them.\n\nSensitivity: P(predict lb | actual lb) = 6 / 12\nSpecificity: P(predict not lb | actual not lb) = 4/ 26\nNegative predictive power: P(actual not lb | predict not lb) = 22 / 28\nPositive predictive power: P(actual lb | predict lb) = 6 / 10\n\n\n\n\nActual\nPredict lb\nPredict not lb\nTOTAL predicted\n\n\n\n\nLb\n6\n6\n12\n\n\nNot lb\n4\n22\n26\n\n\nTOTAL actual\n10\n28\n38"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-8",
    "href": "ae/ae-12-exam-3-review.html#exercise-8",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 8",
    "text": "Exercise 8\nWrite the equation of the model.\nlog(π̂Hnπ̂Cp)=\\log\\Big(\\frac{\\hat{\\pi}_{Hn}}{\\hat{\\pi}_{Cp}}\\Big) = \nlog(π̂Lbπ̂Cp)=\\log\\Big(\\frac{\\hat{\\pi}_{Lb}}{\\hat{\\pi}_{Cp}}\\Big) ="
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-9",
    "href": "ae/ae-12-exam-3-review.html#exercise-9",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nInterpret the intercept for the odds a crab is Hn vs. Cp species.\nInterpret the effect of force on the odds a crab is Lb vs. Cp species."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-10",
    "href": "ae/ae-12-exam-3-review.html#exercise-10",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the effect of force on the odds a crab is in the Hn vs. Lb species.\nCAUTION: We can write an interpretation based on the estimated coefficients; however, we can’t make any inferential conclusions for this question based on the current model. We would need to refit the model with Lb as the baseline category to do so."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-11",
    "href": "ae/ae-12-exam-3-review.html#exercise-11",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 11",
    "text": "Exercise 11\nConditions for multinomial logistic (and logistic models as well):\n\nIndependence:\nRandomness:\nLinearity:\n\nemplogitplot1(lb ~ force, data = claws, ngroups = 10)\nemplogitplot1(lb ~ height, data = claws, ngroups = 10)\n\n\n\n\n\n\n\n\n\n\n# add code here for other species here\n\n\n\n# add code here for other species here"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "href": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "title": "AE 12: Exam 3 Review",
    "section": "Checking for multicollinearity in logistic and multinomial logistic",
    "text": "Checking for multicollinearity in logistic and multinomial logistic\nSimilar to multiple linear regression, we can also check for multicollinearity in logistic and multinomial logistic models.\n\nUse the vif function to check for multicollinearity in logistic regression.\n\n\nThe vif function doesn’t work for the multinomial logistic regression models, so we can look at a correlation matrix of the predictors as a way to assess if the predictors are highly correlated:"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html",
    "href": "ae/ae-8-rail-trail.html",
    "title": "AE 8: Rail Trail",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-8-rail-trail-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#packages-and-data",
    "href": "ae/ae-8-rail-trail.html#packages-and-data",
    "title": "AE 8: Rail Trail",
    "section": "Packages and data",
    "text": "Packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrail_trail &lt;- read_csv(\"data/rail_trail.csv\")"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-1",
    "href": "ae/ae-8-rail-trail.html#exercise-1",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model predicting volume from hightemp and season.\n\nrt_mlr_main_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic       p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   -125.       71.7     -1.75  0.0841       \n2 hightemp         7.54      1.17     6.43  0.00000000692\n3 seasonSpring     5.13     34.3      0.150 0.881        \n4 seasonSummer   -76.8      47.7     -1.61  0.111        \n\n\nRecreate the following visualization which displays the three regression lines we can draw based on the results of this model.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-2",
    "href": "ae/ae-8-rail-trail.html#exercise-2",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 2",
    "text": "Exercise 2\nAdd an interaction effect between hightemp and season and comment on the significance of the interaction predictors. Time permitting, visualize the interaction model as well.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-3",
    "href": "ae/ae-8-rail-trail.html#exercise-3",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a model predicting volume from all available predictors.\n\nrt_full_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ ., data = rail_trail)\n\ntidy(rt_full_fit)\n\n# A tibble: 8 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\nRecreate the following visualization which displays a histogram of residuals and a normal density curve overlaid.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html",
    "href": "ae/ae-4-exam-1-review.html",
    "title": "AE 4: Exam 1 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-4-exam-1-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#packages",
    "href": "ae/ae-4-exam-1-review.html#packages",
    "title": "AE 4: Exam 1 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "href": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "title": "AE 4: Exam 1 Review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips &lt;- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "href": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "title": "AE 4: Exam 1 Review",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\nVisualize, summarize, and describe the relationship between Party and Tip.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#modeling",
    "href": "ae/ae-4-exam-1-review.html#modeling",
    "title": "AE 4: Exam 1 Review",
    "section": "Modeling",
    "text": "Modeling\nLet’s start by fitting a model using Party to predict the Tip at this restaurant.\n\nWrite the statistical model.\nFit the regression line and write the regression equation. Name the model tips_fit and display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret the slope.\nDoes it make sense to interpret the intercept? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#inference",
    "href": "ae/ae-4-exam-1-review.html#inference",
    "title": "AE 4: Exam 1 Review",
    "section": "Inference",
    "text": "Inference\n\nInference for the slope\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we’ll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\nset.seed(1234)\n\nboot_dist &lt;- tips %&gt;%\n  specify(Tip ~ Party) %&gt;%\n  generate(reps = 100, type = \"bootstrap\") %&gt;%\n  fit()\n\n\nUse the bootstrap distribution created in Exercise 6, boot_dist, to construct a 90% confidence interval for the slope using bootstrapping and the percentile method and interpret it in context of the data.\n\n\n# add your code here\n\n\nConduct a hypothesis test at the equivalent significance level using permutation. State the hypotheses and the significance level you’re using explicitly. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercises 7 and 8. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\nNow repeat Exercises 7 and 8 using approaches based on mathematical models.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercise 9. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\n\nInference for a prediction\n\nBased on your model, predict the tip for a party of 4.\n\n\n# add your code here\n\n\nSuppose you’re asked to construct a confidence and a prediction interval for your finding in Exercise 11. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals from Exercise 12 and comment on whether your guess is confirmed.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "href": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "title": "AE 4: Exam 1 Review",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nLeverage (Outliers in x direction)\n\nWhat is the threshold used to identify observations with high leverage? Calculate the threshold and save the value as leverage_threshold.\n\n\n# add your code here\n\n\nMake a plot of the standardized residuals vs. leverage (you can do this with ggplot() or with autoplot(which = 5)). Use geom_vline() to add a vertical line to help identify points with high leverage.\n\n\n# add your code here\n\n\nLet’s dig into the data further. Which observations have high leverage? Why do these points have high leverage?\n\n\n# add your code here\n\n\n\nIdentifying outliers (outliers in y direction)\n\nMake a plot of the residuals vs. fitted values and a plot of the square root of the absolute value of standardized residuals vs. fitted (You can use autoplot(which = c(1, 3)) to display the plots side-by-side).\n\n\nHow are the plots similar? How do they differ?\nWhat is an advantage of using the plot of the residuals vs. fitted to check conditions and model diagnostics?\nWhat is an advantage of using the plot of the |standardized residuals|\\sqrt{|\\text{standardized residuals}|} vs. fitted to check conditions and model diagnostics?\n\n\n# add your code here\n\n\nAre there any observations that are outliers?\n\n\n# add your code here\n\n\n\nCook’s distance\n\nMake a plot to check Cook’s distance (autoplot(which = 4)). Based on this plot, are there any points that have a strong influence on the model coefficients?\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "href": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "title": "AE 4: Exam 1 Review",
    "section": "Adding another variable",
    "text": "Adding another variable\n\nAdd another variable, Alcohol, to your exploratory visualization. Describe any patterns that emerge.\n\n\n# add your code here\n\n\nFit a multiple linear regression model predicting Tip from Party and Alcohol. Display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret each of the slopes.\nDoes it make sense to interpret the intercept? Explain your reasoning.\nAccording to this model, is the rate of change in tip amount the same for various sizes of parties regardless of alcohol consumption or are they different? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#footnotes",
    "href": "ae/ae-4-exam-1-review.html#footnotes",
    "title": "AE 4: Exam 1 Review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.↩︎"
  },
  {
    "objectID": "ae/ae-5-the-office.html",
    "href": "ae/ae-5-the-office.html",
    "title": "AE 5: The Office",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-5-the-office-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-5-the-office.html#packages",
    "href": "ae/ae-5-the-office.html#packages",
    "title": "AE 5: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-5-the-office.html#load-data",
    "href": "ae/ae-5-the-office.html#load-data",
    "title": "AE 5: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\n\nRows: 188 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): title\ndbl  (4): season, episode, imdb_rating, total_votes\ndate (1): air_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "href": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "title": "AE 5: The Office",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nRecreate at least one of the exploratory visualizations from class."
  },
  {
    "objectID": "ae/ae-5-the-office.html#testtrain-split",
    "href": "ae/ae-5-the-office.html#testtrain-split",
    "title": "AE 5: The Office",
    "section": "Test/train split",
    "text": "Test/train split\nSplit your data into testing and training sets."
  },
  {
    "objectID": "ae/ae-5-the-office.html#build-a-recipe",
    "href": "ae/ae-5-the-office.html#build-a-recipe",
    "title": "AE 5: The Office",
    "section": "Build a recipe",
    "text": "Build a recipe\nBuild the recipe from class.\n\nTime permitting…"
  },
  {
    "objectID": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "href": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "title": "AE 5: The Office",
    "section": "Workflows and model fitting",
    "text": "Workflows and model fitting\nBuild the modeling workflow and fit the model to the training data after feature engineering with the recipe."
  },
  {
    "objectID": "hw/hw-2.html",
    "href": "hw/hw-2.html",
    "title": "HW 2 - Multiple linear regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the multiple linear regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multiple linear regression models with main and interaction effects.\nCompare multiple linear regression models.\nReason around multiple linear regression concepts.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-2. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-2.html#introduction",
    "href": "hw/hw-2.html#introduction",
    "title": "HW 2 - Multiple linear regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the multiple linear regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multiple linear regression models with main and interaction effects.\nCompare multiple linear regression models.\nReason around multiple linear regression concepts.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-2. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-2.html#part-1---conceptual",
    "href": "hw/hw-2.html#part-1---conceptual",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 1 - Conceptual",
    "text": "Part 1 - Conceptual\n\nDealing with categorical predictors. Two friends, Elliott and Adrian, want to build a model predicting typing speed (average number of words typed per minute) from whether the person wears glasses or not. Before building the model they want to conduct some exploratory analysis to evaluate the strength of the association between these two variables, but they’re in disagreement about how to evaluate how strongly a categorical predictor is associated with a numerical outcome. Elliott claims that it is not possible to calculate a correlation coefficient to summarize the relationship between a categorical predictor and a numerical outcome, however they’re not sure what a better alternative is. Adrian claims that you can recode a binary predictor as a 0/1 variable (assign one level to be 0 and the other to be 1), thus converting it to a numerical variable. According to Adrian, you can then calculate the correlation coefficient between the predictor and the outcome. Who is right: Elliott or Adrian? If you pick Elliott, can you suggest a better alternative for evaluating the association between the categorical predictor and the numerical outcome?\nHigh correlation, good or bad? Two friends, Frances and Annika, are in disagreement about whether high correlation values are always good in the context of regression. Frances claims that it’s desirable for all variables in the dataset to be highly correlated to each other when building linear models. Annika claims that while it’s desirable for each of the predictors to be highly correlated with the outcome, it is not desirable for the predictors to be highly correlated with each other. Who is right: Frances, Annika, both, or neither? Explain your reasoning using appropriate terminology.\nTraining for the 5K. Nico signs up for a 5K (a 5,000 metre running race) 30 days prior to the race. They decide to run a 5K every day to train for it, and each day they record the following information: days_since_start (number of days since starting training), days_till_race (number of days left until the race), mood (poor, good, awesome), tiredness (1-not tired to 10-very tired), and time (time it takes to run 5K, recorded as mm:ss). Top few rows of the data they collect is shown below.\n\n\n\ndays_since_start\ndays_till_race\nmood\ntiredness\ntime\n\n\n\n\n1\n29\ngood\n3\n25:45\n\n\n2\n28\npoor\n5\n27:13\n\n\n3\n27\nawesome\n4\n24:13\n\n\n…\n…\n…\n…\n…\n\n\n\nUsing these data Nico wants to build a model predicting time from the other variables. Should they include all variables shown above in their model? Why or why not?\nMultiple regression fact checking. Determine which of the following statements are true and false. For each statement that is false, explain why it is false.\n\nIf predictors are colinear, then removing one variable will have no influence on the point estimate of another variable’s coefficient.\nSuppose a numerical predictor xx has a coefficient of β̂1=2.5\\hat{\\beta}_1 = 2.5 in a multiple regression model. Suppose also that the first observation has x1,1=7.2x_{1,1} = 7.2, the second observation has a value of x2,1=8.2x_{2,1} = 8.2, and these two observations have the same values for all other predictors. Then the predicted value of the second observation will be 2.5 higher than the prediction of the first observation based on the multiple regression model.\nIf a regression model’s first predictor has a coefficient of β̂1=5.7\\hat{\\beta}_1 = 5.7 and if we are able to influence the data so that an observation will have its x1x_1 be 1 larger than it would otherwise, the value ŷ1\\hat{y}_1 for this observation would increase by 5.7."
  },
  {
    "objectID": "hw/hw-2.html#part-2---palmer-penguins",
    "href": "hw/hw-2.html#part-2---palmer-penguins",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 2 - Palmer penguins",
    "text": "Part 2 - Palmer penguins\nData were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. (Gorman, Williams, and Fraser 2014)\n\n\n\nArtwork by @allison_horst\n\n\nThese data can be found in the palmerpenguins package. We’re going to be working with the penguins dataset from this package. The dataset contains data for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n\nBody mass. Our first goal is to fit a model predicting body mass (which is more difficult to measure) from bill length, bill depth, flipper length, species, and sex.\n\nFit a model predicting body mass (which is more difficult to measure) from the other variables listed above.\nWrite the equation of the regression model.\nInterpret each one of the slopes in this context.\nCalculate the residual for a male Adelie penguin that weighs 3750 grams with the following body measurements: bill_length_mm = 39.1, bill_depth_mm = 18.7, flipper_length_mm = 181. Does the model overpredict or underpredict this penguin’s weight?\nFind the R2R^2 of this model and interpret this value in context of the data and the model.\n\n\n\n\nBill depth. Next we’ll be focusing on bill depth and bill length and also considering species.\n\nFit a model predicting bill depth from bill length. Find the adjusted R-squared, AIC, and BIC for this model.\nThen, add a new predictor: species. Fit another model predicting bill depth from bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nFinally, add one more predictor: the interaction between bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nUsing the three criteria you recorded for these three models, and with the goal of parsimony, which model is the “best” for predicting bill depth from bill length and/or species. Explain your reasoning.\nCreate a visualization representing your model from part a. Hint: Make a scatterplot of bill depth vs. bill length and add the linear model.\nCreate a visualization representing your model from part b. Hint: Same as part (e), but think about how many lines to plot and whether their slopes should be the same or different.\nCreate a visualization representing your model from part c. Hint: Same as part (f), but think about how many lines to plot and whether their slopes should be the same or different.\nBased on your visualizations from parts e - g, and with the goal of parsimony, is your answer for which model is the “best” for predicting bill depth from bill length and/or species the same as your answer in part d? Explain your reasoning."
  },
  {
    "objectID": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "href": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 3 - Perceived threat of Covid-19",
    "text": "Part 3 - Perceived threat of Covid-19\nGarbe, Rau, and Toppe (2020), published in June 2020, aims to examine the relationship between personality traits, perceived threat of Covid-19 and stockpiling toilet paper. For this study titled Influence of perceived threat of Covid-19 and HEXACO personality traits on toilet paper stockpiling, researchers conducted an online survey March 23 - 29, 2020 and used the results to fit multiple linear regression models to draw conclusions about their research questions. From their survey, they collected data on adults across 35 countries. Given the small number of responses from people outside of the United States, Canada, and Europe, only responses from people in these three locations were included in the regression analysis.\nLet’s consider their results for the model looking at the effect on perceived threat of Covid-19. The model can be found on page 6 of the paper. The perceived threat of Covid was quantified using the responses to the following survey question:\n\nHow threatened do you feel by Coronavirus? [Users select on a 10-point visual analogue scale (Not at all threatened to Extremely Threatened)]\n\n\nInterpret the coefficient of Age (0.072) in the context of the analysis.\nInterpret the coefficient of Place of residence in the context of the analysis.\nThe model includes an interaction between Place of residence and Emotionality (capturing differential tendencies in to worry and be anxious).\n\nWhat does the coefficient for the interaction (0.101) mean in the context of the data?\nInterpret the estimated effect of Emotionality for a person who lives in the US/Canada.\nInterpret the estimated effect of Emotionality for a person who lives in Europe."
  },
  {
    "objectID": "hw/hw-2.html#submission",
    "href": "hw/hw-2.html#submission",
    "title": "HW 2 - Multiple linear regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-2.html#grading",
    "href": "hw/hw-2.html#grading",
    "title": "HW 2 - Multiple linear regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-2.html#footnotes",
    "href": "hw/hw-2.html#footnotes",
    "title": "HW 2 - Multiple linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-4.html",
    "href": "hw/hw-4.html",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multinomial logistic regression models.\nEvaluate model conditions\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-4. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "hw/hw-4.html#introduction",
    "href": "hw/hw-4.html#introduction",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multinomial logistic regression models.\nEvaluate model conditions\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-4. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "hw/hw-4.html#data",
    "href": "hw/hw-4.html#data",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Data",
    "text": "Data\nFor this assignment, we will analyze data from the eye witness identification experiment in Carlson and Carlson (2014). In this experiment, participants were asked to watch a video of a mock crime (from the first person perspective), spend a few minutes completing a random task, and then identify the perpetrator of the mock crime from a line up shown on the screen. Every lineup in this analysis included the true perpetrator from the video. After viewing the line-up , each participant could make one of the following decisions (id):\n\ncorrect: correctly identified the true perpetrator\nfoil: incorrectly identified the “foil”, i.e. a person who looks very similar to the perpetrator\nreject: incorrectly concluded the true perpetrator is not in the lineup\n\nThe main objective of the analysis is to understand how different conditions of the mock crime and suspect lineup affect the decision made by the participant. We will consider the following conditions to describe the decisions:\n\nlineup: How potential suspects are shown to the participants\n\nSimultaneous Lineup: Participants were shown photos of all 6 potential suspects at the same time and were required to make a single decision (identify someone from the lineup or reject the lineup).\nSequential 5 Lineup: Photos of the 6 suspects were shown one at a time. The participant was required to make a decision (choose or don’t choose) as each photo was shown. Once a decision was made, participants were not allowed to reexamine a photo. If the participant made an identification, the remaining photos were not shown. In each of these lineups the true perpetrator was always the 5th photo in the lineup.\n\nweapon: Whether or not a weapon was present in the video of the mock crime.\nfeature: Whether or not the perpetrator had a distinctive marking on his face. In this experiment, the distinctive feature was a large “N” sticker on one cheek. (The letter “N” was chosen to represent the first author’s alma mater - University of Nebraska.)\n\nThe data may be found in eyewitness.csv in the data folder.\n\new &lt;- read_csv(here::here(\"hw\", \"data/eyewitness.csv\"))\new &lt;- ew %&gt;%\n  mutate(id = as_factor(id))"
  },
  {
    "objectID": "hw/hw-4.html#exercises",
    "href": "hw/hw-4.html#exercises",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Exercises",
    "text": "Exercises\n\nLet’s begin by doing some exploratory data analysis. The univariate (single variable) plots for each of the predictor variables and the response variable are shown below.\n\n\n\n\n\n\n\n\n\n\nComplete the exploratory data analysis by creating the plots and/or summary statistics to examine the relationship between the response variable (id) and each of the explanatory variables (lineup, weapon, and feature).\n\nUsing the plots/tables from Exercise 1:\n\n\nWhat is one thing you learn about the data from the univariate plots?\nBased on the bivariate plots, do any of the predictors appear to have a significant effect on the id? Briefly explain.\n\n\nBriefly explain why you should use a multinomial logistic regression model to predict id using lineup, weapon and feature.\nFit the multinomial logistic model that only includes main effects. Display the model output.\n\n\nWhat is the baseline category for the response variable?\nInterpret the intercepts for each part of the model in terms of the odds.\nInterpret the coefficients of lineup for each part of the model in terms of the odds.\n\n\nYou want to consider all possible first-order interaction effects (interaction effects between two variables) for the model.\n\n\nUse the appropriate test to determine if there is at least one significant interaction effect.\nBased on your test, is there evidence of any significant interaction effects?\n\nRegardless of your answer to Question 5, use the model that includes the interaction terms for the remainder of the assignment.\n\nAccording to the model,\n\n\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the log-odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nWhich group of participants (i.e., which set of experimental conditions) is described by the intercept?\n\n\nAre the conditions inference met? List of the conditions, and, if relevant, create visualizations to check the conditions and evaluate whether each condition is met. Include an assessment about each condition and a brief explanation about your conclusion.\nUse the model to predict the decision made by each participant. Make a table of the predicted vs. the actual decisions.\n\n\nBriefly describe how the predicted decision is determined for each participant.\nWhat is the misclassification rate?"
  },
  {
    "objectID": "hw/hw-4.html#submission",
    "href": "hw/hw-4.html#submission",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-4.html#grading",
    "href": "hw/hw-4.html#grading",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nExercises\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-4.html#footnotes",
    "href": "hw/hw-4.html#footnotes",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html",
    "href": "weeks/BSMM_8740_week_1.html",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 1 - Sunday, Sept 15, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html#prepare",
    "href": "weeks/BSMM_8740_week_1.html#prepare",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "Prepare",
    "text": "Prepare\n📖 Read the syllabus\n📖 Read the support resources\n📖 Get familiar with Git by reading Excuse me, do you have a moment to talk about version control?\n📖 Get familiar with Git by reading Excuse me, do you have a moment to talk about version control?\n📖 Read the article What is Tidy Data?\n📖 Read the chapters 2-5 in R for Data Science",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html#participate",
    "href": "weeks/BSMM_8740_week_1.html#participate",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 1 - The tidyverse",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html#practice",
    "href": "weeks/BSMM_8740_week_1.html#practice",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "Practice",
    "text": "Practice\n📋 AE 0 - Movies",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html#perform",
    "href": "weeks/BSMM_8740_week_1.html#perform",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 1 - Git & the tidyverse\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_2.html",
    "href": "weeks/BSMM_8740_week_2.html",
    "title": "Week 2: The Recipes Package",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 2 - Sunday, Sept 22, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_2.html#prepare",
    "href": "weeks/BSMM_8740_week_2.html#prepare",
    "title": "Week 2: The Recipes Package",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Chapter 8: Feature Engineering with recipes in Tidy Modeling in R\n📖 Read Preprocess your data with recipes - Chapter 2: Preprocessing with Recipes\n📖 Familiarize yourself with the recipes package: Package ‘recipes’\n📖 Watch this video from Max Kuhn: Cooking Your Data with Recipes in R with Max Kuhn\n📖 Step through Max Kuhn’s slides: Cooking your data with recipes!!!",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_2.html#participate",
    "href": "weeks/BSMM_8740_week_2.html#participate",
    "title": "Week 2: The Recipes Package",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 2 - The Recipes Package",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_2.html#perform",
    "href": "weeks/BSMM_8740_week_2.html#perform",
    "title": "Week 2: The Recipes Package",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 2 - The Recipes Package\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html",
    "href": "weeks/BSMM_8740_week_7.html",
    "title": "Week 7 - Causality: DAGs",
    "section": "",
    "text": "Important\n\n\n\nDue date: Lab 7 - Sunday, Nov 03, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html#prepare",
    "href": "weeks/BSMM_8740_week_7.html#prepare",
    "title": "Week 7 - Causality: DAGs",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Chp 1 in: Statistical Tools for Causal Inference\n📖 Read Tutorial on Directed Acyclic Graphs\n📖 Read ggdag: An R Package for visualizing and analyzing causal directed acyclic graphs\n📖 Read Causal inference & directed acyclic diagrams (DAGs), Chp 7.3-7.4 in (Mostly Clinical) Epidemiology with R\n📖 Read Propensity Score Analysis: A Primer and Tutorial Chp 1-4:\n📖 Read Margin Effects Zoo, Inverse Probability Weighting, for treatment evaluation using the marginaleffects package.",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html#participate",
    "href": "weeks/BSMM_8740_week_7.html#participate",
    "title": "Week 7 - Causality: DAGs",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 7 - Causality: DAGs",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html#perform",
    "href": "weeks/BSMM_8740_week_7.html#perform",
    "title": "Week 7 - Causality: DAGs",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 7 -Causality: DAGs",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html#podcast",
    "href": "weeks/BSMM_8740_week_7.html#podcast",
    "title": "Week 7 - Causality: DAGs",
    "section": "Podcast",
    "text": "Podcast\nListen here",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html#study",
    "href": "weeks/BSMM_8740_week_7.html#study",
    "title": "Week 7 - Causality: DAGs",
    "section": "Study",
    "text": "Study\n\nShort Answer Questions\nInstructions: Answer the following questions in 2-3 sentences each.\n\nExplain the Fundamental Problem of Causal Inference (FPCI).\nDefine the Average Treatment Effect on the Treated (ATT) and provide an example.\nWhat is selection bias and how does it affect causal inference?\nDifferentiate between a fork and a collider in a DAG and explain their implications for statistical\nExplain the concept of an adjustment set and its role in identifying the causal effect.\nWhat is M-Bias and how can it arise in causal inference?\nDefine an instrumental variable and describe its potential use in causal analysis.\nDifferentiate between a precision variable and a confounder.\nWhat is a tipping point analysis and how is it useful in assessing the robustness of causal inferen\nExplain the concept of Inverse Probability Weighting (IPW) and its application in causal inference.\n\nShort-Answer Answer Key\n\nThe FPCI states that it is impossible to observe both potential outcomes (treated and untreated) for the same unit at the same time. This makes it challenging to determine the true causal effect of a treatment because we can only observe one of the potential outcomes.\nATT is the average difference in outcomes between treated units and what their outcome would have been had they not been treated. For example, if a company offers a training program to its employees, the ATT would measure the average improvement in performance among those who participated in the training compared to what their performance would have been had they not participated.\nSelection bias occurs when the treatment and control groups differ systematically on factors that also influence the outcome. This can lead to inaccurate estimates of the treatment effect because the observed difference in outcomes may be due to these pre-existing differences rather than the treatment itself.\nA fork in a DAG represents a common cause, where a single variable influences both the treatment and the outcome. This can create a spurious association between the treatment and outcome. A collider, on the other hand, is a variable that is caused by both the treatment and the outcome. Adjusting for a collider can introduce bias by creating a spurious association between the treatment and outcome.\nAn adjustment set is a group of variables that, when controlled for, block backdoor paths in a DAG. By controlling for these variables, we can isolate the causal effect of the treatment on the outcome and reduce the risk of confounding.\nM-Bias is a type of selection bias that occurs when we adjust for a collider that is part of an M-shaped path in a DAG. This can open a backdoor path and create a spurious association between the treatment and the outcome.\nAn instrumental variable is a variable that is correlated with the treatment but not with the outcome, except through its effect on the treatment. Instrumental variables can be used to estimate the causal effect of the treatment even when there are unmeasured confounders.\nA precision variable is a variable that is associated with the outcome but not with the treatment. Including precision variables in a model can help to reduce the variance of the estimate of the treatment effect. A confounder, on the other hand, is a variable that is associated with both the treatment and the outcome and can distort the relationship between them.\nTipping point analysis is a sensitivity analysis that explores how strong an unmeasured confounder would have to be to change the conclusions of a study. This is useful for assessing the robustness of causal inferences to the presence of unmeasured confounders.\nInverse Probability Weighting (IPW) is a statistical technique that can be used to adjust for confounding in causal inference. IPW involves creating weights for each individual based on their probability of receiving the treatment, given their observed characteristics. By weighting the data, we can create a pseudo-population in which the treatment and control groups are more comparable.\n\nEssay Questions\n\nDescribe the three main building blocks of the Rubin Causal Model (RCM) and discuss how they relate to the concept of potential outcomes.\nExplain the importance of identifying and controlling for confounders in causal inference. Discuss different methods for controlling for confounding, such as regression adjustment and inverse probability weighting.\nExplain how Directed Acyclic Graphs (DAGs) can be used to represent causal relationships and identify potential sources of bias. Provide an example of a DAG and discuss how it can be used to determine an appropriate adjustment set.\nDiscuss the challenges of estimating causal effects in observational studies. Describe different types of selection bias and explain how they can affect the validity of causal inferences.\nExplain the concept of sensitivity analysis and discuss its importance in causal inference. Describe different types of sensitivity analyses and provide examples of how they can be used to assess the robustness of causal conclusions.\n\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_4.html",
    "href": "weeks/BSMM_8740_week_4.html",
    "title": "Week 4 - The TidyModels Package",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 4 - Sunday, Oct 06, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_4.html#prepare",
    "href": "weeks/BSMM_8740_week_4.html#prepare",
    "title": "Week 4 - The TidyModels Package",
    "section": "Prepare",
    "text": "Prepare\n📖 Read A Review of R Modeling Fundamentals\n📖 Read Build a Model\n📖 Read A gentle Introduction to Tidymodels\n📖 Take a look at Tidymodels: tidy machine learning in R\n📖 Check out Tidymodels Cheatsheet: Tidymodels Functions",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_4.html#participate",
    "href": "weeks/BSMM_8740_week_4.html#participate",
    "title": "Week 4 - The TidyModels Package",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 4 -The TidyModels Package",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_4.html#perform",
    "href": "weeks/BSMM_8740_week_4.html#perform",
    "title": "Week 4 - The TidyModels Package",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 4 -The TidyModels Package\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html",
    "href": "weeks/BSMM_8740_week_10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\nDue date: Project proposal due Fri, Mar 18 at 5:00 pm.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html#prepare",
    "href": "weeks/BSMM_8740_week_10.html#prepare",
    "title": "Week 10",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Logistic regressionStatistical rethinking with brms, ggplot2, and the tidyverse: Second edition",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html#participate",
    "href": "weeks/BSMM_8740_week_10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 10 - Bayesian methods",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html#perform",
    "href": "weeks/BSMM_8740_week_10.html#perform",
    "title": "Week 10",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 10 - Bayesian methods\n✍️ HW 3 - Logistic regression and log transformation",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html#study",
    "href": "weeks/BSMM_8740_week_10.html#study",
    "title": "Week 10",
    "section": "Study",
    "text": "Study\n\nShort Answer Questions\nInstructions: Answer the following questions in 2-3 sentences each.\n\nWhat is the fundamental difference between Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation in the context of Bayesian linear regression?\nExplain the concept of conjugate priors and provide one example.\nWhat are the limitations of using conjugate priors in Bayesian modeling?\nDescribe the key components of a generative Bayesian model.\nList three advantages of using generative Bayesian models.\nHow does the BRMS package simplify Bayesian analysis in R?\nWhat information can be gleaned from the ‘Regression Coefficients’ section of BRMS output?\nExplain the purpose and interpretation of trace plots in assessing MCMC convergence.\nWhat are posterior predictive checks, and how are they useful in evaluating model fit?\nWhat is the purpose of calculating the ‘leave-one-out cross-validation’ (LOO-CV)?\n\n\n\nShort Answer Key\n\nMLE finds parameter values that maximize the likelihood of observing the data, while MAP finds parameter values that maximize the posterior probability, taking into account both the likelihood of the data and the prior distribution of the parameters.\nConjugate priors are prior distributions that, when combined with a particular likelihood function, result in a posterior distribution that belongs to the same family as the prior. This simplifies calculations. An example is the Beta prior for a Binomial likelihood.\nConjugate priors can be restrictive, limiting the choice of priors to specific families. They may not accurately reflect complex prior beliefs or be suitable for complex models with non-standard likelihoods.\nA generative Bayesian model includes: defining priors for parameters, specifying the likelihood function that connects data to parameters, performing posterior inference using Bayes’ theorem, and potentially incorporating latent variables or hierarchical structures.\nGenerative Bayesian models offer interpretability by explicitly modeling the data generation process, strong predictive power due to learning underlying data structures, and robust uncertainty quantification for better decision-making.\nBRMS provides a user-friendly formula-based interface for specifying and fitting Bayesian models in R, leveraging the power of Stan without requiring users to write raw Stan code.\nThe ‘Regression Coefficients’ section provides the estimated posterior means, standard errors, credible intervals, and convergence diagnostics for the regression coefficients in the model.\nTrace plots visualize the sampled values of parameters across MCMC iterations. They help assess convergence by checking for stationarity (horizontal trend), good mixing (overlapping chains), and low autocorrelation (random fluctuations).\nPosterior predictive checks involve simulating data from the fitted model and comparing these simulations to the observed data. This helps assess if the model can generate data similar to the observed data and identify potential misspecifications.\nLOO-CV estimates the model’s predictive performance on unseen data. It involves repeatedly fitting the model on a subset of data and evaluating its predictions on the held-out data point, providing a robust measure of generalization ability.\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_8.html",
    "href": "weeks/BSMM_8740_week_8.html",
    "title": "Week 8 - Causality: Methods",
    "section": "",
    "text": "Important\n\n\n\nDue date: Lab 8 - Sunday, Nov 10, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_8.html#prepare",
    "href": "weeks/BSMM_8740_week_8.html#prepare",
    "title": "Week 8 - Causality: Methods",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Chp 10-14 in: Causal Inference for the Brave and True\n📖 Read Chp 8-10 in: Causal Inference in R\n📖 Read Chp 8-10 in: Causal Inference in R\n📖 Read Matching and Subclassification, Chp 5 in: Causal Inference the Mixtape",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_8.html#participate",
    "href": "weeks/BSMM_8740_week_8.html#participate",
    "title": "Week 8 - Causality: Methods",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 8 - Causality: Methods",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_8.html#perform",
    "href": "weeks/BSMM_8740_week_8.html#perform",
    "title": "Week 8 - Causality: Methods",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 8 - Causality: Methods",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_8.html#study",
    "href": "weeks/BSMM_8740_week_8.html#study",
    "title": "Week 8 - Causality: Methods",
    "section": "Study",
    "text": "Study\n\nShort Answer Questions\nInstructions: Answer the following questions in 2-3 sentences each.\n\nExplain the concept of Inverse Probability Weighting (IPW) and its purpose in causal inference.\nWhat are stabilized weights in IPW, and why are they used?\nHow does regression adjustment help in estimating causal effects?\nDescribe the principle of doubly robust estimation and its advantages.\nExplain the concept of finite sample bias and factors influencing its likelihood.\nWhat is the main idea behind matching in causal inference?\nExplain the concept of subclassification and its limitations in practice.\nHow does matching estimator work, and how is it different from regression adjustment?\nExplain the core principle of difference-in-differences (DID) estimation.\nWhat is the parallel trends assumption in DID, and why is it important?\n\n\n\nShort Answer Key\n\nIPW is a technique that creates a pseudo-population where the distribution of confounders is balanced between the treatment and control groups. This helps to estimate causal effects by reducing bias due to confounding.\nStabilized weights are a modified version of IPW weights that are multiplied by the mean of the treatment or a subset of baseline covariates. They are used to reduce the variance of the IPW estimator and improve stability.\nRegression adjustment estimates causal effects by building a regression model that predicts the outcome based on the treatment and confounders. By controlling for confounders in the model, it aims to isolate the effect of the treatment.\nDoubly robust estimation combines propensity score weighting and regression adjustment. It provides an unbiased estimate of the causal effect if either the propensity score model or the outcome model is correctly specified, offering greater robustness.\nFinite sample bias occurs when estimates of causal effects are biased due to the limited size of the sample. The likelihood of finite sample bias depends on the estimand, overlap between treatment groups, and sample size.\nMatching aims to create comparable treatment and control groups by pairing individuals based on similarities in their observed characteristics (confounders). This reduces bias by creating a more balanced comparison.\nSubclassification involves dividing the data into subgroups based on confounder values and estimating the causal effect within each subgroup. Its practical limitation is the curse of dimensionality, as the number of subgroups grows exponentially with the number of confounders.\nMatching estimator directly pairs treated and control units based on proximity in confounder space, while regression adjustment uses a model to control for confounders. Matching focuses on finding similar individuals, while regression statistically adjusts for differences.\nDID estimates the causal effect of a treatment by comparing the change in outcomes over time between a treatment group and a control group. It leverages both time and group comparisons to isolate the treatment effect.\nThe parallel trends assumption in DID states that the treatment and control groups would have followed similar trends in the outcome variable had the treatment not been implemented. It is crucial because DID estimates the treatment effect based on the difference in these trends.\n\n\n\nEssay Questions\n\nDiscuss the strengths and limitations of both Inverse Probability Weighting (IPW) and regression adjustment in estimating causal effects. When might you prefer one method over the other?\nExplain the concept of doubly robust estimation in detail. Why is it considered “doubly robust”? Provide an example scenario where doubly robust estimation would be particularly advantageous.\nDiscuss the challenges associated with matching in causal inference. How can these challenges be addressed in practice? Explain different distance metrics and their implications in matching.\nDescribe the difference-in-differences (DID) estimator and its underlying assumptions. What are potential threats to the validity of DID estimates, and how can they be mitigated?\nExplain the concept of fixed effects in panel data analysis. How do fixed effects help to control for unobserved confounders? Discuss the advantages and limitations of using fixed effects models.\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\n🔗 on Duke Container Manager\n\n\nCourse GitHub organization\n🔗 on GitHub\n\n\nDiscussion forum\n🔗 on Sakai\n\n\nLecture streaming and recordings\n🔗 on Panopto\n\n\nGradebook\n🔗 on Sakai\n\n\nVirtual meetings\n🔗 on Sakai"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html",
    "href": "labs/BSMM_8740_lab_1.html",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "",
    "text": "This lab will go through many of the same operations we’ve demonstrated in class. The main goal is to reinforce our understanding of tidy data, the tidyverse and the pipe, which we will be using throughout the course.\nAs the labs progress, you are encouraged to explore beyond what the labs require; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R and the tidyverse. Today we begin with exercises in the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\n\n\nBy the end of the lab, you will…\n\nBe familiar with the workflow using R, RStudio, Git, and GitHub\nHave practiced version control using GitHub"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#introduction",
    "href": "labs/BSMM_8740_lab_1.html#introduction",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "",
    "text": "This lab will go through many of the same operations we’ve demonstrated in class. The main goal is to reinforce our understanding of tidy data, the tidyverse and the pipe, which we will be using throughout the course.\nAs the labs progress, you are encouraged to explore beyond what the labs require; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R and the tidyverse. Today we begin with exercises in the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\n\n\nBy the end of the lab, you will…\n\nBe familiar with the workflow using R, RStudio, Git, and GitHub\nHave practiced version control using GitHub"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#getting-started",
    "href": "labs/BSMM_8740_lab_1.html#getting-started",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2025-lab-1-[your github username] repository .\nCreate an R project using your 2025-lab-1-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2025-lab-1.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#packages",
    "href": "labs/BSMM_8740_lab_1.html#packages",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Packages",
    "text": "Packages\n\n\n\n\n\n\nNote\n\n\n\nThe code below will install (if necessary) and load the packages needed in today’s exercises\n\n\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(tidyverse, Lahman, magrittr, gt, gtExtras, ggplot2, skimr\n                 , dplyr, gt, gtExtras, here, readr, rmarkdown)\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#data-yearly-statistics-and-standings-for-baseball-teams",
    "href": "labs/BSMM_8740_lab_1.html#data-yearly-statistics-and-standings-for-baseball-teams",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Data: Yearly statistics and standings for baseball teams",
    "text": "Data: Yearly statistics and standings for baseball teams\nToday’s data is all baseball statistics. The data is in the Lahman package.\n\nView the data\nBefore doing any analysis, you may want to get quick view of the data. This is useful when you’ve imported data to see if your data imported correctly. We can use the view() function to see the entire data set in RStudio. Type the code below in the Console to view the entire dataset.\n\ndim(Teams)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#data-dictionary",
    "href": "labs/BSMM_8740_lab_1.html#data-dictionary",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nThe variable definitions are found in the help for Teams\n\n?Teams\n\n\nExercises\nWrite all code and narrative in your R Markdown file where indicated. Write all narrative in complete sentences. Throughout the assignment, you should periodically render your Quarto document to ensure that all code executes and that your document format is intact, save, stage & commit the changes in the Git pane, and push the updated files to your repository. This ensures that your work is saved.\n\n\n\n\n\n\nTip\n\n\n\nMake sure we can read all of the code in your quarto document. This means you will need to break up long lines of code. One way to help avoid long lines of code is is start a new line after every pipe (%&gt;% or |&gt;) and plus sign (+).\n\n\n\n\nExercise 1\nThe view() function helps us get a quick view of the dataset, but let’s get more detail about its structure. Viewing a summary of the data is a useful starting point for data analysis, especially if the dataset has a large number of observations (rows) or variables (columns). Run the code below to use the dplyr::glimpse() function to see a summary of the ikea dataset.\n\ndplyr::glimpse(Teams[1:10,])\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\nHow many observations are in the Teams dataset? How many variables/measurements?\n\ndplyr::glimpse(Teams[1:10,])\n\nHow many character columns/measurements have missing variables?\n\n# run a data exploration here using the skimr package.\nTeams |&gt; skimr::skim()\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn your lab-1.qmd document you’ll see that we already added the code required for most exercises as well as a sentence where you can fill in the blanks to report the answer.\nAlso note that the code chunk as a label: glimpse-data. It’s not required, but it is good practice and highly encouraged to label your code chunks in this way. If there is an error when you render your document, the code-chunk label will identify where the error is.\n\n\n\n\nExercise 2\nBen Baumer worked for the New York Mets from 2004 to 2012. What was the team W/L record during those years? Use filter() and select() to quickly identify only those pieces of information that we care about.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# fill in the blanks and ensure your code produces the correct result\nmets &lt;- Teams  %&gt;% \n  dplyr::filter(teamID == \"NYN\")\nmy_mets &lt;- mets %&gt;% \n  dplyr::filter(_)\nmy_mets %&gt;% \n  dplyr::select(_,_,_,_)\n\n\n\n\n\nExercise 3\nWe’ve answered the simple question of how the Mets performed during the time that Ben was there, but since we are data scientists, we are interested in deeper questions. For example, some of these seasons were subpar—the Mets had more losses than wins. Did the team just get unlucky in those seasons? Or did they actually play as badly as their record indicates?\nIn order to answer this question, we need a model for expected winning percentage. It turns out that one of the most widely used contributions to the field of baseball analytics (courtesy of Bill James) is exactly that. This model translates the number of runs4 that a team scores and allows over the course of an entire season into an expectation for how many games they should have won. The simplest version of this model is this:\nŴpct=11+(RARS)2\n\\hat{\\text{W}}_{\\text{pct}}=\\frac{1}{1+\\left(\\frac{\\text{RA}}{\\text{RS}}\\right)^{2}}\n\nwhere RA\\text{RA} is the number of runs the team allows to be scored, RS\\text{RS} is the number of runs that the team scores, and Ŵpct\\hat{\\text{W}}_{\\text{pct}} is the team’s expected winning percentage. Luckily for us, the runs scored and allowed are present in the Teams table, so let’s grab them and save them in a new data frame.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# fill in the blanks and ensure your code produces the correct result\nmets_ben &lt;- Teams |&gt;\n  dplyr::select(_, _, _, _, _, _) |&gt;\n  dplyr::filter(_ == \"NYN\" & _ %in% 2004:2012)\nmets_ben\n\nFirst, note that the runs-scored variable is called R in the Teams table, but to stick with our notation we want to rename it RS.\n\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::rename(_ = _)    # new name = old name\nmets_ben\n\n\n\n\nThis is a good place to save, stage, commit, and push changes to your remote lab-1 repository. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message (e.g., “Completed exercises 1 - 3”), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nNext, we need to compute the team’s actual winning percentage in each of these seasons. Thus, we need to add a new column to our data frame, and we do this with the mutate() command.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# add the calculation using the formula above\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::mutate(WPct = _)\nmets_ben\n\nThe expected number of wins is then equal to the product of the expected winning percentage times the number of games.\n\n# fill in the blanks and ensure your code produces the correct result\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::mutate(W_hat = _)\nmets_ben\n\n\n\n\n\nExercise 5\nIn this case, the Mets’ fortunes were better than expected in three of these seasons, and worse than expected in the other six.\nIn how many seasons were the Mets better than expected? How many were they worse than expected?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# repeat the better_than/ worse_than calculation for all the years in the dataset: show your work.\n\n\n\n\nThis is a good place to save, stage, commit, and push changes to your remote lab-1 repo. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message (e.g., “Completed exercises 4 and 5”), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 6\nNaturally, the Mets experienced ups and downs during Ben’s time with the team. Which seasons were best? To figure this out, we can simply sort the rows of the data frame.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\ndplyr::arrange(mets_ben, _)\n\n\n\n\n\nExercise 7\nIn 2006, the Mets had the best record in baseball during the regular season and nearly made the World Series. How do these seasons rank in terms of the team’s performance relative to our model?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nmets_ben %&gt;% \n  dplyr::mutate(Diff = _) |&gt;\n  dplyr::arrange(_)\n\nSummarize the Mets performance:\n\n# fill in the blanks and ensure your code produces the correct result\nmets_ben |&gt;\n  dplyr::summarize(\n    num_years = _, \n    total_W = _, \n    total_L = _, \n    total_WPct = _, \n    sum_resid = _\n  )\n\nIn these nine years, the Mets had a combined record of ? wins and ? losses, for an overall winning percentage of _?.\n\n\n\nThis is a good place to save, stage, commit, and push changes to your remote lab-1 repo. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message (e.g., “Completed exercises 6 - 8”), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 8\nDiscretize the years into three chunks: one for each of the three general managers under whom Ben worked. Jim Duquette was the Mets’ general manager in 2004, Omar Minaya from 2005 to 2010, and Sandy Alderson from 2011 to 2012.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nmets_ben &lt;- mets_ben %&gt;% \n  dplyr::mutate(\n    gm = ifelse(\n      yearID == _, \n      _, \n      ifelse(\n        yearID &gt;= _, \n        _, \n        _)\n    )\n  )\n\nAlternatively, we use the case_when function\n\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::mutate(\n    gm = dplyr::case_when(\n      yearID == _ ~ _, \n      yearID &gt;= _ ~ _, \n      TRUE ~ _\n    )\n  )\n\n\n\n\n\nExercise 9\nThe following dataset is the basis of a model that predicts which businesses are likely to have customer churn at the start of 2015, based on the business type and incorporation_date. This question will give you some practice using the various tidyr:: pivot operations.\n\nset.seed(42)\n\n# read data and drop column 1 (it contains row numbers and doesn't have a column name)\ndf &lt;- readr::read_csv(\"data/monthly_data.csv\", show_col_types = FALSE, col_select = -1)\n\n# Have a glimpse of the data\nglimpse(df)\n\nRows: 902\nColumns: 27\n$ company_id            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, …\n$ `2014-01-01_payments` &lt;dbl&gt; 0, 1, 6, 8, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4, 43, 34…\n$ `2014-02-01_payments` &lt;dbl&gt; 0, 1, 6, 4, 0, 2, 3, 0, 0, 0, 2, 0, 11, 1, 51, 5…\n$ `2014-03-01_payments` &lt;dbl&gt; 0, 3, 6, 7, 39, 1, 1, 6, 0, 0, 0, 8, 0, 1, 44, 4…\n$ `2014-04-01_payments` &lt;dbl&gt; 1, 2, 6, 7, 0, 3, 7, 50, 1, 0, 0, 2, 0, 0, 47, 5…\n$ `2014-05-01_payments` &lt;dbl&gt; 0, 2, 6, 1, 54, 1, 4, 119, 0, 1, 0, 0, 0, 5, 46,…\n$ `2014-06-01_payments` &lt;dbl&gt; 1, 1, 7, 2, 0, 2, 1, 151, 3, 0, 0, 0, 0, 2, 81, …\n$ `2014-07-01_payments` &lt;dbl&gt; 0, 1, 8, 2, 0, 2, 7, 182, 0, 0, 0, 3, 0, 0, 91, …\n$ `2014-08-01_payments` &lt;dbl&gt; 0, 1, 7, 4, 22, 1, 2, 167, 0, 0, 0, 2, 0, 0, 93,…\n$ `2014-09-01_payments` &lt;dbl&gt; 0, 1, 8, 3, 0, 1, 5, 180, 0, 0, 0, 0, 9, 5, 88, …\n$ `2014-10-01_payments` &lt;dbl&gt; 0, 4, 8, 5, 0, 2, 8, 157, 1, 0, 0, 0, 2, 1, 86, …\n$ `2014-11-01_payments` &lt;dbl&gt; 0, 3, 9, 5, 0, 1, 2, 105, 0, 0, 0, 0, 0, 0, 93, …\n$ `2014-12-01_payments` &lt;dbl&gt; 0, 3, 9, 9, 0, 3, 8, 57, 0, 0, 0, 0, 0, 0, 104, …\n$ `2014-01-01_mandates` &lt;dbl&gt; 1, 0, 0, 0, 4, 0, 0, 0, 4, 4, 0, 0, 22, 0, 1, 6,…\n$ `2014-02-01_mandates` &lt;dbl&gt; 2, 0, 0, 0, 31, 1, 0, 2, 3, 8, 0, 0, 20, 12, 0, …\n$ `2014-03-01_mandates` &lt;dbl&gt; 2, 0, 0, 0, 24, 0, 0, 0, 5, 19, 0, 0, 11, 17, 0,…\n$ `2014-04-01_mandates` &lt;dbl&gt; 1, 0, 0, 53, 18, 0, 1, 0, 0, 0, 0, 1, 11, 14, 1,…\n$ `2014-05-01_mandates` &lt;dbl&gt; 0, 0, 2, 0, 8, 0, 0, 0, 0, 0, 0, 1, 15, 0, 0, 3,…\n$ `2014-06-01_mandates` &lt;dbl&gt; 0, 0, 2, 0, 7, 0, 0, 0, 0, 0, 0, 0, 13, 5, 0, 0,…\n$ `2014-07-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 16, 0, 0…\n$ `2014-08-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 8, 0, 0,…\n$ `2014-09-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 2, 1, 1,…\n$ `2014-10-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 16, 1, 0, 2,…\n$ `2014-11-01_mandates` &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 1, 0, …\n$ `2014-12-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 11, 0, 0, 0,…\n$ vertical              &lt;chr&gt; \"gym/fitness\", \"gym/fitness\", \"freelance develop…\n$ incorporation_date    &lt;date&gt; 2013-05-30, 2003-09-25, 2008-10-22, 2005-06-28,…\n\n\nYou’ll notice that the data is not in tidy form: one type of measurement in each column and all measurements that go together in the same row. To put this data into tidy from you’ll need to\n\nTake all the columns with names that start with a date string and use tidyr::pivot_longer to create a column named date containing the original column names and a column named quantity to contain the values.\nIn the date column (which contains strings at this stage), split the date from the remainder of the string, saving the remainder in a column called paymentMandate, and the date string in a column called date. To do this use tidyr::separate_wider_delim with delim = \"_\" and the specified names.\nNext, use tidyr::pivot_wider with names_from = paymentMandate, and values_from = quantity to create payments and mandates columns.\nFinally, use dplyr::mutate to change the type of the data and incorporation_date columns to Date.\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\nWhat is the mean of the sales_per_visit columns/measurement? Are there any grouped observations?\n\n# show your steps and the resulting tibble here:\n\nYour code should produce a tibble that looks like this:\n\n\n# A tibble: 10,824 × 6\n   company_id vertical    incorporation_date date       payments mandates\n        &lt;int&gt; &lt;chr&gt;       &lt;date&gt;             &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1          1 gym/fitness 2013-05-30         2014-01-01        0        1\n 2          1 gym/fitness 2013-05-30         2014-02-01        0        2\n 3          1 gym/fitness 2013-05-30         2014-03-01        0        2\n 4          1 gym/fitness 2013-05-30         2014-04-01        1        1\n 5          1 gym/fitness 2013-05-30         2014-05-01        0        0\n 6          1 gym/fitness 2013-05-30         2014-06-01        1        0\n 7          1 gym/fitness 2013-05-30         2014-07-01        0        0\n 8          1 gym/fitness 2013-05-30         2014-08-01        0        0\n 9          1 gym/fitness 2013-05-30         2014-09-01        0        0\n10          1 gym/fitness 2013-05-30         2014-10-01        0        0\n# ℹ 10,814 more rows\n\n\n\n\n\n\nExercise 10\n\nThe Business Problem\nThe story begins in a fast paced startup. The company is growing fast and the marketing team is looking for ways to increase the sales from existing customers by making them buy more. The main idea is to unlock the potential of the customer base through incentives, in this case a discount. We of course want to measure the effect of the discount on the customer’s behavior. Still, they do not want to waste money giving discounts to users which are not valuable. As always, it is about return on investment (ROI).\nWithout going into specifics about the nature of the discount, it has been designed to provide a positive return on investment if the customer buys more than $1\\$ 1 as a result of the discount. How can we measure the effect of the discount and make sure our experiment has a positive ROI? The marketing team came up with the following strategy:\n\nSelect a sample of existing customers from the same cohort.\nSet a test window of 1 month.\nLook into the historical data of web visits from the last month. The hypothesis is that web visits are a good proxy for the customer’s interest in the product.\nFor customers with a high number of web visits, send them a discount. There will be a hold out group which will not receive the discount within the potential valuable customers based on the number of web visits. For customers with a low number of web visits, do not send them a discount (the marketing team wants to report a positive ROI, so they do not want to waste money on customers which are not valuable). Still, they want to use them to measure the effect of the discount.\nWe also want to use the results of the test to tag loyal customers. These are customers which got a discount (since they showed potential interest in the product) and customers with exceptional sales numbers even if they did not get a discount. The idea is to use this information to target them in the future if the discount strategy is positive.\n\n\n\nThe Data\nThe team collected data from the experiment above and asked the data science team to analyze it and provide insights. In particular they want to know if they should keep the discount strategy. The data consists of the following fields: - visits: Number of visits to the website during the test window. - discount: Whether the customer received a discount or not. - is_loyal: Whether the customer is loyal or not according to the definition above. - sales: Sales in $\\$ during the test window.\n\nPrepare Notebook\nData scientist A was the one in charge of preparing the environment and collecting the data. As an important best practice, they fixed a global seed which initializes the random number generator in order to make sure every part of the analysis was reproducible. This ensures that the calculations are not affected by pure randomness. In addition all the required packages were listed from the start (reproducible R environment).\n\nset.seed(8740)\n\n\n\nRead Data\nThey pulled the data from a csv file and displayed the first 5 measurements.\n\ndata &lt;- readr::read_csv(\"data/sales_dag.csv\", show_col_types = FALSE)\n\ndata |&gt; dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"sample marketing data\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nsample marketing data\n\n\nvisits\ndiscount\nis_loyal\nsales\nsales_per_visit\n\n\n\n\n12\n0\n0\n13.34830\n1.1123585\n\n\n26\n1\n1\n21.70125\n0.8346635\n\n\n13\n0\n0\n14.70040\n1.1308004\n\n\n24\n0\n0\n20.37734\n0.8490557\n\n\n14\n0\n0\n12.63372\n0.9024089\n\n\n\n\n\n\n\nThey then checked for missing values and whether the measurements were in the correct format.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\nWhat is the mean of the sales_per_visit columns/measurement? Are there any grouped observations?\n\n# run a data exploration here using the skimr package.\ndata |&gt; skimr::skim()\n\n\n\n\n\nExploratory Data Analysis\nAs part of the project scope, the data science team in charge of the analysis was asked to provide a summary of the data. The team was also asked to provide a visualization of the data to help the marketing team understand the data better. Data scientist A took over this task.\nThey started by looking at the share of customers which received a discount:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# calculate the % share of customers receiving a discount vs the % not receiving a discount\n\nSimilarly for the share of customers which are loyal:\n\n# calculate the % share of customers that are 'loyal' vs not 'loyal'\n\nTo understand these features better, they also looked at a cross-tab table:\n\n# build a cross-tab table of 'loyal' customers vs customers getting a discount\n\n\n\nNote that all customers with discount are loyal (as required) and that there are loyal users which did not receive a discount. This is because they had exceptional sales numbers. Verify this:\n\ndata |&gt; dplyr::mutate(id = dplyr::row_number(), .before = 1) |&gt; \n  dplyr::filter(discount == 0) |&gt; \n  dplyr::arrange( desc(sales) ) |&gt; \n  dplyr::slice_head(n=10) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"Sales: loyal customers vs others\") |&gt; \n  gtExtras::gt_theme_espn()\n\nThe loyal customers are the top ones in terms of sales. This is good news. It means that the definition of loyal customers is consistent with the data.\nIn order to have orders of magnitude for the sales, the data scientist provided some summary statistics table:\n\ngtExtras::gt_plt_summary(data)\n\nTo have a better glimpse of the data, the data scientist also provided a histogram of the sales:\n\ndata |&gt; \n  ggplot(aes(x=sales)) +\n  geom_histogram(aes(y = ..density..), bins = 30, colour = 1, fill = \"white\") +\n  geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25) +\n  labs(title = \"Sales Distribution\") +\n  theme_minimal()\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 1!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyone’s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#grading",
    "href": "labs/BSMM_8740_lab_1.html#grading",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#resources-for-additional-practice-optional",
    "href": "labs/BSMM_8740_lab_1.html#resources-for-additional-practice-optional",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Resources for additional practice (optional)",
    "text": "Resources for additional practice (optional)\n\nChapter 2: Get Started Data Visualization by Kieran Healy\nChapter 3: Data visualization in R for Data Science by Hadley Wickham\nRStudio Cloud Primers\n\nVisualization Basics: https://rstudio.cloud/learn/primers/1.1\nWork with Data: https://rstudio.cloud/learn/primers/2\nVisualize Data: https://rstudio.cloud/learn/primers/3"
  },
  {
    "objectID": "labs/BSMM_8740_lab_git.html",
    "href": "labs/BSMM_8740_lab_git.html",
    "title": "Lab Git",
    "section": "",
    "text": "This lab is designed to reinforce your use of git and GitHub, the collaboration and version control system that we will be using throughout the course.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\nToday we begin with an introductions to GIT followed by exercises in the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\n\n\nBy the end of the lab, you will…\n\nHave practiced version control using GitHub\nBe familiar with the workflow using R, RStudio, Git, and GitHub\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!"
  },
  {
    "objectID": "labs/BSMM_8740_lab_git.html#introduction",
    "href": "labs/BSMM_8740_lab_git.html#introduction",
    "title": "Lab Git",
    "section": "",
    "text": "This lab is designed to reinforce your use of git and GitHub, the collaboration and version control system that we will be using throughout the course.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\nToday we begin with an introductions to GIT followed by exercises in the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\n\n\nBy the end of the lab, you will…\n\nHave practiced version control using GitHub\nBe familiar with the workflow using R, RStudio, Git, and GitHub\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!"
  },
  {
    "objectID": "labs/BSMM_8740_lab_git.html#lets-git-started",
    "href": "labs/BSMM_8740_lab_git.html#lets-git-started",
    "title": "Lab Git",
    "section": "Let’s Git Started",
    "text": "Let’s Git Started\n\n1. Register a Github account\nRegister an account with GitHub. It’s free!\n\nhttps://github.com\n\n\nUsername advice\n\nIncorporate your actual name! People like to know who they’re dealing with. Also it makes your username easier for people to guess or remember.\nShorter is better than longer.\nMake it timeless. Don’t highlight your current university, employer, or place of residence, e.g. JennyFromTheBlock.\n\nYou can change your username later, but better to get this right the first time.\n\nhttps://help.github.com/articles/changing-your-github-username/\nhttps://help.github.com/articles/what-happens-when-i-change-my-username/\n\n\n\n\n\n\n\nNote\n\n\n\nWe will be switching between the console and the terminal, in this lab and others. The Console is where you can execute R code, while the Terminal is where you can execute system functions like git.\n\n\n\n\n\n\n\n\n\n\n2. Git already installed?\nGo to the Terminal tab in RStudio and enter git --version to see its version:\n\ngit --version\n\nIf this instruction gives an error, it’s possible that git is not installed on your machine. If so, let your instructor know.\n\n\n3. Introduce yourself to Git\n\n\n\n\n\n\nImportant\n\n\n\nmake sure that the package usethis has been installed. You can check under the packages tab in the file & plots viewer (e.g., do a search).\n\n\nYou can set your Git user name and email from within R (i.e. go back to the Console tab):\n\nusethis::use_git_config(\n  # user.name does not have to be your GitHub user name\n  user.name = \"Jane Doe\"\n  # user.email MUST be the email associated with your GitHub account.\n  , user.email = \"jane@example.org\"\n)\n\n\n\n\n\n\n\nNote\n\n\n\nYour commits will be labelled with this user name, so make it informative to potential collaborators and future you.\n\n\n\n\n4. Set up personal access tokens for HTTPS\nThe password that you use to login to GitHub’s website is NOT an acceptable credential when talking to GitHub as a Git server. You can learn more in their blog post Token authentication requirements for Git operations.\nThe recommendation to use a personal access token (PAT) is exactly what we cover here. First you need to create your PAT, and you can do this from R (in the Console):\n\nusethis::create_github_token()\n\nThe usethis approach takes you to a pre-filled form with pre-selected some recommended scopes, which you can look over and adjust before clicking “Generate token”.\nIt is a very good idea to describe the token’s purpose in the Note field, because one day you might have multiple PATs. We recommend naming each token after its use case, such as the computer or project you are using it for, e.g. “personal-macbook-air” or “lab1-course-8740”. In the future, you will find yourself staring at this list of tokens, because inevitably you’ll need to re-generate or delete one of them. Make it easy to figure out which token you’ve come here to fiddle with.\n\n\n\n\n\n\nTip\n\n\n\nIf this is your first time generating a PAT, just accept the defaults and scroll to the bottom of the page and click the green Generate token button.\n\n\n\n4.1 Click “Generate token”.\nYou won’t be able to see this token again, so don’t close or navigate away from this browser window until you store the PAT. Copy the PAT to the clipboard or a text file in RStudio.\nTreat this PAT like a password! Do not ever hard-wire your PAT into your code! A PAT should always be retrieved implicitly, for example, from the Git credential store, a safe place, where command line Git, RStudio, and R packages can discover it.\n\n\n4.2 Save your PAT\n\nCopy the generated PAT to a secure, long-term system for storing secrets, like 1Password or LastPass.\nemail it to yourself.\ncopy it onto a piece of scrap paper.\n\n\n\n4.3 Store your PAT in the Git credential store\nFinally, we store the PAT in a safe place where command line Git, RStudio, and R packages can discover it. To do this call gitcreds::gitcreds_set(). If you don’t have a PAT stored already, it will prompt you to enter your PAT. Paste!\n\ngitcreds::gitcreds_set()\n\nInstead of saving your PAT you could just re-generate the PAT each lab session and re-store it. If you accept the default 30-day expiration period, this is a workflow you’ll be using often anyway.\nOn github.com, assuming you’re signed in, you can manage your personal access tokens from https://github.com/settings/tokens, also reachable via Settings &gt; Developer settings &gt; Personal access tokens.\n\n\n\n\n\n\nImportant\n\n\n\nGiven that the machines start from the same initial state each lab session, you will follow the above steps to initial your machine at the start of each lab session.\n\n\n\n\n\n5. How Git works\nGit has three storages locally: a Working directory, Staging Area, and a Local repository.\n𝟭. 𝗪𝗼𝗿𝗸𝗶𝗻𝗴 𝗗𝗶𝗿𝗲𝗰𝘁𝗼𝗿𝘆 - is where you work, and your files live (“untracked”). GIT is not aware of these files.\n𝟮. 𝗦𝘁𝗮𝗴𝗶𝗻𝗴 𝗔𝗿𝗲𝗮 - When you stage your changes, GIT will start tracking and saving your changes with files. These changes are stored in the .git directory.\n𝟯. 𝗟𝗼𝗰𝗮𝗹 𝗥𝗲𝗽𝗼𝘀𝗶𝘁𝗼𝗿𝘆 - is the area where everything is saved (commits) in the .git directory. So, when you want to move your files from Staging Area to Local Repository, you can use the git commit command. After this, your Staging area will be empty. If you want to see what is in the Local repository, try git log.\nThe workflow looks like this:\n\n\n\nGit workflow\n\n\nYou are now ready interact with GitHub via RStudio!\n\n\nClone the repo & start new RStudio project\n\nFirst make sure you are logged into your own Github account on a web browser.\nNext, in a new browser tab, go to the course organization site at BSMM-8740-Fall-2023 on GitHub. Click on the repo BSMM-lab-1. It contains the starter documents you need to complete the lab.\nClick on the green Use this template button and select Create a new repository. This will make a copy of BSMM-lab-1 in your own github account.\nNext, go back to your Github acount and select your copy of BSMM-lab-1. Click on the green CODE button, select Use HTTPS (this might already be selected by default, and if it is, you’ll see the text Clone with HTTPS). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ➛ New Project ➛Version Control ➛ Git.\nCopy and paste the URL of your assignment repo (the clipboard copy you made in step 4) into the dialog box Repository URL. The project directory name should be automatically populated, but make sure you select a directory in Create project as a subdirectory of.\nFinally, click Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick bsmm-lab-1.qmd to open the template R Markdown file. This is where you will write up your code and narrative for the lab.\n\n\n\nR and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\nYAML\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (`.qmd`) file in your project, change the author name to your name, and render the document. Examine the rendered HTML document in the Files pane.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your *.qmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\nMore on the basic use of git here."
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html",
    "href": "labs/BSMM_8740_lab_6.html",
    "title": "lab 6 - Time Series Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflows with recipes, parsnip models, rsample cross validations, and model comparison in the context of time series data."
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#introduction",
    "href": "labs/BSMM_8740_lab_6.html#introduction",
    "title": "lab 6 - Time Series Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflows with recipes, parsnip models, rsample cross validations, and model comparison in the context of time series data."
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#getting-started",
    "href": "labs/BSMM_8740_lab_6.html#getting-started",
    "title": "lab 6 - Time Series Methods",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-6-[your github username] repository .\nCreate an R project using your 2024-lab-6-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-6.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#packages",
    "href": "labs/BSMM_8740_lab_6.html#packages",
    "title": "lab 6 - Time Series Methods",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#the-data",
    "href": "labs/BSMM_8740_lab_6.html#the-data",
    "title": "lab 6 - Time Series Methods",
    "section": "The Data",
    "text": "The Data\nToday we will be using electricity demand data, based on a paper by James W Taylor:\n\nTaylor, J.W. (2003) Short-term electricity demand forecasting using double seasonal exponential smoothing. Journal of the Operational Research Society, 54, 799-805.\n\nThe data can be found in the timetk package as timetk::taylor_30_min, a tibble with dimensions: 4,032 x 2\n\ndate: A date-time variable in 30-minute increments\nvalue: Electricity demand in Megawatts\n\n\ndata &lt;- timetk::taylor_30_min"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-1-eda",
    "href": "labs/BSMM_8740_lab_6.html#exercise-1-eda",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nPlot the data using the functions timetk::plot_time_series, timetk::plot_acf_diagnostics (using 100 lags), and timetk::plot_seasonal_diagnostics.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# timetk::plot_time_series\n\n\n# timetk::plot_acf_diagnostics\n\n\n# timetk::plot_seasonal_diagnostics"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-2-time-scaling",
    "href": "labs/BSMM_8740_lab_6.html#exercise-2-time-scaling",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 2: Time scaling",
    "text": "Exercise 2: Time scaling\nThe raw data has 30 minute intervals between data points. Downscale the data to 60 minute intervals, using timetk::summarise_by_time, revising the electricity demand (value) variable by adding the two 30-minute intervals in each 60-minute interval. Assign the downscaled data to the variable taylor_60_min.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# downscale the data (down to a lower frequency of measurement)\nset.seed(8740)\ntaylor_60_min &lt;- ?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-3-training-and-test-datasets",
    "href": "labs/BSMM_8740_lab_6.html#exercise-3-training-and-test-datasets",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 3: Training and test datasets",
    "text": "Exercise 3: Training and test datasets\n\nSplit the new (60 min) time series into training and test sets using timetk::time_series_split\n\nset the training period (‘initial’) to ‘2 months’ and the assessment period to ‘1 weeks’\n\nPrepare the data resample specification with timetk::tk_time_series_cv_plan() and plot it with timetk::plot_time_series_cv_plan\nSeparate the training and test data sets using rsample .\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# split\n\n\n# plot splits\n\n\n# separate into train and test data sets"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-4-recipes",
    "href": "labs/BSMM_8740_lab_6.html#exercise-4-recipes",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 4: recipes",
    "text": "Exercise 4: recipes\n\nCreate a base recipe (base_rec) using the formula value ~ date and the training data. This will be used for non-regression models\nCreate a recipe (lm_rec) using the formula value ~ . and the training data. This will be used for regression models. For this recipe:\n\nadd time series signature features using timetk::step_timeseries_signature with the appropriate argument,\nadd a step to select the columns value, date_index.num, date_month.lbl, date_wday.lbl, date_hour ,\nadd a normalization step targeting date_index.num ,\nadd a step to mutate date_hour, changing it to a factor,\nadd a step to one-hot encode nominal predictors.\n\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nbase_rec &lt;- ?\n  \nlm_rec &lt;- ?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-5-models",
    "href": "labs/BSMM_8740_lab_6.html#exercise-5-models",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 5 models",
    "text": "Exercise 5 models\nNow we will create a several models to estimate electricity demand, as follows\n\nCreate a model specification for an exponential smoothing model using engine ‘ets’\nCreate a model specification for an arima model using engine ‘auto_arima’\nCreate a model specification for a linear model using engine ‘glmnet’ and penalty = 0.02, mixture = 0.5\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nmodel_ets &lt;- ?\n  \nmodel_arima &lt;- ?\n  \nmodel_lm &lt;- ?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-6-model-fitting",
    "href": "labs/BSMM_8740_lab_6.html#exercise-6-model-fitting",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 6 model fitting",
    "text": "Exercise 6 model fitting\nCreate a workflow for each model using workflows::workflow.\n\nAdd a recipe to the workflow\n\nthe linear model uses the lm_rec recipe created above\nthe ets and arima models use the base_rec recipe created above\n\nAdd a model to each workflow\nFit with the training data\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nworkflow_fit_ets &lt;- ?\n  \nworkflow_fit_arima &lt;- ?\n  \nworkflow_fit_lm &lt;- ?\n\n\n\n\nThis is a good place to save, stage, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-7-calibrate",
    "href": "labs/BSMM_8740_lab_6.html#exercise-7-calibrate",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 7: calibrate",
    "text": "Exercise 7: calibrate\nIn this exercise we’ll use the testing data with our fitted models.\n\nCreate a table with the fitted workflows using modeltime::modeltime_table\nUsing the table you just created, run a calibration on the test data with the function modeltime::modeltime_calibrate.\nCompare the accuracy of the models using the modeltime::modeltime_accuracy() on the results of the calibration\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nmodel_tbl &lt;- modeltime::modeltime_table( ? )\n\ncalibration_tbl &lt;- ?\n  \ncalibration_tbl |&gt; ?\n\n\n\n\n\n\n\nImportant\n\n\n\nWhich is the best model by the rmse metric?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-8-forecast---training-data",
    "href": "labs/BSMM_8740_lab_6.html#exercise-8-forecast---training-data",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 8: forecast - training data",
    "text": "Exercise 8: forecast - training data\nUse the calibration table with modeltime::modeltime_forecast to graphically compare the fits to the training data with the observed values.\n\n\n\n\n\n\nYOUR ANSWER:"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-9-forecast---future",
    "href": "labs/BSMM_8740_lab_6.html#exercise-9-forecast---future",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 9: forecast - future",
    "text": "Exercise 9: forecast - future\nNow refit the models using the full data set (using the calibration table and modeltime::modeltime_refit). Save the result in the variable refit_tbl.\n\nUse the refit data in the variable refit_tbl, along with modeltime::modeltime_forecast and argument h = ‘2 weeks’ (remember to also set the actual_data argument). This will use the models to forecast electricity demand two weeks into the future.\nPlot the forecast with modeltime::plot_modeltime_forecast\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nrefit_tbl &lt;-  ?\n\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 6!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyone’s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#grading",
    "href": "labs/BSMM_8740_lab_6.html#grading",
    "title": "lab 6 - Time Series Methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html",
    "href": "labs/BSMM_8740_lab_5.html",
    "title": "Lab 5 - Classification and clustering",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering.\n\n\nBy the end of the lab you will…\n\nBe able to build workflows to fit different classification models.\nBe able to build workflows to evaluate different clustering models."
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#introduction",
    "href": "labs/BSMM_8740_lab_5.html#introduction",
    "title": "Lab 5 - Classification and clustering",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering.\n\n\nBy the end of the lab you will…\n\nBe able to build workflows to fit different classification models.\nBe able to build workflows to evaluate different clustering models."
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#getting-started",
    "href": "labs/BSMM_8740_lab_5.html#getting-started",
    "title": "Lab 5 - Classification and clustering",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-5-[your github username] repository .\nCreate an R project using your 2024-lab-5-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-5.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#packages",
    "href": "labs/BSMM_8740_lab_5.html#packages",
    "title": "Lab 5 - Classification and clustering",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, forcats,\n  broom, yardstick, parsnip, workflows, rsample, tune, dials\n)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#the-data",
    "href": "labs/BSMM_8740_lab_5.html#the-data",
    "title": "Lab 5 - Classification and clustering",
    "section": "The Data",
    "text": "The Data\nToday we will be using customer churn data.\nIn the customer management lifecycle, customer churn refers to a decision made by the customer about ending the business relationship. It is also referred as loss of clients or customers. This dataset contains 20 features related to churn in a telecom context and we will look at how to predict churn and estimate the effect of predictors on the customer churn odds ratio.\n\ndata &lt;- \n  readr::read_csv(\"data/Telco-Customer-Churn.csv\", show_col_types = FALSE) |&gt; \n  dplyr::mutate(churn = as.factor(churn))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-1-eda",
    "href": "labs/BSMM_8740_lab_5.html#exercise-1-eda",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nWrite and execute the code to perform summary EDA on the data using the package skimr. Plot histograms for monthly charges and tenure. Tenure measures the strength of the customer relationship by measuring the length of time that a person has been a customer.\n\n\n\n\n\n\nYOUR ANSWER:"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-2-train-test-splits-recipe",
    "href": "labs/BSMM_8740_lab_5.html#exercise-2-train-test-splits-recipe",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 2: train / test splits & recipe",
    "text": "Exercise 2: train / test splits & recipe\nWrite and execute code to create training and test datasets. Have the training dataset represent 70% of the total data.\nNext create a recipe where churn is related to all the other variables, and\n\nnormalize the numeric variables\ncreate dummy variables for the ordinal predictors\n\nMake sure the steps are in a sequence that preserves the (0,1) dummy variables.\nPrep the data on the training data and show the result.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nset.seed(8740)\n\n# split data\n\n\n# create a recipe"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-3-logistic-modeling",
    "href": "labs/BSMM_8740_lab_5.html#exercise-3-logistic-modeling",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 3: logistic modeling",
    "text": "Exercise 3: logistic modeling\n\nCreate a linear model using logistic regression to predict churn. for the set engine stage use “glm,” and set the mode to “classification.”\nCreate a workflow using the recipe of the last exercise and the model if the last step.\nWith the workflow, fit the training data\nCombine the training data and the predictions from step 3 using broom::augment , and assign the result to a variable\nCreate a combined metric function using yardstick::metric_set as show in the code below:\nUse the variable from step 4 as the first argument to the function from step 5. The other arguments are truth = churn (from the data) and estimate=.pred_class (from step 4). Make a note of the numerical metrics.\nUse the variable from step 4 as the first argument to the functions listed below, with arguments truth = churn and estimate =``.pred_No.\n\nyardstick::roc_auc\nyardstick::roc_curve followed by ggplot2::autoplot().\n\n\n\n\n\n\n\n\nrank-deficiency\n\n\n\nYou can ignore this message. It means that there are a lot of predictors.\n\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# create a linear regression model\n\n# create a workflow\n\n# fit the workflow\n\n# augment the training data with the fitted data\n\n\n# create the metrics function\nm_set_fn &lt;- \n  yardstick::metric_set(\n    yardstick::accuracy\n    , yardstick::precision\n    , yardstick::recall\n    , yardstick::f_meas\n    , yardstick::spec\n    , yardstick::sens\n    , yardstick::ppv\n    , yardstick::npv\n)\n\n\n# compute roc_auc and plot the roc_curve"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-4-effects",
    "href": "labs/BSMM_8740_lab_5.html#exercise-4-effects",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 4: effects",
    "text": "Exercise 4: effects\nUse broom::tidy() on the fit object from exercise 4 to get the predictor coefficients. Sort them in decreasing order by absolute value.\nWhat is the effect of one additional year of tenure on the churn odds ratio?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# \n\nThe effect of one additional year of tenure on the churn odds ratio is __."
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-5-knn-modeling",
    "href": "labs/BSMM_8740_lab_5.html#exercise-5-knn-modeling",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 5 knn modeling",
    "text": "Exercise 5 knn modeling\nNow we will create a K-nearest neighbours model to estimate churn. To do this, write the code for the following steps:\n\nCreate a K-nearest neighbours model to predict churn using parsnip::nearest_neighbor with argument neighbors = 3 which will use the three most similar data points from the training set to predict churn. For the set engine stage use “kknn,” and set the mode to “classification.”\nTake the workflow from exercise 3 and create a new workflow by updating the original workflow. Use workflows::update_model to swap out the original logistic model for the nearest neighbour model.\nUse the new workflow to fit the training data. Take the fit and use broom::augment to augment the fit with the training data.\nUse the augmented data from step 3 to plot the roc curve, using yardstick::roc_curve(.pred_No, truth = churn) as in exercise 3. How do you interpret his curve?\nTake the fit from step 3 and use broom::augment to augment the fit with the test data.\nRepeat step 4 using the augmented data from step 5.\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?parsnip::nearest_neighbor\n\n# create a knn classification model model\n\n# create a workflow\n\n# fit the workflow\n\n# augment the training data with the fitted data\n\n\n# compute the metrics\n\n\n?yardstick::roc_curve\n# compute roc_auc and plot the roc_curve"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-6-cross-validation",
    "href": "labs/BSMM_8740_lab_5.html#exercise-6-cross-validation",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 6 cross validation",
    "text": "Exercise 6 cross validation\nFollowing the last exercise, we should have some concerns about over-fitting by the nearest-neighbour model.\nTo address this we will use cross validation to tune the model and evaluate the fits.\n\nCreate a cross-validation dataset based on 5 folds using rsample::vfold_cv.\nUsing the knn workflow from exercise 5, apply tune::fit_resamples with arguments resamples and control where the resamples are the dataset created in step 1 and control is tune::control_resamples(save_pred = TRUE), which will ensure that the predictions are saved.\nUse tune::collect_metrics() on the results from step 2\nUse tune::collect_predictions() on the results from step 2 to plot the roc_auc curve as in exercise 5. Has it changed much from exercise 5?\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?rsample::vfold_cv\n\n# create v-fold cross validation data\n\n# use tune::fit on the cv dat, saving the predictions\n\n\n?tune::fit_resamples\n\n# collect the metrics\n\n# compute the roc_curve\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-7-tuning-for-k",
    "href": "labs/BSMM_8740_lab_5.html#exercise-7-tuning-for-k",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 7: tuning for k",
    "text": "Exercise 7: tuning for k\nIn this exercise we’ll tune the number of nearest neighbours in our model to see if we can improve performance.\n\nRedo exercise 5 steps 1 and 2, setting neighbors = tune::tune() for the model, and then updating the workflow with workflows::update_model.\nUse dials::grid_regular(dials::neighbors(), levels = 10) to create a grid for tuning k.\nUse tune::tune_grid with tune::control_grid(save_pred = TRUE) and yardstick::metric_set(yardstick::accuracy, yardstick::roc_auc) to generate tuning results\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?tune::tune_grid\n\n# re-specify the model for tuning\n\n\n# update the workflow\n\n\n# make a grid for tuning\n\n\n# use the grid to tune the model\n\n\n# show the tuning results dataframe"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-8",
    "href": "labs/BSMM_8740_lab_5.html#exercise-8",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 8",
    "text": "Exercise 8\nUse tune::collect_metrics() to collect the metrics from the tuning results in exercise 7 and then plot the metrics as a function of k using the code below.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# collect the metrics\n\n# plot the collected metrics as a function of K\n_your_metrics_ |&gt;\nggplot(aes(neighbors,mean)) +\n  geom_line(linewidth = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-9",
    "href": "labs/BSMM_8740_lab_5.html#exercise-9",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 9",
    "text": "Exercise 9\nUse tune::show_best and tune::select_best with argument “roc_auc” to find the best k for the knn classification model. Then\n\nupdate the workflow using tune::finalize_workflow to set the best k value.\nuse tune::last_fit with the updated workflow from step 1, evaluated on the split data from exercise 2 to finalize the fit.\nuse tune::collect_metrics() to get the metrics for the best fit\nuse tune::collect_predictions() to get the predictions and plot the roc_auc as in the prior exercises\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?tune::finalize_workflow\n# show the roc_auc metrics\n\n# select the best roc_auc metric (using a function from tune::)\n\n# finalize the workflow with the best nn metric from the last step\n\n# use  tune::last_fit with the finaized workflow on the data_split (ex 2)\n\n# collect the metrics from the final fit\n\n# collect the predictions from the final fit and plot the roc_curve"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-10-clustering",
    "href": "labs/BSMM_8740_lab_5.html#exercise-10-clustering",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 10: clustering",
    "text": "Exercise 10: clustering\nLoad the data for this exercise as below and plot it, and then create an analysis dataset with the cluster labels removed\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# read the data\nlabelled_points &lt;- readr::read_csv(\"data/lab_5_clusters.csv\", show_col_types = FALSE)\n\n# plot the clusters\nlabelled_points |&gt; ggplot(aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3) + \n  theme(legend.position=\"none\")\n\n# remove cluster labels to make the analysis dataset\npoints &lt;-\n  labelled_points |&gt;\n  select(?)\n\nYou have frequently used broom::augment to combine a model with the data set, and broom::tidy to summarize model components; broom::glance is used to similarly to summarize goodness-of-fit metrics.\nNow perform k-means clustering on the points data for different values of k as follows:\n\nkclusts &lt;-\n  # number of clusters from 1-9\n  tibble(k = 1:9) |&gt;\n  # mutate to add columns\n  mutate(\n    # a list-column with the results of the kmeans function (clustering)\n    kclust = purrr::map(k, ~stats::kmeans(points, .x)),\n    # a list-column with the results broom::tidy applied to the clustering results\n    tidied = purrr::map(kclust, broom::tidy),\n    # a list-column with the results broom::glance applied to the clustering results\n    glanced = purrr::map(kclust, broom::glance),\n    # a list-column with the results broom::augment applied to the clustering results\n    augmented = purrr::map(kclust, broom::augment, points)\n  )\n\n(i) Create 3 variables by tidyr::unnesting the appropriate columns of kclusts\n\n# take kclusts and use tidy::unnest() on the appropriate columns\n\nclusters &lt;- ?\n\nassignments &lt;- ?\n\nclusterings &lt;- ?\n\n(ii) Use the assignments variable to plot the cluster assignments generated by stats::kmeans\n\n# plot the points assigned to each cluster\np &lt;- assignments |&gt; ggplot(aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) +\n  facet_wrap(~ k) + theme(legend.position=\"none\")\np\n\n(iii) Use the clusters variable to add the cluster centers to the plot\n\n# on the last plot, mark the cluster centres with an X\np + geom_point(data = clusters, size = 10, shape = \"x\")\n\n(iv) Use the clusterings variable to plot the total within sum of squares value by number of clusters.\n\n# make a separate line-and-point plot with the tot-withinss data by cluster number\nclusterings |&gt; ggplot(aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n\n(v) Using the results of parts (iii) and (iv), the k (number of clusters) that gives the best results is __.\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 5!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyone’s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#grading",
    "href": "labs/BSMM_8740_lab_5.html#grading",
    "title": "Lab 5 - Classification and clustering",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html",
    "title": "Lab 6 - Time Series Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflows with recipes, parsnip models, rsample cross validations, and model comparison in the context of timeseries data."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#introduction",
    "title": "Lab 6 - Time Series Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflows with recipes, parsnip models, rsample cross validations, and model comparison in the context of timeseries data."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#packages",
    "title": "Lab 6 - Time Series Methods",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#the-data",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#the-data",
    "title": "Lab 6 - Time Series Methods",
    "section": "The Data",
    "text": "The Data\nToday we will be using electricity demand data, based on a paper by James W Taylor:\n\nTaylor, J.W. (2003) Short-term electricity demand forecasting using double seasonal exponential smoothing. Journal of the Operational Research Society, 54, 799-805.\n\nThe data can be found in the timetk package as timetk::taylor_30_min, a tibble with demensions: 4,032 x 2\n\ndate: A date-time variable in 30-minute increments\nvalue: Electricity demand in Megawatts\n\n\ndata &lt;- timetk::taylor_30_min"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-1-eda",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-1-eda",
    "title": "Lab 6 - Time Series Methods",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nPlot the data using the functions timetk::plot_time_series, timetk::plot_acf_diagnostics (using 100 lags), and timetk::plot_seasonal_diagnostics.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# - plot the data\ntimetk::taylor_30_min |&gt;\n  timetk::plot_time_series(\n    date\n    , value\n    , .title = \"Short-term electricity demand (30 min)\"\n  )\n\n\n\n\n\n\n# - plot the acf, pacf\ntimetk::taylor_30_min |&gt;\n  timetk::plot_acf_diagnostics(\n    date\n    , value\n    , .lags = 100\n    , .title = \"Lag Diagnostics - Short-term electricity demand (30 min)\")\n\n\n\n\n\n\n# - plot the acf, pacf\ntimetk::taylor_30_min |&gt;\n  timetk::plot_seasonal_diagnostics(\n    date\n    , value\n    , .title = \"Seasonal Diagnostics - Short-term electricity demand (30 min)\")"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-2-time-scaling",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-2-time-scaling",
    "title": "Lab 6 - Time Series Methods",
    "section": "Exercise 2: Time scaling",
    "text": "Exercise 2: Time scaling\nThe raw data has 30 minutes intervals between data points. Downscale the data to 60 minute intervals, using timetk::summarise_by_time, revising the electricity demand (value) variable by adding the two 30-minute intervals in each 60-minute interval. Assign the downscaled data to the variable taylor_60_min.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# downscale the data (down to a lower frequency of measurement)\nset.seed(8740)\ntaylor_60_min &lt;-\n  timetk::taylor_30_min |&gt;\n  timetk::summarise_by_time(\n    .date_var = date\n    , .by = \"hour\"\n    , value = sum(value)\n  )"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-3-training-and-test-datasets",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-3-training-and-test-datasets",
    "title": "Lab 6 - Time Series Methods",
    "section": "Exercise 3: Training and test datasets",
    "text": "Exercise 3: Training and test datasets\n\nSplit the new (60 min) time series into training and test sets using timetk::time_series_split\n\nset the training period (‘initial’) to ‘2 months’ and the assessment period to ‘1 weeks’\n\nPrepare the data resample specification with timetk::tk_time_series_cv_plan() and plot it with timetk::plot_time_series_cv_plan\nSeparate the training and test data sets using rsample.\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# split\nsplits &lt;-\n  taylor_60_min |&gt;\n  timetk::time_series_split(\n    initial = \"2 months\"\n    , assess = \"1 weeks\"\n  )\n\n# plot\nsplits |&gt;\n  timetk::tk_time_series_cv_plan() |&gt;\n  timetk::plot_time_series_cv_plan(\n    .date_var = date\n    , .value = value\n    , .title = \"Cross Validation Plan - Short-term electricity demand (30 min)\")\n\n\n\n\n# separate\ntrain &lt;- rsample::training(splits) \ntest  &lt;- rsample::testing(splits)"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-4-recipes",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-4-recipes",
    "title": "Lab 6 - Time Series Methods",
    "section": "Exercise 4: recipes",
    "text": "Exercise 4: recipes\n\nCreate a base recipe (base_rec) using the formula value ~ date and the training data. This will be used for non-regression models\nCreate a recipe (lm_rec) using the formula value ~ . and the training data. This will be used for regression models. For this recipe:\n\nadd time series signature features using timetk::step_timeseries_signature with the appropriate argument,\nadd a step to select the columns value, date_index.num, date_month.lbl, date_wday.lbl, date_hour ,\nadd a normalization step targeting date_index.num ,\nadd a step to mutate date_hour, changing it to a factor,\nadd a step to one-hot encode nominal predictors.\n\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nbase_rec &lt;- \n  train |&gt;\n  recipes::recipe(value ~ date)\n  \nlm_rec &lt;- train |&gt;\n  recipes::recipe(value ~ .) |&gt;\n  timetk::step_timeseries_signature(date) |&gt;\n  recipes::step_select( value, date_index.num, date_month.lbl, date_wday.lbl, date_hour ) |&gt;\n  step_normalize(date_index.num) |&gt;\n  recipes::step_mutate(date_hour = date_hour |&gt; as.factor()) |&gt;\n  recipes::step_dummy(all_nominal(), one_hot = TRUE)"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-5-models",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-5-models",
    "title": "Lab 6 - Time Series Methods",
    "section": "Exercise 5 models",
    "text": "Exercise 5 models\nNow we will create a several models to estimate electricity demand, as follows\n\nCreate a model specification for an exponential smoothing model using engine ‘ets’\nCreate a model specification for an arima model using engine ‘auto_arima’\nCreate a model specification for a linear model using engine ‘glmnet’ and penalty = 0.02, mixture = 0.5\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmodel_ets &lt;- modeltime::exp_smoothing() |&gt;\n  parsnip::set_engine(engine = \"ets\")\n\nmodel_arima &lt;- modeltime::arima_reg() |&gt;\n  parsnip::set_engine(\"auto_arima\")\n\nmodel_lm &lt;- parsnip::linear_reg(penalty = 0.02, mixture = 0.5) |&gt;\n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-6-model-fitting",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-6-model-fitting",
    "title": "Lab 6 - Time Series Methods",
    "section": "Exercise 6 model fitting",
    "text": "Exercise 6 model fitting\nCreate a workflow for each model using workflows::workflow.\n\nAdd a recipe to the workflow\n\nthe linear model uses the lm_rec recipe created above\nthe ets and arima models use the base_rec recipe created above\n\nAdd a model to each workflow\nFit with the training data\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nworkflow_fit_ets &lt;- workflows::workflow() |&gt;\n  workflows::add_recipe(base_rec) |&gt;\n  workflows::add_model(model_ets) |&gt;\n  parsnip::fit(train)\n  \nworkflow_fit_arima &lt;- workflows::workflow() |&gt;\n  workflows::add_recipe(base_rec) |&gt;\n  workflows::add_model(model_arima) |&gt;\n  parsnip::fit(train)\n  \nworkflow_fit_lm &lt;- workflows::workflow() |&gt;\n  workflows::add_recipe(lm_rec) |&gt;\n  workflows::add_model(model_lm) |&gt;\n  parsnip::fit(train)"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-7-calibrate",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-7-calibrate",
    "title": "Lab 6 - Time Series Methods",
    "section": "Exercise 7: calibrate",
    "text": "Exercise 7: calibrate\nIn this exercise we’ll use the testing data with our fitted models.\n\nCreate a table with the fitted workflows using modeltime::modeltime_table\nUsing the table you just created, run a calibration on the test data with the function modeltime::modeltime_calibrate.\nCompare the accuracy of the models using the modeltime::modeltime_accuracy() on the results of the calibration\n\n\n\n\n\n\n\nImportant\n\n\n\nWhich is the best model by the rmse metric?\n\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmodel_tbl &lt;- modeltime::modeltime_table(\n  workflow_fit_ets\n  , workflow_fit_arima\n  , workflow_fit_lm\n)\n\ncalibration_tbl &lt;- model_tbl |&gt;\n  modeltime::modeltime_calibrate( test )\n  \ncalibration_tbl |&gt; modeltime::modeltime_accuracy()\n\n# A tibble: 3 × 9\n  .model_id .model_desc             .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 ETS(M,AD,M)             Test  6529. 10.6   2.68 11.0  7222. 0.735\n2         2 ARIMA(3,0,0)(2,1,0)[24] Test  8071. 12.7   3.32 13.8  9511. 0.718\n3         3 GLMNET                  Test  3022.  5.25  1.24  5.43 3573. 0.936\n\n\nIt looks like the linear model is the best fit per the rmse metric. This is likely because the data shows the electricity demand is very periodic, and the linear model explicitly includes fourier (periodic) components in the model. The other two models are more general purpose."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-8-forecast---training-data",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-8-forecast---training-data",
    "title": "Lab 6 - Time Series Methods",
    "section": "Exercise 8: forecast - training data",
    "text": "Exercise 8: forecast - training data\nUse the calibration table with modeltime::modeltime_forecast to graphically compare the fits to the testing data with the observed values.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ncalibration_tbl |&gt;\n  modeltime::modeltime_forecast(\n    new_data    = test,\n    actual_data = taylor_60_min\n  ) |&gt;\n  modeltime::plot_modeltime_forecast()"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-9-forecast---future",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#exercise-9-forecast---future",
    "title": "Lab 6 - Time Series Methods",
    "section": "Exercise 9: forecast - future",
    "text": "Exercise 9: forecast - future\nNow refit the models using the full data set (using the calibration table and modeltime::modeltime_refit). Save the result in the variable refit_tbl.\n\nUse the refit data in the variable refit_tbl, along with modeltime::modeltime_forecast and argument h = ‘2 weeks’ (remember to also set the actual_data argument). This will use the models to forecast electricity demand two weeks into the future.\nPlot the forecast with modeltime::plot_modeltime_forecast.\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nrefit_tbl &lt;- calibration_tbl |&gt;\n  modeltime::modeltime_refit(data = taylor_60_min)\n\nrefit_tbl |&gt;\n  modeltime::modeltime_forecast(h = \"2 weeks\", actual_data = taylor_60_min) %&gt;%\n  modeltime::plot_modeltime_forecast(\n    .legend_max_width = 12, # For mobile screens\n    .interactive      = TRUE\n  )"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_6_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_6_solutions.html#grading",
    "title": "Lab 6 - Time Series Methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#introduction",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#packages",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(tidyverse, magrittr, gt, gtExtras, tidymodels, ggplot2)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#the-data",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#the-data",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "The Data",
    "text": "The Data\nToday we will be using customer churn data.\nIn the customer management lifecycle, customer churn refers to a decision made by the customer about ending the business relationship. It is also referred as loss of clients or customers. This dataset contains 20 features related to churn in a telecom context and we will look at how to predict churn and estimate the effect of predictors on the customer churn odds ratio.\n\ndata &lt;- \n  readr::read_csv(\"data/Telco-Customer-Churn.csv\", show_col_types = FALSE) |&gt;\n  dplyr::mutate(churn = as.factor(churn))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-1-eda",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-1-eda",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nWrite and execute the code to perform summary EDA on the data using the package skimr. Plot histograms for monthly charges and tenure. Tenure measures the strength of the customer relationship by measuring the length of time that a person has been a customer.\n\n\n\n\n\n\nSOLUTION:\n\n\n\nskimr::skim(data)\ndata |&gt;\n  ggplot(aes(x=monthly_charges)) + geom_histogram()\ndata |&gt;\n  ggplot(aes(x=tenure)) + geom_histogram()\n\n\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n7032\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n17\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngender\n0\n1\n4\n6\n0\n2\n0\n\n\nsenior_citizen\n0\n1\n2\n3\n0\n2\n0\n\n\npartner\n0\n1\n2\n3\n0\n2\n0\n\n\ndependents\n0\n1\n2\n3\n0\n2\n0\n\n\ntenure_interval\n0\n1\n9\n11\n0\n7\n0\n\n\nphone_service\n0\n1\n2\n3\n0\n2\n0\n\n\nmultiple_lines\n0\n1\n2\n16\n0\n3\n0\n\n\ninternet_service\n0\n1\n2\n11\n0\n3\n0\n\n\nonline_security\n0\n1\n2\n19\n0\n3\n0\n\n\nonline_backup\n0\n1\n2\n19\n0\n3\n0\n\n\ndevice_protection\n0\n1\n2\n19\n0\n3\n0\n\n\ntech_support\n0\n1\n2\n19\n0\n3\n0\n\n\nstreaming_tv\n0\n1\n2\n19\n0\n3\n0\n\n\nstreaming_movies\n0\n1\n2\n19\n0\n3\n0\n\n\ncontract\n0\n1\n8\n14\n0\n3\n0\n\n\npaperless_billing\n0\n1\n2\n3\n0\n2\n0\n\n\npayment_method\n0\n1\n12\n25\n0\n4\n0\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nchurn\n0\n1\nFALSE\n2\nNo: 5163, Yes: 1869\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntenure\n0\n1\n32.42\n24.55\n1.00\n9.00\n29.00\n55.00\n72.00\n▇▃▃▃▅\n\n\nmonthly_charges\n0\n1\n64.80\n30.09\n18.25\n35.59\n70.35\n89.86\n118.75\n▇▅▆▇▅\n\n\ntotal_charges\n0\n1\n2283.30\n2266.77\n18.80\n401.45\n1397.47\n3794.74\n8684.80\n▇▂▂▂▁"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-2-train-test-splits-recipe",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-2-train-test-splits-recipe",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 2: train / test splits & recipe",
    "text": "Exercise 2: train / test splits & recipe\nWrite and execute code to create training and test datasets. Have the training dataset represent 70% of the total data.\nNext create a recipe where churn is related to all the other variables, and\n\nnormalize the numeric variables\ncreate dummy variables for the ordinal predictors\n\nMake sure the steps are in a sequence that preserves the (0,1) dummy variables.\nPrep the data on the training data and show the result.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nset.seed(8740)\n\n# split data\ndata_split    &lt;- rsample::initial_split(data, prop = 0.7)\ndefault_train &lt;- rsample::training(data_split)\ndefault_test  &lt;- rsample::testing(data_split)\n\n# create a recipe\ndefault_recipe &lt;- default_train |&gt;\n  recipes::recipe(formula = churn ~ .) |&gt;\n  recipes::step_normalize(recipes::all_numeric_predictors()) |&gt;\n  recipes::step_dummy(recipes::all_nominal_predictors())\n\ndefault_recipe |&gt; recipes::prep(default_train) |&gt; \n  summary()\n\n# A tibble: 37 × 4\n   variable                     type      role      source  \n   &lt;chr&gt;                        &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 tenure                       &lt;chr [2]&gt; predictor original\n 2 monthly_charges              &lt;chr [2]&gt; predictor original\n 3 total_charges                &lt;chr [2]&gt; predictor original\n 4 churn                        &lt;chr [3]&gt; outcome   original\n 5 gender_Male                  &lt;chr [2]&gt; predictor derived \n 6 senior_citizen_Yes           &lt;chr [2]&gt; predictor derived \n 7 partner_Yes                  &lt;chr [2]&gt; predictor derived \n 8 dependents_Yes               &lt;chr [2]&gt; predictor derived \n 9 tenure_interval_X0.6.Month   &lt;chr [2]&gt; predictor derived \n10 tenure_interval_X12.24.Month &lt;chr [2]&gt; predictor derived \n# ℹ 27 more rows"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-3-logistic-modeling",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-3-logistic-modeling",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 3: logistic modeling",
    "text": "Exercise 3: logistic modeling\n\nCreate a linear model using logistic regression to predict churn. for the set engine stage use “glm,” and set the mode to “classification.”\nCreate a workflow using the recipe of the last exercise and the model if the last step.\nWith the workflow, fit the training data\nCombine the training data and the predictions from step 3 using broom::augment , and assign the result to a variable\nCreate a combined metric function as show in the code below:\nUse the variable from step 4 as the first argument to the function from step 5. The other arguments are truth = churn (from the data) and estimate=.pred_class (from step 4). Make a note of the numerical metrics.\nUse the variable from step 4 as the first argument to the functions below, with arguments truth = churn and estimate =.pred_No.\n\nyardstick::roc_auc\nyardstick::roc_curve followed by ggplot2::autoplot().\n\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create a linear regression model\ndefault_model &lt;- parsnip::logistic_reg() |&gt;\n  parsnip::set_engine(\"glm\") |&gt;\n  parsnip::set_mode(\"classification\")\n\n# create a workflow\ndefault_workflow &lt;- workflows::workflow() |&gt;\n  workflows::add_recipe(default_recipe) |&gt;\n  workflows::add_model(default_model)\n\n# fit the workflow\nlm_fit &lt;-\n  default_workflow |&gt;\n  parsnip::fit(default_train)\n\n# training dataset\ntraining_results &lt;-\n  broom::augment(lm_fit , default_train)\n\n\n# create the metrics function\nm_set_fn &lt;- \n  yardstick::metric_set(\n    yardstick::accuracy\n    , yardstick::precision\n    , yardstick::recall\n    , yardstick::f_meas\n    , yardstick::spec\n    , yardstick::sens\n    , yardstick::ppv\n    , yardstick::npv\n)\ntraining_results |&gt; m_set_fn(truth = churn, estimate = .pred_class)\n\n# A tibble: 8 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.807\n2 precision binary         0.842\n3 recall    binary         0.905\n4 f_meas    binary         0.872\n5 spec      binary         0.546\n6 sens      binary         0.905\n7 ppv       binary         0.842\n8 npv       binary         0.683\n\n\n# compute roc_auc and plot the roc_curve\ntraining_results |&gt;\n  yardstick::roc_auc(.pred_No, truth = churn)\ntraining_results |&gt;\n  yardstick::roc_curve(.pred_No, truth = churn) |&gt; autoplot()\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.854\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild your own\n\n\n\nYou can construct the roc_auc curve directly, as follows:\n\n# use the augmented training results, and find points on the curve for a given threshold\nbuild_roc_aus &lt;- function(threshold, dat = training_results){\n  # threshold on predictions\n  log_preds &lt;- ifelse(dat$.pred_Yes &gt; threshold, 1, 0)\n  # compute predictions and reference/truth\n  log_preds &lt;- factor(log_preds, levels = c(0,1), labels = c('No','Yes'))\n\n  return(\n    tibble::tibble(\n      threshold = threshold\n      , \"1-Specificity\" = \n        1 - yardstick::specificity_vec(truth = dat$churn, estimate = log_preds)\n      , Sensitivity = \n          yardstick::sensitivity_vec(truth = dat$churn, estimate = log_preds)\n    )\n  )\n}\n\n\n#take thesholds in [0,1] and calculate the corresponding x,y points\n((0:50)/50) |&gt; purrr::map(~build_roc_aus(.x)) |&gt; \n  dplyr::bind_rows() |&gt; \n    ggplot(aes(x=`1-Specificity`, y=Sensitivity)) +\n    geom_point(size=1) + coord_fixed() +\n    geom_abline(intercept=0, slope=1)"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-4-effects",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-4-effects",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 4: effects",
    "text": "Exercise 4: effects\nUse broom::tidy() on the fit object from exercise 4 to get the predictor coefficients. Sort them in decreasing order by absolute value.\nWhat is the effect of one additional year of tenure on the churn odds ratio?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nfit0_tbl &lt;- lm_fit |&gt; broom::tidy() |&gt;\n  dplyr::arrange(desc(abs(estimate)))\n\nfit0_tbl\n\n# A tibble: 37 × 5\n   term                         estimate std.error statistic  p.value\n   &lt;chr&gt;                           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 tenure_interval_X6.12.Month     -3.04     0.820    -3.71  2.11e- 4\n 2 tenure_interval_X12.24.Month    -2.78     0.705    -3.94  8.02e- 5\n 3 tenure_interval_X0.6.Month      -2.68     0.905    -2.96  3.09e- 3\n 4 tenure_interval_X24.36.Month    -2.20     0.554    -3.96  7.51e- 5\n 5 tenure                          -2.10     0.386    -5.43  5.67e- 8\n 6 (Intercept)                      1.91     1.62      1.18  2.37e- 1\n 7 contract_Two.year               -1.45     0.217    -6.67  2.60e-11\n 8 tenure_interval_X36.48.Month    -1.28     0.402    -3.19  1.44e- 3\n 9 phone_service_Yes               -1.23     0.782    -1.57  1.17e- 1\n10 monthly_charges                  1.15     1.15      0.997 3.19e- 1\n# ℹ 27 more rows\n\n\n\n# pull the tenure coefficient and exponentiate it\nfit0_tbl |&gt; dplyr::filter(term == 'tenure') |&gt; \n  dplyr::pull(estimate) |&gt; \n  exp()\n\n[1] 0.1228617"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-5-knn-modeling",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-5-knn-modeling",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 5 knn modeling",
    "text": "Exercise 5 knn modeling\nNow we will create a K-nearest neighbours model to estimate churn. To do this, write the code for the following steps:\n\nCreate a K-nearest neighbours model to predict churn using parsnip::nearest_neighbor with argument neighbors = 3 which will use the three most similar data points from the training set to predict churn. For the set engine stage use “kknn,” and set the mode to “classification.”\nTake the workflow from exercise 3 and create a new workflow by updating the original workflow. Use workflows::update_model to swap out the original logistic model for the nearest neighbour model.\nUse the new workflow to fit the training data. Take the fit and use broom::augment to augment the fit with the training data.\nUse the augmented data from step 3 to plot the roc curve, using yardstick::roc_curve(.pred_No, truth = churn) as in exercise 3. How do you interpret his curve?\nTake the fit from step 3 and use broom::augment to augment the fit with the test data.\nRepeat step 4 using the augmented data from step 5.\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create a knn classification model\ndefault_model_knn &lt;- parsnip::nearest_neighbor(neighbors = 3) |&gt;\n  parsnip::set_engine(\"kknn\") |&gt;\n  parsnip::set_mode(\"classification\")\n\n# create a workflow\ndefault_workflow_knn &lt;- default_workflow |&gt;\n  workflows::update_model(default_model_knn)\n\n# fit the workflow\nlm_fit_knn &lt;-\n  default_workflow_knn |&gt;\n  parsnip::fit(default_train)\n\n# augment the training data with the fitted data\ntraining_results_knn &lt;-\n  broom::augment(lm_fit_knn , default_train)\n\n\n# compute the metrics\ntraining_results_knn |&gt; m_set_fn(truth = churn, estimate = .pred_class)\n\n# A tibble: 8 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.999\n2 precision binary         0.999\n3 recall    binary         1.00 \n4 f_meas    binary         0.999\n5 spec      binary         0.998\n6 sens      binary         1.00 \n7 ppv       binary         0.999\n8 npv       binary         0.999\n\n\n# compute roc_auc and plot the roc_curve\ntraining_results_knn |&gt;\n  yardstick::roc_auc(.pred_No, truth = churn)\ntraining_results_knn |&gt;\n  yardstick::roc_curve(.pred_No, truth = churn) |&gt; autoplot()\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.999"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-6-cross-validation",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-6-cross-validation",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 6 cross validation",
    "text": "Exercise 6 cross validation\nFollowing the last exercise, we should have some concerns about over-fitting by the nearest-neighbour model.\nTo address this we will use cross validation to tune the model and evaluate the fits.\n\nCreate a cross-validation dataset based on 5 folds using rsample::vfold_cv.\nUsing the knn workflow from exercise 5, apply tune::fit_resamples with arguments resamples and control where the resamples are the dataset created in step 1 and control is tune::control_resamples(save_pred = TRUE), which will ensure that the predictions are saved.\nUse tune::collect_metrics() on the results from step 2\nUse tune::collect_predictions() on the results from step 2 to plot the roc_auc curve as in exercise 5. Has it changed much from exercise 5?\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create v-fold cross validation data\ndata_vfold_cv &lt;- data |&gt; rsample::vfold_cv(v=5)\n\n# use tune::fit on the cv dat, saving the predictions\nrf_fit_rs &lt;-\n  default_workflow_knn |&gt;\n  tune::fit_resamples(data_vfold_cv, control = tune::control_resamples(save_pred = TRUE))\n\n# collect the metrics\nrf_fit_rs |&gt; tune::collect_metrics()\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.729     5 0.00480 Preprocessor1_Model1\n2 brier_class binary     0.207     5 0.00374 Preprocessor1_Model1\n3 roc_auc     binary     0.746     5 0.00790 Preprocessor1_Model1\n\n# compute the roc_curve\nrf_fit_rs |&gt; tune::collect_predictions() |&gt;\n  yardstick::roc_curve(.pred_No, truth = churn) |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-7-tuning-for-k",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-7-tuning-for-k",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 7: tuning for k",
    "text": "Exercise 7: tuning for k\nIn this exercise we’ll tune the number of nearest neighbours in our model to see if we can improve performance.\n\nRedo exercise 5 steps 1 and 2, setting neighbors = tune::tune() for the model, and then updating the workflow with workflows::update_model.\nUse dials::grid_regular(dials::neighbors(), levels = 10) to create a grid for tuning k.\nUse tune::tune_grid with tune::control_grid(save_pred = TRUE) and yardstick::metric_set(yardstick::accuracy, yardstick::roc_auc) to generate tuning results\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n# re-specify the model for tuning\ndefault_model_knn_tuned &lt;- parsnip::nearest_neighbor(neighbors = tune::tune()) |&gt;\n  parsnip::set_engine(\"kknn\") |&gt;\n  parsnip::set_mode(\"classification\")\n\n# update the workflow\ndefault_workflow_knn &lt;- default_workflow |&gt;\n  workflows::update_model(default_model_knn_tuned)\n\n# make a grid for tuning\nclust_num_grid &lt;-\n  dials::grid_regular(dials::neighbors(), levels = 10)\n\n# use the grid to tune the model\ntune_results &lt;- tune::tune_grid(\n  default_workflow_knn,\n  resamples = data_vfold_cv,\n  grid = clust_num_grid,\n  control = tune::control_grid(save_pred = TRUE)\n  , metrics =\n    yardstick::metric_set(yardstick::accuracy, yardstick::roc_auc)\n)\n\n# show the tuning results dataframe\ntune_results\n\n\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits              id    .metrics          .notes           .predictions\n  &lt;list&gt;              &lt;chr&gt; &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [5625/1407]&gt; Fold1 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n2 &lt;split [5625/1407]&gt; Fold2 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n3 &lt;split [5626/1406]&gt; Fold3 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n4 &lt;split [5626/1406]&gt; Fold4 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n5 &lt;split [5626/1406]&gt; Fold5 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-8",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-8",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 8",
    "text": "Exercise 8\nUse tune::collect_metrics() to collect the metrics from the tuning results in exercise 7 and then plot the metrics as a function of k using the code below.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n# collect the metrics\ntune_results |&gt;\n  tune::collect_metrics()\n# plot the collected metrics as a function of K\ntune_results |&gt;\n  tune::collect_metrics() |&gt;\n  ggplot(aes(neighbors,mean)) +\n  geom_line(linewidth = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2)\n\n\n\n# A tibble: 20 × 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1         1 accuracy binary     0.729     5 0.00471 Preprocessor1_Model01\n 2         1 roc_auc  binary     0.656     5 0.00416 Preprocessor1_Model01\n 3         2 accuracy binary     0.729     5 0.00460 Preprocessor1_Model02\n 4         2 roc_auc  binary     0.718     5 0.00752 Preprocessor1_Model02\n 5         3 accuracy binary     0.729     5 0.00480 Preprocessor1_Model03\n 6         3 roc_auc  binary     0.746     5 0.00790 Preprocessor1_Model03\n 7         4 accuracy binary     0.730     5 0.00470 Preprocessor1_Model04\n 8         4 roc_auc  binary     0.760     5 0.00707 Preprocessor1_Model04\n 9         5 accuracy binary     0.729     5 0.00469 Preprocessor1_Model05\n10         5 roc_auc  binary     0.767     5 0.00698 Preprocessor1_Model05\n11         6 accuracy binary     0.757     5 0.00608 Preprocessor1_Model06\n12         6 roc_auc  binary     0.777     5 0.00670 Preprocessor1_Model06\n13         7 accuracy binary     0.763     5 0.00566 Preprocessor1_Model07\n14         7 roc_auc  binary     0.783     5 0.00655 Preprocessor1_Model07\n15         8 accuracy binary     0.768     5 0.00578 Preprocessor1_Model08\n16         8 roc_auc  binary     0.789     5 0.00668 Preprocessor1_Model08\n17         9 accuracy binary     0.770     5 0.00639 Preprocessor1_Model09\n18         9 roc_auc  binary     0.793     5 0.00662 Preprocessor1_Model09\n19        10 accuracy binary     0.771     5 0.00686 Preprocessor1_Model10\n20        10 roc_auc  binary     0.797     5 0.00659 Preprocessor1_Model10"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-9",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-9",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 9",
    "text": "Exercise 9\nUse tune::show_best and tune::select_best with argument “roc_auc” to find the best k for the knn classification model. Then\n\nupdate the workflow using tune::finalize_workflow to set the best k value.\nuse tune::last_fit with the updated workflow from step 1, evaluated on the split data from exercise 2 to finalize the fit.\nuse tune::collect_metrics() to get the metrics for the best fit\nuse tune::collect_predictions() to get the predictions and plot the roc_auc as in the prior exercises\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n# show the roc_auc metrics\ntune_results |&gt;\n  tune::show_best(metric = \"roc_auc\")\n# select the best roc_auc metric\nbest_nn &lt;- tune_results |&gt;\n  tune::select_best(metric = \"roc_auc\")\n\n# finalize the workflow with the best nn metric from the last step\nfinal_wf &lt;- default_workflow_knn |&gt;\n  tune::finalize_workflow(best_nn)\n\n# use  tune::last_fit with the finaized workflow on the data_split (ex 2)\nfinal_fit &lt;-\n  final_wf |&gt;\n  tune::last_fit(data_split)\n\n# collect the metrics from the final fit\nfinal_fit |&gt;\n  tune::collect_metrics()\nfinal_fit |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_curve(.pred_No, truth = churn) |&gt;\n  autoplot()\n\n\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        10 roc_auc binary     0.797     5 0.00659 Preprocessor1_Model10\n2         9 roc_auc binary     0.793     5 0.00662 Preprocessor1_Model09\n3         8 roc_auc binary     0.789     5 0.00668 Preprocessor1_Model08\n4         7 roc_auc binary     0.783     5 0.00655 Preprocessor1_Model07\n5         6 roc_auc binary     0.777     5 0.00670 Preprocessor1_Model06\n\n\n\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.773 Preprocessor1_Model1\n2 roc_auc     binary         0.798 Preprocessor1_Model1\n3 brier_class binary         0.157 Preprocessor1_Model1"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-10-clustering",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-10-clustering",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 10: clustering",
    "text": "Exercise 10: clustering\nLoad the data for this exercise as below and plot it, and then create an analysis dataset with the cluster labels removed\n\n#\n# read the data\nlabelled_points &lt;- readr::read_csv(\"data/lab_5_clusters.csv\", show_col_types = FALSE)\n\n# plot the clusters\nlabelled_points |&gt; ggplot(aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3) + \n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n# remove cluster labels to make the analysis dataset\npoints &lt;-\n  labelled_points |&gt;\n  select(-cluster)\n\nYou have frequently used broom::augment to combine a model with the data set, and broom::tidy to summarize model components; broom::glance is used to similarly to summarize goodness-of-fit metrics.\nNow perform k-means clustering on the points data for different values of k as follows:\n\nkclusts &lt;-\n  # number of clusters from 1-9\n  tibble(k = 1:9) |&gt;\n  # mutate to add columns\n  mutate(\n    # a list-column with the results of the kmeans function (clustering)\n    kclust = purrr::map(k, ~stats::kmeans(points, .x)),\n    # a list-column with the results broom::tidy applied to the clustering results\n    tidied = purrr::map(kclust, broom::tidy),\n    # a list-column with the results broom::glance applied to the clustering results\n    glanced = purrr::map(kclust, broom::glance),\n    # a list-column with the results broom::augment applied to the clustering results\n    augmented = purrr::map(kclust, broom::augment, points)\n  )\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n(i) Create 3 variables by tidyr::unnesting the appropriate columns of kclusts\n\n# take kclusts and use tidy::unnest() on the appropriate columns\nclusters &lt;-\n  kclusts |&gt;\n  tidyr::unnest(cols = c(tidied))\n\nassignments &lt;-\n  kclusts |&gt;\n  tidyr::unnest(cols = c(augmented))\n\nclusterings &lt;-\n  kclusts |&gt;\n  tidyr::unnest(cols = c(glanced))\n\n(ii) Use the assignment variable to plot the cluster assignments generated by stats::kmeans\n\n# plot the points assigned to each cluster\np &lt;- assignments |&gt; ggplot(aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) +\n  facet_wrap(~ k) + theme(legend.position=\"none\")\np\n\n\n\n\n\n\n\n\n(iii) Use the clusters variable to add the cluster centers to the plot\n\n# on the last plot, mark the cluster centres with an X\np + geom_point(data = clusters, size = 10, shape = \"x\")\n\n\n\n\n\n\n\n\n(iv) Use the clusterings variable to plot the total within sum of squares value by number of clusters.\n\n# make a separate line-and-point plot with the tot-withinss data by cluster number\nclusterings |&gt; ggplot(aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\n(v) Visually and by the “elbow” heursistic, we should use k=3, i.e. k=3 should give good results: good fit with low model complexity."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#submission",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#submission",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are saved, staged, committed, and pushed to your repository on the course github site.\nRemember – you do not have to turn in an *.html file. I will be pulling your work directly from your repository on the course website."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#grading",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_10_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_10_solutions.html",
    "title": "lab 10 - Bayesian Methods",
    "section": "",
    "text": "In this lab, you’ll practice creating Bayesian models."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_10_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_10_solutions.html#introduction",
    "title": "lab 10 - Bayesian Methods",
    "section": "",
    "text": "In this lab, you’ll practice creating Bayesian models."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_10_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_10_solutions.html#packages",
    "title": "lab 10 - Bayesian Methods",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_10_solutions.html#exercise-1-bayesian-regression",
    "href": "labs/solutions/BSMM_8740_lab_10_solutions.html#exercise-1-bayesian-regression",
    "title": "lab 10 - Bayesian Methods",
    "section": "Exercise 1: Bayesian Regression",
    "text": "Exercise 1: Bayesian Regression\nData:\nThis data contains roughly 2000 trials of a mouse-tracking experiment,\nIt is a preprocessed data set from an experiment conducted by Kieslich et al. (2020) in which participants classified specific animals into broader categories. The data set contains response times, MAD, AUC and other attributes as well as all experimental conditions.\n\ndolphin &lt;- aida::data_MT\n\n# aggregate\ndolphin_agg &lt;- dolphin |&gt;\n  dplyr::filter(correct == 1) |&gt; # only correct values\n  dplyr::group_by(subject_id) |&gt; \n  dplyr::summarize(       # use the median AUC/MAD\n    AUC = median(AUC, na.rm = TRUE),\n    MAD = median(MAD, na.rm = TRUE)) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(\n    # the function scale centers and scales the data \n    AUC = scale(AUC),\n    MAD = scale(MAD)\n  )\n  \n# let's take a look\nhead(dolphin_agg)\n\n# A tibble: 6 × 3\n  subject_id AUC[,1] MAD[,1]\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1       1001   0.643   0.593\n2       1002   0.732   0.367\n3       1003  -0.839  -0.833\n4       1004  -0.551  -0.535\n5       1005   0.619   0.436\n6       1006   0.748   1.02 \n\n\nPlot AUC vs MAD\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nggplot(data = dolphin_agg, \n       aes(x = AUC, \n           y = MAD)) + \n  geom_point(size = 3, alpha = 0.3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph displays the distribution of AUC and MAD values. We can see that there is a strong relationship between AUC and MAD. And that makes a lot of sense. The more the cursor strives toward the competitor, the larger is the overall area under the curve.\n\n\nRegress AUC against MAD using a bayesian regression using the dolphin_agg data. Save the results in the fits directory as “model1.” Use tidybayes::summarise_draws to summarize the model\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmodel1 &lt;- brms::brm(\n  # model formula\n  AUC ~ MAD, \n  # data\n  data = dolphin_agg,\n  file = \"fits/model1\"\n)\n# show summary in tidy format\ntidybayes::summarise_draws(model1)\n\n# A tibble: 6 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 b_Intercept  5.49e-4  5.58e-4 0.0328  0.0319   -0.0536   0.0552  1.00    3967.\n2 b_MAD        9.39e-1  9.39e-1 0.0348  0.0344    0.882    0.996   1.00    4384.\n3 sigma        3.49e-1  3.47e-1 0.0251  0.0248    0.311    0.393   1.00    3838.\n4 Intercept    5.49e-4  5.58e-4 0.0328  0.0319   -0.0536   0.0552  1.00    3967.\n5 lprior      -3.16e+0 -3.16e+0 0.00237 0.00223  -3.16    -3.16    1.00    3890.\n6 lp__        -4.32e+1 -4.29e+1 1.30    1.02    -45.8    -41.8     1.00    2030.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\n\nExtract the model coefficients, then redo the graph to add a geom_abline with the model slope and intercept.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# extract model parameters:\nmodel_intercept &lt;- summary(model1)$fixed[1,1]\nmodel_slope &lt;- summary(model1)$fixed[2,1]\n\nggplot(data = dolphin_agg, \n       aes(x = AUC, \n           y = MAD)) + \n  geom_abline(intercept = model_intercept, slope = model_slope, color = project_colors[2], size  = 1) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1]) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nUse tidybayes::get_variables to get the model variables, and then use tidybayes::spread_draws with the model1 to get the draws for “b_MAD” and “b_Intercept” as a tibble, bound to the variable posteriors1.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ntidybayes::get_variables(model1)\n\n [1] \"b_Intercept\"   \"b_MAD\"         \"sigma\"         \"Intercept\"    \n [5] \"lprior\"        \"lp__\"          \"accept_stat__\" \"stepsize__\"   \n [9] \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\nposteriors1 &lt;- model1 |&gt;\n  tidybayes::spread_draws(b_MAD, b_Intercept) |&gt;\n  select(b_MAD, b_Intercept)\n\nposteriors1\n\n# A tibble: 4,000 × 2\n   b_MAD b_Intercept\n   &lt;dbl&gt;       &lt;dbl&gt;\n 1 0.933     0.0294 \n 2 0.938    -0.0331 \n 3 0.991    -0.0276 \n 4 0.942    -0.0118 \n 5 0.928    -0.0234 \n 6 0.895     0.00991\n 7 0.923    -0.0467 \n 8 0.917    -0.0472 \n 9 0.962     0.103  \n10 0.981     0.110  \n# ℹ 3,990 more rows\n\n\n\n\nRepeat the last operation, but this time use ndraws = 100 to limit the number of draws (call this posterior2). Again using the first graph, add a line with intercept and slope corresponding to each row of posterior2 (i.e. add 100 lines).\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# wrangle data frame\nposteriors2 &lt;- model1 |&gt;\n  # parameter 'ndraws' requests 100 random subsamples\n  tidybayes::spread_draws(b_MAD, b_Intercept, ndraws = 100) |&gt;\n  select(b_MAD, b_Intercept)\n  \n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(data = posteriors2,\n              aes(intercept = b_Intercept, slope = b_MAD), \n              color = project_colors[2], size  = 0.1, alpha = 0.4) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\n\n\n\n\n\n\nUse model1 and tidybayes::gather_draws, to get the draws for “b_MAD” as a tibble in long form, bound to the variable posteriors3. Rename ‘.variable’ to ‘parameter’ and ‘.value’ to ‘posterior’ and then keep only those two columns.\nCalculate the mean, the lower and the upper bound of a 90% CrI, using the function tidybayes::hdi().\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nposteriors3 &lt;- model1 |&gt;\n   # use the gather_draws() function for \"long data\"\n   tidybayes::gather_draws(b_MAD) |&gt; \n   # change names of columns\n   dplyr::rename(parameter = .variable,\n          posterior = .value) |&gt; \n   # select only those columns that are relevant\n   dplyr::select(parameter, posterior)\n\nhead(posteriors3)\n\n# A tibble: 6 × 2\n# Groups:   parameter [1]\n  parameter posterior\n  &lt;chr&gt;         &lt;dbl&gt;\n1 b_MAD         0.933\n2 b_MAD         0.938\n3 b_MAD         0.991\n4 b_MAD         0.942\n5 b_MAD         0.928\n6 b_MAD         0.895\n\n\n\nposteriors3_agg &lt;- posteriors3 |&gt; \n  dplyr::group_by(parameter) |&gt; \n  dplyr::summarise(\n    `90lowerCrI`   = tidybayes::hdi(posterior, credMass = 0.90)[1],\n    mean_posterior = mean(posterior),\n    `90higherCrI`  = tidybayes::hdi(posterior, credMass = 0.90)[2])\n\nposteriors3_agg \n\n# A tibble: 1 × 4\n  parameter `90lowerCrI` mean_posterior `90higherCrI`\n  &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1 b_MAD            0.870          0.939          1.01"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_10_solutions.html#exercise-2-elasticity",
    "href": "labs/solutions/BSMM_8740_lab_10_solutions.html#exercise-2-elasticity",
    "title": "lab 10 - Bayesian Methods",
    "section": "Exercise 2: elasticity",
    "text": "Exercise 2: elasticity\nIn this exercise we will estimate price elasticity. The dataset contains price and monthly unit-volume data for four sites over 12 months. Since the unit-volumes are counts of unit sold, we’ll need a discrete pmf to model the data generation process.\n\n# read the data\ndat &lt;- \n  readr::read_csv(\"data/price_data.csv\",show_col_types = FALSE) |&gt; \n  dplyr::filter(site == \"Windsor\")\n# plot the data\ndat |&gt; \n  ggplot(aes(x=price, y=volume, group=site)) +\n  geom_point()\n\n\n\n\n\n\n\n\nSince elasticity is defined as the percentage change in volume (ΔV/V\\Delta V/V) for a given percentage change in price (Δp/p\\Delta p/p), then with elasticity parameter β\\beta we write:\nΔVV=β×Δpp∂VV=β×∂pp∂log(V)=β×∂log(p)\n\\begin{align*}\n\\frac{\\Delta V}{V} & = \\beta\\times\\frac{\\Delta p}{p} \\\\\n\\frac{\\partial V}{V} & = \\beta\\times\\frac{\\partial p}{p} \\\\\n\\partial\\log(V) & = \\beta\\times\\partial\\log(p)\n\\end{align*}\n\nThis equation is the justification for the log-log regression model of elasticity, and this model has solution V=KpβV = Kp^\\beta, where KK is a constant.\nAs written, the value of KK is either the volume when p=1p=1 which may or may not be useful, or it is the volume when β=0\\beta=0, which is uninteresting.\nTo make the interpretation of the constant KK more useful, the model can be written as\n∂log(V)=β×∂log(p/pbaseline);V=K(ppbaseline)β\n\\partial\\log(V) = \\beta\\times\\partial\\log(p/p_{\\text{baseline}});\\qquad V = K\\left(\\frac{p}{p_{\\text{baseline}}}\\right)^{\\beta}\n\nin which case the constant is interpreted as the volume when the price equals the baseline price; the elasticity parameter β\\beta is unchanged.\nThe implies that our regression model should be volume=log(K)+βlog(p)\\mathrm{volume} = \\log(K) + \\beta\\log(p) given the log\\log link in the Poisson model. To analyze the data\n\nscale and transform the price data by dividing by the mean price and then taking the log. Assign the modified data to the variable dat01.\nbuild and fit a Bayesian Poisson model with\n\na normal(0,5) prior on the intercept, and\na cauchy(0,10) prior on the independent variable with upper bound 0 (to ensure that elasticities are negative).\nassign the fit to the variable windsor_01 and save the fit in the folder “fits/windsor_01”\n\nonce the model is fit, summarize the draws\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ndat01 &lt;- dat |&gt; \n  dplyr::mutate( price = log( price/mean(price)) )\n\nwindsor_01 &lt;- \n  brm(data =\n    dat01,\n    family = poisson(),\n    volume ~ 1 + price,\n    prior = c(prior(normal(0, 5), class = Intercept),\n              prior(cauchy(0, 10), ub = 0, class = b)\n    ),\n    iter = 4000, warmup = 1000, chains = 4, cores = 4,\n    seed = 4,\n    file = \"fits/windsor_01\")\n\ntidybayes::summarise_draws(windsor_01)\n\n# A tibble: 5 × 10\n  variable      mean median     sd    mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b_Intercept   4.18   4.18 0.0361 0.0360   4.12   4.24  1.00    6649.    6665.\n2 b_price      -3.76  -3.76 1.35   1.38    -6.03  -1.58  1.00    5322.    3119.\n3 Intercept     4.18   4.18 0.0360 0.0358   4.12   4.24  1.00    6660.    6704.\n4 lprior       -5.78  -5.76 0.0881 0.0885  -5.94  -5.66  1.00    5496.    3277.\n5 lp__        -58.3  -57.9  1.14   0.792  -60.5  -57.2   1.00    3802.    3857.\n\n\n\npairs(windsor_01, variable = c('b_Intercept', 'b_price'))\n\n\n\n\n\n\n\n\n\n\nIn a Poisson model, the mean and variance of the dependent variable are equal. This is clearly not the case here (check this). So we might not expect the Poisson generating process to be a good fit.\nAn alternative discrete pmf to the Poisson data generation process is the negative binomial process.\n\nbuild and fit a Bayesian Negative Binomial model using the same priors as in the Poisson model\nassign the fit to the variable windsor_02 and save the fit in the folder “fits/windsor_02”\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nwindsor_02 &lt;- \n  brm(data =\n    dat01,\n    family = negbinomial(),\n    volume ~ 1 + price,\n    prior = c(prior(normal(0, 5), class = Intercept),\n              prior(cauchy(0, 10), ub = 0, class = b)\n    ),\n    iter = 4000, warmup = 1000, chains = 4, cores = 4,\n    seed = 4,\n    file = \"fits/windsor_02\")\n\ntidybayes::summarise_draws(windsor_02)\n\n# A tibble: 6 × 10\n  variable    mean median      sd     mad     q5     q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b_Inter…  4.18e0   4.19 7.22e-2  0.0647   4.07   4.30   1.00    5451.    5032.\n2 b_price  -3.93e0  -3.78 2.16e+0  2.21    -7.79  -0.720  1.00    4462.    2829.\n3 shape     1.40e4  26.5  1.08e+6 16.2      8.92  88.3    1.00    5367.    4462.\n4 Interce…  4.19e0   4.19 7.22e-2  0.0646   4.07   4.30   1.00    5445.    5062.\n5 lprior   -1.17e1 -11.7  1.01e+0  0.892  -13.4  -10.2    1.00    5273.    4638.\n6 lp__     -5.68e1 -56.4  1.47e+0  1.21   -59.7  -55.2    1.00    3606.    3985.\n\n\n\npairs(windsor_02, variable = c('b_Intercept', 'b_price') )\n\n\n\n\n\n\n\n\n\n\nSince we have discrete outcomes, the continuous distribution that is the default output of brms::pp_check is not appropriate here.\nInstead use the type = “rootogram” argument of brms::pp_check to plot posterior predictive checks for the Poisson and NB models.\nRootograms graphically compare frequencies of empirical distributions and expected (fitted) probability models. For the observed distribution the histogram is drawn on a square root scale (hence the name) and superimposed with a line for the expected frequencies.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\np1 &lt;- brms::pp_check(windsor_01, type = \"rootogram\") + xlim(20,120)\np2 &lt;- brms::pp_check(windsor_02, type = \"rootogram\") + xlim(20,120)\np1/p2\n\n\n\n\n\n\n\n\n\n\nFinally, compare the two model fits using brms::loo and brms::loo_compare and comment on the model comparison based on all the analyses performed in this exercise.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nbrms::loo_compare(brms::loo(windsor_01), brms::loo(windsor_02))\n\n           elpd_diff se_diff\nwindsor_02  0.0       0.0   \nwindsor_01 -6.5       4.0   \n\n\nThe loo comparison suggests that negative binomial model is better than the Poisson model, however, as you can see from the plot of the data and the histograms (in the rootograms) there just isn’t much data so you might proceed with caution. Note that the NB model does give higher likelihood to the largest value n the right in the histograms (just less than 100), consistent with it being the better model per the loo comparison.\nOne approach in this case it to use all the sites, estimating a hierachical model for elasticity, with a group level elasticity and site separate site-level adjustment. This might be more useful in practice."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_10_solutions.html#exercise-3",
    "href": "labs/solutions/BSMM_8740_lab_10_solutions.html#exercise-3",
    "title": "lab 10 - Bayesian Methods",
    "section": "Exercise 3:",
    "text": "Exercise 3:\nConsider the a complicated manufacturing process as follows, where production is one unit per day on average and manufacturing equipment maintenance takes 20% of the time on average.\n\n# define parameters\nprob_maintenance &lt;- 0.2  # 20% of days\nrate_work        &lt;- 1    # average 1 unit per day\n\n# sample one year of production\nn &lt;- 365\n\n# simulate days of maintenance\nset.seed(365)\nmaintenance &lt;- rbinom(n, size = 1, prob = prob_maintenance)\n\n# simulate units completed\ny &lt;- (1 - maintenance) * rpois(n, lambda = rate_work)\n\ndat &lt;-\n  tibble::tibble(maintenance = factor(maintenance, levels = 1:0), y = y)\n  \ndat %&gt;% \n  ggplot(aes(x = y)) +\n  geom_histogram(aes(fill = maintenance),\n                 binwidth = 1, linewidth = 1/10, color = \"grey92\") +\n  scale_fill_manual(values = ggthemes::canva_pal(\"Green fields\")(4)[1:2]) +\n  xlab(\"Units completed\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWith this data, the likelihood of observing zero on y, (i.e., the likelihood zero units were completed on a given day) is\nPr(0|p,λ)=Pr(maintenance|p)+Pr(work|p)×Pr(0|λ)=p+(1−p)exp(−λ).\n\\begin{align*}\n\\operatorname{Pr}(0 | p, \\lambda) & = \\operatorname{Pr}(\\text{maintenance} | p) + \\operatorname{Pr}(\\text{work} | p) \\times \\operatorname{Pr}(0 | \\lambda) \\\\\n                                   & = p + (1 - p) \\exp (- \\lambda).\n\\end{align*}\n\nAnd the likelihood of a non-zero value yy is:\nPr(y|y&gt;0,p,λ)=Pr(maintenance|p)(0)+Pr(work|p)Pr(y|λ)=(1−p)λyexp(−λ)y!\n\\operatorname{Pr}(y | y &gt; 0, p, \\lambda) = \\operatorname{Pr}(\\text{maintenance} | p) (0) + \\operatorname{Pr}(\\text{work} | p) \\operatorname{Pr}(y | \\lambda) = (1 - p) \\frac {\\lambda^y \\exp (- \\lambda)}{y!}\n\nSo letting pp be the probability that yy is zero and lambdalambda be the shape of the distribution, the zero-inflated Poisson (ZIPoisson) regression model might take the basic form:\nyi∼ZIPoisson(pi,λi)logit(pi)=αp+βpxilog(λi)=αλ+βλxi,\n\\begin{align*}\ny_{i} & \\sim\\mathrm{ZIPoisson}(p_{i},\\lambda_{i})\\\\\n\\mathrm{logit}(p_{i}) & =\\alpha_{p}+\\beta_{p}x_{i}\\\\\n\\log(\\lambda_{i}) & =\\alpha_{\\lambda}+\\beta_{\\lambda}x_{i},\n\\end{align*}\n where both parameters in the likelihood, pip_i and λi\\lambda_i might get their own statistical model. In brms, pip_i is denoted by zi.\nCreate a Bayesian zero-inflated Poisson Model (family = zero_inflated_poisson) with a normal(1,0.5) normal prior on the intercept and a beta(2,6) prior on zi (class = zi) and save the model in “fits/zi_model”\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nzi_model &lt;- \n  brms::brm(data = dat, \n      family = zero_inflated_poisson,\n      y ~ 1,\n      prior = c(prior(normal(1, 0.5), class = Intercept),\n                prior(beta(2, 6), class = zi)),  # the brms default is beta(1, 1)\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 12,\n      file = \"fits/zi_model\") \nprint(zi_model)\n\n Family: zero_inflated_poisson \n  Links: mu = log; zi = identity \nFormula: y ~ 1 \n   Data: dat (Number of observations: 365) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.09    -0.15     0.19 1.00     1333     1641\n\nFurther Distributional Parameters:\n   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nzi     0.23      0.05     0.12     0.34 1.00     1463     1475\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nfixef(zi_model)\n\n            Estimate  Est.Error       Q2.5     Q97.5\nIntercept 0.02353553 0.08690357 -0.1504926 0.1878018\n\n\nExpress the fitted parameters as probabilities:\nlambda = exp(Intercept) = exp(0.02) = 1.02\np = zi = 0.23"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_10_solutions.html#exercise-4",
    "href": "labs/solutions/BSMM_8740_lab_10_solutions.html#exercise-4",
    "title": "lab 10 - Bayesian Methods",
    "section": "Exercise 4:",
    "text": "Exercise 4:\nLoad the regional sales data below\n\ndat &lt;- readr::read_csv(\"data/regional_sales.csv\", show_col_types = FALSE)\n\nBuild the following sales model. How are the terms (1|region_id) and (1|store_id) referred to in the output, and how do you interpret them.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# Fit hierarchical Bayesian model\nsales_model &lt;- brm(\n  formula = log_sales ~ 1 + \n    scale(store_size) + \n    scale(competition_density) + \n    scale(marketing_spend) +\n    seasonal_effect + \n    (1|region_id) + \n    (1|store_id),\n  data = dat,\n  family = gaussian(),\n  prior = c(\n    prior(normal(10, 1), class = \"Intercept\"),\n    prior(normal(0, 0.5), class = \"b\"),\n    prior(cauchy(0, 0.2), class = \"sd\"),\n    prior(cauchy(0, 0.2), class = \"sigma\")\n  ),\n  chains = 4,\n  cores = 4,\n  seed = 456,\n  file = \"fits/sales_model\"\n)\n\nThe terms (1|region_id) and (1|store_id) are called random effects or random intercepts in a mixed-effects model.\nRandom intercepts allow different baseline levels for different groups (in this case, regions and stores) while assuming these levels come from a common distribution. This creates a partial pooling of information across groups - a middle ground between treating groups completely separately and completely the same.\nIn the model specification:\n\n(1|region_id) means each region gets its own intercept adjustment\n(1|store_id) means each store gets its own intercept adjustment\n\n\n\nCreate a tibble and bind it to the variable new_store_data. Give it the following columns:\n\nstore_size = 5000\ncompetition_density = 5\nmarketing_spend = 10000\nseasonal_effect = 0\nregion_id = 1\nstore_id = max(data$store_id) + 1\n\nUse the model and this data to predict sales for the new store and give the confidence intervals for the prediction.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nnew_store_data &lt;- tibble::tibble(\n  store_size = 5000,\n  competition_density = 5,\n  marketing_spend = 10000,\n  seasonal_effect = 0,\n  region_id = 1,\n  store_id = max(dat$store_id) + 1\n)\n\n\n# Make predictions\npredict(sales_model, newdata = new_store_data, allow_new_levels=TRUE)\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,]  10.9829 0.0967135 10.79838 11.17192"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_10_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_10_solutions.html#grading",
    "title": "lab 10 - Bayesian Methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 4\n30"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html",
    "href": "labs/solutions/2024-lab-5.html",
    "title": "Lab 5 - Classification and clustering",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering.\n\n\nBy the end of the lab you will…\n\nBe able to build workflows to fit different classification models.\nBe able to build workflows to evaluate different clustering models."
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#introduction",
    "href": "labs/solutions/2024-lab-5.html#introduction",
    "title": "Lab 5 - Classification and clustering",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering.\n\n\nBy the end of the lab you will…\n\nBe able to build workflows to fit different classification models.\nBe able to build workflows to evaluate different clustering models."
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#getting-started",
    "href": "labs/solutions/2024-lab-5.html#getting-started",
    "title": "Lab 5 - Classification and clustering",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-5-[your github username] repository .\nCreate an R project using your 2024-lab-5-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-5.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#packages",
    "href": "labs/solutions/2024-lab-5.html#packages",
    "title": "Lab 5 - Classification and clustering",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, forcats,\n  broom, yardstick, parsnip, workflows, rsample, tune, dials\n)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#the-data",
    "href": "labs/solutions/2024-lab-5.html#the-data",
    "title": "Lab 5 - Classification and clustering",
    "section": "The Data",
    "text": "The Data\nToday we will be using customer churn data.\nIn the customer management lifecycle, customer churn refers to a decision made by the customer about ending the business relationship. It is also referred as loss of clients or customers. This dataset contains 20 features related to churn in a telecom context and we will look at how to predict churn and estimate the effect of predictors on the customer churn odds ratio.\n\ndata &lt;- \n  readr::read_csv(\"data/Telco-Customer-Churn.csv\", show_col_types = FALSE) |&gt; \n  dplyr::mutate(churn = as.factor(churn))"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-1-eda",
    "href": "labs/solutions/2024-lab-5.html#exercise-1-eda",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nWrite and execute the code to perform summary EDA on the data using the package skimr. Plot histograms for monthly charges and tenure. Tenure measures the strength of the customer relationship by measuring the length of time that a person has been a customer.\n\n\n\n\n\n\nYOUR ANSWER:"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-2-train-test-splits-recipe",
    "href": "labs/solutions/2024-lab-5.html#exercise-2-train-test-splits-recipe",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 2: train / test splits & recipe",
    "text": "Exercise 2: train / test splits & recipe\nWrite and execute code to create training and test datasets. Have the training dataset represent 70% of the total data.\nNext create a recipe where churn is related to all the other variables, and\n\nnormalize the numeric variables\ncreate dummy variables for the ordinal predictors\n\nMake sure the steps are in a sequence that preserves the (0,1) dummy variables.\nPrep the data on the training data and show the result.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nset.seed(8740)\n\n# split data\n\n\n# create a recipe"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-3-logistic-modeling",
    "href": "labs/solutions/2024-lab-5.html#exercise-3-logistic-modeling",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 3: logistic modeling",
    "text": "Exercise 3: logistic modeling\n\nCreate a linear model using logistic regression to predict churn. for the set engine stage use “glm,” and set the mode to “classification.”\nCreate a workflow using the recipe of the last exercise and the model if the last step.\nWith the workflow, fit the training data\nCombine the training data and the predictions from step 3 using broom::augment , and assign the result to a variable\nCreate a combined metric function using yardstick::metric_set as show in the code below:\nUse the variable from step 4 as the first argument to the function from step 5. The other arguments are truth = churn (from the data) and estimate=.pred_class (from step 4). Make a note of the numerical metrics.\nUse the variable from step 4 as the first argument to the functions listed below, with arguments truth = churn and estimate =``.pred_No.\n\nyardstick::roc_auc\nyardstick::roc_curve followed by ggplot2::autoplot().\n\n\n\n\n\n\n\n\nrank-deficiency\n\n\n\nYou can ignore this message. It means that there are a lot of predictors.\n\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# create a linear regression model\n\n# create a workflow\n\n# fit the workflow\n\n# augment the training data with the fitted data\n\n\n# create the metrics function\nm_set_fn &lt;- \n  yardstick::metric_set(\n    yardstick::accuracy\n    , yardstick::precision\n    , yardstick::recall\n    , yardstick::f_meas\n    , yardstick::spec\n    , yardstick::sens\n    , yardstick::ppv\n    , yardstick::npv\n)\n\n\n# compute roc_auc and plot the roc_curve"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-4-effects",
    "href": "labs/solutions/2024-lab-5.html#exercise-4-effects",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 4: effects",
    "text": "Exercise 4: effects\nUse broom::tidy() on the fit object from exercise 4 to get the predictor coefficients. Sort them in decreasing order by absolute value.\nWhat is the effect of one additional year of tenure on the churn odds ratio?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# \n\nThe effect of one additional year of tenure on the churn odds ratio is __."
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-5-knn-modeling",
    "href": "labs/solutions/2024-lab-5.html#exercise-5-knn-modeling",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 5 knn modeling",
    "text": "Exercise 5 knn modeling\nNow we will create a K-nearest neighbours model to estimate churn. To do this, write the code for the following steps:\n\nCreate a K-nearest neighbours model to predict churn using parsnip::nearest_neighbor with argument neighbors = 3 which will use the three most similar data points from the training set to predict churn. For the set engine stage use “kknn,” and set the mode to “classification.”\nTake the workflow from exercise 3 and create a new workflow by updating the original workflow. Use workflows::update_model to swap out the original logistic model for the nearest neighbour model.\nUse the new workflow to fit the training data. Take the fit and use broom::augment to augment the fit with the training data.\nUse the augmented data from step 3 to plot the roc curve, using yardstick::roc_curve(.pred_No, truth = churn) as in exercise 3. How do you interpret his curve?\nTake the fit from step 3 and use broom::augment to augment the fit with the test data.\nRepeat step 4 using the augmented data from step 5.\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?parsnip::nearest_neighbor\n\n# create a knn classification model model\n\n# create a workflow\n\n# fit the workflow\n\n# augment the training data with the fitted data\n\n\n# compute the metrics\n\n\n?yardstick::roc_curve\n# compute roc_auc and plot the roc_curve"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-6-cross-validation",
    "href": "labs/solutions/2024-lab-5.html#exercise-6-cross-validation",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 6 cross validation",
    "text": "Exercise 6 cross validation\nFollowing the last exercise, we should have some concerns about over-fitting by the nearest-neighbour model.\nTo address this we will use cross validation to tune the model and evaluate the fits.\n\nCreate a cross-validation dataset based on 5 folds using rsample::vfold_cv.\nUsing the knn workflow from exercise 5, apply tune::fit_resamples with arguments resamples and control where the resamples are the dataset created in step 1 and control is tune::control_resamples(save_pred = TRUE), which will ensure that the predictions are saved.\nUse tune::collect_metrics() on the results from step 2\nUse tune::collect_predictions() on the results from step 2 to plot the roc_auc curve as in exercise 5. Has it changed much from exercise 5?\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?rsample::vfold_cv\n\n# create v-fold cross validation data\n\n# use tune::fit on the cv dat, saving the predictions\n\n\n?tune::fit_resamples\n\n# collect the metrics\n\n# compute the roc_curve\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-7-tuning-for-k",
    "href": "labs/solutions/2024-lab-5.html#exercise-7-tuning-for-k",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 7: tuning for k",
    "text": "Exercise 7: tuning for k\nIn this exercise we’ll tune the number of nearest neighbours in our model to see if we can improve performance.\n\nRedo exercise 5 steps 1 and 2, setting neighbors = tune::tune() for the model, and then updating the workflow with workflows::update_model.\nUse dials::grid_regular(dials::neighbors(), levels = 10) to create a grid for tuning k.\nUse tune::tune_grid with tune::control_grid(save_pred = TRUE) and yardstick::metric_set(yardstick::accuracy, yardstick::roc_auc) to generate tuning results\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?tune::tune_grid\n\n# re-specify the model for tuning\n\n\n# update the workflow\n\n\n# make a grid for tuning\n\n\n# use the grid to tune the model\n\n\n# show the tuning results dataframe"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-8",
    "href": "labs/solutions/2024-lab-5.html#exercise-8",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 8",
    "text": "Exercise 8\nUse tune::collect_metrics() to collect the metrics from the tuning results in exercise 7 and then plot the metrics as a function of k using the code below.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# collect the metrics\n\n# plot the collected metrics as a function of K\n_your_metrics_ |&gt;\nggplot(aes(neighbors,mean)) +\n  geom_line(linewidth = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2)"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-9",
    "href": "labs/solutions/2024-lab-5.html#exercise-9",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 9",
    "text": "Exercise 9\nUse tune::show_best and tune::select_best with argument “roc_auc” to find the best k for the knn classification model. Then\n\nupdate the workflow using tune::finalize_workflow to set the best k value.\nuse tune::last_fit with the updated workflow from step 1, evaluated on the split data from exercise 2 to finalize the fit.\nuse tune::collect_metrics() to get the metrics for the best fit\nuse tune::collect_predictions() to get the predictions and plot the roc_auc as in the prior exercises\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?tune::finalize_workflow\n# show the roc_auc metrics\n\n# select the best roc_auc metric (using a function from tune::)\n\n# finalize the workflow with the best nn metric from the last step\n\n# use  tune::last_fit with the finaized workflow on the data_split (ex 2)\n\n# collect the metrics from the final fit\n\n# collect the predictions from the final fit and plot the roc_curve"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#exercise-10-clustering",
    "href": "labs/solutions/2024-lab-5.html#exercise-10-clustering",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 10: clustering",
    "text": "Exercise 10: clustering\nLoad the data for this exercise as below and plot it, and then create an analysis dataset with the cluster labels removed\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# read the data\nlabelled_points &lt;- readr::read_csv(\"data/lab_5_clusters.csv\", show_col_types = FALSE)\n\n# plot the clusters\nlabelled_points |&gt; ggplot(aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3) + \n  theme(legend.position=\"none\")\n\n# remove cluster labels to make the analysis dataset\npoints &lt;-\n  labelled_points |&gt;\n  select(?)\n\nYou have frequently used broom::augment to combine a model with the data set, and broom::tidy to summarize model components; broom::glance is used to similarly to summarize goodness-of-fit metrics.\nNow perform k-means clustering on the points data for different values of k as follows:\n\nkclusts &lt;-\n  # number of clusters from 1-9\n  tibble(k = 1:9) |&gt;\n  # mutate to add columns\n  mutate(\n    # a list-column with the results of the kmeans function (clustering)\n    kclust = purrr::map(k, ~stats::kmeans(points, .x)),\n    # a list-column with the results broom::tidy applied to the clustering results\n    tidied = purrr::map(kclust, broom::tidy),\n    # a list-column with the results broom::glance applied to the clustering results\n    glanced = purrr::map(kclust, broom::glance),\n    # a list-column with the results broom::augment applied to the clustering results\n    augmented = purrr::map(kclust, broom::augment, points)\n  )\n\n(i) Create 3 variables by tidyr::unnesting the appropriate columns of kclusts\n\n# take kclusts and use tidy::unnest() on the appropriate columns\n\nclusters &lt;- ?\n\nassignments &lt;- ?\n\nclusterings &lt;- ?\n\n(ii) Use the assignments variable to plot the cluster assignments generated by stats::kmeans\n\n# plot the points assigned to each cluster\np &lt;- assignments |&gt; ggplot(aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) +\n  facet_wrap(~ k) + theme(legend.position=\"none\")\np\n\n(iii) Use the clusters variable to add the cluster centers to the plot\n\n# on the last plot, mark the cluster centres with an X\np + geom_point(data = clusters, size = 10, shape = \"x\")\n\n(iv) Use the clusterings variable to plot the total within sum of squares value by number of clusters.\n\n# make a separate line-and-point plot with the tot-withinss data by cluster number\nclusterings |&gt; ggplot(aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n\n(v) Using the results of parts (iii) and (iv), the k (number of clusters) that gives the best results is __.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 5!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyone’s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/solutions/2024-lab-5.html#grading",
    "href": "labs/solutions/2024-lab-5.html#grading",
    "title": "Lab 5 - Classification and clustering",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html",
    "title": "Lab 7 - Causality: DAGs",
    "section": "",
    "text": "In today’s lab, you’ll practice working with DAGs and building a causal workflow.\n\n\nBy the end of the lab you will…\n\nBe able to build DAGs to model causal assumptions and use the causal model to extract implications for answering causal questions.\nBe able to build a causal workflow to answer causal questions."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html#introduction",
    "title": "Lab 7 - Causality: DAGs",
    "section": "",
    "text": "In today’s lab, you’ll practice working with DAGs and building a causal workflow.\n\n\nBy the end of the lab you will…\n\nBe able to build DAGs to model causal assumptions and use the causal model to extract implications for answering causal questions.\nBe able to build a causal workflow to answer causal questions."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html#packages",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, broom, rsample, ggdag, causaldata, halfmoon, ggokabeito, malcolmbarrett/causalworkshop\n  , magrittr, ggplot2, estimatr, Formula, r-causal/propensity, gt, gtExtras)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-1-dags-and-open-paths",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-1-dags-and-open-paths",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 1: DAGs and open paths",
    "text": "Exercise 1: DAGs and open paths\nFind the open paths from D (treatment) to Y (outcome) in the four DAGs below.\nYou can examine the DAGS to identify (this is the recommended first step) and then use the code for the DAGs (in your repo), along with the function dagitty::paths to confirm\n\n\nFour example DAGs\n# (1)\ndag_1 &lt;- \nggdag::dagify(\n  A ~ D\n  , Y ~ A\n  , B ~ A\n  , coords = ggdag::time_ordered_coords(\n    list(\n      \"D\"             # time point 1\n      , c(\"A\",\"B\")    # time point 2\n      , \"Y\"           # time point 3\n    )\n  ),\n  exposure = \"D\",\n  outcome = \"Y\"\n) \n\n# (2)\ndag_2 &lt;- \nggdag::dagify(\n  D ~ A\n  , E ~ D\n  , Y ~ E\n  , E ~ F\n  , B ~ F\n  , B ~ A\n  , C ~ B\n  , coords = ggdag::time_ordered_coords(\n    list(\n      c(\"A\",\"D\")             # time point 1\n      , c(\"B\",\"F\",\"E\")    # time point 2\n      , c(\"C\",\"Y\")           # time point 3\n    )\n  ),\n  exposure = \"D\",\n  outcome = \"Y\"\n) \n\n# (3)\ndag_3 &lt;- \nggdag::dagify(\n  A ~ D\n  , Y ~ D\n  , D ~ B\n  , A ~ B\n  , Y ~ B\n  , coords = ggdag::time_ordered_coords(\n    list(\n      \"D\"             # time point 1\n      , c(\"B\",\"A\")    # time point 2\n      , \"Y\"           # time point 3\n    )\n  ),\n  exposure = \"D\",\n  outcome = \"Y\"\n) \n\n# (4)\ndag_4 &lt;- \n  ggdag::dagify(\n    Y ~ D\n    , Y ~ C\n    , D ~ B\n    , D ~ A\n    , B ~ C\n    , B ~ A\n    , coords = ggdag::time_ordered_coords(\n      list(\n        c(\"A\",\"D\")             # time point 1\n        , \"B\"    # time point 2\n        , c(\"C\",\"Y\")           # time point 3\n      )\n    ),\n    exposure = \"D\",\n    outcome = \"Y\"\n  ) \n# %&gt;% \n#   ggdag::ggdag(text = TRUE) +\n#   ggdag::theme_dag()\n\ndag_flows &lt;- \n  purrr::map(\n    list(dag_1 = dag_1, dag_2 = dag_2, dag_3 = dag_3, dag_4 = dag_4)\n    , ggdag::tidy_dagitty\n  ) |&gt; \n  purrr::map(\"data\") |&gt; \n  purrr::list_rbind(names_to = \"dag\") |&gt; \n  dplyr::mutate(dag = factor(dag, levels = c(\"dag_1\", \"dag_2\", \"dag_3\", \"dag_4\")))\n\ndag_flows |&gt; \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  ggdag::geom_dag_edges(edge_width = 1) + \n  ggdag::geom_dag_point() + \n  ggdag::geom_dag_text() + \n  facet_wrap(~ dag) +\n  ggdag::expand_plot(\n    expand_x = expansion(c(0.2, 0.2)),\n    expand_y = expansion(c(0.2, 0.2))\n  ) +\n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nDAG_1\n\n\n# open paths DAG 1: \"D -&gt; A -&gt; Y\"\ndag_1 %&gt;% dagitty::paths()\n\n$paths\n[1] \"D -&gt; A -&gt; Y\"\n\n$open\n[1] TRUE\n\n\n\nDAG_2\n\n\n# open paths DAG 2: \"D -&gt; E -&gt; Y\"\ndag_2 %&gt;% dagitty::paths()\n\n$paths\n[1] \"D -&gt; E -&gt; Y\"                \"D &lt;- A -&gt; B &lt;- F -&gt; E -&gt; Y\"\n\n$open\n[1]  TRUE FALSE\n\n\n\nDAG_3\n\n\n# open paths DAG 3: \"D -&gt; Y\" and \"D &lt;- B -&gt; Y\"\ndag_3 %&gt;% dagitty::paths()\n\n$paths\n[1] \"D -&gt; A &lt;- B -&gt; Y\" \"D -&gt; Y\"           \"D &lt;- B -&gt; Y\"     \n\n$open\n[1] FALSE  TRUE  TRUE\n\n\n\nDAG_4\n\n\n# open paths DAG 4: \"D -&gt; Y\" and \"D &lt;- B &lt;- C -&gt; Y\"\ndag_4 %&gt;% dagitty::paths()\n\n$paths\n[1] \"D -&gt; Y\"                \"D &lt;- A -&gt; B &lt;- C -&gt; Y\" \"D &lt;- B &lt;- C -&gt; Y\"     \n\n$open\n[1]  TRUE FALSE  TRUE"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-2-building-a-dag",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-2-building-a-dag",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 2: Building a DAG",
    "text": "Exercise 2: Building a DAG\nYou work for a company that sells a commodity to retail customers, and your management is interested in the relationship between your price and the demand for the commodity at your outlets. You have one competitor and your pricing tactic is to set your price at slightly less that your competitor’s. Your company surveys the competitors prices several times per day and once you know the competitor’s price, the pricing team resets your prices according to the pricing tactic. The public is well informed of both prices when they make their choice to buy.\nYou and your competitor buy from the wholesaler at a price that is set by the global market, and the wholesaler’s price is reset at the beginning of the each day according to the market price at the end of the day before. As the market is traded globally it reflects global demand for the commodity as well as other global and local economic shocks that you customers might be exposed to (interest rates, general business conditions, wages, etc.).\nYour company has data on its prices, competitor prices and sales, and has asked you to do an analysis of the pricing tactics to increase demand.\nTo confirm your understanding of the business, perhaps identify missing data, and to inform your analysis, create a DAG describing the assumed relationships between the driving factors for this problem.\nWhat data might be missing from dataset provided by the company?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n(1) The DAG: The description above describes the DAG shown below\n\n\nprice-demand DAG\nset.seed(8740)\nggdag::dagify(\n  demand ~ price + economy + c_price\n  , price ~ c_price + wholesale\n  , c_price ~ wholesale\n  , wholesale ~ economy\n  , exposure = \"price\"\n  , outcome = \"demand\"\n  , labels = c(\n      price = \"price\",\n      c_price = \"c_price\",\n      economy = \"economy\",\n      demand = \"demand\",\n      wholesale = \"wholesale\"\n    )\n) %&gt;%\n  ggdag::ggdag(use_labels = \"label\", text = FALSE) +\n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\n(2) The missing data: There is an open path: price -&gt; c_price -&gt; demand, and clearly our competitor’s price is a confounder of our price effect on demand - but it can be controlled for since we have the competitor price data.\nHowever, there is also an open path: price -&gt; wholesale -&gt; economy -&gt; demand, and this needs to be closed if we are to accurately estimate our price effect on demand. We could close this path if we had data on the wholesale prices or on the ‘economy,’ as controlling for either would close the path, but the latter is unmeasured and even if there were some measure of the ‘economy,’ the wholesale prices would likely have less measurement error. So both those datasets are missing, but we should ask management for, or otherwise acquire, the wholesale price history.\nWe might be able to do a bit better too, perhaps by reducing variance by including data on weather, traffic, holidays, etc., expanding our model as follows:\n\n\nrevised price-demand DAG\nset.seed(8740)\nggdag::dagify(\n  demand ~ price + traffic + c_price + economy\n  , traffic ~ economy\n  , demand ~ weather\n  , demand ~ holidays\n  , price ~ c_price + wholesale\n  , c_price ~ wholesale\n  , wholesale ~ economy\n  , exposure = \"price\"\n  , outcome = \"demand\"\n  , labels = c(\n      price = \"price\",\n      c_price = \"c_price\",\n      economy = \"economy\",\n      demand = \"demand\",\n      wholesale = \"wholesale\",\n      traffic = \"traffic\",\n      weather = \"weather\",\n      holidays = \"holidays\"\n    )\n) %&gt;%\n  ggdag::ggdag(use_labels = \"label\", text = FALSE) +\n  ggdag::theme_dag()"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-3-inverse-probability-weights-ipws",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-3-inverse-probability-weights-ipws",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 3: Inverse Probability Weights (IPWs)",
    "text": "Exercise 3: Inverse Probability Weights (IPWs)\nIn class we used the function propensity::wt_ate to calculate inverse probability weights, starting from the propensity scores, as in the code below:\n\npropensity_model &lt;- glm(\n  net ~ income + health + temperature,\n  data = causalworkshop::net_data,\n  family = binomial()\n)\n\nRepeat the calculation of the IPWs, using the definition of the weight as the inverse probability, and show that your calculated weights are the same as those computed by propensity::wt_ate\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# calculate inverse probability weights\nnet_data_wts &lt;- propensity_model |&gt;\n  # augment the origiinal data with the predictions from that data per the model\n  broom::augment(newdata = causalworkshop::net_data, type.predict = \"response\") |&gt;\n  # add columns for \n  # - directly calculated weights and \n  # - weights from propensity::wt_ate, and\n  # - eqquality comparison of the two weights\n  dplyr::mutate(\n    ip_wts =\n      # NOTE: the weight is 1/(probability of treatment)- so 1/.fitted if net = TRUE and 1/(1-.fitted) otherwise\n      dplyr::case_when(\n        net ~ 1/.fitted\n        , TRUE ~ 1/(1-.fitted)\n      )\n    , wts = propensity::wt_ate(.fitted, net)\n    , \"ip_wts = wts\" = ip_wts == wts\n)\n\nnet_data_wts |&gt; dplyr::select(net, ip_wts, wts, `ip_wts = wts`)\n\n# A tibble: 1,752 × 4\n   net   ip_wts   wts `ip_wts = wts`\n   &lt;lgl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;         \n 1 FALSE   1.33  1.33 TRUE          \n 2 FALSE   1.28  1.28 TRUE          \n 3 FALSE   1.48  1.48 TRUE          \n 4 FALSE   1.30  1.30 TRUE          \n 5 FALSE   1.39  1.39 TRUE          \n 6 FALSE   1.44  1.44 TRUE          \n 7 FALSE   1.50  1.50 TRUE          \n 8 FALSE   1.20  1.20 TRUE          \n 9 FALSE   1.29  1.29 TRUE          \n10 FALSE   1.34  1.34 TRUE          \n# ℹ 1,742 more rows\n\n\n\n# summarize by evaluating if all the weights are positive\nnet_data_wts |&gt; dplyr::select(net, ip_wts, wts, `ip_wts = wts`) |&gt; \n  dplyr::summarise(\"all equal\" = all(`ip_wts = wts`) )\n\n# A tibble: 1 × 1\n  `all equal`\n  &lt;lgl&gt;      \n1 TRUE"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-4-randomized-controlled-trials",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-4-randomized-controlled-trials",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 4: Randomized Controlled Trials",
    "text": "Exercise 4: Randomized Controlled Trials\nThe essence of exchangeability is that the treated and untreated groups are very similar with respect to values of potential confounders. Randomization of treatment makes outcomes independent of treatment, and also makes the treated and untreated groups very similar with respect to values of potential confounders.\nShow that this is the case for our mosquito net data by simulating random treatment assignment as follows:\n\n# use this data - mosquito net data plus a row id numer\nsmpl_dat &lt;- causalworkshop::net_data |&gt;\n  tibble::rowid_to_column()\n\n\nuse tidysmd::tidy_smd with smpl_dat and group=net to calculate the standardized mean differences (SMDs) for the confounders income, health and temperature.\nuse dplyr::slice_sample to randomly sample from smpl_dat, with proportion 0.5. Give this sample data a name.\nmutate the sample to add a column with smpl = 1.\ntake the data not in the first sample and form a second sample (start with the original data (smpl_dat) and remove the rows that appear in the sample of step 1. This is why we added a row id. Give this second sample data a name.\nmutate the second sample to add a column with smpl = 0.\nbind the two samples together by rows (e.g. dplyr::bind_rows).\nuse tidysmd::tidy_smd with the combined samples from step 6 and group=smpl to calculate the standardized mean differences (SMDs) for the confounders income, health and temperature.\n\nDid randomization make the treatment groups more alike with respect to income, health and temperature?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create the first random sample of the data (sample 0)\nsplit_0 &lt;- smpl_dat |&gt;\n  dplyr::slice_sample(prop=0.5) |&gt;\n  dplyr::mutate(smpl = 0)\n\n# create the second random sample from the first (sample 1)\nsplit_data &lt;- split_0 |&gt;\n  dplyr::bind_rows(\n    smpl_dat[-split_0$rowid,] |&gt;\n      dplyr::mutate(smpl = 1)\n  )\n\nsmd_dat &lt;- tidysmd::tidy_smd(\n  split_data,\n  c(income, health, temperature),\n  .group = smpl,\n) \nsmd_dat\n\n# A tibble: 3 × 4\n  variable    method   smpl      smd\n  &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt;\n1 income      observed 1      0.0305\n2 health      observed 1      0.0511\n3 temperature observed 1     -0.0291\n\n\n\n# plot the differences\nsmd_dat |&gt; dplyr::mutate( samples = \"random\" ) |&gt; \n  dplyr::bind_rows(\n    tidysmd::tidy_smd(\n      causalworkshop::net_data,\n      c(income, health, temperature),\n      .group = net,\n    ) |&gt; dplyr::mutate( samples = \"non-random\" )\n  ) |&gt; \n  ggplot(\n    aes(x = abs(smd), y = variable, group = samples, color = samples)\n  ) + tidysmd::geom_love()"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-5-frisch-waugh-lovell-theorem",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-5-frisch-waugh-lovell-theorem",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 5: Frisch-Waugh-Lovell Theorem",
    "text": "Exercise 5: Frisch-Waugh-Lovell Theorem\nHere we’ll look at credit and default-risk data. First we’ll load the data:\n\n# load data\nrisk_data = readr::read_csv(\"data/risk_data.csv\", show_col_types = FALSE)\n\nThe FWL Theorem states that a multivariate linear regression can be estimated all at once or in three separate steps. For example, you can regress default on the financial variables credit_limit, wage, credit_score1, and credit_score2 as follows:\n\n# regress default on financial variabbles in the dataset\nmodel &lt;- lm(default ~ credit_limit + wage +  credit_score1 + credit_score2, data = risk_data)\n\nmodel |&gt; broom::tidy(conf.int = TRUE)\n\n# A tibble: 5 × 7\n  term             estimate  std.error statistic  p.value     conf.low conf.high\n  &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.404      0.00860        46.9  0             3.87e-1   4.21e-1\n2 credit_limit   0.00000306 0.00000154      1.99 4.69e- 2      4.16e-8   6.08e-6\n3 wage          -0.0000882  0.00000607    -14.5  8.33e-48     -1.00e-4  -7.63e-5\n4 credit_score1 -0.0000417  0.0000183      -2.28 2.28e- 2     -7.77e-5  -5.82e-6\n5 credit_score2 -0.000304   0.0000152     -20.1  4.10e-89     -3.34e-4  -2.74e-4\n\n\nPer FWL you can also break this down into\n\na de-biasing step, where you regress the treatment (credit_limit) on the financial confounders wage, credit_score1, and credit_score2 , obtaining the residuals\na de-noising step, where you regress the outcome (default) on the financial confounders, obtaining the residuals\nan outcome model, where you regress the outcome residuals from step 2 on the treatment residuals of step 1.\n\nDue to confounding, the data looks like this, with default percentage trending down by credit limit.\n\nrisk_data |&gt; \n  dplyr::group_by(credit_limit) |&gt; \n  # add columns for number of measurements in the group, and the mean of the group\n  dplyr::mutate(size = n(), default = mean(default), ) |&gt; \n  # pull ot the distict values\n  dplyr::distinct(default,credit_limit, size) |&gt; \n  ggplot(aes(x = credit_limit, y = default, size = size)) +\n  geom_point() +\n  labs(title = \"Default Rate by Credit Limit\", x = \"credit_limit\", y = \"default\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nStep 1:\n\nCreate the debiasing model, and\nadd the residuals to the risk_data, saving the result in risk_data_deb in a column credit_limit_res.\nplot the de-biased data\nregress the outcome (default) on credit_limit_res\n\n\n\n\n\n\n\nSOLUTION - step 1:\n\n\n\n\n# create debiasing model\ndebiasing_model &lt;- \n  lm(\n    # the de-biasing step predicts the treatment will all variables **except** the treatment\n    credit_limit ~ wage + credit_score1 + credit_score2 \n    , data = risk_data\n  )\n\n# add a column with the residuals of the debiasing model \n# (add the residuals to the mean credit limit to give a nice interpretation to a zero residual)\nrisk_data_deb &lt;- risk_data |&gt; \n  dplyr::mutate(\n    credit_limit_res = \n      mean(credit_limit) + debiasing_model$residuals \n  )\n\nrisk_data_deb |&gt; \n  # round the residuals prior to grouping\n  dplyr::mutate(credit_limit_res = round(credit_limit_res, digits=-2)) |&gt; \n  dplyr::group_by(credit_limit_res) |&gt; \n  # add columns for number of measurements in the group, and the mean of the group\n  dplyr::mutate(size = n(), default = mean(default), ) |&gt; \n  # only plot the residual groups with 'large' numbers of cases\n  dplyr::filter(size&gt;30) |&gt; \n  # pull oot the distinct values\n  dplyr::distinct(default,credit_limit_res, size) |&gt; \n  ggplot(aes(x = credit_limit_res, y = default, size = size)) +\n  geom_point() +\n  labs(title = \"Default Rate by Debiased Credit Limit\", x = \"credit_limit\", y = \"default\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# predict the outcome with the de-biasing residual data\nlm(default ~ credit_limit_res, data = risk_data_deb) |&gt; \n  # note the confidence intervals on the estimate of credit_limit_res\n    broom::tidy(conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term               estimate  std.error statistic   p.value conf.low  conf.high\n  &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)      0.142      0.00474        30.0  5.28e-196  1.33e-1 0.151     \n2 credit_limit_res 0.00000306 0.00000156      1.96 5.03e-  2 -4.29e-9 0.00000613\n\n\nIn this last regression, the coefficient estimated for credit_limit_res should be the same as the coefficient estimated for credit_limit in the initial regression.\nNote the difference in the confidence intervals though.\n\n\nThe de-biasing step is crucial for estimating the correct causal effect, while the de-noising step is nice to have, since it reduces the variance of the coefficient estimate and narrows the confidence interval.\nStep 2:\n\ncreate the de-noising model,\nadd the residuals to the de-biased data (risk_data_deb), saving the result in risk_data_denoise in a column default_res.\n\n\n\n\n\n\n\nSOLUTION - step 2:\n\n\n\n\n# create the de-noising model\ndenoising_model &lt;- \n    lm(\n      # the de-noising step predicts the outcome with all variables **except** the treatment\n      default ~ wage + credit_score1  + credit_score2\n      , data = risk_data_deb\n    )\n\nrisk_data_denoise &lt;-risk_data_deb |&gt; \n  # add a column with the residuals of the de-noised model \n  # (add the residuals to the mean default to give a nice interpretation to the zero residual)\n  dplyr::mutate(default_res = denoising_model$residuals + mean(default))\n\n\n\nStep 3:\n\nregress the default residuals (default_res) on the credit limit residuals (credit_limit_res)\n\n\n\n\n\n\n\nSOLUTION - step 3:\n\n\n\n\nlm(\n  # the final step regresses the outcome residuals the treatment residuals\n  default_res ~ credit_limit_res\n  , data = risk_data_denoise\n) |&gt; \n    broom::tidy(conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term               estimate  std.error statistic   p.value  conf.low conf.high\n  &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)      0.142      0.00466        30.5  6.74e-202   1.33e-1   1.51e-1\n2 credit_limit_res 0.00000306 0.00000154      1.99 4.69e-  2   4.17e-8   6.08e-6\n\n\nHow do the coefficients and confidence intervals from the FWL steps above compare to the original multivariate regression?\nThey are essentially identical, showing that the two methods are equivalent."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-6-causal-modeling",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html#exercise-6-causal-modeling",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 6: Causal Modeling",
    "text": "Exercise 6: Causal Modeling\nIn this guided exercise, we attempt to answer a causal question: does quitting smoking make you gain weight? Causal modeling has a special place in the history of smoking research: the studies that demonstrated that smoking causes lung cancer were observational. Thanks to other studies, we also know that, if you’re already a smoker, quitting smoking reduces your risk of lung cancer. However, some have observed that former smokers tend to gain weight. Is this the result of quitting smoking, or does something else explain this effect? In the book Causal Inference by Hernán and Robins, the authors analyze this question using several causal inference techniques.\nTo answer this question, we’ll use causal inference methods to examine the relationship between quitting smoking and gaining weight. First, we’ll draw our assumptions with a causal diagram (a directed acyclic graph, or DAG), which will guide our model. Then, we’ll use a modeling approach called inverse probability weighting–one of many causal modeling techniques–to estimate the causal effect we’re interested in.\nWe’ll use data from NHEFS to try to estimate the causal effect of quitting smoking on weight game. NHEFS is a longitudinal, observational study that has many of the variables we’ll need. Take a look at causaldata::nhefs_codebook if you want to know more about the variables in this data set. These data are included in the {causaldata} package. We’ll use the causaldata::nhefs_complete data set, but we’ll remove people who were lost to follow-up.\n\n\nNHEFS data\nnhefs_complete_uc &lt;- causaldata::nhefs_complete |&gt;\n  dplyr::filter(censored == 0)\nnhefs_complete_uc\n\n\n# A tibble: 1,566 × 67\n    seqn  qsmk death yrdth modth dadth   sbp   dbp sex     age race  income\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1   233     0     0    NA    NA    NA   175    96 0        42 1         19\n 2   235     0     0    NA    NA    NA   123    80 0        36 0         18\n 3   244     0     0    NA    NA    NA   115    75 1        56 1         15\n 4   245     0     1    85     2    14   148    78 0        68 1         15\n 5   252     0     0    NA    NA    NA   118    77 0        40 0         18\n 6   257     0     0    NA    NA    NA   141    83 1        43 1         11\n 7   262     0     0    NA    NA    NA   132    69 1        56 0         19\n 8   266     0     0    NA    NA    NA   100    53 1        29 0         22\n 9   419     0     1    84    10    13   163    79 0        51 0         18\n10   420     0     1    86    10    17   184   106 0        43 0         16\n# ℹ 1,556 more rows\n# ℹ 55 more variables: marital &lt;dbl&gt;, school &lt;dbl&gt;, education &lt;fct&gt;, ht &lt;dbl&gt;,\n#   wt71 &lt;dbl&gt;, wt82 &lt;dbl&gt;, wt82_71 &lt;dbl&gt;, birthplace &lt;dbl&gt;,\n#   smokeintensity &lt;dbl&gt;, smkintensity82_71 &lt;dbl&gt;, smokeyrs &lt;dbl&gt;,\n#   asthma &lt;dbl&gt;, bronch &lt;dbl&gt;, tb &lt;dbl&gt;, hf &lt;dbl&gt;, hbp &lt;dbl&gt;,\n#   pepticulcer &lt;dbl&gt;, colitis &lt;dbl&gt;, hepatitis &lt;dbl&gt;, chroniccough &lt;dbl&gt;,\n#   hayfever &lt;dbl&gt;, diabetes &lt;dbl&gt;, polio &lt;dbl&gt;, tumor &lt;dbl&gt;, …\n\n\nLet’s look at the distribution of weight gain between the two groups.\n\n\nweight gain distributions\nnhefs_complete_uc |&gt;\n  ggplot(aes(wt82_71, fill = factor(qsmk))) + \n  geom_vline(xintercept = 0, color = \"grey60\", linewidth = 1) +\n  geom_density(color = \"white\", alpha = .75, linewidth = .5) +\n  ggokabeito::scale_color_okabe_ito(order = c(1, 5)) + \n  theme_minimal() +\n  theme(legend.position = \"bottom\") + \n  labs(\n    x = \"change in weight (kg)\",\n    fill = \"quit smoking (1 = yes)\"\n  )\n\n\n\n\n\n\n\n\n\nThere’s a difference–former smokers do seemed to have gained a bit more weight–but there’s also a lot of variation. Let’s look at the numeric summaries.\n\n\nweight gain summaries\n# ~4.5 kg gained for quit vs. not quit\nnhefs_complete_uc |&gt;\n  dplyr::group_by(qsmk) |&gt;\n  dplyr::summarize(\n    mean_weight_change = mean(wt82_71), \n    sd = sd(wt82_71),\n    .groups = \"drop\"\n  )\n\n\n# A tibble: 2 × 3\n   qsmk mean_weight_change    sd\n  &lt;dbl&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1     0               1.98  7.45\n2     1               4.53  8.75\n\n\nHere, it looks like those who quit smoking gained, on average, 4.5 kg. But is there something else that could explain these results? There are many factors associated with both quitting smoking and gaining weight; could one of those factors explain away the results we’re seeing here?\nTo truly answer this question, we need to specify a causal diagram based on domain knowledge. Sadly, for most circumstances, there is no data-driven approach that consistently identify confounders. Only our causal assumptions can help us identify them. Causal diagrams are a visual expression of those assumptions linked to rigorous mathematics that allow us to understand what we need to account for in our model.\nIn R, we can visualize and analyze our DAGs with the {ggdag} package. {ggdag} uses {ggplot2} and {ggraph} to visualize diagrams and {dagitty} to analyze them. Let’s set up our assumptions. The dagify() function takes formulas, much like lm() and friends, to express assumptions. We have two basic causal structures: the causes of quitting smoking and the causes of gaining weight. Here, we’re assuming that the set of variables here affect both. Additionally, we’re adding qsmk as a cause of wt82_71, which is our causal question; we also identify these as our outcome and exposure. Finally, we’ll add some labels so the diagram is easier to understand. The result is a dagitty object, and we can transform it to a tidy_dagitty data set with tidy_dagitty().\n\n\ntidy version of the DAG\n# set up DAG\nsmk_wt_dag &lt;- ggdag::dagify(\n  # specify causes of quitting smoking and weight gain:\n  qsmk ~ sex + race + age + education + \n    smokeintensity + smokeyrs + exercise + active + wt71,\n  wt82_71 ~ qsmk + sex + race + age + education + \n    smokeintensity + smokeyrs + exercise + active + wt71,\n  # specify causal question:\n  exposure = \"qsmk\", \n  outcome = \"wt82_71\",\n  coords = ggdag::time_ordered_coords(),\n  # set up labels:\n  # here, I'll use the same variable names as the data set, but I'll label them\n  # with clearer names\n  labels = c(\n    # causal question\n    \"qsmk\" = \"quit\\nsmoking\",\n    \"wt82_71\" = \"change in\\nweight\",\n    \n    # demographics\n    \"age\" = \"age\",\n    \"sex\" = \"sex\",\n    \"race\" = \"race\",\n    \"education\" = \"education\",\n    \n    # health\n    \"wt71\" = \"baseline\\nweight\",\n    \"active\" = \"daily\\nactivity\\nlevel\",\n    \"exercise\" = \"exercise\",\n    \n    # smoking history\n    \"smokeintensity\" = \"smoking\\nintensity\",\n    \"smokeyrs\" = \"yrs of\\nsmoking\"\n  )\n) |&gt;\n  ggdag::tidy_dagitty()\n\nsmk_wt_dag\n\n\n# A DAG with 11 nodes and 19 edges\n#\n# Exposure: qsmk\n# Outcome: wt82_71\n#\n# A tibble: 20 × 9\n   name               x     y direction to       xend  yend circular label      \n   &lt;chr&gt;          &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    &lt;chr&gt;      \n 1 active             1    -4 -&gt;        qsmk        2     0 FALSE    \"daily\\nac…\n 2 active             1    -4 -&gt;        wt82_71     3     0 FALSE    \"daily\\nac…\n 3 age                1    -3 -&gt;        qsmk        2     0 FALSE    \"age\"      \n 4 age                1    -3 -&gt;        wt82_71     3     0 FALSE    \"age\"      \n 5 education          1    -2 -&gt;        qsmk        2     0 FALSE    \"education\"\n 6 education          1    -2 -&gt;        wt82_71     3     0 FALSE    \"education\"\n 7 exercise           1    -1 -&gt;        qsmk        2     0 FALSE    \"exercise\" \n 8 exercise           1    -1 -&gt;        wt82_71     3     0 FALSE    \"exercise\" \n 9 qsmk               2     0 -&gt;        wt82_71     3     0 FALSE    \"quit\\nsmo…\n10 race               1     0 -&gt;        qsmk        2     0 FALSE    \"race\"     \n11 race               1     0 -&gt;        wt82_71     3     0 FALSE    \"race\"     \n12 sex                1     1 -&gt;        qsmk        2     0 FALSE    \"sex\"      \n13 sex                1     1 -&gt;        wt82_71     3     0 FALSE    \"sex\"      \n14 smokeintensity     1     2 -&gt;        qsmk        2     0 FALSE    \"smoking\\n…\n15 smokeintensity     1     2 -&gt;        wt82_71     3     0 FALSE    \"smoking\\n…\n16 smokeyrs           1     3 -&gt;        qsmk        2     0 FALSE    \"yrs of\\ns…\n17 smokeyrs           1     3 -&gt;        wt82_71     3     0 FALSE    \"yrs of\\ns…\n18 wt71               1     4 -&gt;        qsmk        2     0 FALSE    \"baseline\\…\n19 wt71               1     4 -&gt;        wt82_71     3     0 FALSE    \"baseline\\…\n20 wt82_71            3     0 &lt;NA&gt;      &lt;NA&gt;       NA    NA FALSE    \"change in…\n\n\nLet’s visualize our assumptions with ggdag().\n\n\ngraphic version of the DAG\nsmk_wt_dag |&gt;\n  ggdag::ggdag(text = FALSE, use_labels = \"label\") + ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\nWhat do we need to control for to estimate an unbiased effect of quitting smoking on weight gain? In many DAGs, there will be many sets of variables–called adjustment sets–that will give us the right effect (assuming our DAG is correct–a big, unverifiable assumption!). ggdag_adjustment_set() can help you visualize them. Here, there’s only one adjustment set: we need to control for everything! While we’re add it, since a {ggdag} plot is just a {ggplot2} plot, let’s clean it up a bit, too.\n\n\nDAG adjustment sets\nsmk_wt_dag |&gt;\n  ggdag::ggdag_adjustment_set(text = FALSE, use_labels = \"label\") +\n  ggdag::theme_dag() +\n  ggokabeito::scale_color_okabe_ito(order = c(1, 5)) + \n  ggokabeito::scale_fill_okabe_ito(order = c(1, 5))\n\n\n\n\n\n\n\n\n\nLet’s fit a model with these variables. Note that we’ll fit all continuous variables with squared terms, as well, to allow them a bit of flexibility.\n\n\nfit of data using all variables\nlm(\n  wt82_71~ qsmk + sex + \n    race + age + I(age^2) + education + \n    smokeintensity + I(smokeintensity^2) + \n    smokeyrs + I(smokeyrs^2) + exercise + active + \n    wt71 + I(wt71^2), \n  data = nhefs_complete_uc\n) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"qsmk\")\n\n\n# A tibble: 1 × 7\n  term  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 qsmk      3.46     0.438      7.90 5.36e-15     2.60      4.32\n\n\nWhen we adjust for the variables in our DAG, we get an estimate of about 3.5 kg–people who quit smoking gained about this amount of weight. However, we are trying to answer a specific causal question: how much weight would a person gain if the quit smoking vs. if the same person did not quit smoking? Let’s use an inverse probability weighting model to try to estimate that effect at the population level (what if everyone quit smoking vs what if no one quit smoking).\nFor a simple IPW model, we have two modeling steps. First, we fit a propensity score model, which predicts the probability that you received a treatment or exposure (here, that a participant quit smoking). We use this model to calculate inverse probability weights–1 / your probability of treatment. Then, in the second step, we use this weights in the outcome model, which estimates the effect of exposure on the outcome (here, the effect of quitting smoking on gaining weight).\nFor the propensity score model, we’ll use logistic regression (since quitting smoking is a binary variable). The outcome is quitting smoking, and the variables in the model are all those included in our adjustment set. Then, we’ll use augment() from {broom} (which calls predict() on the inside) to calculate our weights using propensity::wt_ate() and save it back into our data set.\n\n\npropensity score calculations\npropensity_model &lt;- glm(\n  qsmk ~ sex + \n    race + age + I(age^2) + education + \n    smokeintensity + I(smokeintensity^2) + \n    smokeyrs + I(smokeyrs^2) + exercise + active + \n    wt71 + I(wt71^2), \n  family = binomial(), \n  data = nhefs_complete_uc\n)\n\nnhefs_complete_uc &lt;- propensity_model |&gt;\n  # predict whether quit smoking\n  broom::augment(type.predict = \"response\", data = nhefs_complete_uc) |&gt;\n  # calculate inverse probability\n  dplyr::mutate(wts = propensity::wt_ate(.fitted, qsmk))\n\nnhefs_complete_uc |&gt;\n  dplyr::select(qsmk, .fitted, wts)\n\n\n# A tibble: 1,566 × 3\n    qsmk .fitted   wts\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1     0  0.0987  1.11\n 2     0  0.140   1.16\n 3     0  0.126   1.14\n 4     0  0.400   1.67\n 5     0  0.294   1.42\n 6     0  0.170   1.20\n 7     0  0.220   1.28\n 8     0  0.345   1.53\n 9     0  0.283   1.40\n10     0  0.265   1.36\n# ℹ 1,556 more rows\n\n\nLet’s look at the distribution of the weights.\n\n\npropensity weight distribution\nggplot(nhefs_complete_uc, aes(wts)) +\n  geom_histogram(color = \"white\", fill = \"#E69F00\", bins = 50) + \n  #  use a log scale for the x axis\n  scale_x_log10() + \n  theme_minimal(base_size = 20) + \n  xlab(\"Weights\")\n\n\n\n\n\n\n\n\n\nIt looks a little skewed, particularly that there are some participants with much higher weights. There are a few techniques for dealing with this – trimming weights and stabilizing weights – but we’ll keep it simple for now and just use them as is.\nThe main goal here is to break the non-causal associations between quitting smoking and gaining weight–the other paths that might distort our results. In other words, if we succeed, there should be no differences in these variables between our two groups, those who quit smoking and those who didn’t. This is where randomized trials shine; you can often assume that there is no baseline differences among potential confounders between your treatment groups (of course, no study is perfect, and there’s a whole set of literature on dealing with this problem in randomized trials).\nStandardized mean differences (SMD) are a simple measurement of differences that work across variable types. In general, the closer to 0 we are, the better job we have done eliminating the non-causal relationships we drew in our DAG. Note that low SMDs for everything we adjust for does not mean that there is not something else that might confound our study. Unmeasured confounders or misspecified DAGs can still distort our effects, even if our SMDs look great!\nWe’ll use the {halfmoon} package to calculate the SMDs, then visualize them.\n\n\nStandardized mean differences plot\nvars &lt;- c(\n  \"sex\", \"race\", \"age\", \"education\", \n  \"smokeintensity\", \"smokeyrs\", \n  \"exercise\", \"active\", \"wt71\"\n)\n\nplot_df &lt;- halfmoon::tidy_smd(\n    nhefs_complete_uc,\n    all_of(vars),\n    qsmk,\n    wts\n)\n\nggplot(\n    data = plot_df,\n    mapping = aes(x = abs(smd), y = variable, group = method, color = method)\n) +\n    halfmoon::geom_love()\n\n\n\n\n\n\n\n\n\nThese look pretty good! Some variables are better than others, but weighting appears to have done a much better job eliminating these differences than an unadjusted analysis.\nWe can also use halfmoon’s geom_mirror_histogram() to visualize the impact that the weights are having on our population.\n\n\nInverse Probability Weight distributions\nnhefs_complete_uc |&gt;\n  dplyr::mutate(qsmk = factor(qsmk)) |&gt;\n  ggplot(aes(.fitted)) +\n  halfmoon::geom_mirror_histogram(\n    aes(group = qsmk),\n    bins = 50\n  ) +\n  halfmoon::geom_mirror_histogram(\n    aes(fill = qsmk, weight = wts),\n    bins = 50,\n    alpha = .5\n  ) +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\") + \n  theme_minimal(base_size = 20)\n\n\n\n\n\n\n\n\n\nBoth groups are being upweighted so that their distributions of propensity scores are much more similar.\nWe could do more here to analyze our assumptions, but let’s move on to our second step: fitting the outcome model weighted by our inverse probabilities. Some researchers call these Marginal Structural Models, in part because the model is marginal; we only need to include our outcome (wt82_71) and exposure (qsmk). The other variables aren’t in the model; they are accounted for with the IPWs!\n\n\nFit using inverse probability weights\nipw_model &lt;- lm(\n  wt82_71 ~ qsmk, \n  data = nhefs_complete_uc, \n  weights = wts # inverse probability weights\n) \n\nipw_estimate &lt;- ipw_model |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"qsmk\")\n\nipw_estimate\n\n\n# A tibble: 1 × 7\n  term  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 qsmk      3.44     0.408      8.43 7.47e-17     2.64      4.24\n\n\nThis estimate is pretty similar to what we saw before, if a little smaller. In fact, for simple causal questions, this is often the case: adjusting for confounders directly in your regression model sometimes estimates the same effect as IPWs and other causal techniques. Causal techniques are special, though, in that they use counterfactual modeling, which allows you to deal with many circumstances, such as when you have selection bias or time-dependendent confounding. They also often have variance properties.\nBut we have other problem that we need to address. While we’re just using lm() to estimate our IPW model, it doesn’t properly account for the weights. That means our standard error is too small, which will artificially narrow confidence intervals and artificially shrink p-values. There are many ways to address this, including robust estimators. We’ll focus on using the bootstrap via the {rsamples} package in this lab, but here’s one way to do it with robust standard errors:\n\n\nRobust linear fit without bootstrap\n# also see robustbase, survey, gee, and others\nlibrary(estimatr)\nipw_model_robust &lt;- estimatr::lm_robust( \n  wt82_71 ~ qsmk, \n  data = nhefs_complete_uc, \n  weights = wts \n) \n\nipw_estimate_robust &lt;- ipw_model_robust |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"qsmk\")\n\nipw_estimate_robust\n\n\n  term estimate std.error statistic      p.value conf.low conf.high   df\n1 qsmk 3.440535 0.5264638  6.535179 8.573524e-11 2.407886  4.473185 1564\n  outcome\n1 wt82_71\n\n\nNow let’s try the bootstrap. First, we need to wrap our model in a function so we can call it many times on our bootstrapped data. A function like this might be your instinct; however, it’s not quite right.\n\n\nnot quite right function for bootstrapped estimates\n# fit ipw model for a single bootstrap sample\nfit_ipw_not_quite_rightly &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  .df &lt;- rsample::analysis(split)\n  \n  # fit ipw model\n  lm(wt82_71 ~ qsmk, data = .df, weights = wts) |&gt;\n    tidy()\n}\n\n\nThe problem is that we need to account for the entire modeling process, so we need to include the first step of our analysis – fitting the inverse probability weights.\n\n\nright function for bootstrapped estimates\nfit_ipw &lt;- function(split, ...) {\n  .df &lt;- rsample::analysis(split)\n  \n  # fit propensity score model\n  propensity_model &lt;- glm(\n    qsmk ~ sex + \n      race + age + I(age^2) + education + \n      smokeintensity + I(smokeintensity^2) + \n      smokeyrs + I(smokeyrs^2) + exercise + active + \n      wt71 + I(wt71^2), \n    family = binomial(), \n    data = .df\n  )\n  \n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(\n      .fitted,\n      qsmk, \n      exposure_type = \"binary\"\n    ))\n  \n  # fit correctly bootstrapped ipw model\n  lm(wt82_71 ~ qsmk, data = .df, weights = wts) |&gt;\n    tidy()\n}\n\n\n{rsample} makes the rest easy for us: bootstraps() resamples our data 1000 times, then we can use purrr::map() to apply our function to each resampled set (splits). {rsample}’s int_*() functions help us get confidence intervals for our estimate.\n\n\nfit ipw model to bootstrapped estimates\n# fit ipw model to bootstrapped samples\nipw_results &lt;- rsample::bootstraps(causaldata::nhefs_complete, 1000, apparent = TRUE) |&gt;\n  dplyr::mutate(results = purrr::map(splits, fit_ipw))\n\n# get t-statistic-based CIs\nboot_estimate &lt;- rsample::int_t(ipw_results, results) |&gt;\n  dplyr::filter(term == \"qsmk\")\n\nboot_estimate\n\n\n# A tibble: 1 × 6\n  term  .lower .estimate .upper .alpha .method  \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n1 qsmk    2.47      3.46   4.45   0.05 student-t\n\n\nLet’s compare to our naive weighted model that just used a single estimate from lm()\n\n\ncomparison of robust and naive weighted models\ndplyr::bind_rows(\n  ipw_estimate |&gt;\n    dplyr::select(estimate, conf.low, conf.high) |&gt;\n    dplyr::mutate(type = \"ols\"),\n  ipw_estimate_robust |&gt;\n    dplyr::select(estimate, conf.low, conf.high) |&gt;\n    dplyr::mutate(type = \"robust\"),\n  boot_estimate |&gt;\n    dplyr::select(estimate = .estimate, conf.low = .lower, conf.high = .upper) |&gt;\n    dplyr::mutate(type = \"bootstrap\")\n) |&gt;\n  #  calculate CI width to sort by it\n  dplyr::mutate(width = conf.high - conf.low) |&gt;\n  dplyr::arrange(width) |&gt;\n  #  fix the order of the model types for the plot  \n  dplyr::mutate(type = forcats::fct_inorder(type)) |&gt;\n  ggplot(aes(x = type, y = estimate, ymin = conf.low, ymax = conf.high)) + \n    geom_pointrange(color = \"#0172B1\", size = 1, fatten = 3) +\n    coord_flip() +\n    theme_minimal(base_size = 20) +\n    theme(axis.title.y = element_blank())\n\n\n\n\n\n\n\n\n\nOur bootstrapped confidence intervals are wider, which is expected; remember that they were artificially narrow in the naive OLS model!\nSo, we have a final estimate for our causal effect: on average, a person who quits smoking will gain 3.5 kg (95% CI 2.4 kg, 4.4 kg) versus if they had not quit smoking.\n\n\nQuestions:\n\nPlease enumerate the steps in the causal analysis workflow\nWhat do you think? Is this estimate reliable? Did we do a good job addressing the assumptions we need to make for a causal effect, particularly that there is no confounding? How might you criticize this model, and what would you do differently?\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ncausal workflow steps:\n\nSpecify a causal question\nDraw assumptions using a causal diagram\nModel assumptions\nDiagnose models\nEstimate the causal effect\nConduct sensitivity analysis on the effect estimate\n\ncritique of the results of the exercise:\n\nWe have many confounders in this problem. What if we missed an important one?\nThe one step of the workflow that we missed is to conduct a sensitivity analysis and so that should certainly be done before reporting the results of the exercise."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_7_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_7_solutions.html#grading",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 6\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_10.html",
    "href": "labs/BSMM_8740_lab_10.html",
    "title": "lab 10 - Bayesian Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice creating Bayesian models."
  },
  {
    "objectID": "labs/BSMM_8740_lab_10.html#introduction",
    "href": "labs/BSMM_8740_lab_10.html#introduction",
    "title": "lab 10 - Bayesian Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice creating Bayesian models."
  },
  {
    "objectID": "labs/BSMM_8740_lab_10.html#getting-started",
    "href": "labs/BSMM_8740_lab_10.html#getting-started",
    "title": "lab 10 - Bayesian Methods",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-10-[your github username] repository .\nCreate an R project using your 2024-lab-10-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-10.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_10.html#packages",
    "href": "labs/BSMM_8740_lab_10.html#packages",
    "title": "lab 10 - Bayesian Methods",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n    tidyverse, broom, rsample, ggdag, causaldata, magrittr, gt, gtExtras, halfmoon, ggokabeito, \n    ggplot2, survey, cardx, gtsummary,brms, shinystan, patchwork, tidybayes, ggthemes,\n    michael-franke/aida-package, michael-franke/faintr, CogSciPrag/cspplot\n)\n\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_10.html#exercise-1",
    "href": "labs/BSMM_8740_lab_10.html#exercise-1",
    "title": "lab 10 - Bayesian Methods",
    "section": "Exercise 1:",
    "text": "Exercise 1:\nData:\nThis data contains roughly 2000 trials of a mouse-tracking experiment,\nIt is a preprocessed data set from an experiment conducted by Kieslich et al. (2020) in which participants classified specific animals into broader categories. The data set contains response times, MAD, AUC and other attributes as well as all experimental conditions.\n\ndolphin &lt;- aida::data_MT\n\n# aggregate\ndolphin_agg &lt;- dolphin |&gt;\n  dplyr::filter(correct == 1) |&gt; # only correct values\n  dplyr::group_by(subject_id) |&gt; \n  dplyr::summarize(       # use the median AUC/MAD\n    AUC = median(AUC, na.rm = TRUE),\n    MAD = median(MAD, na.rm = TRUE)) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(\n    # the function scale centers and scales the data \n    AUC = scale(AUC),\n    MAD = scale(MAD)\n  )\n  \n# let's take a look\nhead(dolphin_agg)\n\n# A tibble: 6 × 3\n  subject_id AUC[,1] MAD[,1]\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1       1001   0.643   0.593\n2       1002   0.732   0.367\n3       1003  -0.839  -0.833\n4       1004  -0.551  -0.535\n5       1005   0.619   0.436\n6       1006   0.748   1.02 \n\n\nPlot AUC vs MAD. What does the relationship show? Does the relationship make sense?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# the transition matrix is\nggplot(data = dolphin_agg, \n       aes(x = ?, \n           y = ?)) + \n  geom_point(size = 3, alpha = 0.3) \n\n\n\nRegress AUC against MAD using a bayesian regression with the dolphin_agg data. Save the results in the fits directory as “model1.” Use all the brms defaults\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# regress AUC against MAD\nmodel1 &lt;- \n\n  \n# show summary in tidy format\n\n\n\nExtract the model coefficients, then redo the graph to add a geom_abline with the model slope and intercept.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# extract model parameters:\n\n\n# re-create the original plot, with a regression line based on the model coefficients\nggplot(data = dolphin_agg, \n       aes(x = AUC, y = MAD)) + \n  ? +\n  theme_minimal()\n\n\n\nUse tidybayes::get_variables to get the model variables, and then use tidybayes::spread_draws with the model1 to get the draws for “b_MAD” and “b_Intercept” as a tibble, bound to the variable posteriors1.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# get all the variable names for model1\n\n\n# get the draws for \"b_MAD\" and \"b_Intercept\"\nposteriors1 &lt;- \n  \nposteriors1  \n\n\n\nRepeat the last operation, but this time use ndraws = 100 to limit the number of draws (call this posterior2). Again using the first graph, add a line with intercept and slope corresponding to each row of posterior2 (i.e. add 100 lines).\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# spread draws for b_MAD and b_Intercept\nposteriors2 &lt;- \n  \n# plot the data along with 100 regression lines, one from each of 100 draws\nggplot(data = dolphin_agg, \n       aes(x = MAD, y = AUC)) + \n  ? +\n  geom_point(size = 3, alpha = 0.3, color = ?)\n\n\n\nUse model1 and tidybayes::gather_draws, to get the draws for “b_MAD” as a tibble in long form, bound to the variable posteriors3. Rename ‘.variable’ to ‘parameter’ and ‘.value’ to ‘posterior’ and then keep only those two columns.\nCalculate the mean, the lower and the upper bound of a 90% CrI, using the function tidybayes::hdi().\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# gather draws for b_MAD\nposteriors3 &lt;-\n  \n\n# show the top 5 rows  \nhead(posteriors3)\n\n\n# use dplyr::summarise to calculate the mean and credible intervals for B_MAD\nposteriors3_agg &lt;- \n\n# display the result \nposteriors3_agg"
  },
  {
    "objectID": "labs/BSMM_8740_lab_10.html#exercise-2-elasticity",
    "href": "labs/BSMM_8740_lab_10.html#exercise-2-elasticity",
    "title": "lab 10 - Bayesian Methods",
    "section": "Exercise 2: elasticity",
    "text": "Exercise 2: elasticity\nIn this exercise we will estimate price elasticity. The dataset contains price and monthly unit-volume data for four sites over 12 months. Since the unit-volumes are counts of unit sold, we’ll need a discrete pmf to model the data generation process.\n\n# read the data\ndat &lt;- \n  readr::read_csv(\"data/price_data.csv\",show_col_types = FALSE) |&gt; \n  dplyr::filter(site == \"Windsor\")\n# plot the data\ndat |&gt; \n  ggplot(aes(x=price, y=volume, group=site)) +\n  geom_point()\n\nSince elasticity is defined as the percentage change in volume (ΔV/V\\Delta V/V) for a given percentage change in price (Δp/p\\Delta p/p), then with elasticity parameter β\\beta we write:\nΔVV=β×Δpp∂VV=β×∂pp∂log(V)=β×∂log(p)\n\\begin{align*}\n\\frac{\\Delta V}{V} & = \\beta\\times\\frac{\\Delta p}{p} \\\\\n\\frac{\\partial V}{V} & = \\beta\\times\\frac{\\partial p}{p} \\\\\n\\partial\\log(V) & = \\beta\\times\\partial\\log(p)\n\\end{align*}\n\nThis equation is the justification for the log-log regression model of elasticity, and this model has solution V=KpβV = Kp^\\beta, where KK is a constant.\nAs written, the value of KK is either the volume when p=1p=1 which may or may not be useful, or it is the volume when β=0\\beta=0, which is uninteresting.\nTo make the interpretation of the constant KK more useful, the model can be written as\n∂log(V)=β×∂log(p/pbaseline);V=K(ppbaseline)β\n\\partial\\log(V) = \\beta\\times\\partial\\log(p/p_{\\text{baseline}});\\qquad V = K\\left(\\frac{p}{p_{\\text{baseline}}}\\right)^{\\beta}\n\nin which case the constant is interpreted as the volume when the price equals the baseline price; the elasticity parameter β\\beta is unchanged.\nThe implies that our regression model should be volume=log(K)+βlog(p)\\mathrm{volume} = \\log(K) + \\beta\\log(p) given the log\\log link in the Poisson model. To analyze the data\n\nscale and transform the price data by dividing by the mean price and then taking the log. Assign the modified data to the variable dat01.\nbuild and fit a Bayesian Poisson model with\n\na normal(0,5) prior on the intercept, and\na cauchy(0,10) prior on the independent variable with upper bound 0 (to ensure that elasticities are negative).\nassign the fit to the variable windsor_01 and save the fit in the folder “fits/windsor_01”\ncomment on the posterior distributions in the pairs plot\n\nonce the model is fit, summarize the draws and show the the pairs plot\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\ndat01 &lt;- dat |&gt; \n  ?\n\nwindsor_01 &lt;- \n  ?\n\ntidybayes::summarise_draws(?)\n\n\npairs(?, variable = c('b_Intercept', 'b_price'))\n\nThe pairs plot ?\n\n\nIn a Poisson model, the mean and variance of the dependent variable are equal. This is clearly not the case here (check this). So we might not expect the Poisson generating process to be a good fit.\nAn alternative discrete pmf to the Poisson data generation process is the negative binomial process.\n\nbuild and fit a Bayesian Negative Binomial model (family = negbinomial()) using the same priors as in the Poisson model\nassign the fit to the variable windsor_02 and save the fit in the folder “fits/windsor_02”\nonce the model is fit, summarize the draws and show the the pairs plot\ncomment on the posterior distributions in the pairs plot\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nwindsor_02 &lt;- \n  ?\n\ntidybayes::summarise_draws(?)\n\n\npairs(?, variable = c('b_Intercept', 'b_price'))\n\nThe pairs plot ?\n\n\nSince we have discrete outcomes, the continuous distribution that is the default output of brms::pp_check is not appropriate here.\nInstead use the type = “rootogram” argument of brms::pp_check to plot posterior predictive checks for the Poisson and NB models.\nRootograms graphically compare frequencies of empirical distributions and expected (fitted) probability models. For the observed distribution the histogram is drawn on a square root scale (hence the name) and superimposed with a line for the expected frequencies.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\np1 &lt;- brms::pp_check(?, type = \"rootogram\") + xlim(20,120)\np2 &lt;- brms::pp_check(?, type = \"rootogram\") + xlim(20,120)\np1/p2\n\n\n\nFinally, compare the two model fits using brms::loo and brms::loo_compare and comment on the model comparison based on all the analyses performed in this exercise.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nbrms::loo_compare(brms::loo(?), brms::loo(?))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_10.html#exercise-3",
    "href": "labs/BSMM_8740_lab_10.html#exercise-3",
    "title": "lab 10 - Bayesian Methods",
    "section": "Exercise 3:",
    "text": "Exercise 3:\nConsider the a complicated manufacturing process as follows, where production is one unit per day on average and manufacturing equipment maintenance takes 20% of the time on average.\n\n# define parameters\nprob_maintenance &lt;- 0.2  # 20% of days\nrate_work        &lt;- 1    # average 1 unit per day\n\n# sample one year of production\nn &lt;- 365\n\n# simulate days of maintenance\nset.seed(365)\nmaintenance &lt;- rbinom(n, size = 1, prob = prob_maintenance)\n\n# simulate units completed\ny &lt;- (1 - maintenance) * rpois(n, lambda = rate_work)\n\ndat &lt;-\n  tibble::tibble(maintenance = factor(maintenance, levels = 1:0), y = y)\n  \ndat %&gt;% \n  ggplot(aes(x = y)) +\n  geom_histogram(aes(fill = maintenance),\n                 binwidth = 1, linewidth = 1/10, color = \"grey92\") +\n  scale_fill_manual(values = ggthemes::canva_pal(\"Green fields\")(4)[1:2]) +\n  xlab(\"Units completed\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWith this data, the likelihood of observing zero on y, (i.e., the likelihood zero units were completed on a given day) is\nPr(0|p,λ)=Pr(maintenance|p)+Pr(work|p)×Pr(0|λ)=p+(1−p)exp(−λ).\n\\begin{align*}\n\\operatorname{Pr}(0 | p, \\lambda) & = \\operatorname{Pr}(\\text{maintenance} | p) + \\operatorname{Pr}(\\text{work} | p) \\times \\operatorname{Pr}(0 | \\lambda) \\\\\n                                   & = p + (1 - p) \\exp (- \\lambda).\n\\end{align*}\n\nAnd the likelihood of a non-zero value yy is:\nPr(y|y&gt;0,p,λ)=Pr(maintenance|p)(0)+Pr(work|p)Pr(y|λ)=(1−p)λyexp(−λ)y!\n\\operatorname{Pr}(y | y &gt; 0, p, \\lambda) = \\operatorname{Pr}(\\text{maintenance} | p) (0) + \\operatorname{Pr}(\\text{work} | p) \\operatorname{Pr}(y | \\lambda) = (1 - p) \\frac {\\lambda^y \\exp (- \\lambda)}{y!}\n\nSo letting pp be the probability that yy is zero and lambdalambda be the shape of the distribution, the zero-inflated Poisson (ZIPoisson) regression model might take the basic form:\nyi∼ZIPoisson(pi,λi)logit(pi)=αp+βpxilog(λi)=αλ+βλxi,\n\\begin{align*}y_{i} & \\sim\\mathrm{ZIPoisson}(p_{i},\\lambda_{i})\\\\\n\\mathrm{logit}(p_{i}) & =\\alpha_{p}+\\beta_{p}x_{i}\\\\\n\\log(\\lambda_{i}) & =\\alpha_{\\lambda}+\\beta_{\\lambda}x_{i},\n\\end{align*}\n\nwhere both parameters in the likelihood, pip_i and λi\\lambda_i might get their own statistical model. In brms, pip_i is denoted by zi.\nCreate a Bayesian zero-inflated Poisson Model (family = zero_inflated_poisson) with a normal(1,0.5) normal prior on the intercept and a beta(2,6) prior on zi (class = zi) and save the model in “fits/zi_model”\n\nuse print() to display a summary of the model, and\nnoting the link functions in the second line of the model summary, express the parameters a probabilities.\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nzi_model &lt;- ?\n  \nprint(?)\n\nExpress the fitted parameters as probabilities:\nlambda =\np ="
  },
  {
    "objectID": "labs/BSMM_8740_lab_10.html#exercise-4",
    "href": "labs/BSMM_8740_lab_10.html#exercise-4",
    "title": "lab 10 - Bayesian Methods",
    "section": "Exercise 4:",
    "text": "Exercise 4:\nLoad the regional sales data below\n\ndat &lt;- readr::read_csv(\"data/regional_sales.csv\", show_col_types = FALSE)\n\nBuild the following sales model. How are the terms (1|region_id) and (1|store_id) referred to in the output, and how do you interpret them.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# Fit hierarchical Bayesian model\nsales_model &lt;- brm(\n  formula = log_sales ~ 1 + \n    scale(store_size) + \n    scale(competition_density) + \n    scale(marketing_spend) +\n    seasonal_effect + \n    (1|region_id) + \n    (1|store_id),\n  data = dat,\n  family = gaussian(),\n  prior = c(\n    prior(normal(10, 1), class = \"Intercept\"),\n    prior(normal(0, 0.5), class = \"b\"),\n    prior(cauchy(0, 0.2), class = \"sd\"),\n    prior(cauchy(0, 0.2), class = \"sigma\")\n  ),\n  chains = 4,\n  cores = 4,\n  seed = 456,\n  file = \"fits/sales_model\"\n)\n\nThe terms (1|region_id) and (1|store_id) are called __? in the output, and they are interpreted as __?\n\n\nCreate a tibble and bind it to the variable new_store_data. Give it the following columns:\n\nstore_size = 5000\ncompetition_density = 5\nmarketing_spend = 10000\nseasonal_effect = 0\nregion_id = 1\nstore_id = max(data$store_id) + 1\n\nUse the model and this data to predict sales for the new store and give the confidence intervals for the prediction.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nnew_store_data &lt;- tibble::tibble()\n\n\n# Make predictions\npredict(?, newdata = ?, allow_new_levels=TRUE)\n\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 6!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyone’s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_10.html#grading",
    "href": "labs/BSMM_8740_lab_10.html#grading",
    "title": "lab 10 - Bayesian Methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 4\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html",
    "href": "labs/BSMM_8740_lab_8.html",
    "title": "Lab 8 - Causality: Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice making causal estimates.\nBy the end of the lab you will…\n\nBe able to estimate causal effects via regression adjustment, doubly robust estimates and two-way fixed effects.\nBe able to use causal estimation to build uplift curves - a improvement over logistic regression in marketing applications."
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#introduction",
    "href": "labs/BSMM_8740_lab_8.html#introduction",
    "title": "Lab 8 - Causality: Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice making causal estimates.\nBy the end of the lab you will…\n\nBe able to estimate causal effects via regression adjustment, doubly robust estimates and two-way fixed effects.\nBe able to use causal estimation to build uplift curves - a improvement over logistic regression in marketing applications."
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#getting-started",
    "href": "labs/BSMM_8740_lab_8.html#getting-started",
    "title": "Lab 8 - Causality: Methods",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-8-[your github username] repository .\nCreate an R project using your 2024-lab-8-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-8.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#packages",
    "href": "labs/BSMM_8740_lab_8.html#packages",
    "title": "Lab 8 - Causality: Methods",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#q1",
    "href": "labs/BSMM_8740_lab_8.html#q1",
    "title": "Lab 8 - Causality: Methods",
    "section": "Q1",
    "text": "Q1\nRecall that in our last lecture we used several methods to understand the effect of nets on Malaria risk. The regression adjustment approach gave results that were lower than those using IPW or Doubly Robust Estimation.\nThis is partly due to the regression specification we used, which as a second-order, fixed effects model did not fully capture the relationship between the covariates and the outcome. One simple way to enhance the model is to relax the fixed effects assumption, which you will do here, in the context of a completely different approach.\n\ng-computation\nThe g-computation method (g for general) is good to know because it works for both binary/discrete treatments and continuous treatments\n\ndat_ &lt;- causalworkshop::net_data |&gt; dplyr::mutate(net = as.numeric(net))\n\nIn this question we’ll use g-computation to estimate the effect of net use on Malaria risk. Run the following steps:\n\nMake two copies of the data. Keep the original copy (you’ll have three in total).\nMutate the copied data so that one copy has net == 1 and the other copy has net == 0.\nBind the data copies together by row to produce a test dataset.\nModel the relationship between net use and malaria risk, incorporating all confounders. The linear model from the lecture is a good start. Fit the model with the original data.\nUse the model to predict the outcomes in the test dataset.\nGroup the test dataset by net use, compute the average outcome by group (the effect), and find the difference in effects (the contrast).\n\n\n\n\n\n\n\nHINT\n\n\n\nTry using (net + income + health + temperature + insecticide_resistance)^2 in the RHS of your formula. This will add interaction terms to your model (i.e. effects of each variable won’t be fixed)\n\n\n\n\n\n\n\n\nYOUR ANSWER\n\n\n\n\n# copy data\n\n# mutate data to fix treatment (one of each) & bind data into single dataset\n\n# Model the outcome, and fit it using the original (unmutated) data. Show the coefficient estimates with broom::tidy() \n\n# Use the model fit to predict the outcomes in the test data, Use broom::augment to predict the response for each row of the data\n\n# average the predicted outcomes and compute the contrast.\n\nThe contrast between using a net and not using a net is a increase | reduction in malaria risk\n\n\nIn summary the g-computation is as follows\n\nFit a standardized model with all covariates/confounders. Then, use cloned data sets with values set on each level of the exposure you want to study.\nUse the model to predict the values for that level of the exposure and compute the effect estimate of interest."
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#q2",
    "href": "labs/BSMM_8740_lab_8.html#q2",
    "title": "Lab 8 - Causality: Methods",
    "section": "Q2",
    "text": "Q2\nSuppose you work for a big tech company and you want to estimate the impact of a billboard marketing campaign on in-app purchases. When you look at data from the past, you see that the marketing department tends to spend more to place billboards in cities where the purchase level is lower. This makes sense right? They wouldn’t need to do lots of advertisement if sales were skyrocketing. If you run a regression model on this data, it looks like higher cost in marketing leads to lower in-app purchase amounts, but only because marketing investments are biased towards low spending regions.\n\ntoy_panel &lt;-tibble::tibble(\n    \"mkt_costs\" = c(5,4,3.5,3, 10,9.5,9,8, 4,3,2,1, 8,7,6,4),\n    \"purchase\" = c(12,9,7.5,7, 9,7,6.5,5, 15,14.5,14,13, 11,9.5,8,5),\n    \"city\" = \n      c(\"Windsor\",\"Windsor\",\"Windsor\",\"Windsor\"\n        , \"London\",\"London\",\"London\",\"London\"\n        , \"Toronto\",\"Toronto\",\"Toronto\",\"Toronto\", \"Tilbury\",\"Tilbury\",\"Tilbury\",\"Tilbury\")\n)\n\nfit_lm &lt;- lm(purchase ~ mkt_costs, data = toy_panel)\n\ntoy_panel |&gt; \n  ggplot(aes(x = mkt_costs, y = purchase)) +\n  geom_point(color = 'blue') +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1], color = 'purple') +\n  labs(title = \"Simple OLS Model\", x = \"Marketing Costs (in 1000)\", y = \"In-app Purchase (in 1000)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nKnowing a lot about causal inference (and Simpson’s Paradox), you decide to run a fixed effect model, adding the city’s indicator as a dummy variable to your model. The fixed effect model controls for city specific characteristics that are constant in time, so if a city is less open to your product, it will capture that.\n\n\n\n\n\n\nYOUR ANSWER\n\n\n\n\n# mutate the data to make the column 'city' a factor\n\n\n# fit the data using the new data and augment the data with predictions from the model\nfe &lt;- ...\n\n\n# augment the fit with predictions\nfe_toy &lt;- ...\n\n# plot the data points\np &lt;- fe_toy |&gt; \n  ggplot(aes(x = mkt_costs, y = purchase, color  = city)) +\n  geom_point() + \n  labs(title = \"Fixed Effect Model\", x = \"Marketing Costs (in 1000)\", y = \"In-app Purchase (in 1000)\") +\n  theme_minimal() \n\n  intcpt &lt;- fe$coefficients[1]; slp = fe$coefficients[2]\n  for ( inc in c(0,fe$coefficients[-c(1,2)]) ){\n    p &lt;- p + geom_abline(slope = slp, intercept = intcpt + inc, color = 'purple') \n  }\np\n\nWhat is the image above is telling you about what fixed effect is doing?\nHow do you interpret the slopes of the lines?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#q3",
    "href": "labs/BSMM_8740_lab_8.html#q3",
    "title": "Lab 8 - Causality: Methods",
    "section": "Q3",
    "text": "Q3\nYou are an advanced analytics analyst working at a private equity firm. One of the partners, an MBA by training, has suggested that the firm eliminate portfolio companies that are run by their founders or their families, on the basis of poor management quality.\n\n\nportfolio data\ndata &lt;- readr::read_csv(\"data/portfolio_companies.csv\", show_col_types = FALSE)\n\n\nThe partner opened the data on the portfolio companies in excel and did a simple calculation, the equivalent of the following:\n\n\nmanagement score difference\ndata %&gt;%\n  dplyr::group_by(foundfam_owned) %&gt;%\n  dplyr::summarise (\n    \"mean management score\" = mean(management)\n    , \"management score stdev\" = sd(management)\n  ) |&gt; \n  dplyr::mutate(delta = `mean management score` - dplyr::lag(`mean management score`))\n\n\n# A tibble: 2 × 4\n  foundfam_owned `mean management score` `management score stdev`  delta\n           &lt;dbl&gt;                   &lt;dbl&gt;                    &lt;dbl&gt;  &lt;dbl&gt;\n1              0                    3.08                    0.632 NA    \n2              1                    2.68                    0.621 -0.396\n\n\nThe partner concluded that the portfolio firms that are run by their founders or their families result in a management quality that is worse (per the management score) in comparison to other portfolio firms, a significant difference - almost 2/3 of a standard deviation worse.\nYou, being an advanced-analytics analyst, gently suggest that this is a question of causality, and that there may be other factors related to both firm ownership and management quality that bias the management score. The other partners all agree, and ask you to estimate the real effect of firm ownership type on management quality.\nSo you start by interviewing the partners and others to identify other factors, particularly those that might be related to variations in either ownership structure or management quality.\n\nPotential variables\nOne source of variation in ownership is how a firm starts, whether they were started by a founder or perhaps they were spin-offs, joint ventures, or affiliates of other companies. You don’t have this kind of data, but you do have data on the production technology the firm uses. Some technologies are very capital intensive, so they are unlikely to be used by start-ups, even those that become successful. So the technology a firm uses is a source of variation in ownership.\nWhether firms start as founder-owned or involve outside owners also depend on cultural or institutional factors in society. This may be important in data collected from firms in many countries, and even within countries. Similar factors may affect management quality.\nSome founder/family businesses are sold to investors, so variation may depend on supply and demand (i.e. the level of M&A business). Firm size and age may also be a factor in whether a firm is acquired.\nSimilarly the competition in the industry may be a factor in both ownership and management quality, as highly competitive industries may have fewer founder owned firms and better management quality.\nYou build a DAG to represent these assumptions, as follows:\n\n\nbusiness DAG\nset.seed(3534)\nset.seed(534)\n\nfq_dag &lt;- ggdag::dagify(\n  mq ~ ff + m + c + ci + ct,\n  ff ~ c + ct + ci + fsa + fc,\n  m ~ ff,\n  es ~ mq + ff,\n  #fsa ~ mq,\n  exposure = \"ff\",\n  outcome = \"mq\",\n  labels = c(\n    mq = \"management_quality\",\n    ff = \"founder_family\",\n    m = \"managers\",\n    fsa = \"firm_size_age\",\n    c = \"competition\",\n    ci = \"culture_institutions\",\n    ct = \"complex_technology\",\n    fc = \"family_circumstances\",\n    es = \"export share\"\n  )\n)\n\nfq_dag |&gt;\n  ggdag::tidy_dagitty() |&gt; \n  ggdag::node_status() |&gt;\n  ggplot(\n    aes(x, y, xend = xend, yend = yend, color = status)\n  ) +\n  ggdag::geom_dag_edges() +\n  ggdag::geom_dag_point() +\n  ggdag::geom_dag_label_repel(aes(label = label)) +\n  ggokabeito::scale_color_okabe_ito(na.value = \"darkgrey\") +\n  ggdag::theme_dag() +\n  theme(legend.position = \"none\") +\n  coord_cartesian(clip = \"off\")\n\n\n\n\n\n\n\n\n\n\n\nData\nNext you look for data that can measure the casual factors in your DAG. You have the following data:\n\nemployment count\nage of firm\nproportion of employees with a college education (except management)\nlevel of competition\nindustry classification\ncountry of origin\nshare of exports in sales\n\nOf these:\n\nindustry can be used as a measure of technology complexity, as is the share of college educated employees.\nnumber of competitors is a measure of competition strength\nthe country of origin is a measure of cultural and institutional factors\nthe number of employees is a measure of firm size\nthe age of the firm is missing for about 14% of the observations\n\nYou have data for the share of exports in sales, but as it is a collider you decide not to condition on this variable in your analysis.\n\n\nQ3(1)\nHere are the variables you have decided to use in your analysis:\n\nY &lt;- \"management\"      # outcome\nD &lt;- \"foundfam_owned\"  # treatment\n\ncontrol_vars &lt;- c(\"degree_nm\", \"degree_nm_sq\", \"compet_moder\", \"compet_strong\", \n                  \"lnemp\", \"age_young\", \"age_old\", \"age_unknown\")\ncontrol_vars_to_interact &lt;- c(\"industry\", \"countrycode\")\n\n# confounders\nX &lt;- c(control_vars, control_vars_to_interact)\n\nAnd here are the formulas for the models you have decided to test:\n\n# basic model without confounders\nformula1 &lt;- as.formula(paste0(Y, \" ~ \",D))\n\n# basic model with confounders\nformula2 &lt;- as.formula(paste0(Y, \" ~ \",D,\" + \", \n                  paste(X, collapse = \" + \")))\n\n# model with interactions between variables\nformula3 &lt;- as.formula(paste(Y, \" ~ \",D,\" + \", \n    paste(control_vars_to_interact, collapse = \":\"), \n    \" + (\", paste(control_vars, collapse = \"+\"),\")*(\",\n    paste(control_vars_to_interact, collapse = \"+\"),\")\",sep=\"\"))\n\nUse linear regression to fit each model, and\n\nuse fit |&gt; broom::tidy() |&gt; dplyr::slice(2) to extract the estimated effect for each model, and\nuse fit |&gt; broom::glance() to extract model performance data\nrank the models by comparing AIC and BIC and select the most suitable.\n\n\n\n\n\n\n\nYOUR ANSWER\n\n\n\n\n# fit formula1\nols1 &lt;- ?\n\n# fit formula2\nols2 &lt;- ?\n\n# fit formula3\nols3 &lt;- ?\n\n\n# effect and metrics: formula1\n\n# effect and metrics: formula2\n\n# effect and metrics: formula3\n\nConsidering the trade-off between model fit and model complexity, the most suitable model is ____\n\n\n\n\nQ3(2)\nHaving settled on a formula/model, use the selected model in the doubly-robust effect estimation function from class to:\n\nestimate the effect.\ncalculate the confidence intervals on the effect estimate, using rsample::bootstraps with times = 100.\n\n\n\n\n\n\n\nYOUR ANSWER\n\n\n\n\n# double robust estimate using the best model\n\n\n# bootstrap CI estimates\n\n\n\n\n\nOmitted variables and bias\nIf all potential confounders are captured and included in the models, then we can put the expected effect of changing ownership within our 95% confidence intervals we calculated.\nHowever, we suspect that we do not have a full set of confounders in the data set, either because our measurements don’t capture all the variation due to a variable or because we just don’t have the data at all.\nFor example, we don’t have information on the the city the business is located in; being in a large city may make a business more attractive to outside investors, reducing the number of family-owned business while making the quality of family-owned businesses better. If this assumption is correct, then without this variable/data in the model, our estimate will be negatively biased and the effect is probably weaker than suggested by our estimates.\nIf other omitted variables behaved the same way we would see an even smaller effect.\nGiven your estimated effect and considering omitted variable bias, please provide two bullet points about your analysis for the partners:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nbullet #1: (comment on the comparison of the robust causal estimate and the mean differences)\nbullet #2: (comment on omitted variable bias and small sample size bias in context)’"
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#q4",
    "href": "labs/BSMM_8740_lab_8.html#q4",
    "title": "Lab 8 - Causality: Methods",
    "section": "Q4",
    "text": "Q4\nYour firm has a customer intervention that management would like to evaluate. The data from a test of the intervention is as follows (where DD is the (binary intervention, YY is the outcome (in units of $1,000 dollars), and X1,…,X5X_1,\\ldots,X_5 are variables in the adjustment set). Each row in the data represents measurements for a distinct customer:\n\nset.seed(8740)\nn &lt;- 2000; p &lt;- 5;\n\ntest_dat &lt;- matrix(rnorm(n * p), n, p, dimnames = list(NULL, paste0(\"X\",1:p)) ) |&gt;\n  tibble::as_tibble() |&gt;\n  dplyr::mutate(\n    D = rbinom(n, 1, 0.5) * as.numeric(abs(X3) &lt; 2)\n    , Y = pmax(X1, 0) * D + X2 + pmin(X3, 0) + rnorm(n)\n  )\n\nEstimate the ATE as follows (hint: it is &gt;0):\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# (1) using regression adjustment, with formula Y ~ D*X1 + X2 + X3 + X4 + X5\nols &lt;- \n\n# (2) using doubly robust estimation\ndre &lt;- \n\n\n# summarize results\ntibble::tibble(\n  dre_ATE = dre\n  , ols_ATE = \n    ols |&gt; broom::tidy() |&gt; \n    dplyr::filter(term == \"D\") |&gt; \n    dplyr::pull(estimate)\n)\n\n\n\nGiven that the ATE is &gt;0, the firm would like to roll out the intervention with 1,000 new customers, and the project is budgeted as follows\n\ncost of the intervention is $100 (cost = 0.1 in the scale of the data).\nthe per-customer average incremental revenue on making the intervention is (ATE−0.1)×1000(\\textrm{ATE}-0.1)\\times 1000.\n\nGiven the substantial return on the investment, the budget is approved and the firm decides to implement the intervention with the 1,000 new customers.\nHowever, the firm’s good fortune is that you are in the room with management, and you suggest an alternative strategy, based on predicting the individual treatment effects for each new customer using the customer data.\nThe new customer data is:\n\n# new customer data\nnew_dat &lt;- \n  matrix(rnorm(n * p), n, p, dimnames = list(NULL, paste0(\"X\",1:p)) ) |&gt;\n  tibble::as_tibble() \n\nYou analyse your strategy for management as follows:\n\nmake two copies of the new data; mutate one to add a column D=1 and mutate the other to add a column D=0, and\ntake the regression model used to estimate the ATE, and predict the treatment effects for each customer, using the two data sets from steps 1 (i.e. use broom::augment with each dataset),\nselect the columns with the predicted responses from each dataset and bind them columnwise. Name the columns r1 (response when D=1) and r0 (response when D=0). Then\n\nmutate to compute the contrast r1-r0 and subtract the cost of the intervention. Call this new column ‘lift’ (standing for your new strategy based on estimating individual treatment effects)\nmutate to add a new column with the ATE and subtract the cost of the intervention. Call this new column ‘baseline’ (standing for baseline strategy)\nsort the rows in descending order by lift\nadd two new columns: one for the cumulative sum of the lifts and the other for the cumulative sum of the baseline. Call these columns cumulative_lift and cumulative_baseline respectively.\n\nPlot the cumulative results of the baseline and the lift strategies along with the % of budget spent\nFind the maximum difference between the lift and baseline strategies, and the percent of budget spent at the maximum.\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# make two copies of new_dat and,\n\n# mutate one setting the intervention D=1\nnew_dat1 &lt;- \n\n# mutate the other setting the intervention D=0  \nnew_dat0 &lt;-\n\n\n# use the linear model (ols) to predict the responses under the two treatments\npredicted1 &lt;-\n  \npredicted0 &lt;- \n\n\n# combine the columns containing the predicted responses to create a two column dataframe, and\n# - add columns for the lift net of costs and the baseline net of costs\n# - sort in descending order by lift, then\n# - add an ID column, using tibble::rowid_to_column(\"ID\")\n# - in the last step, add a column for the cumulative % of budget (use the ID column for this)\n#   along with columns for the cumulative sums of lift and baseline \n\nresult &lt;- \n\n\n# plot the cumulative results to compare strategies \nresult |&gt;\n  tidyr::pivot_longer(c(cumulative_baseline, cumulative_lift)) |&gt;\n  ggplot(aes(x=cumulative_pct, y=value, color = name)) + geom_line() +\n  theme_minimal(base_size = 18) + \n  theme(legend.title = element_blank(),legend.position = \"top\") +\n  labs(title = \" Stategy Comparison\", x = \"cumulative budget spend (%)\", y = \"net revenue (000's)\")\n\n\n# use the result data to \n# - find the maximum difference predicted between lift and baseline strategies in $'s\n\n# - find the maximum net revenue under either strategy in $'s\n\n# - indicate the percent of original budget necessary for each maximum."
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#q5",
    "href": "labs/BSMM_8740_lab_8.html#q5",
    "title": "Lab 8 - Causality: Methods",
    "section": "Q5",
    "text": "Q5\nEstimate the causal effect of smoking cessation on weight, using the dataset data(nhefs) where the treatment variable is quit_smoking, the outcome variable is wt82_71, and the covariates of interest are age, wt71, smokeintensity, exercise, education, sex, and race.\nEstimate the causal effect using the matching estimator described in the lecture.\n\n# load data\nnhefs &lt;- \n  readr::read_csv('data/nhefs_data.csv', show_col_types = FALSE) |&gt; \n  dplyr::select(wt82_71, quit_smoking, age, wt71, smokeintensity, exercise, education, sex, race)\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# (1)\n# create a table to calculate the mean difference in effects for the two treatment groups in the raw data\n\n\n#(2)\n# create recipe to normalize the numerical covariates of interest (note - some covariates are factors)\nnhefs_data &lt;-\n\n\n#(3)\n# using nhefs_data calculate the un-corrected effect estimate, per the matching method we used in class\n# NOTE: this takes some time to run\n\nThe un-corrected treatment effect is: ____\n\n#(4)\n# use the method from class to calculate the correction terms, can compute the revised estimate \n# NOTE: this takes some time to run\n\n\n#(5)\n# calculate the corrected causal estimate for the effect of smoking cessation on weight\n\nThe un-corrected treatment effect is: ____\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 7!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub."
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#submission",
    "href": "labs/BSMM_8740_lab_8.html#submission",
    "title": "Lab 8 - Causality: Methods",
    "section": "Submission",
    "text": "Submission\nI will pull (copy) everyone’s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_8.html#grading",
    "href": "labs/BSMM_8740_lab_8.html#grading",
    "title": "Lab 8 - Causality: Methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 5\n30"
  },
  {
    "objectID": "misc/10-continuous-g-computation-exercises.html",
    "href": "misc/10-continuous-g-computation-exercises.html",
    "title": "Continuous exposures and g-computation",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(touringplans)\nlibrary(splines)\nFor this set of exercises, we’ll use g-computation to calculate a causal effect for continuous exposures.\nIn the touringplans data set, we have information about the posted waiting times for rides. We also have a limited amount of data on the observed, actual times. The question that we will consider is this: Do posted wait times (avg_spostmin) for the Seven Dwarves Mine Train at 8 am affect actual wait times (avg_sactmin) at 9 am? Here’s our DAG:\nFirst, let’s wrangle our data to address our question: do posted wait times at 8 affect actual weight times at 9? We’ll join the baseline data (all covariates and posted wait time at 8) with the outcome (average actual time). We also have a lot of missingness for avg_sactmin, so we’ll drop unobserved values for now.\nYou don’t need to update any code here, so just run this.\neight &lt;- seven_dwarfs_train_2018 |&gt;\n  dplyr::filter(wait_hour == 8) |&gt;\n  dplyr::select(-wait_minutes_actual_avg)\n\nnine &lt;- seven_dwarfs_train_2018 |&gt;\n  dplyr::filter(wait_hour == 9) |&gt;\n  dplyr::select(park_date, wait_minutes_actual_avg)\n\nwait_times &lt;- eight |&gt;\n  dplyr::left_join(nine, by = \"park_date\") |&gt;\n  tidyr::drop_na(wait_minutes_actual_avg)"
  },
  {
    "objectID": "misc/10-continuous-g-computation-exercises.html#stretch-goal-boostrapped-intervals",
    "href": "misc/10-continuous-g-computation-exercises.html#stretch-goal-boostrapped-intervals",
    "title": "Continuous exposures and g-computation",
    "section": "Stretch goal: Boostrapped intervals",
    "text": "Stretch goal: Boostrapped intervals\nLike propensity-based models, we need to do a little more work to get correct standard errors and confidence intervals. In this stretch goal, use rsample to bootstrap the estimates we got from the G-computation model.\nRemember, you need to bootstrap the entire modeling process, including the regression model, cloning the data sets, and calculating the effects.\n\nset.seed(1234)\nlibrary(rsample)\n\nfit_gcomp &lt;- function(split, ...) { \n  .df &lt;- analysis(split) \n  \n  # fit outcome model. remember to model using `.df` instead of `wait_times`\n  standardized_model &lt;-  lm(\n    wait_minutes_actual_avg ~ ns(wait_minutes_posted_avg, df = 2)*park_extra_magic_morning + park_temperature_high + park_ticket_season + park_close, \n    data = .df\n  )\n  \n  # clone datasets. remember to clone `.df` instead of `wait_times`\n  thirty &lt;- .df |&gt;\n    mutate(wait_minutes_posted_avg = 30)\n  \n  sixty &lt;- .df |&gt;\n    mutate(wait_minutes_posted_avg = 60)\n  \n  # predict actual wait time for each cloned dataset\n  predicted_thirty &lt;- standardized_model |&gt;\n    augment(newdata = thirty) |&gt;\n    select(thirty_posted_minutes = .fitted)\n  \n  predicted_sixty &lt;- standardized_model |&gt;\n    augment(newdata = sixty) |&gt;\n    select(sixty_posted_minutes = .fitted)\n  \n  # calculate ATE\n  bind_cols(predicted_thirty, predicted_sixty) |&gt;\n    summarize(\n      mean_thirty = mean(thirty_posted_minutes),\n      mean_sixty = mean(sixty_posted_minutes),\n      difference = mean_sixty - mean_thirty\n    ) |&gt;\n    # rsample expects a `term` and `estimate` column\n    pivot_longer(everything(), names_to = \"term\", values_to = \"estimate\")\n}\n\ngcomp_results &lt;- bootstraps(wait_times, 1000, apparent = TRUE) |&gt;\n  mutate(results = map(splits, fit_gcomp))\n\n# using bias-corrected confidence intervals\nboot_estimate &lt;- int_bca(gcomp_results, results, .fn = fit_gcomp)\n\nboot_estimate\n\n# A tibble: 3 × 6\n  term        .lower .estimate .upper .alpha .method\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n1 difference   -24.8     -1.23   28.6   0.05 BCa    \n2 mean_sixty    16.8     34.6    58.1   0.05 BCa    \n3 mean_thirty   24.2     35.9    49.0   0.05 BCa"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html",
    "href": "misc/in_process/Causal_via_Prediction_R.html",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "",
    "text": "The traditional go-to tool for causal inference is multiple regression: Yi=δDi+Xi′β+εi,\nY_i = \\delta D_i + X_i'\\beta+\\varepsilon_i,\n where DiD_i is the “treatment” or causal variable whose effects we are interested in, and XiX_i is a vector of controls, conditional on which we are willing to assume DiD_i is as good as randomly assigned.\n\nexample: Suppose we are interested in the magnitude of racial discrimination in the labor market. One way to conceptualize this is the difference in earnings between two workers who are identical in productivity, but differ in their race, or, the “effect” of race. Then DiD_i would be an indicator for, say, a Black worker. YiY_i would be earnings, and XiX_i would be characteristics that capture determinants of productivity, including educational attainment, cognitive ability, and other background characteristics.\n\nWhere does machine learning fit into causal inference? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Don’t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}. Modern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient δ\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate δ\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, Ỹ\\tilde{Y}% _{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where ŶiOLS=Xi′(X′X)−1X′Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime }\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, D̃\\tilde{D}% _{i}=D_{i}-\\hat{D}_{i}^{OLS}, where D̂iOLS=Xi′(X′X)−1X′D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime }\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress Ỹi\\tilde{Y}_{i} on D̃i\\tilde{D}_{i}.\n\nSteps 1 and 2 are prediction exercises–ML’s wheelhouse. When OLS isn’t the right tool for the job, we can replace OLS in those steps with machine learning:\n\nPredict YiY_{i} based on XiX_{i} using ML and compute the residuals, Ỹ\\tilde{Y}% _{i}=Y_{i}-\\hat{Y}_{i}^{ML}, where ŶiML\\hat{Y}_{i}^{ML} is the prediction from an ML algorithm\nPredict DiD_{i} based on XiX_{i} using ML and compute the residuals, D̃\\tilde{D}% _{i}=D_{i}-\\hat{D}_{i}^{ML}, where D̂iML\\hat{D}_{i}^{ML} is the prediction from an ML algorithm\nRegress Ỹi\\tilde{Y}_{i} on D̃i\\tilde{D}_{i}.\n\nThis is the basis for the two major methods we’ll look at today: The first is “Post-Double Selection Lasso” (Belloni, Chernozhukov, Hansen). The second is “Double-Debiased Machine Learning” (Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins)"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#define-outcome-regressor-of-interest",
    "href": "misc/in_process/Causal_via_Prediction_R.html#define-outcome-regressor-of-interest",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Define outcome & regressor of interest",
    "text": "Define outcome & regressor of interest\n\ny: lnw_2016\nd: black"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#simple-regression-with-no-controls",
    "href": "misc/in_process/Causal_via_Prediction_R.html#simple-regression-with-no-controls",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Simple Regression with no Controls",
    "text": "Simple Regression with no Controls\nRegress y on d and print out coefficient\n\nlm(lnw_2016 ~ black, data = nlsy)\n\n\nCall:\nlm(formula = lnw_2016 ~ black, data = nlsy)\n\nCoefficients:\n(Intercept)        black  \n     3.1792      -0.3817  \n\n\n\n…\nIs this the effect we’re looking for?\nLet’s try a regression where we control for a few things: education (linearly), experience (linearly), and cognitive ability (afqt, linearly).\n\nfit &lt;- lm(lnw_2016 ~ black + educ + exp + afqt, data = nlsy)\n\nbroom::tidy(fit)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   1.15      0.499       2.31 0.0210    \n2 black        -0.262     0.0639     -4.10 0.0000444 \n3 educ          0.0893    0.0195      4.57 0.00000532\n4 exp           0.0362    0.0167      2.17 0.0305    \n5 afqt          0.0371    0.0102      3.63 0.000296  \n\n\n\n\n…\nHow does it compare to the simple regression?\nBut who is to say the controls we included are sufficient? We have a whole host (hundred!) of other potential controls, not to mention that perhaps the controls we did put in enter linearly. This is a job for ML!\nTo prep, let’s define a matrix X with all of our potential controls:\n\npotential_controls &lt;- setdiff(colnames(nlsy), c(\"lnw_2016\", \"black\"))"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#post-double-selection-lasso",
    "href": "misc/in_process/Causal_via_Prediction_R.html#post-double-selection-lasso",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Post Double Selection Lasso",
    "text": "Post Double Selection Lasso\n\nStep 1: Lasso the outcome on X\nDon’t forget to standard Xs, or choose the normalize=True option\n\nnlsy_rec_y &lt;- nlsy %&gt;% \n  recipes::recipe(lnw_2016 ~ .) %&gt;% \n  recipes::step_rm(black)\n\nnlsy_rec_d &lt;- nlsy %&gt;% \n  recipes::recipe(black ~ .) %&gt;% \n  recipes::step_rm(lnw_2016)\n\n\n# Run cross-validation for y\nlasso_y &lt;- \n  glmnet::cv.glmnet(\n    x = model.matrix(lnw_2016 ~ ., data = nlsy_bake)\n    , y = nlsy_bake$lnw_2016\n  )\n\n\nset.seed(8740)\n\nlr_cv   &lt;- rsample::vfold_cv(nlsy)\nlr_grid &lt;- tibble::tibble(penalty = 10^seq(-4,1, length.out = 200))\n\nlr_mod  &lt;- parsnip::linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  parsnip::set_engine(\"glmnet\") %&gt;% \n  parsnip::set_mode(\"regression\")\n\nlr_wf_y &lt;- workflows::workflow() %&gt;%\n  workflows::add_model(lr_mod) %&gt;%\n  workflows::add_recipe(nlsy_rec_y)\n\nlr_best_y &lt;- lr_wf_y %&gt;%\n  tune::tune_grid(\n    resamples = lr_cv,\n    grid = lr_grid\n  ) %&gt;%\n  tune::select_best(metric = \"rmse\")\n\ntidy_keep_y &lt;- lr_wf_y %&gt;% \n  tune::finalize_workflow(lr_best_y) %&gt;%\n  parsnip::fit(nlsy) %&gt;%\n  workflows::extract_fit_parsnip() %&gt;%\n  broom::tidy() %&gt;% \n  dplyr::filter(estimate != 0 & term != '(Intercept)') %&gt;% \n  dplyr::pull(term)\n\n\nX = as.matrix(nlsy[, potential_controls])\ny = nlsy[[\"lnw_2016\"]]\n\n# Run cross-validation for y\nlasso_y &lt;- glmnet::cv.glmnet(x=X, y=y)\n\nlasso_y_coefs = coef(lasso_y, lasso_y$lambda.1se)\nlasso_y_coefs = as.matrix(lasso_y_coefs)\n\nkeep_y = rownames(lasso_y_coefs)[lasso_y_coefs != 0]\n# Don't need intercept\nkeep_y = setdiff(keep_y, \"(Intercept)\")\n\n\n\nStep 2: Lasso the treatment on d\n\nlr_wf_d &lt;- lr_wf_y %&gt;% workflows::update_recipe(nlsy_rec_d)\n\nlr_best_d &lt;- lr_wf_d %&gt;%\n  tune::tune_grid(\n    resamples = lr_cv,\n    grid = lr_grid\n  ) %&gt;%\n  tune::select_best(metric = \"rmse\")\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x10\n\n\n\n\ntidy_keep_d &lt;- lr_wf_d %&gt;% \n  tune::finalize_workflow(lr_best_d) %&gt;%\n  parsnip::fit(nlsy) %&gt;%\n  workflows::extract_fit_parsnip() %&gt;%\n  broom::tidy() %&gt;% \n  dplyr::filter(estimate != 0 & term != '(Intercept)') %&gt;% \n  dplyr::pull(term)\n\n\n# Run cross-validation for d\nd = nlsy[[\"black\"]]\nlasso_d &lt;- cv.glmnet(x=X, y=d)\n\nlasso_d_coefs = coef(lasso_d, lasso_d$lambda.1se)\nlasso_d_coefs = as.matrix(lasso_d_coefs)\n\nkeep_d = rownames(lasso_d_coefs)[lasso_d_coefs != 0]\n# Don't need intercept\nkeep_d = setdiff(keep_d, \"(Intercept)\")\n\n\n\nStep 3: Form the union of controls\n\ntidy_keep &lt;- union(tidy_keep_y, tidy_keep_d)\n\n\nkeep = union(keep_y, keep_d)\n\n\n\nConcatenate treatment with union of controls and regress y on that and print out estimate\n\n# Need `` surrounding variables since some variables start with underscore\nformula = paste(\n  \"lnw_2016 ~ black + \", \n  paste0(\"`\", tidy_keep, \"`\", collapse = \" + \")\n)\nformula = as.formula(formula)\n\nlm(formula, data = nlsy) %&gt;% broom::tidy()\n\n# A tibble: 147 × 5\n   term                  estimate std.error statistic       p.value\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 (Intercept)             1.92      0.313       6.15 0.00000000108\n 2 black                  -0.235     0.0912     -2.57 0.0102       \n 3 educ                    0.0506    0.0109      4.62 0.00000425   \n 4 afqt                    0.0310    0.0113      2.74 0.00633      \n 5 youth_bothbio_01_1997   0.205     0.194       1.05 0.292        \n 6 p4_001_1997            -0.0557    0.0316     -1.76 0.0787       \n 7 `_BGypc9_001_3`        -0.181     0.0563     -3.21 0.00134      \n 8 `_BGypc12_02b4`        -0.216     0.0862     -2.51 0.0123       \n 9 `_BGyysaq_29b7`         0.258     0.0839      3.07 0.00219      \n10 `_BGyfp_yhro_21`       -0.372     0.130      -2.87 0.00413      \n# ℹ 137 more rows\n\n# lr_wf_final &lt;- lr_wf_d %&gt;% workflows::update_formula(as.formula(formula))\n\n\n# Need `` surrounding variables since some variables start with underscore\nformula = paste(\n  \"lnw_2016 ~ black + \", \n  paste0(\"`\", keep, \"`\", collapse = \" + \")\n)\nformula = as.formula(formula)\n\n(fullreg = feols(formula, data = nlsy))\n\nThe variables '`_BGpp4_029__1`' and '`_BGpfp_yfmr_4`' have been removed because of collinearity (see $collin.var).\n\n\nOLS estimation, Dep. Var.: lnw_2016\nObservations: 1,266\nStandard-errors: IID \n                            Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)                 2.341612   0.225027 10.405919  &lt; 2.2e-16 ***\nblack                      -0.141191   0.086111 -1.639644 1.0134e-01    \neduc                        0.054290   0.010662  5.091736 4.1163e-07 ***\nafqt                        0.038466   0.011318  3.398817 6.9899e-04 ***\nhispanic                    0.068641   0.079383  0.864671 3.8739e-01    \nyhea_2200_1997              0.000995   0.000701  1.419437 1.5603e-01    \np4_001_1997                -0.067009   0.026496 -2.528978 1.1567e-02 *  \ncv_bio_mom_age_child1_1997 -0.002313   0.004404 -0.525223 5.9953e-01    \n... 58 coefficients remaining (display them with summary() or use argument n)\n... 2 variables were removed because of collinearity (`_BGpp4_029__1` and `_BGpfp_yfmr_4`)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.802714   Adj. R2: 0.100286"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#double-debiased-machine-learning",
    "href": "misc/in_process/Causal_via_Prediction_R.html#double-debiased-machine-learning",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Double-Debiased Machine Learning",
    "text": "Double-Debiased Machine Learning\nFor simplicity, we will first do it without sample splitting\n\nStep 1: Ridge outcome on Xs, get residuals\n\n# Run cross-validation for y\nridge_y &lt;- cv.glmnet(x=X, y=y, alpha = 0)\n\ny_hat = predict(ridge_y, ridge_y$lambda.1se, newx = X)\nnlsy$y_resid = nlsy$lnw_2016 - as.numeric(y_hat)\n\n\n\nStep 2: Ridge treatment on Xs, get residuals\n\n# Run cross-validation for y\nridge_d &lt;- cv.glmnet(x=X, y=d, alpha = 0)\n\nd_hat = predict(ridge_d, ridge_d$lambda.1se, newx = X)\nnlsy$d_resid = nlsy$black - as.numeric(d_hat)\n\n\n\nStep 3: Regress y resids on d resids and print out estimate\n\nfeols(y_resid ~ d_resid, nlsy)\n\nOLS estimation, Dep. Var.: y_resid\nObservations: 1,266\nStandard-errors: IID \n                 Estimate Std. Error      t value   Pr(&gt;|t|)    \n(Intercept) -1.320000e-15   0.024100 -5.50000e-14 1.00000000    \nd_resid     -3.167224e-01   0.083147 -3.80917e+00 0.00014613 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.856812   Adj. R2: 0.010567\n\n\n\n\nThe real thing: with sample splitting\n\nset.seed(5)\nN_folds = 5\n# 5 folds with equal \nnlsy$fold_id = sample(1:N_folds, size = nrow(nlsy), replace = T)\n\nnlsy$y_resid = 0\nnlsy$d_resid = 0\n\n# Loop through each fold, use other 4 folds to estimate\nfor(i in 1:5) {\n  in_training = (nlsy$fold_id != i)\n  in_test = (nlsy$fold_id == i)\n\n  # Ridge regression for y using training\n  ridge_y = cv.glmnet(\n    x=X[in_training,], y=y[in_training], alpha = 0\n  )\n  # Calculate residuals for testing\n  nlsy[in_test, \"y_resid\"] =\n    y[in_test] - predict(ridge_y, newx = X[in_test, ])\n\n  # Ridge regression for d using training\n  ridge_d = cv.glmnet(\n    x=X[in_training,], y=d[in_training], alpha = 0\n  )\n  # Calculate residuals for testing\n  nlsy[in_test, \"d_resid\"] = \n    d[in_test] - predict(ridge_d, newx = X[in_test, ])\n}\n\n# k-fold cross-validation ensures standard errors are fine\nfeols(\n  y_resid ~ d_resid, data = nlsy, vcov = \"hc1\"\n)\n\nOLS estimation, Dep. Var.: y_resid\nObservations: 1,266\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept) -0.000992   0.024262 -0.040884 0.96739479    \nd_resid     -0.302294   0.077506 -3.900259 0.00010114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.862505   Adj. R2: 0.013436"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#now-do-dml-using-random-forest",
    "href": "misc/in_process/Causal_via_Prediction_R.html#now-do-dml-using-random-forest",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Now do DML using Random Forest!",
    "text": "Now do DML using Random Forest!\n\nset.seed(5)\nN_folds = 5\n# 5 folds with equal \nnlsy$fold_id = sample(1:N_folds, size = nrow(nlsy), replace = T)\n\nnlsy$y_resid = 0\nnlsy$d_resid = 0\n\n# Loop through each fold, use other 4 folds to estimate\nfor(i in 1:5) {\n  in_training = (nlsy$fold_id != i)\n  in_test = (nlsy$fold_id == i)\n\n  # Ridge regression for y using training\n  ridge_y = randomForest(\n    x = X[in_training,], y = y[in_training]\n  )\n  # Calculate residuals for testing\n  nlsy[in_test, \"y_resid\"] =\n    y[in_test] - predict(ridge_y, newdata = X[in_test, ])\n\n  # Ridge regression for d using training\n  ridge_d = randomForest(\n    x = X[in_training,], y = d[in_training]\n  )\n  # Calculate residuals for testing\n  nlsy[in_test, \"d_resid\"] = \n    d[in_test] - predict(ridge_d, newdata = X[in_test, ])\n}\n\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\n\n# k-fold cross-validation ensures standard errors are fine\nfeols(\n  y_resid ~ d_resid, data = nlsy, vcov = \"hc1\"\n)\n\nOLS estimation, Dep. Var.: y_resid\nObservations: 1,266\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept) -0.007765   0.023164 -0.335205  0.73753    \nd_resid     -0.136571   0.074804 -1.825707  0.06813 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.823375   Adj. R2: 0.002127"
  },
  {
    "objectID": "misc/rcts_to_regression.html",
    "href": "misc/rcts_to_regression.html",
    "title": "RCTs to Regression",
    "section": "",
    "text": "Treatment indicator: Di∈0,1D_i\\in {0,1}\n\nexample: eligibility for expanded Medicaid\n\nOutcome: YiY_{i}\n\nexample: number of doctor visits in past 6 months\n\nPotential outcomes Yi0,Yi1Y_{i}^{0},Y_{i}^{1}\nIndividual-level treatment effect ΔiY=Yi1−Yi0\\Delta_i^Y=Y_i^1-Y_i^0 (can never know this).\nUnbiased estimate of average treatment effect:\nΔY=𝔼[Yi1−Yi0]\n\\Delta^{Y}=\\mathbb{E}\\left[Y_{i}^{1}-Y_{i}^{0}\\right]\n\nor OLS coefficient on 𝐷𝑖 from this regression:\nYi=α+δDi+ϵiY_{i}=\\alpha+\\delta D_{i}+\\epsilon_{i}\n\nurl_str  &lt;- \n  'https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/'\nfile_str &lt;- \n  'oregon_hie_table5.csv?raw=true'\n  \ndat &lt;- readr::read_csv(\n  paste0(url_str, file_str, show_col_types = FALSE)\n) %&gt;% \n  dplyr::select(doc_num, treatment, weight, dplyr::starts_with(\"ddd\")) %&gt;% \n  tidyr::drop_na()\n\n\nfit &lt;- lm(doc_num ~ treatment, data = dat, weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits: {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits: 0.268\n\n\n\n\n\nThe bivariate regression above leans heavily on random assignment of treatment:\nDi⟂⟂Yi0,Yi1D_{i}\\perp\\!\\!\\!\\!\\perp Y_{i}^{0},Y_{i}^{1}\nSometimes, even in an RCT, treatment is assigned randomly only conditional on some set of covariates XiX_i.\n\nexample: in the Oregon HIE, eligibility for Medicaid was granted via lottery, but households with more members could have more lottery entries. So the lottery outcome is random only conditional on household size.\n\nSo what happens if we don’t have random assignment? In terms of our regression model above, it means ϵi\\epsilon_i may be correlated with DiD_i. For example, perhaps household size, XiX_i, which increases the probability of treatment, is also associated with more doctor visits. If XiX_i is omitted from the model, it is buried in the error term:\nϵi=βXi+ηi\n\\epsilon_{i}=\\beta X_{i}+\\eta_{i}\n We’ll assume for now that everything else related to doctor visits (ηi\\eta_i) is unrelated to treatment. What does our bivariate regression coefficient deliver in this case?\nδ̂OLS=Cov(Yi,Di)Var(Di)=δ+βCov(Xi,Di)Var(Di)(1)\\hat{\\delta}^{\\text{OLS}}=\\frac{Cov\\left(Y_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}=\\delta+\\beta\\frac{Cov\\left(X_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}\n \\qquad(1)\nSimple regression gives us what we want (δ\\delta) plus an omitted variables bias term. The form of this term tells us what kinds of XiX_i variables we should take care to control for in our regressions.\nAccording to the ommitted variable bias (OVB) formula (Equation 1), what kinds of variables should you be be sure to control for in regressions?\nCareful investigators will find a set of regressors XiX_i for which they are willing to assume treatment is as good as randomly assigned:\nDi⟂⟂(Yi(0),Yi(1))|Xi.\nD_i\\perp\\!\\!\\!\\!\\perp\\left( Y_{i}\\left( 0\\right) ,Y_{i}\\left( 1\\right) \\right) |X_{i}\n\\text{.}\n This combined with a linear model for the conditional expectation of Yi0Y_{i}^0 and Yi1Y_{i}^1 given XiX_{i} means we can estimate the average treatment via OLS on the following regression equation:\nYi=δDi+Xi′β+εi.\nY_{i}=\\delta D_{i}+X_{i}^{\\prime }\\beta +\\varepsilon _{i}.\n\n\n# Add the household size indicators to our regressor set and run regression:\nfit &lt;- lm(doc_num ~ ., data = dat %&gt;% dplyr::select(-weight), weights = dat$weight)\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (with controls): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (with controls): 0.314\n\n\nHow did the estimate of the effect of Medicaid eligiblity change? What does that tell us about the relationship between the included regressors and the outcome and treatment?\n\n\n\nWhere does machine learning fit into this? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Don’t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}.\nModern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient δ\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate δ\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, Ỹi=Yi−ŶiOLS\\tilde{Y}_{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where ŶiOLS=Xi′(X′X)−1X′Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, D̃i=Di−D̂iOLS\\tilde{D}_{i}=D_{i}-\\hat{D}_{i}^{OLS}, where D̂iOLS=Xi′(X′X)−1X′D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress Ỹi\\tilde{Y}_{i} on D̃i\\tilde{D}_{i}.\n\nLet’s try it!\n\n# Regress outcome on covariates (not treatment)\nyreg &lt;- lm(\n  doc_num ~ .\n  , data = dat %&gt;% dplyr::select(doc_num, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\nytilde = yreg$residuals\n\n# regress treatment on covariates (not outcome)\ndreg &lt;- lm(\n  treatment ~ .\n  , data = dat %&gt;% dplyr::select(treatment, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\ndtilde = dreg$residuals\n\n# regress ytilde on dtilde\nfit &lt;- lm(\n  ytilde ~ dtilde\n  , data = tibble::tibble(ytilde = ytilde, dtilde = dtilde)\n  , weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'dtilde') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (partialled out): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (partialled out): 0.314\n\n\nML enters the picture by providing an alternate way to generate Ŷi\\hat{Y}_i and D̂i\\hat{D}_i when OLS is not the best tool for the job. The first two steps are really just prediction exercises, and in principle any supervised machine learning algorithm can step in here.\nBack to the whiteboard!"
  },
  {
    "objectID": "misc/rcts_to_regression.html#rcts-to-regression",
    "href": "misc/rcts_to_regression.html#rcts-to-regression",
    "title": "RCTs to Regression",
    "section": "",
    "text": "Treatment indicator: Di∈0,1D_i\\in {0,1}\n\nexample: eligibility for expanded Medicaid\n\nOutcome: YiY_{i}\n\nexample: number of doctor visits in past 6 months\n\nPotential outcomes Yi0,Yi1Y_{i}^{0},Y_{i}^{1}\nIndividual-level treatment effect ΔiY=Yi1−Yi0\\Delta_i^Y=Y_i^1-Y_i^0 (can never know this).\nUnbiased estimate of average treatment effect:\nΔY=𝔼[Yi1−Yi0]\n\\Delta^{Y}=\\mathbb{E}\\left[Y_{i}^{1}-Y_{i}^{0}\\right]\n\nor OLS coefficient on 𝐷𝑖 from this regression:\nYi=α+δDi+ϵiY_{i}=\\alpha+\\delta D_{i}+\\epsilon_{i}\n\nurl_str  &lt;- \n  'https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/'\nfile_str &lt;- \n  'oregon_hie_table5.csv?raw=true'\n  \ndat &lt;- readr::read_csv(\n  paste0(url_str, file_str, show_col_types = FALSE)\n) %&gt;% \n  dplyr::select(doc_num, treatment, weight, dplyr::starts_with(\"ddd\")) %&gt;% \n  tidyr::drop_na()\n\n\nfit &lt;- lm(doc_num ~ treatment, data = dat, weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits: {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits: 0.268\n\n\n\n\n\nThe bivariate regression above leans heavily on random assignment of treatment:\nDi⟂⟂Yi0,Yi1D_{i}\\perp\\!\\!\\!\\!\\perp Y_{i}^{0},Y_{i}^{1}\nSometimes, even in an RCT, treatment is assigned randomly only conditional on some set of covariates XiX_i.\n\nexample: in the Oregon HIE, eligibility for Medicaid was granted via lottery, but households with more members could have more lottery entries. So the lottery outcome is random only conditional on household size.\n\nSo what happens if we don’t have random assignment? In terms of our regression model above, it means ϵi\\epsilon_i may be correlated with DiD_i. For example, perhaps household size, XiX_i, which increases the probability of treatment, is also associated with more doctor visits. If XiX_i is omitted from the model, it is buried in the error term:\nϵi=βXi+ηi\n\\epsilon_{i}=\\beta X_{i}+\\eta_{i}\n We’ll assume for now that everything else related to doctor visits (ηi\\eta_i) is unrelated to treatment. What does our bivariate regression coefficient deliver in this case?\nδ̂OLS=Cov(Yi,Di)Var(Di)=δ+βCov(Xi,Di)Var(Di)(1)\\hat{\\delta}^{\\text{OLS}}=\\frac{Cov\\left(Y_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}=\\delta+\\beta\\frac{Cov\\left(X_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}\n \\qquad(1)\nSimple regression gives us what we want (δ\\delta) plus an omitted variables bias term. The form of this term tells us what kinds of XiX_i variables we should take care to control for in our regressions.\nAccording to the ommitted variable bias (OVB) formula (Equation 1), what kinds of variables should you be be sure to control for in regressions?\nCareful investigators will find a set of regressors XiX_i for which they are willing to assume treatment is as good as randomly assigned:\nDi⟂⟂(Yi(0),Yi(1))|Xi.\nD_i\\perp\\!\\!\\!\\!\\perp\\left( Y_{i}\\left( 0\\right) ,Y_{i}\\left( 1\\right) \\right) |X_{i}\n\\text{.}\n This combined with a linear model for the conditional expectation of Yi0Y_{i}^0 and Yi1Y_{i}^1 given XiX_{i} means we can estimate the average treatment via OLS on the following regression equation:\nYi=δDi+Xi′β+εi.\nY_{i}=\\delta D_{i}+X_{i}^{\\prime }\\beta +\\varepsilon _{i}.\n\n\n# Add the household size indicators to our regressor set and run regression:\nfit &lt;- lm(doc_num ~ ., data = dat %&gt;% dplyr::select(-weight), weights = dat$weight)\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (with controls): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (with controls): 0.314\n\n\nHow did the estimate of the effect of Medicaid eligiblity change? What does that tell us about the relationship between the included regressors and the outcome and treatment?\n\n\n\nWhere does machine learning fit into this? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Don’t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}.\nModern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient δ\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate δ\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, Ỹi=Yi−ŶiOLS\\tilde{Y}_{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where ŶiOLS=Xi′(X′X)−1X′Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, D̃i=Di−D̂iOLS\\tilde{D}_{i}=D_{i}-\\hat{D}_{i}^{OLS}, where D̂iOLS=Xi′(X′X)−1X′D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress Ỹi\\tilde{Y}_{i} on D̃i\\tilde{D}_{i}.\n\nLet’s try it!\n\n# Regress outcome on covariates (not treatment)\nyreg &lt;- lm(\n  doc_num ~ .\n  , data = dat %&gt;% dplyr::select(doc_num, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\nytilde = yreg$residuals\n\n# regress treatment on covariates (not outcome)\ndreg &lt;- lm(\n  treatment ~ .\n  , data = dat %&gt;% dplyr::select(treatment, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\ndtilde = dreg$residuals\n\n# regress ytilde on dtilde\nfit &lt;- lm(\n  ytilde ~ dtilde\n  , data = tibble::tibble(ytilde = ytilde, dtilde = dtilde)\n  , weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'dtilde') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (partialled out): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (partialled out): 0.314\n\n\nML enters the picture by providing an alternate way to generate Ŷi\\hat{Y}_i and D̂i\\hat{D}_i when OLS is not the best tool for the job. The first two steps are really just prediction exercises, and in principle any supervised machine learning algorithm can step in here.\nBack to the whiteboard!"
  },
  {
    "objectID": "misc/in_process/Kalman.html",
    "href": "misc/in_process/Kalman.html",
    "title": "Kalman",
    "section": "",
    "text": "Sure, here is an example of how you might use the Kalman filter to predict customer churn in a banking context using R and the tidyverse package. Note that this is a simplified illustration and assumes you have a dataset with relevant variables.\n\nSample Data\nLet’s assume we have a dataset customer_data with the following columns: - customer_id: Unique identifier for each customer. - transaction_freq: Frequency of transactions in the current month. - avg_balance: Average account balance in the current month. - cust_service_calls: Number of customer service interactions in the current month. - churn: Whether the customer churned (1) or not (0).\n\n\nSample Code\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(KFAS)\n\n# Generate sample data\nset.seed(123)\ncustomer_data &lt;- tibble(\n  customer_id = 1:100,\n  transaction_freq = rnorm(100, mean = 10, sd = 2),\n  avg_balance = rnorm(100, mean = 5000, sd = 1000),\n  cust_service_calls = rpois(100, lambda = 2),\n  churn = sample(c(0, 1), 100, replace = TRUE)\n)\n\n# Define the state space model\n# For simplicity, we'll use a local level model for each state variable\n# Assume normal distribution for observations\n\n# Build a state space model\nbuild_model &lt;- function(data) {\n  ssmodel &lt;- SSModel(\n    H = matrix(NA, 3, 3),\n    Q = diag(NA, 3),\n    Z = diag(3),\n    T = diag(3),\n    R = diag(3),\n    a1 = c(mean(data$transaction_freq), mean(data$avg_balance), mean(data$cust_service_calls)),\n    P1 = diag(NA, 3)\n  )\n  \n  ssmodel$y &lt;- t(as.matrix(data %&gt;% select(transaction_freq, avg_balance, cust_service_calls)))\n  return(ssmodel)\n}\n\n# Fit the model\nfit_model &lt;- function(ssmodel) {\n  fit &lt;- fitSSM(ssmodel, inits = rep(0, 6), method = \"BFGS\")\n  return(fit$model)\n}\n\n# Kalman filter application\napply_kalman_filter &lt;- function(model) {\n  kf &lt;- KFS(model, filtering = \"mean\", smoothing = \"mean\")\n  return(kf)\n}\n\n# Predict churn probabilities based on filtered states\npredict_churn &lt;- function(kf, threshold = 0.5) {\n  smoothed_states &lt;- kf$a\n  churn_prob &lt;- 1 / (1 + exp(-rowMeans(smoothed_states))) # Logistic function for churn probability\n  churn_pred &lt;- ifelse(churn_prob &gt; threshold, 1, 0)\n  return(churn_pred)\n}\n\n# Build, fit, and apply the Kalman filter model\nssmodel &lt;- build_model(customer_data)\nfitted_model &lt;- fit_model(ssmodel)\nkf_results &lt;- apply_kalman_filter(fitted_model)\n\n# Predict churn\ncustomer_data &lt;- customer_data %&gt;%\n  mutate(predicted_churn = predict_churn(kf_results))\n\n# Display results\nprint(customer_data)\n\n# Evaluate the model\nconfusion_matrix &lt;- table(customer_data$churn, customer_data$predicted_churn)\nprint(confusion_matrix)\naccuracy &lt;- sum(diag(confusion_matrix)) / sum(confusion_matrix)\nprint(paste(\"Accuracy:\", accuracy))\n\n\nExplanation:\n\nData Generation: Simulate customer data for the example.\nState Space Model Definition: Define a simple state space model where each state variable is modeled independently.\nModel Fitting: Fit the state space model using the Kalman filter.\nFiltering and Smoothing: Apply the Kalman filter and smoother to estimate the hidden states.\nChurn Prediction: Use the smoothed states to predict churn probabilities, applying a logistic function.\nEvaluation: Compare the predicted churn with the actual churn to evaluate model performance.\n\nThis code provides a simplified illustration. In a real-world application, you would need to preprocess the data, handle missing values, and potentially use more complex state space models to capture the dynamics of customer behavior more accurately.\n\n\n\n\n\n\nNote\n\n\n\nPlease cite KFAS in publications by using: \n\nJouni Helske (2017). KFAS: Exponential Family State Space Models in R. Journal of Statistical Software, 78(10), 1-39. doi:10.18637/jss.v078.i10.\n\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(KFAS)\n\n# Generate sample data\nset.seed(123)\ncustomer_data &lt;- tibble(\n  customer_id = 1:100,\n  transaction_freq = rnorm(100, mean = 10, sd = 2),\n  avg_balance = rnorm(100, mean = 5000, sd = 1000),\n  cust_service_calls = rpois(100, lambda = 2),\n  churn = sample(c(0, 1), 100, replace = TRUE)\n)\n\ncustomer_data\n\n\n# Define the state space model\n# For simplicity, we'll use a local level model for each state variable\n# Assume normal distribution for observations\n\n# Build a state space model\n# build_model &lt;- function(data) {\n#   ssmodel &lt;- KFAS::SSModel(\n#     H = matrix(NA, 3, 3),\n#     Q = diag(NA, 3),\n#     Z = diag(3),\n#     T = diag(3),\n#     R = diag(3),\n#     a1 = c(mean(data$transaction_freq), mean(data$avg_balance), mean(data$cust_service_calls)),\n#     P1 = diag(NA, 3)\n#   )\n#   \n#   ssmodel$y &lt;- t(as.matrix(data %&gt;% select(transaction_freq, avg_balance, cust_service_calls)))\n#   return(ssmodel)\n# }\n\nbuild_model &lt;- function(data) {\n  # Define the state space model using a formula\n  model &lt;- SSModel(\n    transaction_freq + avg_balance + cust_service_calls ~ \n      SSMtrend(1, Q = list(NA)),  # Local level model for each variable\n    H = matrix(NA, 3, 3),  # Observation noise covariance matrix\n    data = data\n  )\n  \n  # Assign observed data to the model\n  model$y &lt;- t(as.matrix(data %&gt;% select(transaction_freq, avg_balance, cust_service_calls)))\n  \n  return(model)\n}\n\n# Fit the model\nfit_model &lt;- function(ssmodel) {\n  fit &lt;- fitSSM(ssmodel, inits = rep(0, 6), method = \"BFGS\")\n  return(fit$model)\n}\n\n# Kalman filter application\napply_kalman_filter &lt;- function(model) {\n  kf &lt;- KFS(model, filtering = \"mean\", smoothing = \"mean\")\n  return(kf)\n}\n\n# Predict churn probabilities based on filtered states\npredict_churn &lt;- function(kf, threshold = 0.5) {\n  smoothed_states &lt;- kf$a\n  churn_prob &lt;- 1 / (1 + exp(-rowMeans(smoothed_states))) # Logistic function for churn probability\n  churn_pred &lt;- ifelse(churn_prob &gt; threshold, 1, 0)\n  return(churn_pred)\n}\n\n\n# Build, fit, and apply the Kalman filter model\nssmodel &lt;- build_model(customer_data)\nfitted_model &lt;- fit_model(ssmodel)\nkf_results &lt;- apply_kalman_filter(fitted_model)\n\n# Predict churn\ncustomer_data &lt;- customer_data %&gt;%\n  mutate(predicted_churn = predict_churn(kf_results))"
  },
  {
    "objectID": "misc/causal_scratch.html",
    "href": "misc/causal_scratch.html",
    "title": "causal_scratch",
    "section": "",
    "text": "Treatment indicator: Di∈0,1D_i\\in {0,1}\n\nexample: eligibility for expanded Medicaid\n\nOutcome: YiY_{i}\n\nexample: number of doctor visits in past 6 months\n\nPotential outcomes Yi0,Yi1Y_{i}^{0},Y_{i}^{1}\nIndividual-level treatment effect ΔiY=Yi1−Yi0\\Delta_i^Y=Y_i^1-Y_i^0 (can never know this).\nUnbiased estimate of average treatment effect:\nΔY=𝔼[Yi1−Yi0]\n\\Delta^{Y}=\\mathbb{E}\\left[Y_{i}^{1}-Y_{i}^{0}\\right]\n\nor OLS coefficient on 𝐷𝑖 from this regression:\nYi=α+δDi+ϵiY_{i}=\\alpha+\\delta D_{i}+\\epsilon_{i}\n\nurl_str  &lt;- \n  'https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/'\nfile_str &lt;- \n  'oregon_hie_table5.csv?raw=true'\n  \ndat &lt;- readr::read_csv(\n  paste0(url_str, file_str, show_col_types = FALSE)\n) %&gt;% \n  dplyr::select(doc_num, treatment, weight, dplyr::starts_with(\"ddd\")) %&gt;% \n  tidyr::drop_na()\n\n\nfit &lt;- lm(doc_num ~ treatment, data = dat, weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits: {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits: 0.268\n\n\n\n\n\nThe bivariate regression above leans heavily on random assignment of treatment:\nDi⟂⟂Yi0,Yi1D_{i}\\perp\\!\\!\\!\\!\\perp Y_{i}^{0},Y_{i}^{1}\nSometimes, even in an RCT, treatment is assigned randomly only conditional on some set of covariates XiX_i.\n\nexample: in the Oregon HIE, eligibility for Medicaid was granted via lottery, but households with more members could have more lottery entries. So the lottery outcome is random only conditional on household size.\n\nSo what happens if we don’t have random assignment? In terms of our regression model above, it means ϵi\\epsilon_i may be correlated with DiD_i. For example, perhaps household size, XiX_i, which increases the probability of treatment, is also associated with more doctor visits. If XiX_i is omitted from the model, it is buried in the error term:\nϵi=βXi+ηi\n\\epsilon_{i}=\\beta X_{i}+\\eta_{i}\n We’ll assume for now that everything else related to doctor visits (ηi\\eta_i) is unrelated to treatment. What does our bivariate regression coefficient deliver in this case?\nδ̂OLS=Cov(Yi,Di)Var(Di)=δ+βCov(Xi,Di)Var(Di)(1)\\hat{\\delta}^{\\text{OLS}}=\\frac{Cov\\left(Y_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}=\\delta+\\beta\\frac{Cov\\left(X_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}\n \\qquad(1)\nSimple regression gives us what we want (δ\\delta) plus an omitted variables bias term. The form of this term tells us what kinds of XiX_i variables we should take care to control for in our regressions.\nAccording to the ommitted variable bias (OVB) formula (Equation 1), what kinds of variables should you be be sure to control for in regressions?\nCareful investigators will find a set of regressors XiX_i for which they are willing to assume treatment is as good as randomly assigned:\nDi⟂⟂(Yi(0),Yi(1))|Xi.\nD_i\\perp\\!\\!\\!\\!\\perp\\left( Y_{i}\\left( 0\\right) ,Y_{i}\\left( 1\\right) \\right) |X_{i}\n\\text{.}\n This combined with a linear model for the conditional expectation of Yi0Y_{i}^0 and Yi1Y_{i}^1 given XiX_{i} means we can estimate the average treatment via OLS on the following regression equation:\nYi=δDi+Xi′β+εi.\nY_{i}=\\delta D_{i}+X_{i}^{\\prime }\\beta +\\varepsilon _{i}.\n\n\n# Add the household size indicators to our regressor set and run regression:\nfit &lt;- lm(doc_num ~ ., data = dat %&gt;% dplyr::select(-weight), weights = dat$weight)\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (with controls): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (with controls): 0.314\n\n\nHow did the estimate of the effect of Medicaid eligiblity change? What does that tell us about the relationship between the included regressors and the outcome and treatment?\n\n\n\nWhere does machine learning fit into this? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Don’t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}.\nModern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient δ\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate δ\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, Ỹi=Yi−ŶiOLS\\tilde{Y}_{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where ŶiOLS=Xi′(X′X)−1X′Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, D̃i=Di−D̂iOLS\\tilde{D}_{i}=D_{i}-\\hat{D}_{i}^{OLS}, where D̂iOLS=Xi′(X′X)−1X′D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress Ỹi\\tilde{Y}_{i} on D̃i\\tilde{D}_{i}.\n\nLet’s try it!\n\n# Regress outcome on covariates (not treatment)\nyreg &lt;- lm(\n  doc_num ~ .\n  , data = dat %&gt;% dplyr::select(doc_num, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\nytilde = yreg$residuals\n\n# regress treatment on covariates (not outcome)\ndreg &lt;- lm(\n  treatment ~ .\n  , data = dat %&gt;% dplyr::select(treatment, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\ndtilde = dreg$residuals\n\n# regress ytilde on dtilde\nfit &lt;- lm(\n  ytilde ~ dtilde\n  , data = tibble::tibble(ytilde = ytilde, dtilde = dtilde)\n  , weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'dtilde') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (partialled out): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (partialled out): 0.314\n\n\nML enters the picture by providing an alternate way to generate Ŷi\\hat{Y}_i and D̂i\\hat{D}_i when OLS is not the best tool for the job. The first two steps are really just prediction exercises, and in principle any supervised machine learning algorithm can step in here.\nBack to the whiteboard!"
  },
  {
    "objectID": "misc/causal_scratch.html#rcts-to-regression",
    "href": "misc/causal_scratch.html#rcts-to-regression",
    "title": "causal_scratch",
    "section": "",
    "text": "Treatment indicator: Di∈0,1D_i\\in {0,1}\n\nexample: eligibility for expanded Medicaid\n\nOutcome: YiY_{i}\n\nexample: number of doctor visits in past 6 months\n\nPotential outcomes Yi0,Yi1Y_{i}^{0},Y_{i}^{1}\nIndividual-level treatment effect ΔiY=Yi1−Yi0\\Delta_i^Y=Y_i^1-Y_i^0 (can never know this).\nUnbiased estimate of average treatment effect:\nΔY=𝔼[Yi1−Yi0]\n\\Delta^{Y}=\\mathbb{E}\\left[Y_{i}^{1}-Y_{i}^{0}\\right]\n\nor OLS coefficient on 𝐷𝑖 from this regression:\nYi=α+δDi+ϵiY_{i}=\\alpha+\\delta D_{i}+\\epsilon_{i}\n\nurl_str  &lt;- \n  'https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/'\nfile_str &lt;- \n  'oregon_hie_table5.csv?raw=true'\n  \ndat &lt;- readr::read_csv(\n  paste0(url_str, file_str, show_col_types = FALSE)\n) %&gt;% \n  dplyr::select(doc_num, treatment, weight, dplyr::starts_with(\"ddd\")) %&gt;% \n  tidyr::drop_na()\n\n\nfit &lt;- lm(doc_num ~ treatment, data = dat, weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits: {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits: 0.268\n\n\n\n\n\nThe bivariate regression above leans heavily on random assignment of treatment:\nDi⟂⟂Yi0,Yi1D_{i}\\perp\\!\\!\\!\\!\\perp Y_{i}^{0},Y_{i}^{1}\nSometimes, even in an RCT, treatment is assigned randomly only conditional on some set of covariates XiX_i.\n\nexample: in the Oregon HIE, eligibility for Medicaid was granted via lottery, but households with more members could have more lottery entries. So the lottery outcome is random only conditional on household size.\n\nSo what happens if we don’t have random assignment? In terms of our regression model above, it means ϵi\\epsilon_i may be correlated with DiD_i. For example, perhaps household size, XiX_i, which increases the probability of treatment, is also associated with more doctor visits. If XiX_i is omitted from the model, it is buried in the error term:\nϵi=βXi+ηi\n\\epsilon_{i}=\\beta X_{i}+\\eta_{i}\n We’ll assume for now that everything else related to doctor visits (ηi\\eta_i) is unrelated to treatment. What does our bivariate regression coefficient deliver in this case?\nδ̂OLS=Cov(Yi,Di)Var(Di)=δ+βCov(Xi,Di)Var(Di)(1)\\hat{\\delta}^{\\text{OLS}}=\\frac{Cov\\left(Y_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}=\\delta+\\beta\\frac{Cov\\left(X_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}\n \\qquad(1)\nSimple regression gives us what we want (δ\\delta) plus an omitted variables bias term. The form of this term tells us what kinds of XiX_i variables we should take care to control for in our regressions.\nAccording to the ommitted variable bias (OVB) formula (Equation 1), what kinds of variables should you be be sure to control for in regressions?\nCareful investigators will find a set of regressors XiX_i for which they are willing to assume treatment is as good as randomly assigned:\nDi⟂⟂(Yi(0),Yi(1))|Xi.\nD_i\\perp\\!\\!\\!\\!\\perp\\left( Y_{i}\\left( 0\\right) ,Y_{i}\\left( 1\\right) \\right) |X_{i}\n\\text{.}\n This combined with a linear model for the conditional expectation of Yi0Y_{i}^0 and Yi1Y_{i}^1 given XiX_{i} means we can estimate the average treatment via OLS on the following regression equation:\nYi=δDi+Xi′β+εi.\nY_{i}=\\delta D_{i}+X_{i}^{\\prime }\\beta +\\varepsilon _{i}.\n\n\n# Add the household size indicators to our regressor set and run regression:\nfit &lt;- lm(doc_num ~ ., data = dat %&gt;% dplyr::select(-weight), weights = dat$weight)\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (with controls): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (with controls): 0.314\n\n\nHow did the estimate of the effect of Medicaid eligiblity change? What does that tell us about the relationship between the included regressors and the outcome and treatment?\n\n\n\nWhere does machine learning fit into this? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Don’t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}.\nModern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient δ\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate δ\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, Ỹi=Yi−ŶiOLS\\tilde{Y}_{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where ŶiOLS=Xi′(X′X)−1X′Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, D̃i=Di−D̂iOLS\\tilde{D}_{i}=D_{i}-\\hat{D}_{i}^{OLS}, where D̂iOLS=Xi′(X′X)−1X′D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress Ỹi\\tilde{Y}_{i} on D̃i\\tilde{D}_{i}.\n\nLet’s try it!\n\n# Regress outcome on covariates (not treatment)\nyreg &lt;- lm(\n  doc_num ~ .\n  , data = dat %&gt;% dplyr::select(doc_num, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\nytilde = yreg$residuals\n\n# regress treatment on covariates (not outcome)\ndreg &lt;- lm(\n  treatment ~ .\n  , data = dat %&gt;% dplyr::select(treatment, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\ndtilde = dreg$residuals\n\n# regress ytilde on dtilde\nfit &lt;- lm(\n  ytilde ~ dtilde\n  , data = tibble::tibble(ytilde = ytilde, dtilde = dtilde)\n  , weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'dtilde') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (partialled out): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (partialled out): 0.314\n\n\nML enters the picture by providing an alternate way to generate Ŷi\\hat{Y}_i and D̂i\\hat{D}_i when OLS is not the best tool for the job. The first two steps are really just prediction exercises, and in principle any supervised machine learning algorithm can step in here.\nBack to the whiteboard!"
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html",
    "href": "labs/BSMM_8740_lab_9.html",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice sampling from distribuions and working with Markov chains."
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html#introduction",
    "href": "labs/BSMM_8740_lab_9.html#introduction",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice sampling from distribuions and working with Markov chains."
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html#getting-started",
    "href": "labs/BSMM_8740_lab_9.html#getting-started",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-9-[your github username] repository .\nCreate an R project using your 2024-lab-9-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-9.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html#packages",
    "href": "labs/BSMM_8740_lab_9.html#packages",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html#exercise-1-markov-chains",
    "href": "labs/BSMM_8740_lab_9.html#exercise-1-markov-chains",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "Exercise 1: Markov Chains",
    "text": "Exercise 1: Markov Chains\nHere is a four-state Markov chain that could model customer loyalty for a subscription-based service, with one month between steps in the chain.\nStates:\n\nState A (New Customer): The customer has just signed up.\nState B (Engaged Customer): The customer is actively using the service and seems satisfied.\nState C (At-Risk Customer): The customer is showing signs of disengagement (e.g., reduced usage or negative feedback).\nState D (Churned Customer): The customer has canceled their subscription.\n\nTransition Probabilities:\n\nFrom State A (New Customer), there’s a high chance the customer either becomes engaged (State B) or starts showing signs of disengagement (State C).\nFrom State B (Engaged Customer), there’s a probability of remaining engaged or transitioning to at-risk (State C), and a smaller probability of churning (State D).\nFrom State C (At-Risk Customer), the customer may either re-engage (return to State B) or churn (State D).\nFrom State D (Churned Customer), it’s possible the company might re-acquire the customer through marketing efforts, which would move them back to State A.\n\nThis type of Markov model can help businesses predict customer behavior, optimize marketing efforts, and focus on retention strategies.\nWhat is the probability that a customer that has just signed up is still a customer after 6 months?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# the transition matrix is\nP &lt;- \n  matrix(\n    c(0, 0.6, 0.4, 0,\n      0, 0.75, 0.25, 0,\n      0, 0.5, 0, 0.5,\n      0.3, 0, 0, 0.7\n      )\n    , nrow =4, byrow = TRUE\n  )\n\n# use %^% from the expm package to compute the k-th power of a matrix (k = 6 months)\n\n# sum the probabilities of the non-churned customer states after 6 steps\n\nThe probability that a customer that has just signed up is still a customer after 6 months is ___%"
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html#exercise-2-markov-chains",
    "href": "labs/BSMM_8740_lab_9.html#exercise-2-markov-chains",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "Exercise 2: Markov Chains",
    "text": "Exercise 2: Markov Chains\nA simpler customer churn model for each monthly period is as follows:\n\na current subscriber cancels their subscription with probability 0.2\na current non-subscriber starts their subscription with probability with probability 0.06\n\nwrite the state transition matrix 𝖯i,j\\mathsf{P}_{i,j}, and compute the stationary distribution π\\pi for this Markov Chain, confirming that π𝖯=π\\pi\\mathsf{P}=\\pi and that the sum of the elements of π\\pi equals 1.01.0.\nWhat percent of customers remain once the chain has reached the steady state?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# replace the placeholders '_' with the state transition probabilities\nP &lt;- \n  matrix(\n    c(_, _,\n      _, _\n      )\n    , nrow =2, byrow = TRUE\n  )\n\n\n# compute transpose(I-P) and add a row of 1's to the bottom \n# call the resulting matrix A\nA &lt;- \n\n# create a vector called b with the # of elements equal to the number of rows of A\n# with elements all zero but the last one\nb &lt;- \n  \n# compute pi by solving (A x pi) = b using qr.solve\npi &lt;- \n\n# confirm (pi x P) = pi and \n  \n# confirm pi[1] + pi[2] == 1\n\nIn the steady state, the probability of being a current customer is ___%"
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html#exercise-3-acceptance-probability",
    "href": "labs/BSMM_8740_lab_9.html#exercise-3-acceptance-probability",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "Exercise 3: Acceptance probability",
    "text": "Exercise 3: Acceptance probability\nWe want to sample from the Poisson distribution ℙ(X=x)∼λxe−λ/x!\\mathbb{P}(X=x)\\sim \\lambda^xe^{-\\lambda}/x! using a Metropolis Hastings algorithm.\nFor the proposal we toss a fair coin and add or subtract 1 from xx to obtain yy as follows:\nq(y|x)={12x≥1,y=x±11x=0,y=10otherwise\nq(y|x)=\\begin{cases}\n\\frac{1}{2} & x\\ge1,\\,y=x\\pm1\\\\\n1 & x=0,\\,y=1\\\\\n0 & \\mathrm{otherwise}\n\\end{cases}\n show that the acceptance probability is\nα(y|x)={min(1,λx+1)x≥1,y=x+1min(1,xλ)x≥2,y=x−1\n\\alpha(y|x)=\\begin{cases}\n\\min\\left(1,\\frac{\\lambda}{x+1}\\right) & x\\ge1,\\,y=x+1\\\\\n\\min\\left(1,\\frac{x}{\\lambda}\\right) & x\\ge2,\\,y=x-1\n\\end{cases}\n\nand α(1|0)=min(1,λ/2)\\alpha(1|0)=\\min(1,\\lambda/2), α(0|1)=min(1,2/λ)\\alpha(0|1)=\\min(1,2/\\lambda)\n\n\n\n\n\n\nYOUR ANSWER:"
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html#exercise-4-samples-from-poisson-pmf",
    "href": "labs/BSMM_8740_lab_9.html#exercise-4-samples-from-poisson-pmf",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "Exercise 4: Samples from Poisson pmf",
    "text": "Exercise 4: Samples from Poisson pmf\nGiven the following function for the acceptance probability\n\nalpha &lt;- function(y,x, lambda){\n  if(x &gt;= 1 & y == x+1){\n    min(1,lambda/(x+1))\n  }else if(x &gt;= 2 & y == x-1){\n    min(1,x/lambda)\n  }else if(x == 0 & y == 1){\n   min(1,lambda/2)\n  }else{\n    min(1,2/lambda)\n  }\n}\n\n\nWrite a MH algorithm to draw 2000 samples from a from a Poisson pmf with λ=20\\lambda = 20 starting from x0=1x_0=1.\nCompare the sample quantiles at probabilities c(0.1,.25,0.5, 0.75, 0.9) with the theoretical quantiles for the Poisson distribution (using the qpois function)\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# MH algorithm drawing samples from a Poisson(20) pmf\n\n\n# comparison of quantiles between the samples and the theoretical."
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html#exercise-5-a-loan-portfolio",
    "href": "labs/BSMM_8740_lab_9.html#exercise-5-a-loan-portfolio",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "Exercise 5: A Loan Portfolio",
    "text": "Exercise 5: A Loan Portfolio\nOur client is a bank with both asset and liability products in retail bank industry. Most of the bank’s assets are loans, and these loans generate the majority of the total revenue earned by the bank. Hence, it is essential for the bank to understand the proportion of loans that have a high propensity to be paid in full and those which will finally become Bad loans.\nAll the loans that have been issued by the bank are classified into one of four categories/states :\n\nGood Loans : These are the loans which are in progress but are given to low risk customers. We expect most of these loans will be paid up in full with time.\nRisky loans : These are also the loans which are in progress but are given to medium or high risk customers. We expect a good number of these customers will default.\nBad loans : The customer to whom these loans were given have already defaulted.\nPaid up loans : These loans have already been paid in full.\n\nYour research has suggested the following state transition matrix for the bank loans\n\n# the 1-year state transition matrix for loans is:\nP &lt;- \n  matrix(\n    c(0.7, 0.05, 0.03, 0.22,\n      0.05, 0.55, 0.35, 0.05,\n      0, 0, 1, 0, \n      0, 0, 0, 1\n      )\n    , nrow =4, byrow = TRUE\n  )\n\nAnswer the following questions, given that the bank’s records indicate 60% of the loans on the books are ‘good loans’ and 40% are ‘risky loans’.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# describe the current loan portfolio by state at the end of one year and two years.  \n\n\n# describe the current loan portfolio by state at the end of two years.\n\n\n# What percentage of good loans are paid in full after 20 years\n\n___? percent of good loans are paid in full after 20 years\n\n# What percentage of good loans are paid in full after 20 years\n\n___? percent of risk loans are paid in full after 20 years\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 6!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyone’s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_9.html#grading",
    "href": "labs/BSMM_8740_lab_9.html#grading",
    "title": "lab 9 - Monte Carlo Methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 5\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html",
    "title": "Lab 3 - Regression",
    "section": "",
    "text": "In today’s lab, you’ll explore several data sets and practice building and evaluating regression models.\n\n\nBy the end of the lab you will…\n\nBe able to use different regression models to predict a response/target/outcome as a function of a set of variates."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#introduction",
    "title": "Lab 3 - Regression",
    "section": "",
    "text": "In today’s lab, you’ll explore several data sets and practice building and evaluating regression models.\n\n\nBy the end of the lab you will…\n\nBe able to use different regression models to predict a response/target/outcome as a function of a set of variates."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#packages",
    "title": "Lab 3 - Regression",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in today’s lab.\n\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, knitr, ISLR2, stats, xgboost, see\n)\ntheme_set(theme_bw(base_size = 12))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#data-boston-house-values",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#data-boston-house-values",
    "title": "Lab 3 - Regression",
    "section": "Data: Boston House Values",
    "text": "Data: Boston House Values\nThe Boston House Values dataset (usually referred to as the Boston dataset) appears in several R packages in different versions and is based on economic studies published in the late 1970’s.\nThis dataset contains the following information for each cocktail:\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ncrim\nper capita crime rate by town.\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus\nproportion of non-retail business acres per town.\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox\nnitrogen oxides concentration (parts per 10 million).\n\n\nrm\naverage number of rooms per dwelling.\n\n\nage\nproportion of owner-occupied units built prior to 1940.\n\n\ndis\nweighted mean of distances to five Boston employment centres.\n\n\nrad\nindex of accessibility to radial highways.\n\n\ntax\nfull-value property-tax rate per $10,000.\n\n\nptratio\npupil-teacher ratio by town.\n\n\nlstat\nlower status of the population (percent).\n\n\nmedv\nmedian value of owner-occupied homes in $1000s.\n\n\n\nUse the code below to load the Boston Cocktail Recipes data set.\n\nboston &lt;- ISLR2::Boston"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#exercises",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#exercises",
    "title": "Lab 3 - Regression",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nPlot the median value of owner-occupied homes (medv) vs the percentage of houses with lower socioeconomic status (lstat) then use lm to model medv ~ lstat and save the result in a variable for use later.\nNext prepare a summary of the model. What is the intercept and the coefficient of lstat in this model?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nboston %&gt;% \n  ggplot(aes(x=lstat, y = medv)) + geom_point()\n\n\n\n\n\n\n\n\nlm_medv_lstat &lt;- lm(medv ~ lstat, data = boston)\nsummary(lm_medv_lstat)\n# alternatively:\nlm_medv_lstat %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::select(1:2)\n\n\n\n\nCall:\nlm(formula = medv ~ lstat, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n# A tibble: 2 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)   34.6  \n2 lstat         -0.950\n\n\n\n\n\n\n\nExercise 2\nUsing the result from Exercise 1, and the data below, use the predict function (stats::predict.lm or just predict) with the argument interval = “confidence” to prepare a summary table with columns lstat, fit, lwr, upr.\n\ntibble(lstat = c(5, 10, 15, 20))\n\nFinally, use your model to plot some performance checks using the performance::check_model function with arguments check=c(\"linearity\",\"qq\",\"homogeneity\", \"outliers\").\nAre there any overly influential observations in this dataset?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ntibble::tibble(lstat = c(5, 10, 15, 20)) %&gt;% \n  # create a nested column\n  dplyr::mutate(\n    results =\n      purrr::map(\n        lstat                         # the data is in column lstat\n        , ~stats::predict.lm(         # the function is based on predict,\n          lm_medv_lstat               # with first argument from the fit \n          , tibble::tibble(lstat = .) # and newdata argument from lstat column \n          , interval = \"confidence\"   # and setting interval argument\n          ) %&gt;% \n          # predict returns a vector; make it a tibble,\n          # with columns fit, lwr, upr\n          # where the last two set lower and upper confidence intervals \n          tibble::as_tibble()\n      )\n  ) %&gt;% \n  tidyr::unnest(results)\n\n# A tibble: 4 × 4\n  lstat   fit   lwr   upr\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     5  29.8  29.0  30.6\n2    10  25.1  24.5  25.6\n3    15  20.3  19.7  20.9\n4    20  15.6  14.8  16.3\n\n\n\n# Alternatively, and more directly\ntibble::tibble(lstat = c(5, 10, 15, 20)) %&gt;% \n  dplyr::bind_cols(\n    stats::predict.lm(         \n          lm_medv_lstat                \n          , tibble::tibble(lstat = c(5, 10, 15, 20))\n          , interval = \"confidence\"   \n          )\n  )\n\n# A tibble: 4 × 4\n  lstat   fit   lwr   upr\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     5  29.8  29.0  30.6\n2    10  25.1  24.5  25.6\n3    15  20.3  19.7  20.9\n4    20  15.6  14.8  16.3\n\n\n\n# Or most directly\ntibble::tibble(lstat = c(5, 10, 15, 20)) %&gt;% \n  broom::augment(\n    lm_medv_lstat, newdata = ., interval = \"confidence\"\n  )\n\n# A tibble: 4 × 4\n  lstat .fitted .lower .upper\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     5    29.8   29.0   30.6\n2    10    25.1   24.5   25.6\n3    15    20.3   19.7   20.9\n4    20    15.6   14.8   16.3\n\n\nCheck the help for stats::predict.lm, dplyr::bind_cols, and broom::augment to more detail on each method.\nFinally\n\nlm_medv_lstat %&gt;% \n  performance::check_model(\n    check = \n      c(\n        \"linearity\"      # linear fit\n        , \"qq\"           # Normal residuals\n        , \"homogeneity\"  # constant variance \n        , \"outliers\"     # any influential observations?\n      ) \n  )\n\n\n\n\n\n\n\n\nSince no points lie outside the dashed lines of the Influential Observations chart, there are no influential observations.\n\n\n\n\nExercise 3\nFit medv to all predictors in the dataset and use the performance::check_collinearity function on the resulting model to check if any predictors are redundant.\nThe variance inflation factor is a measure of the magnitude of multicollinearity of model terms. A VIF less than 5 indicates a low correlation of that predictor with other predictors. A value between 5 and 10 indicates a moderate correlation, while VIF values larger than 10 are a sign for high, not tolerable correlation of model predictors.\nWhich predictors in this dataset might be redundant for predicting medv?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# fit the model\nlm_medv_all &lt;- lm(medv ~ ., data = boston)\n# check for collinearity\nperformance::check_collinearity(lm_medv_all)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n    Term  VIF    VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n    crim 1.77 [1.58,  2.01]         1.33      0.57     [0.50, 0.63]\n      zn 2.30 [2.03,  2.64]         1.52      0.44     [0.38, 0.49]\n   indus 3.99 [3.45,  4.64]         2.00      0.25     [0.22, 0.29]\n    chas 1.07 [1.02,  1.28]         1.03      0.93     [0.78, 0.98]\n     nox 4.37 [3.77,  5.09]         2.09      0.23     [0.20, 0.26]\n      rm 1.91 [1.70,  2.19]         1.38      0.52     [0.46, 0.59]\n     age 3.09 [2.69,  3.57]         1.76      0.32     [0.28, 0.37]\n     dis 3.95 [3.42,  4.60]         1.99      0.25     [0.22, 0.29]\n ptratio 1.80 [1.61,  2.05]         1.34      0.56     [0.49, 0.62]\n   lstat 2.87 [2.51,  3.32]         1.69      0.35     [0.30, 0.40]\n\nModerate Correlation\n\n Term  VIF    VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n  rad 7.45 [6.37,  8.73]         2.73      0.13     [0.11, 0.16]\n  tax 9.00 [7.69, 10.58]         3.00      0.11     [0.09, 0.13]\n\n\nThe variance inflation factor (VIF) is moderate (between 5-10) for rad and tax, suggesting these may be redundant fo predicting the outcome (medv).\n\n\n\n\nExercise 4\nIn this exercise you will compare and interpret the results of linear regression on two similar datasets.\nThe first dataset (dat0 - generated below) has demand0 and price0 variables along with an unobserved variable (unobserved0 - so not in our dataset) that doesn’t change the values of demand0 and price0. Use lm to build a model to predict demand0 from price0 . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset dat0?\n\nN &lt;- 500\nset.seed(1966)\n\ndat0 &lt;- tibble::tibble(\n  price0 = 10+rnorm(500)\n  , demand0 = 30-(price0 + rnorm(500))\n  , unobserved0 = 0.45*price0 + 0.77*demand0 + rnorm(500)\n)\n\nThe second dataset (dat1 - generated below) has demand1 and price1 variables, along with a variable unobserved1 that is completely random and is not observed, so it isn’t in our dataset. Use lm to build a model to predict demand1 from price1 . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset dat1?\n\nset.seed(1966)\n\ndat1 &lt;- tibble::tibble(\n  unobserved1 = rnorm(500)\n  , price1 = 10 + unobserved1 + rnorm(500)\n  , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))\n)\n\nWhich linear model returns the (approximately) correct dependence of demand on price, as given in the data generation process?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# DAT0\n# fit the model\nfit0 &lt;- lm(demand0 ~ price0, data = dat0)\n# get the estimated coefficients for dataset 0\nest0  &lt;- fit0 %&gt;% broom::tidy()\n# plot the data\ndat0 %&gt;% ggplot(aes(x=price0,y=demand0)) +\n  geom_point() + \n  geom_abline(\n    data = est0 %&gt;% \n      dplyr::select(1:2) %&gt;% \n      tidyr::pivot_wider(names_from = term, values_from =estimate)\n    , aes(intercept = `(Intercept)`, slope = price0)\n    , colour = \"red\"\n  )\n\n\n\n\n\n\n\n\n\n#DAT1\n# fit the model\nfit1 &lt;- lm(demand1 ~ price1, data = dat1)\n# get the estimated coefficients for dataset 0\nest1  &lt;- fit1 %&gt;% broom::tidy()\n# plot the data\ndat1 %&gt;% ggplot(aes(x=price1,y=demand1)) +\n  geom_point() + \n  geom_abline(\n    data = est1 %&gt;% dplyr::select(1:2) %&gt;% tidyr::pivot_wider(names_from = term, values_from =estimate)\n    , aes(intercept = `(Intercept)`, slope = price1)\n    , colour = \"red\"\n  )\n\n\n\n\n\n\n\n\nThe linear fit to the two datasets (without the unobserved variables, because they are, well, unobserved) gives very similar intercepts and slopes (dependence on price).\nIf you look at the plots the fits look good, and this is consistent with the estimates.\n### dataset 0\nest0 %&gt;% dplyr::select(1:2) %&gt;% tibble::add_column(data = 'dat0')\n### dataset 1\nest1 %&gt;% dplyr::select(1:2) %&gt;% tibble::add_column(data = 'dat1')\n\n\n\n# A tibble: 2 × 3\n  term        estimate data \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;\n1 (Intercept)    30.5  dat0 \n2 price0         -1.04 dat0 \n\n\n# A tibble: 2 × 3\n  term        estimate data \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;\n1 (Intercept)   27.8   dat1 \n2 price1        -0.989 dat1 \n\n\n\nHowever, now go back and look at the way the datasets were generated.\nIn dat0, the coefficient of price in the equation for demand is -1, consistent with the linear fit.\nBut in dat1, the coefficient of price in the equation for demand is -0.5, yet the linear fit estimates the value as close to 1.\nSo here the model doesn’t give the answers that the knowledge of the data generation process would lead us to expect. What if we can observe the unobservables?\n\n\n\n\nExercise 5\nNow repeat the modeling of exercise 4, but assuming that the formerly unobservable variables are now observable, and so can be included in the linear regression models.\nWhich model returns the (approximately) correct dependence of demand on price, as given in the data generation process?\nWhat can you conclude from these two exercises?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# fit the model with dat0\nlm(demand0 ~ price0 + unobserved0, data = dat0) %&gt;% \n  # pull out the coefficient estimates as a tibble\n  broom::tidy() %&gt;% \n  # combine with another table\n  dplyr::bind_rows(\n    # fit the model with dat1\n    lm(demand1 ~ price1 + unobserved1, data = dat1) %&gt;% \n      broom::tidy()\n  )\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   18.9      0.710      26.6  1.33e- 97\n2 price0        -0.878    0.0351    -25.0  9.06e- 90\n3 unobserved0    0.497    0.0267     18.6  4.29e- 59\n4 (Intercept)   22.4      0.443      50.5  1.10e-197\n5 price1        -0.443    0.0444     -9.98 1.74e- 21\n6 unobserved1   -1.08     0.0638    -17.0  3.21e- 51\n\n\nWhen we include the formerly unobserved variables we find that\n\nagain the coefficients of price are similar between the models, but\nnow the coefficient of price1 is correct (should be -0.5) while the coefficient of price0 is not (should be -1.0)\n\nThe conclusion is that whether a covariate is included or not requires thinking about the data generation process. We generally only have the observations, but not the exact process that produced them, but we can hypothesize the process and test the conclusions.\nCould adding more data improve our estimates? It depends.\nIn dataset dat1, the unobserved variable affects the values of both price1 and demand1, which adds a correlation to the relationship between price1 and demand1 that is in addition to their direct relationship (with price1 coefficient -0.5). Because of this, the unobserved variable needs to be included in the regression, to control for it and remove the bias due the extra correlation.\nBy contrast, in dataset dat0, the unobserved variable depends on the values of both price0 and demand0, and there is no correlation when this variable is not included in the regression, which estimates the price0 coefficient correctly as -1.0. But if the unobserved variable is included in the regression, because a regression estimate is ‘with all other variables held constant,’ this induces a new correlation between price0 and demand0, exactly because they both affect the unobserved variable - with a fixed value for the unobserved variable, price0 and demand0 are no longer independent. This additional correlation creates the bias in our estimate of the coefficient of price0.\n\n\n\n\nExercise 6\nFor the next several exercises, we’ll work with a new dataset. This dataset is taken from an EPA site on fuel economy, in particular the fuel economy dataset for 2023.\nUse the code below to load the FE Guide data set.\n\ndat &lt;- \n  readxl::read_xlsx( \"data/2023 FE Guide for DOE-release dates before 7-28-2023.xlsx\")\n\nFrom the raw data in dat, we’ll make a smaller dataset, and we’ll need to do some cleaning to make it useable.\nFirst select the columns “Comb FE (Guide) - Conventional Fuel”, “Eng Displ”,‘# Cyl’, Transmission , “# Gears”, “Air Aspiration Method Desc”, “Regen Braking Type Desc”, “Batt Energy Capacity (Amp-hrs)” , “Drive Desc”, “Fuel Usage Desc - Conventional Fuel”, “Cyl Deact?”, and “Var Valve Lift?” and then clean the column names using janitor::janitor::clean_names(). Assign the revised data to the variable cars_23.\nPerform a quick check of the data using DataExplorer::introduce() and DataExplorer::plot_missing() and modify the data as follows\n\nmutate the columns comb_fe_guide_conventional_fuel, number_cyl, and number_gears to ensure that they contain integers values, not doubles.\nuse tidyr::replace_na to replace any missing values in batt_energy_capacity_amp_hrs column with zeros, and replace and missing values in regen_braking_type_desc with empty strings (““).\nfinally, mutate the columns ‘transmission’,‘air_aspiration_method_desc’,‘regen_braking_type_desc’,‘drive_desc’ ,‘fuel_usage_desc_conventional_fuel’,‘cyl_deact’,‘var_valve_lift’ so their values are factors.\n\nPrepare a recipe to pre-process cars_23 ahead of modelling, using comb_fe_guide_conventional_fuel as the outcome, with the following steps.\n\nCentering for: recipes::all_numeric()\nScaling for: recipes::all_numeric()\nDummy variables from: recipes::all_factor()\n\nHow many predictor variables are there in cars_23 ?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# CLEANING\n# select the columns\ncars_23 &lt;- dat %&gt;% dplyr::select(\n    \"Comb FE (Guide) - Conventional Fuel\",\n    \"Eng Displ\",'# Cyl',Transmission\n    ,\"# Gears\",\"Air Aspiration Method Desc\"\n    ,\"Regen Braking Type Desc\",\"Batt Energy Capacity (Amp-hrs)\"\n    ,\"Drive Desc\",\"Fuel Usage Desc - Conventional Fuel\"\n    ,\"Cyl Deact?\", \"Var Valve Lift?\"\n  ) %&gt;% \n  # clean the names\n  janitor::clean_names()\n\n# Explore the data\ncars_23 %&gt;% DataExplorer::introduce()\ncars_23 %&gt;% DataExplorer::plot_missing()\n\n\n\n# A tibble: 1 × 9\n   rows columns discrete_columns continuous_columns all_missing_columns\n  &lt;int&gt;   &lt;int&gt;            &lt;int&gt;              &lt;int&gt;               &lt;int&gt;\n1  1119      12                7                  5                   0\n# ℹ 4 more variables: total_missing_values &lt;int&gt;, complete_rows &lt;int&gt;,\n#   total_observations &lt;int&gt;, memory_usage &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n# convert integer values\ncars_23 %&lt;&gt;% \n  dplyr::mutate( \n    dplyr::across(\n      .cols = all_of(\n        c('comb_fe_guide_conventional_fuel',\"number_cyl\",\"number_gears\"))\n      , .fns = as.integer\n    ) \n  ) \n# replace na values\ncars_23 %&lt;&gt;% \n  tidyr::replace_na(\n    list(\n      batt_energy_capacity_amp_hrs = 0\n      , regen_braking_type_desc = \"\"\n    )\n  ) \n\n\n# convert to factors\ncars_23 %&lt;&gt;% \n  dplyr::mutate( \n    dplyr::across(\n      .cols = all_of(\n        c( 'transmission','air_aspiration_method_desc'\n           ,'regen_braking_type_desc','drive_desc'\n           ,'fuel_usage_desc_conventional_fuel'\n           ,'cyl_deact','var_valve_lift' )\n      )\n      , .fns = as.factor\n    ) \n  ) \n\n\n# create a recipe for this data\ncars_23_rec &lt;- cars_23 %&gt;% \n  recipes::recipe(comb_fe_guide_conventional_fuel~.) %&gt;% \n  recipes::step_center(recipes::all_numeric()) %&gt;%\n  recipes::step_scale(recipes::all_numeric()) %&gt;% \n  recipes::step_dummy(recipes::all_factor())\n\nsummary(cars_23_rec)\n\n# A tibble: 12 × 4\n   variable                          type      role      source  \n   &lt;chr&gt;                             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 eng_displ                         &lt;chr [2]&gt; predictor original\n 2 number_cyl                        &lt;chr [2]&gt; predictor original\n 3 transmission                      &lt;chr [3]&gt; predictor original\n 4 number_gears                      &lt;chr [2]&gt; predictor original\n 5 air_aspiration_method_desc        &lt;chr [3]&gt; predictor original\n 6 regen_braking_type_desc           &lt;chr [3]&gt; predictor original\n 7 batt_energy_capacity_amp_hrs      &lt;chr [2]&gt; predictor original\n 8 drive_desc                        &lt;chr [3]&gt; predictor original\n 9 fuel_usage_desc_conventional_fuel &lt;chr [3]&gt; predictor original\n10 cyl_deact                         &lt;chr [3]&gt; predictor original\n11 var_valve_lift                    &lt;chr [3]&gt; predictor original\n12 comb_fe_guide_conventional_fuel   &lt;chr [2]&gt; outcome   original\n\n\nThere are 11 predictors at this stage in cars_23.\n\n\n\n\nExercise 7\nFor this exercise, set a sample size equal to 75% of the observations of cars_23 and split the data as follows:\n\nset.seed(1966)\n\n# sample 75% of the rows of the cars_23 dataset to make the training set\ntrain &lt;- cars_23 %&gt;% \n  # make an ID column for use as a key\n  tibble::rowid_to_column(\"ID\") %&gt;% \n  # sample the rows\n  dplyr::sample_frac(0.75)\n\n# remove the training dataset from the original dataset to make the training set\ntest  &lt;- \n  dplyr::anti_join(\n    cars_23 %&gt;% tibble::rowid_to_column(\"ID\") # add a key column to the original data\n    , train\n    , by = 'ID'\n  )\n\n# drop the ID column from training and test datasets\ntrain %&lt;&gt;% dplyr::select(-ID); test %&lt;&gt;% dplyr::select(-ID)\n\nNext prep the recipe created in the last exercise using recipes::prep on the training data, and then use the result of the prep step to recipes::bake with the training and test data. Save the baked data in separate variables for use later.\nAfter these two steps how many columns are in the data? Why does this differ from the last step?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# prep the recipe with the training data\ncars_23_prep &lt;- cars_23_rec %&gt;% \n  recipes::prep(\n    training = train    # NOTE: training data\n    , verbose = FALSE\n    , retain = TRUE\n  )\n\ncars_23_train &lt;- cars_23_prep %&gt;% \n  # create a training dataset by baking the training data\n  recipes::bake(new_data=NULL)\n  \ncars_23_test  &lt;- cars_23_prep %&gt;% \n  # create a test dataset by baking the test data\n  recipes::bake(new_data=test)\n\n# columns in the data\ncars_23_train %&gt;% dim()\n# predictors in the recipe\nsummary(cars_23_prep)\n\n\n\n[1] 839  45\n\n\n# A tibble: 45 × 4\n   variable                        type      role      source  \n   &lt;chr&gt;                           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 eng_displ                       &lt;chr [2]&gt; predictor original\n 2 number_cyl                      &lt;chr [2]&gt; predictor original\n 3 number_gears                    &lt;chr [2]&gt; predictor original\n 4 batt_energy_capacity_amp_hrs    &lt;chr [2]&gt; predictor original\n 5 comb_fe_guide_conventional_fuel &lt;chr [2]&gt; outcome   original\n 6 transmission_Auto.A6.           &lt;chr [2]&gt; predictor derived \n 7 transmission_Auto.A8.           &lt;chr [2]&gt; predictor derived \n 8 transmission_Auto.A9.           &lt;chr [2]&gt; predictor derived \n 9 transmission_Auto.AM.S6.        &lt;chr [2]&gt; predictor derived \n10 transmission_Auto.AM.S7.        &lt;chr [2]&gt; predictor derived \n# ℹ 35 more rows\n\n\n\nThere are now 45 columns in the data and 45 predictors in the recipe. This is because the recipe converted the factors to dummy variables.\n\n\n\n\nExercise 8\nIn this exercise we will run xgboost::xgboost to evaluate the regression.\nFirst run fit the model with default meta-parameters for max_depth and eta, using the training data per the code below:\n\nuntuned_xgb &lt;-\n  xgboost::xgboost(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    max_depth = 6,\n    eta = .25\n    , verbose = FALSE\n  )\n\nNext use the fitted model to predict the outcome using the test data:\n\n# create predictions using the test data and the fitted model\nyhat &lt;- predict(\n  untuned_xgb\n  , cars_23_test %&gt;% \n    dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% \n    as.matrix() \n)\n\nFinally, pull out the comb_fe_guide_conventional_fuel column from the test data, assign it to the variable y and then use caret::postResample with arguments yhat and y to evaluate how well the model fits.\nWhat is the RMSE for the un-tuned model?\n\n\n\n\n\n\nSOLUTION:\n\n\n\nThe RMSE is approximately 0.245.\n\n# select the observations in the test data\ny &lt;- cars_23_test %&gt;% \n    dplyr::pull(comb_fe_guide_conventional_fuel) \n\n# compare observations and predictions\ncaret::postResample(yhat, y)\n\n     RMSE  Rsquared       MAE \n0.2446266 0.9379850 0.1729323 \n\n\n\n\n\n\nExercise 9\nIn this exercise we are going to tune the model using cross validation. First we create a tuning grid for the parameters and then fit the model for all the values in the grid, saving the results.\nFinally, we select the best parameters by least RMSE.\n\n#create hyperparameter grid\nhyper_grid &lt;- expand.grid(max_depth = seq(3, 6, 1), eta = seq(.2, .35, .01))  \n\n# initialize our metric variables\nxgb_train_rmse &lt;- NULL\nxgb_test_rmse  &lt;- NULL\n\nfor (j in 1:nrow(hyper_grid)) {\n  set.seed(123)\n  m_xgb_untuned &lt;- xgboost::xgb.cv(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    nfold = 5,\n    max_depth = hyper_grid$max_depth[j],\n    eta = hyper_grid$eta[j],\n    verbose = FALSE\n  )\n  \n  xgb_train_rmse[j] &lt;- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]\n  xgb_test_rmse[j] &lt;- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]\n}    \n\nbest &lt;- hyper_grid[which(xgb_test_rmse == min(xgb_test_rmse)),]; best # there may be ties\n\n   max_depth  eta\n50         4 0.32\n\n\nre-run the code from the last exercise and evaluate the fit using the best tuning parameters.\nIs the tuned model better than the un-tuned model? If better, how much has the RMSE improved (in %).\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ntuned_xgb &lt;-\n  xgboost::xgboost(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    max_depth = best[1,1],        # this is the best-fit max_depth\n    eta = best[1,2],              # this is the best-fit eta\n    verbose = FALSE\n  ) \n\nNow the RMSE is 0.2425948/0.2446266 - about 1% better than the untuned model\n\nyhat &lt;- predict(\n  tuned_xgb\n  , cars_23_test %&gt;% \n    dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% \n    as.matrix() \n)\n\ny &lt;- cars_23_test %&gt;% \n    dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% \n    as.matrix() \ncaret::postResample(yhat, y)\n\n     RMSE  Rsquared       MAE \n0.2425948 0.9387286 0.1709782 \n\n\n\n\n\n\nExercise 10\nUsing xgboost::xgb.importance rank the importance of each predictor in the model. Finally, take the top 10 predictors by importance and plot them using xgboost::xgb.plot.importance.\nPer this model, what is the most important feature for predicting fuel efficiency?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create the importance matrix\nimportance_matrix &lt;- xgboost::xgb.importance(model = tuned_xgb)\n\n# plot the importance measures\nxgboost::xgb.plot.importance(importance_matrix[1:10,], xlab = \"Feature Importance\")\n\n\n\n\n\n\n\n\nPer the model, engine displacement is the most important feature for predicting fuel efficiency."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#grading",
    "title": "Lab 3 - Regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html",
    "title": "Lab 4 - The Models package",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison.\n\n\nBy the end of the lab you will…\n\nBe able to build workflows to evaluate different models and featuresets."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#introduction",
    "title": "Lab 4 - The Models package",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison.\n\n\nBy the end of the lab you will…\n\nBe able to build workflows to evaluate different models and featuresets."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#packages",
    "title": "Lab 4 - The Models package",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(tidyverse, magrittr, gt, gtExtras, tidymodels, ggplot2)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#the-data",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#the-data",
    "title": "Lab 4 - The Models package",
    "section": "The Data",
    "text": "The Data\nToday we will be using the Ames Housing Data.\nThis is a data set from De Cock (2011) has 82 fields were recorded for 2,930 properties in Ames Iowa in the US. The version in the modeldata package is copied from the AmesHousing package but does not include a few quality columns that appear to be outcomes rather than predictors.\n\ndat &lt;- modeldata::ames\n\nThe data dictionary can be found on the internet:\n\ncat(readr::read_file(\"http://jse.amstat.org/v19n3/decock/DataDocumentation.txt\"))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-1-eda",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-1-eda",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nWrite and execute the code to perform summary EDA on the Ames Housing data using the package skimr.\n\n\n\n\n\n\nSOLUTION:\n\n\n\ndat |&gt; skimr::skim()\n\n\n\n\nData summary\n\n\nName\ndat\n\n\nNumber of rows\n2930\n\n\nNumber of columns\n74\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n40\n\n\nnumeric\n34\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nMS_SubClass\n0\n1\nFALSE\n16\nOne: 1079, Two: 575, One: 287, One: 192\n\n\nMS_Zoning\n0\n1\nFALSE\n7\nRes: 2273, Res: 462, Flo: 139, Res: 27\n\n\nStreet\n0\n1\nFALSE\n2\nPav: 2918, Grv: 12\n\n\nAlley\n0\n1\nFALSE\n3\nNo_: 2732, Gra: 120, Pav: 78\n\n\nLot_Shape\n0\n1\nFALSE\n4\nReg: 1859, Sli: 979, Mod: 76, Irr: 16\n\n\nLand_Contour\n0\n1\nFALSE\n4\nLvl: 2633, HLS: 120, Bnk: 117, Low: 60\n\n\nUtilities\n0\n1\nFALSE\n3\nAll: 2927, NoS: 2, NoS: 1\n\n\nLot_Config\n0\n1\nFALSE\n5\nIns: 2140, Cor: 511, Cul: 180, FR2: 85\n\n\nLand_Slope\n0\n1\nFALSE\n3\nGtl: 2789, Mod: 125, Sev: 16\n\n\nNeighborhood\n0\n1\nFALSE\n28\nNor: 443, Col: 267, Old: 239, Edw: 194\n\n\nCondition_1\n0\n1\nFALSE\n9\nNor: 2522, Fee: 164, Art: 92, RRA: 50\n\n\nCondition_2\n0\n1\nFALSE\n8\nNor: 2900, Fee: 13, Art: 5, Pos: 4\n\n\nBldg_Type\n0\n1\nFALSE\n5\nOne: 2425, Twn: 233, Dup: 109, Twn: 101\n\n\nHouse_Style\n0\n1\nFALSE\n8\nOne: 1481, Two: 873, One: 314, SLv: 128\n\n\nOverall_Cond\n0\n1\nFALSE\n9\nAve: 1654, Abo: 533, Goo: 390, Ver: 144\n\n\nRoof_Style\n0\n1\nFALSE\n6\nGab: 2321, Hip: 551, Gam: 22, Fla: 20\n\n\nRoof_Matl\n0\n1\nFALSE\n8\nCom: 2887, Tar: 23, WdS: 9, WdS: 7\n\n\nExterior_1st\n0\n1\nFALSE\n16\nVin: 1026, Met: 450, HdB: 442, Wd : 420\n\n\nExterior_2nd\n0\n1\nFALSE\n17\nVin: 1015, Met: 447, HdB: 406, Wd : 397\n\n\nMas_Vnr_Type\n0\n1\nFALSE\n5\nNon: 1775, Brk: 880, Sto: 249, Brk: 25\n\n\nExter_Cond\n0\n1\nFALSE\n5\nTyp: 2549, Goo: 299, Fai: 67, Exc: 12\n\n\nFoundation\n0\n1\nFALSE\n6\nPCo: 1310, CBl: 1244, Brk: 311, Sla: 49\n\n\nBsmt_Cond\n0\n1\nFALSE\n6\nTyp: 2616, Goo: 122, Fai: 104, No_: 80\n\n\nBsmt_Exposure\n0\n1\nFALSE\n5\nNo: 1906, Av: 418, Gd: 284, Mn: 239\n\n\nBsmtFin_Type_1\n0\n1\nFALSE\n7\nGLQ: 859, Unf: 851, ALQ: 429, Rec: 288\n\n\nBsmtFin_Type_2\n0\n1\nFALSE\n7\nUnf: 2499, Rec: 106, LwQ: 89, No_: 81\n\n\nHeating\n0\n1\nFALSE\n6\nGas: 2885, Gas: 27, Gra: 9, Wal: 6\n\n\nHeating_QC\n0\n1\nFALSE\n5\nExc: 1495, Typ: 864, Goo: 476, Fai: 92\n\n\nCentral_Air\n0\n1\nFALSE\n2\nY: 2734, N: 196\n\n\nElectrical\n0\n1\nFALSE\n6\nSBr: 2682, Fus: 188, Fus: 50, Fus: 8\n\n\nFunctional\n0\n1\nFALSE\n8\nTyp: 2728, Min: 70, Min: 65, Mod: 35\n\n\nGarage_Type\n0\n1\nFALSE\n7\nAtt: 1731, Det: 782, Bui: 186, No_: 157\n\n\nGarage_Finish\n0\n1\nFALSE\n4\nUnf: 1231, RFn: 812, Fin: 728, No_: 159\n\n\nGarage_Cond\n0\n1\nFALSE\n6\nTyp: 2665, No_: 159, Fai: 74, Goo: 15\n\n\nPaved_Drive\n0\n1\nFALSE\n3\nPav: 2652, Dir: 216, Par: 62\n\n\nPool_QC\n0\n1\nFALSE\n5\nNo_: 2917, Exc: 4, Goo: 4, Typ: 3\n\n\nFence\n0\n1\nFALSE\n5\nNo_: 2358, Min: 330, Goo: 118, Goo: 112\n\n\nMisc_Feature\n0\n1\nFALSE\n6\nNon: 2824, She: 95, Gar: 5, Oth: 4\n\n\nSale_Type\n0\n1\nFALSE\n10\nWD : 2536, New: 239, COD: 87, Con: 26\n\n\nSale_Condition\n0\n1\nFALSE\n6\nNor: 2413, Par: 245, Abn: 190, Fam: 46\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nLot_Frontage\n0\n1\n57.65\n33.50\n0.00\n43.00\n63.00\n78.00\n313.00\n▇▇▁▁▁\n\n\nLot_Area\n0\n1\n10147.92\n7880.02\n1300.00\n7440.25\n9436.50\n11555.25\n215245.00\n▇▁▁▁▁\n\n\nYear_Built\n0\n1\n1971.36\n30.25\n1872.00\n1954.00\n1973.00\n2001.00\n2010.00\n▁▂▃▆▇\n\n\nYear_Remod_Add\n0\n1\n1984.27\n20.86\n1950.00\n1965.00\n1993.00\n2004.00\n2010.00\n▅▂▂▃▇\n\n\nMas_Vnr_Area\n0\n1\n101.10\n178.63\n0.00\n0.00\n0.00\n162.75\n1600.00\n▇▁▁▁▁\n\n\nBsmtFin_SF_1\n0\n1\n4.18\n2.23\n0.00\n3.00\n3.00\n7.00\n7.00\n▃▂▇▁▇\n\n\nBsmtFin_SF_2\n0\n1\n49.71\n169.14\n0.00\n0.00\n0.00\n0.00\n1526.00\n▇▁▁▁▁\n\n\nBsmt_Unf_SF\n0\n1\n559.07\n439.54\n0.00\n219.00\n465.50\n801.75\n2336.00\n▇▅▂▁▁\n\n\nTotal_Bsmt_SF\n0\n1\n1051.26\n440.97\n0.00\n793.00\n990.00\n1301.50\n6110.00\n▇▃▁▁▁\n\n\nFirst_Flr_SF\n0\n1\n1159.56\n391.89\n334.00\n876.25\n1084.00\n1384.00\n5095.00\n▇▃▁▁▁\n\n\nSecond_Flr_SF\n0\n1\n335.46\n428.40\n0.00\n0.00\n0.00\n703.75\n2065.00\n▇▃▂▁▁\n\n\nGr_Liv_Area\n0\n1\n1499.69\n505.51\n334.00\n1126.00\n1442.00\n1742.75\n5642.00\n▇▇▁▁▁\n\n\nBsmt_Full_Bath\n0\n1\n0.43\n0.52\n0.00\n0.00\n0.00\n1.00\n3.00\n▇▆▁▁▁\n\n\nBsmt_Half_Bath\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\n\n\nFull_Bath\n0\n1\n1.57\n0.55\n0.00\n1.00\n2.00\n2.00\n4.00\n▁▇▇▁▁\n\n\nHalf_Bath\n0\n1\n0.38\n0.50\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▁\n\n\nBedroom_AbvGr\n0\n1\n2.85\n0.83\n0.00\n2.00\n3.00\n3.00\n8.00\n▁▇▂▁▁\n\n\nKitchen_AbvGr\n0\n1\n1.04\n0.21\n0.00\n1.00\n1.00\n1.00\n3.00\n▁▇▁▁▁\n\n\nTotRms_AbvGrd\n0\n1\n6.44\n1.57\n2.00\n5.00\n6.00\n7.00\n15.00\n▁▇▂▁▁\n\n\nFireplaces\n0\n1\n0.60\n0.65\n0.00\n0.00\n1.00\n1.00\n4.00\n▇▇▁▁▁\n\n\nGarage_Cars\n0\n1\n1.77\n0.76\n0.00\n1.00\n2.00\n2.00\n5.00\n▅▇▂▁▁\n\n\nGarage_Area\n0\n1\n472.66\n215.19\n0.00\n320.00\n480.00\n576.00\n1488.00\n▃▇▃▁▁\n\n\nWood_Deck_SF\n0\n1\n93.75\n126.36\n0.00\n0.00\n0.00\n168.00\n1424.00\n▇▁▁▁▁\n\n\nOpen_Porch_SF\n0\n1\n47.53\n67.48\n0.00\n0.00\n27.00\n70.00\n742.00\n▇▁▁▁▁\n\n\nEnclosed_Porch\n0\n1\n23.01\n64.14\n0.00\n0.00\n0.00\n0.00\n1012.00\n▇▁▁▁▁\n\n\nThree_season_porch\n0\n1\n2.59\n25.14\n0.00\n0.00\n0.00\n0.00\n508.00\n▇▁▁▁▁\n\n\nScreen_Porch\n0\n1\n16.00\n56.09\n0.00\n0.00\n0.00\n0.00\n576.00\n▇▁▁▁▁\n\n\nPool_Area\n0\n1\n2.24\n35.60\n0.00\n0.00\n0.00\n0.00\n800.00\n▇▁▁▁▁\n\n\nMisc_Val\n0\n1\n50.64\n566.34\n0.00\n0.00\n0.00\n0.00\n17000.00\n▇▁▁▁▁\n\n\nMo_Sold\n0\n1\n6.22\n2.71\n1.00\n4.00\n6.00\n8.00\n12.00\n▅▆▇▃▃\n\n\nYear_Sold\n0\n1\n2007.79\n1.32\n2006.00\n2007.00\n2008.00\n2009.00\n2010.00\n▇▇▇▇▃\n\n\nSale_Price\n0\n1\n180796.06\n79886.69\n12789.00\n129500.00\n160000.00\n213500.00\n755000.00\n▇▇▁▁▁\n\n\nLongitude\n0\n1\n-93.64\n0.03\n-93.69\n-93.66\n-93.64\n-93.62\n-93.58\n▅▅▇▆▁\n\n\nLatitude\n0\n1\n42.03\n0.02\n41.99\n42.02\n42.03\n42.05\n42.06\n▂▂▇▇▇"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-2-train-test-splits",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-2-train-test-splits",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 2: Train / Test Splits",
    "text": "Exercise 2: Train / Test Splits\nWrite and execute code to create training and test datasets. Have the training dataset represent 75% of the total data.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nset.seed(8740)\n# split the data, with 75% in the training set\ndata_split &lt;- rsample::initial_split(dat, strata = \"Sale_Price\", prop = 0.75)\n\n# extract the training set\names_train &lt;- rsample::training(data_split)\n# extract the text set\names_test  &lt;- rsample::testing(data_split)"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-3-data-preprocessing",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-3-data-preprocessing",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 3: Data Preprocessing",
    "text": "Exercise 3: Data Preprocessing\ncreate a recipe based on the formula Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold with the following steps:\n\ntransform the outcome variable Sale_Price to log(Sale_Price) (natural log)\ncenter and scale all numeric predictors\ntransform the categorical variable Neighborhood to pool infrequent values (see recipes::step_other)\ncreate dummy variables for all nominal predictors\n\nFinally prep the recipe.\nMake sure you consider the order of the operations (hint: step_dummy turns factors into multiply integer (numeric) predictor, so consider when to scale numeric predictors relative to creating dummy predictors.\nYou can use broom::tidy() on the recipe to examine whether the prepped data is correct.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nnorm_recipe &lt;- \n  # create a recipe with the specified formula and data\n  recipes::recipe(\n    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, \n    data = ames_train\n  ) |&gt;\n  # center all predictors\n  recipes::step_center(all_numeric_predictors()) |&gt;\n  # scales all predictors\n  recipes::step_scale(all_numeric_predictors()) |&gt;\n  # transformm the outcome using log-base-e (natural log)\n  recipes::step_log(Sale_Price, base = exp(1)) |&gt; \n  # pool categories with few members into a new category - 'other'\n  recipes::step_other(Neighborhood) |&gt; \n  # create dummy variables for all categories\n  recipes::step_dummy(all_nominal_predictors()) \n\n\nnorm_recipe |&gt; \n  # estimate the means and standard deviations by prepping the data\n  recipes::prep(training = ames_train, retain = TRUE) |&gt; broom::tidy()\n\n# A tibble: 5 × 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      center TRUE    FALSE center_8P2vV\n2      2 step      scale  TRUE    FALSE scale_5uUEH \n3      3 step      log    TRUE    FALSE log_Y9hV0   \n4      4 step      other  TRUE    FALSE other_JBWlv \n5      5 step      dummy  TRUE    FALSE dummy_yvcFM"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-4-modeling",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-4-modeling",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 4 Modeling",
    "text": "Exercise 4 Modeling\nCreate three regression models using the parsnip:: package and assign each model to its own variable\n\na base regression model using lm\na regression model using glmnet; set the model parameters penalty and mixture for tuning\na tree model using the ranger engine; set the model parameters min_n and trees for tuning\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# linear model: engine lm, mode regression\nlm_mod_base &lt;- parsnip::linear_reg() |&gt;\n  parsnip::set_engine(\"lm\")  |&gt; \n  parsnip::set_mode(\"regression\")\n\n# tuned linear model: engine glmnet, mode regression; tuning penalty and mixture\nlm_mod_glmnet &lt;- \n  parsnip::linear_reg( penalty = parsnip::tune(), mixture = parsnip::tune() ) |&gt; \n  parsnip::set_engine(\"glmnet\") |&gt; \n  parsnip::set_mode(\"regression\")\n\n# random forest model: engine lm, mode regression\nlm_mod_rforest &lt;- \n  parsnip::rand_forest( min_n = parsnip::tune(), trees = parsnip::tune() ) |&gt; \n  parsnip::set_engine(\"ranger\") |&gt; \n  parsnip::set_mode(\"regression\")\n\n\nlm_mod_base\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nlm_mod_glmnet\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = parsnip::tune()\n  mixture = parsnip::tune()\n\nComputational engine: glmnet \n\n\n\nlm_mod_rforest\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = parsnip::tune()\n  min_n = parsnip::tune()\n\nComputational engine: ranger"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-5",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-5",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse parsnip::translate() on each model to see the model template for each method of fitting.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# lm model\nlm_mod_base |&gt; parsnip::translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\n\n# glmnet model\nlm_mod_glmnet |&gt; parsnip::translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = parsnip::tune()\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    alpha = parsnip::tune(), family = \"gaussian\")\n\n\n\n# rforest model\nlm_mod_rforest |&gt; parsnip::translate()\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = parsnip::tune()\n  min_n = parsnip::tune()\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    num.trees = parsnip::tune(), min.node.size = min_rows(~parsnip::tune(), \n        x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5, \n        1))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-6-bootstrap",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-6-bootstrap",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 6 Bootstrap",
    "text": "Exercise 6 Bootstrap\nCreate bootstrap samples for the training dataset. You can leave the parameters set to their defaults\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nset.seed(8740)\ntrain_resamples &lt;- rsample::bootstraps(ames_train)"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-7",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-7",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 7",
    "text": "Exercise 7\nCreate workflows with workflowsets::workflow_set using your recipe and models. Show the resulting datastructure, noting the number of columns, and then use tidyr:: to unnest the info column and show its contents.\n\n\n\n\n\n\nSOLUTION:\n\n\n\nall_workflows &lt;- \n  workflowsets::workflow_set(\n    preproc = list(base = norm_recipe),\n    models = list(base = lm_mod_base, glmnet = lm_mod_glmnet, forest = lm_mod_rforest)\n  )\n\nall_workflows # four columns\nall_workflows |&gt; tidyr::unnest(info)\n\n\n\n# A workflow set/tibble: 3 × 4\n  wflow_id    info             option    result    \n  &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 base_base   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_glmnet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 base_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\n\n\n# A tibble: 3 × 7\n  wflow_id    workflow   preproc model       comment option    result    \n  &lt;chr&gt;       &lt;list&gt;     &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;   &lt;list&gt;    &lt;list&gt;    \n1 base_base   &lt;workflow&gt; recipe  linear_reg  \"\"      &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_glmnet &lt;workflow&gt; recipe  linear_reg  \"\"      &lt;opts[0]&gt; &lt;list [0]&gt;\n3 base_forest &lt;workflow&gt; recipe  rand_forest \"\"      &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-8",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-8",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 8",
    "text": "Exercise 8\nUse workflowsets::workflow_map to map the default function (tune::tune_grid() - look at the help for workflowsets::workflow_map ) across the workflows in the workflowset you just created and update the variable all_workflows with the result.\n\nall_workflows &lt;- all_workflows |&gt; \n  workflowsets::workflow_map(\n    verbose = TRUE                # enable logging\n    , resamples = train_resamples # a parameter passed to tune::tune_grid()\n    , grid = 5                    # a parameter passed to tune::tune_grid()\n  )\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 1 of 3 resampling: base_base\n\n\n✔ 1 of 3 resampling: base_base (792ms)\n\n\ni 2 of 3 tuning:     base_glmnet\n\n\n✔ 2 of 3 tuning:     base_glmnet (2.4s)\n\n\ni 3 of 3 tuning:     base_forest\n\n\n✔ 3 of 3 tuning:     base_forest (1m 21.7s)\n\n\nThe updated variable all_workflows contains a nested column named result, and each cell of the column result is a tibble containing a nested column named .metrics. Write code to\n\nun-nest the metrics in the column .metrics\nfilter out the rows for the metric rsq\ngroup by wflow_id, order the .estimate column from highest to lowest, and pick out the first row of each group.\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nall_workflows |&gt; \n  # unnest the result column\n  dplyr::select(wflow_id,result) |&gt; \n  tidyr::unnest(result) |&gt; \n  # unnest the .metrics column\n  tidyr::unnest(.metrics) |&gt; \n  # filter out the metric rsq\n  dplyr::filter(.metric == 'rsq') |&gt; \n  # group by wflow_id\n  dplyr::group_by(wflow_id) |&gt; \n  # order by .estimate, starting with the largest value\n  dplyr::arrange(desc(.estimate) ) |&gt; \n  # select the first row for each group (i.e. highest rsq)\n  dplyr::slice(1)\n\n# A tibble: 3 × 12\n# Groups:   wflow_id [3]\n  wflow_id    splits             id         .metric .estimator .estimate .config\n  &lt;chr&gt;       &lt;list&gt;             &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;  \n1 base_base   &lt;split [2197/798]&gt; Bootstrap… rsq     standard       0.448 Prepro…\n2 base_forest &lt;split [2197/797]&gt; Bootstrap… rsq     standard       0.712 Prepro…\n3 base_glmnet &lt;split [2197/798]&gt; Bootstrap… rsq     standard       0.448 Prepro…\n# ℹ 5 more variables: penalty &lt;dbl&gt;, mixture &lt;dbl&gt;, trees &lt;int&gt;, min_n &lt;int&gt;,\n#   .notes &lt;list&gt;"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-9",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-9",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 9",
    "text": "Exercise 9\nRun the code below and compare to your results from exercise 8.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nworkflowsets::rank_results(\n  all_workflows\n  , rank_metric = 'rsq'\n  , select_best = TRUE\n)\n\n# A tibble: 6 × 9\n  wflow_id    .config       .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 base_forest Preprocessor… rmse    0.240 0.00255    25 recipe       rand…     1\n2 base_forest Preprocessor… rsq     0.657 0.00471    25 recipe       rand…     1\n3 base_base   Preprocessor… rmse    0.321 0.00366    25 recipe       line…     2\n4 base_base   Preprocessor… rsq     0.376 0.00836    25 recipe       line…     2\n5 base_glmnet Preprocessor… rmse    0.321 0.00364    25 recipe       line…     3\n6 base_glmnet Preprocessor… rsq     0.375 0.00836    25 recipe       line…     3"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-10",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-10",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 10",
    "text": "Exercise 10\nSelect the best model per the rsq metric using its id.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# extract the best model (per rsq)\nbest_model_workflow &lt;- \n  all_workflows |&gt; \n  workflowsets::extract_workflow(\"base_forest\")\n\n\n# finalize the workflow\nbest_model_workflow &lt;- \n  best_model_workflow |&gt; \n  tune::finalize_workflow(\n    tibble::tibble(trees = 1971, min_n = 2) # enter the name and value of the best-fit parameters\n  ) \n\n# having trained the model, compare test and training performance\ntraining_fit &lt;- best_model_workflow |&gt; \n  fit(data = ames_train)\ntraining_fit\ntesting_fit &lt;- best_model_workflow |&gt; \n  fit(data = ames_test)\ntesting_fit\n\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_center()\n• step_scale()\n• step_log()\n• step_other()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1971,      min.node.size = min_rows(~2, x), num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1971 \nSample size:                      2197 \nNumber of independent variables:  12 \nMtry:                             3 \nTarget node size:                 2 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.05686626 \nR squared (OOB):                  0.6558831 \n\n\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_center()\n• step_scale()\n• step_log()\n• step_other()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1971,      min.node.size = min_rows(~2, x), num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1971 \nSample size:                      733 \nNumber of independent variables:  12 \nMtry:                             3 \nTarget node size:                 2 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.06798341 \nR squared (OOB):                  0.5976383 \n\n\n\n\n\n\n\n\n\nWhat is the ratio of the OOB prediction errors (MSE): test/train?\nThe ratio is 0.06821716 / 0.05747611 - 1 = 0.1868785, or about 19% higher in the test dataset than in the training dataset."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#grading",
    "title": "Lab 4 - The Models package",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_9_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_9_solutions.html",
    "title": "Lab 9 - Monte Carlo Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice sampling from distributions and working with Markov chains."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_9_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_9_solutions.html#introduction",
    "title": "Lab 9 - Monte Carlo Methods",
    "section": "",
    "text": "In today’s lab, you’ll practice sampling from distributions and working with Markov chains."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_9_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_9_solutions.html#packages",
    "title": "Lab 9 - Monte Carlo Methods",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(expm, ggplot2)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-1-markov-chains",
    "href": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-1-markov-chains",
    "title": "Lab 9 - Monte Carlo Methods",
    "section": "Exercise 1: Markov Chains",
    "text": "Exercise 1: Markov Chains\nHere is a four-state Markov chain that could model customer loyalty for a subscription-based service, with one month between steps in the chain.\nStates:\n\nState A (New Customer): The customer has just signed up.\nState B (Engaged Customer): The customer is actively using the service and seems satisfied.\nState C (At-Risk Customer): The customer is showing signs of disengagement (e.g., reduced usage or negative feedback).\nState D (Churned Customer): The customer has canceled their subscription.\n\nTransition Probabilities:\n\nFrom State A (New Customer), there’s a high chance the customer either becomes engaged (State B) or starts showing signs of disengagement (State C).\nFrom State B (Engaged Customer), there’s a probability of remaining engaged or transitioning to at-risk (State C), and a smaller probability of churning (State D).\nFrom State C (At-Risk Customer), the customer may either re-engage (return to State B) or churn (State D).\nFrom State D (Churned Customer), it’s possible the company might re-acquire the customer through marketing efforts, which would move them back to State A.\n\nThis type of Markov model can help businesses predict customer behavior, optimize marketing efforts, and focus on retention strategies.\nWhat is the probability that a customer that has just signed up is still a customer after 6 months?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# the transition matrix is\nP &lt;- \n  matrix(\n    c(0, 0.6, 0.4, 0,\n      0, 0.75, 0.25, 0,\n      0, 0.5, 0, 0.5,\n      0.3, 0, 0, 0.7\n      )\n    , nrow =4, byrow = TRUE\n  )\n\n# use the %^% operator from the expm package to compute the k-th power of a matrix (k = 6 months)\n\nP6 &lt;- P %^% 6\n\n# sum the probabilities of the non-churned customer states after 6 steps\n\n(c(1,0,0,0) %*% P6) %*% c(1,1,1,0)\n\n          [,1]\n[1,] 0.7485331\n\n\nThe probability that a customer that has just signed up is still a customer after 6 months is about 75%"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-2-markov-chains",
    "href": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-2-markov-chains",
    "title": "Lab 9 - Monte Carlo Methods",
    "section": "Exercise 2: Markov Chains",
    "text": "Exercise 2: Markov Chains\nA simpler customer churn model for each monthly period is as follows:\n\na current subscriber cancels their subscription with probability 0.2\na current non-subscriber starts their subscription with probability with probability 0.06\n\nwrite the state transition matrix 𝖯i,j\\mathsf{P}_{i,j}, and compute the stationary distribution π\\pi for this Markov Chain, confirming that π𝖯=π\\pi\\mathsf{P}=\\pi and that the sum of the elements of π\\pi equals 1.01.0.\nWhat percent of customers remain once the chain has reached the steady state?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# the state transition matrix is:\nP &lt;- \n  matrix(\n    c(0.8, 0.2,\n      0.06, 0.94\n      )\n    , nrow =2, byrow = TRUE\n  )\n\n\n# compute transpose(I-P) and add a row of 1's to the bottom \n# call the resulting matrix A\nA &lt;- t( diag(2) - P ) |&gt; rbind(c(1,1))\n\n# create a vector called b with the # of elements equal to the number of rows of A\n# with elements all zero but the last one\nb &lt;- c(0,0,1)\n\n# compute pi by solving (pi x A) = b using qr.solve\npi &lt;- qr.solve(A,b)\n\n# confirm (pi x A) = pi\npi %*% P - pi\n\n             [,1]          [,2]\n[1,] 8.326673e-17 -1.110223e-16\n\nsum(pi)\n\n[1] 1\n\n\nIn the steady state, the probability of being a current customer is pi[1] = 23%"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-3-acceptance-probability",
    "href": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-3-acceptance-probability",
    "title": "Lab 9 - Monte Carlo Methods",
    "section": "Exercise 3: Acceptance probability",
    "text": "Exercise 3: Acceptance probability\nWe want to sample from the Poisson distribution ℙ(X=x)∼λxe−λ/x!\\mathbb{P}(X=x)\\sim \\lambda^xe^{-\\lambda}/x! using a Metropolis Hastings algorithm.\nFor the proposal we toss a fair coin and add or subtract 1 from xx to obtain yy as follows:\nq(y|x)={12x≥1,y=x±11x=0,y=10otherwise\nq(y|x)=\\begin{cases}\n\\frac{1}{2} & x\\ge1,\\,y=x\\pm1\\\\\n1 & x=0,\\,y=1\\\\\n0 & \\mathrm{otherwise}\n\\end{cases}\n show that the acceptance probability is\nα(y|x)={min(1,λx+1)x≥1,y=x+1min(1,xλ)x≥2,y=x−1\n\\alpha(y|x)=\\begin{cases}\n\\min\\left(1,\\frac{\\lambda}{x+1}\\right) & x\\ge1,\\,y=x+1\\\\\n\\min\\left(1,\\frac{x}{\\lambda}\\right) & x\\ge2,\\,y=x-1\n\\end{cases}\n\nand α(1|0)=min(1,λ/2)\\alpha(1|0)=\\min(1,\\lambda/2), α(0|1)=min(1,2/λ)\\alpha(0|1)=\\min(1,2/\\lambda)\n\n\n\n\n\n\nSOLUTION:\n\n\n\nFor the MH algorithm, the acceptance probability is α(Xn+1|Xn)=min(1,π(Xn+1)π(Xn)q(Xn+1,Xn)q(Xn,Xn+1))\\alpha(X_{n+1}\\vert X_n)=\\min\\left(1,\\frac{\\pi(X_{n+1})}{\\pi(X_n)}\\frac{q(X_{n+1},X_n)}{q(X_n,X_{n+1})}\\right)\ncase 1: for x≥1,y=x+1x\\ge1, y=x+1, then\nα(x+1|x)=λx+1e−λ/(x+1)!λxe−λ/(x)!1/21/2=λx+1\n\\alpha(x+1\\vert x)=\\frac{\\lambda^{x+1}e^{-\\lambda}/(x+1)!}{\\lambda^{x}e^{-\\lambda}/(x)!}\\frac{1/2}{1/2}=\\frac{\\lambda}{x+1}\n\ncase 2: for x≥2,y=x−1x\\ge2, y=x-1, then\nα(x−1|x)=λx−1e−λ/(x−1)!λxe−λ/(x)!1/21/2=xλ\n\\alpha(x-1\\vert x)=\\frac{\\lambda^{x-1}e^{-\\lambda}/(x-1)!}{\\lambda^{x}e^{-\\lambda}/(x)!}\\frac{1/2}{1/2}=\\frac{x}{\\lambda}\n\ncase 3: for x=0,y=1x=0, y=1, then\nα(1|0)=λ1e−λ/(1)!λ0e−λ/(0)!1/21=λ2\n\\alpha(1\\vert 0)=\\frac{\\lambda^{1}e^{-\\lambda}/(1)!}{\\lambda^{0}e^{-\\lambda}/(0)!}\\frac{1/2}{1}=\\frac{\\lambda}{2}\n\ncase 4: for x=1,y=0x=1, y=0, then\nα(0|1)=λ0e−λ/(0)!λ1e−λ/(1)!11/2=2λ\n\\alpha(0\\vert 1)=\\frac{\\lambda^{0}e^{-\\lambda}/(0)!}{\\lambda^{1}e^{-\\lambda}/(1)!}\\frac{1}{1/2}=\\frac{2}{\\lambda}"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-4-samples-from-poisson-pmf",
    "href": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-4-samples-from-poisson-pmf",
    "title": "Lab 9 - Monte Carlo Methods",
    "section": "Exercise 4: Samples from Poisson pmf",
    "text": "Exercise 4: Samples from Poisson pmf\nGiven the following function for the acceptance probability\n\nalpha &lt;- function(y,x, lambda){\n  if(x &gt;= 1 & y == x+1){\n    min(1,lambda/(x+1))\n  }else if(x &gt;= 2 & y == x-1){\n    min(1,x/lambda)\n  }else if(x == 0 & y == 1){\n   min(1,lambda/2)\n  }else{\n    min(1,2/lambda)\n  }\n}\n\nq &lt;- function(y,x){\n  if(x &gt;= 1 & y %in% c(x-1,x+1) ){\n    0.5\n  }else if(x == 0 & y == 1){\n    1\n  }else{\n    0\n  }\n}\n\n\nWrite a MH algorithm to draw 2000 samples from a from a Poisson pmf with λ=20\\lambda = 20 starting from x0=1x_0=1.\nCompare the sample quantiles at probabilities c(0.1,.25,0.5, 0.75, 0.9) with the theoretical quantiles for the Poisson distribution (using the qpois function)\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# MH algorithm drawing 2000 samples from a Poisson(20) pmf\nset.seed(8740)\n# initialize\nn = 2000; x = 1; lambda = 20; samples = c(x,rep(NA,n-1))\n\nfor ( t in 2:n ){\n  if(x==0){\n    x = 1\n  }else{\n    # proposal\n    y = ifelse(runif(1) &lt;= 0.5, x+1, x-1)\n    x = ifelse( runif(1) &lt;= alpha(y,x,lambda), y, x)\n  }\n  samples[t] = x\n}\n\n# Comparison of theoretical quantiles and sample quantiles\ntibble::tibble(\n  \"sample quantiles\" = quantile(samples, c(0.1,.25,0.5, 0.75, 0.9))\n  , \"poisson quantiles\" = qpois(c(0.1,.25,0.5, 0.75, 0.9), lambda, lower.tail = TRUE, log.p = FALSE)\n)\n\n# A tibble: 5 × 2\n  `sample quantiles` `poisson quantiles`\n               &lt;dbl&gt;               &lt;dbl&gt;\n1                 14                  14\n2                 17                  17\n3                 20                  20\n4                 23                  23\n5                 26                  26"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-5-a-loan-portfolio",
    "href": "labs/solutions/BSMM_8740_lab_9_solutions.html#exercise-5-a-loan-portfolio",
    "title": "Lab 9 - Monte Carlo Methods",
    "section": "Exercise 5: A Loan Portfolio",
    "text": "Exercise 5: A Loan Portfolio\nOur client is a bank with both asset and liability products in retail bank industry. Most of the bank’s assets are loans, and these loans generate the majority of the total revenue earned by the bank. Hence, it is essential for the bank to understand the proportion of loans that have a high propensity to be paid in full and those which will finally become Bad loans.\nAll the loans that have been issued by the bank are classified into one of four categories :\n\nGood Loans : These are the loans which are in progress but are given to low risk customers. We expect most of these loans will be paid up in full with time.\nRisky loans : These are also the loans which are in progress but are given to medium or high risk customers. We expect a good number of these customers will default.\nBad loans : The customer to whom these loans were given have already defaulted.\nPaid up loans : These loans have already been paid in full.\n\nYour research has suggested the following state transition matrix for the bank loans\nc(0.6, 0.4, 0, 0) %*% P\n\n# the 1-year state transition matrix for loans is:\nP &lt;- \n  matrix(\n    c(0.7, 0.05, 0.03, 0.22,\n      0.05, 0.55, 0.35, 0.05,\n      0, 0, 1, 0, \n      0, 0, 0, 1\n      )\n    , nrow =4, byrow = TRUE\n  )\n\n\n\nc(1, 0.0, 0, 0) %*% (P %^% 20)\n\n            [,1]        [,2]      [,3]      [,4]\n[1,] 0.001121588 0.000338478 0.2334278 0.7651121\n\nc(0, 1, 0, 0) %*% (P %^% 20)\n\n            [,1]         [,2]      [,3]      [,4]\n[1,] 0.000338478 0.0001061542 0.8036091 0.1959463\n\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# describe the current loan portfolio by state at the end of one year. \nc(0.6, 0.4, 0, 0) %*% P\n\n     [,1] [,2]  [,3]  [,4]\n[1,] 0.44 0.25 0.158 0.152\n\n# describe the current loan portfolio by state at the end of two years.\nc(0.6, 0.4, 0, 0) %*% (P %^% 2)\n\n       [,1]   [,2]   [,3]   [,4]\n[1,] 0.3205 0.1595 0.2587 0.2613\n\n\n\n# What percentage of good loans are paid in full after 20 years\nc(1, 0.0, 0, 0) %*% (P %^% 20)\n\n            [,1]        [,2]      [,3]      [,4]\n[1,] 0.001121588 0.000338478 0.2334278 0.7651121\n\n\n76.5 percent of good loans are paid in full after 20 years\n\n# What percentage of risky loans are paid in full after 20 years\nc(0, 1, 0, 0) %*% (P %^% 20)\n\n            [,1]         [,2]      [,3]      [,4]\n[1,] 0.000338478 0.0001061542 0.8036091 0.1959463\n\n\n19.6 percent of good loans are paid in full after 20 years"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_9_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_9_solutions.html#grading",
    "title": "Lab 9 - Monte Carlo Methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 5\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_8_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_8_solutions.html",
    "title": "Lab 8 - Causality: Methods",
    "section": "",
    "text": "Recall that in our last lecture we used several methods to understand the effect of nets on Malaria risk. The regression adjustment approach gave results that were lower than those using IPW or Doubly Robust Estimation.\nThis is partly due to the regression specification we used, which as a second-order, fixed effects model did not fully capture the relationship between the covariates and the outcome. One simple way to enhance the model is to relax the fixed effects assumption, which you will do here, in the context of a completely different approach.\n\n\nThe g-computation method (g for general) is good to know because it works for both binary/discrete treatments and continuous treatments\n\ndat_ &lt;- causalworkshop::net_data |&gt; dplyr::mutate(net = as.numeric(net))\n\nIn this question we’ll use g-computation to estimate the effect of net use on Malaria risk. Run the following steps:\n\nMake two copies of the data. Keep the original copy (you’ll have three in total).\nMutate the copied data so that one copy has net == 1 and the other copy has net ==0.\nBind the data together by row to produce a test dataset.\nModel the relationship between net use and malaria risk, incorporating all confounders. The linear model from the lecture is a good start. Fit the model with the original data.\nUse the model to predict the outcomes in the test dataset.\nGroup the test dataset by net use, compute the average outcome by group (the effect), and find the difference in effects.\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# copy data\n# mutate data to fix treatment (one of each) & bind data into single dataset\ndat_stacked &lt;-\n  dplyr::bind_rows(\n    dat_ |&gt; dplyr::mutate(net = 1)\n    , dat_ |&gt; dplyr::mutate(net = 0)\n  )\n\n\n# Model the outcome, and fit it using the original (unmutated) data\nrisk_model_net_fit &lt;- glm(\n  malaria_risk ~ (net + income + health + temperature + insecticide_resistance)^2, data = dat_)\n\nrisk_model_net_fit |&gt; broom::tidy()\n\n# A tibble: 16 × 5\n   term                                 estimate std.error statistic  p.value\n   &lt;chr&gt;                                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                         49.8      3.63          13.7  1.08e-40\n 2 net                                -24.5      1.65         -14.9  2.59e-47\n 3 income                              -0.0560   0.00543      -10.3  2.95e-24\n 4 health                              -0.228    0.0548        -4.15 3.44e- 5\n 5 temperature                          2.07     0.137         15.1  9.01e-49\n 6 insecticide_resistance               0.484    0.0441        11.0  3.61e-27\n 7 net:income                           0.0303   0.00167       18.1  2.88e-67\n 8 net:health                          -0.0870   0.0165        -5.26 1.64e- 7\n 9 net:temperature                     -0.357    0.0488        -7.31 4.04e-13\n10 net:insecticide_resistance          -0.0557   0.0142        -3.92 9.13e- 5\n11 income:health                        0.000410 0.0000210     19.5  1.06e-76\n12 income:temperature                  -0.00105  0.000187      -5.61 2.37e- 8\n13 income:insecticide_resistance       -0.000449 0.0000524     -8.58 2.02e-17\n14 health:temperature                  -0.00266  0.00184       -1.45 1.48e- 1\n15 health:insecticide_resistance        0.00156  0.000522       2.98 2.89e- 3\n16 temperature:insecticide_resistance   0.00287  0.00148        1.94 5.30e- 2\n\n\n\n# Use the model fit to predict the outcomes in the test data\npredictions &lt;-\n  risk_model_net_fit |&gt;\n  broom::augment(newdata = dat_stacked, type.predict = \"response\")\n\n\n# average the predicted outcomes and compute the contrast.\npredictions |&gt;\n  dplyr::group_by(net) |&gt;\n  dplyr::summarize(mean_malaria_risk = mean(.fitted)) |&gt;\n  dplyr::mutate(contrast = mean_malaria_risk - dplyr::lag(mean_malaria_risk))\n\n# A tibble: 2 × 3\n    net mean_malaria_risk contrast\n  &lt;dbl&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n1     0              42.7     NA  \n2     1              29.7    -13.0\n\n\nThe contrast between using a net and not using a net is a -13.02 reduction in malaria risk\n\n\nIn summary the g-computation is as follows\n\nFit a standardized model with all covariates/confounders. Then, use cloned data sets with values set on each level of the exposure you want to study.\nUse the model to predict the values for that level of the exposure and compute the effect estimate of interest."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_8_solutions.html#q1",
    "href": "labs/solutions/BSMM_8740_lab_8_solutions.html#q1",
    "title": "Lab 8 - Causality: Methods",
    "section": "",
    "text": "Recall that in our last lecture we used several methods to understand the effect of nets on Malaria risk. The regression adjustment approach gave results that were lower than those using IPW or Doubly Robust Estimation.\nThis is partly due to the regression specification we used, which as a second-order, fixed effects model did not fully capture the relationship between the covariates and the outcome. One simple way to enhance the model is to relax the fixed effects assumption, which you will do here, in the context of a completely different approach.\n\n\nThe g-computation method (g for general) is good to know because it works for both binary/discrete treatments and continuous treatments\n\ndat_ &lt;- causalworkshop::net_data |&gt; dplyr::mutate(net = as.numeric(net))\n\nIn this question we’ll use g-computation to estimate the effect of net use on Malaria risk. Run the following steps:\n\nMake two copies of the data. Keep the original copy (you’ll have three in total).\nMutate the copied data so that one copy has net == 1 and the other copy has net ==0.\nBind the data together by row to produce a test dataset.\nModel the relationship between net use and malaria risk, incorporating all confounders. The linear model from the lecture is a good start. Fit the model with the original data.\nUse the model to predict the outcomes in the test dataset.\nGroup the test dataset by net use, compute the average outcome by group (the effect), and find the difference in effects.\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# copy data\n# mutate data to fix treatment (one of each) & bind data into single dataset\ndat_stacked &lt;-\n  dplyr::bind_rows(\n    dat_ |&gt; dplyr::mutate(net = 1)\n    , dat_ |&gt; dplyr::mutate(net = 0)\n  )\n\n\n# Model the outcome, and fit it using the original (unmutated) data\nrisk_model_net_fit &lt;- glm(\n  malaria_risk ~ (net + income + health + temperature + insecticide_resistance)^2, data = dat_)\n\nrisk_model_net_fit |&gt; broom::tidy()\n\n# A tibble: 16 × 5\n   term                                 estimate std.error statistic  p.value\n   &lt;chr&gt;                                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                         49.8      3.63          13.7  1.08e-40\n 2 net                                -24.5      1.65         -14.9  2.59e-47\n 3 income                              -0.0560   0.00543      -10.3  2.95e-24\n 4 health                              -0.228    0.0548        -4.15 3.44e- 5\n 5 temperature                          2.07     0.137         15.1  9.01e-49\n 6 insecticide_resistance               0.484    0.0441        11.0  3.61e-27\n 7 net:income                           0.0303   0.00167       18.1  2.88e-67\n 8 net:health                          -0.0870   0.0165        -5.26 1.64e- 7\n 9 net:temperature                     -0.357    0.0488        -7.31 4.04e-13\n10 net:insecticide_resistance          -0.0557   0.0142        -3.92 9.13e- 5\n11 income:health                        0.000410 0.0000210     19.5  1.06e-76\n12 income:temperature                  -0.00105  0.000187      -5.61 2.37e- 8\n13 income:insecticide_resistance       -0.000449 0.0000524     -8.58 2.02e-17\n14 health:temperature                  -0.00266  0.00184       -1.45 1.48e- 1\n15 health:insecticide_resistance        0.00156  0.000522       2.98 2.89e- 3\n16 temperature:insecticide_resistance   0.00287  0.00148        1.94 5.30e- 2\n\n\n\n# Use the model fit to predict the outcomes in the test data\npredictions &lt;-\n  risk_model_net_fit |&gt;\n  broom::augment(newdata = dat_stacked, type.predict = \"response\")\n\n\n# average the predicted outcomes and compute the contrast.\npredictions |&gt;\n  dplyr::group_by(net) |&gt;\n  dplyr::summarize(mean_malaria_risk = mean(.fitted)) |&gt;\n  dplyr::mutate(contrast = mean_malaria_risk - dplyr::lag(mean_malaria_risk))\n\n# A tibble: 2 × 3\n    net mean_malaria_risk contrast\n  &lt;dbl&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n1     0              42.7     NA  \n2     1              29.7    -13.0\n\n\nThe contrast between using a net and not using a net is a -13.02 reduction in malaria risk\n\n\nIn summary the g-computation is as follows\n\nFit a standardized model with all covariates/confounders. Then, use cloned data sets with values set on each level of the exposure you want to study.\nUse the model to predict the values for that level of the exposure and compute the effect estimate of interest."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_8_solutions.html#q2",
    "href": "labs/solutions/BSMM_8740_lab_8_solutions.html#q2",
    "title": "Lab 8 - Causality: Methods",
    "section": "Q2",
    "text": "Q2\nSuppose you work for a big tech company and you want to estimate the impact of a billboard marketing campaign on in-app purchases. When you look at data from the past, you see that the marketing department tends to spend more to place billboards in cities where the purchase level is lower. This makes sense because they wouldn’t need to do lots of advertisement if sales were skyrocketing. If you run a regression model on this data, it looks like higher cost in marketing leads to lower in-app purchase amounts, but only because marketing investments are biased towards low spending regions.\n\ntoy_panel &lt;-tibble::tibble(\n    \"mkt_costs\" = c(5,4,3.5,3, 10,9.5,9,8, 4,3,2,1, 8,7,6,4),\n    \"purchase\" = c(12,9,7.5,7, 9,7,6.5,5, 15,14.5,14,13, 11,9.5,8,5),\n    \"city\" = \n      c(\"Windsor\",\"Windsor\",\"Windsor\",\"Windsor\"\n        , \"London\",\"London\",\"London\",\"London\"\n        , \"Toronto\",\"Toronto\",\"Toronto\",\"Toronto\", \"Tilbury\",\"Tilbury\",\"Tilbury\",\"Tilbury\")\n)\n\nfit_lm &lt;- lm(purchase ~ mkt_costs, data = toy_panel)\n\ntoy_panel |&gt; \n  ggplot(aes(x = mkt_costs, y = purchase)) +\n  geom_point(color = 'blue') +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1], color = 'purple') +\n  labs(title = \"Simple OLS Model\", x = \"Marketing Costs (in 1000)\", y = \"In-app Purchase (in 1000)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nKnowing a lot about causal inference (and Simpson’s Paradox), you decide to run a fixed effect model, adding the city’s indicator as a dummy variable to your model. The fixed effect model controls for city specific characteristics that are constant in time, so if a city is less open to your product, it will capture that. When you run that model, you can finally see that more marketing costs leads to higher in-app purchase.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# mutate the data to make the column 'city' a factor\ntoy_panel &lt;- toy_panel |&gt; dplyr::mutate(city = factor(city))\n\n\n# fit the data using the new data and augment with predictions\nfe &lt;- lm(purchase ~ mkt_costs + city, data = toy_panel)\n\n\n# augment the fit with predictions\nfe_toy &lt;- fe |&gt; broom::augment(newdata = toy_panel)\n\n\n# plot the data points\np &lt;- fe_toy |&gt; \n  ggplot(aes(x = mkt_costs, y = purchase, color  = city)) +\n  geom_point() + \n  labs(title = \"Fixed Effect Model\", x = \"Marketing Costs (in 1000)\", y = \"In-app Purchase (in 1000)\") +\n  theme_minimal() \n\n  intcpt &lt;- fe$coefficients[1]; slp = fe$coefficients[2]\n  for ( inc in c(0,fe$coefficients[-c(1,2)]) ){\n    p &lt;- p + geom_abline(slope = slp, intercept = intcpt + inc, color = 'purple') \n  }\np\n\n\n\n\n\n\n\n\nTake a minute to appreciate what the image above is telling you about what fixed effect is doing. Notice that fixed effect is fitting one regression line per city. Also notice that the lines are parallel. The slope of the line is the effect of marketing costs on in-app purchase. So the fixed effect is assuming that the causal effect is constant across all entities, which are cities in this case. This can be a weakness or an advantage, depending on how you see it. It is a weakness if you are interested in finding the causal effect per city. Since the FE model assumes this effect is constant across entities, you won’t find any difference in the causal effect. However, if you want to find the overall impact of marketing on in-app purchase, the panel structure of the data is a very useful leverage that fixed effects can explore."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_8_solutions.html#q3",
    "href": "labs/solutions/BSMM_8740_lab_8_solutions.html#q3",
    "title": "Lab 8 - Causality: Methods",
    "section": "Q3",
    "text": "Q3\nYou are an advanced-analytics analyst working at a private equity firm. One of the partners, an MBA by training, has suggested that the firm eliminate portfolio companies that are run by their founders or their families, on the basis of poor management quality.\n\n\nportfolio data\ndata &lt;- readr::read_csv(\"data/portfolio_companies.csv\", show_col_types = FALSE)\n\n\nThe partner opened the data on the portfolio companies in excel and did a simple calculation, the equivalent of the following:\n\n\nmanagement score difference\ndata |&gt;\n  dplyr::group_by(foundfam_owned) |&gt;\n  dplyr::summarise (\n    \"mean management score\" = mean(management)\n    , \"management score stdev\" = sd(management)\n  ) |&gt; \n  dplyr::mutate(delta = `mean management score` - dplyr::lag(`mean management score`))\n\n\n# A tibble: 2 × 4\n  foundfam_owned `mean management score` `management score stdev`  delta\n           &lt;dbl&gt;                   &lt;dbl&gt;                    &lt;dbl&gt;  &lt;dbl&gt;\n1              0                    3.08                    0.632 NA    \n2              1                    2.68                    0.621 -0.396\n\n\nThe partner concluded that the portfolio firms that are run by their founders or their families result in a management quality that is worse (per the management score) in comparison to other portfolio firms, a significant difference - almost 2/3 of a standard deviation worse.\nYou, being an advanced-analytics analyst, gently suggest that this is a question of causality, and that there may be other factors related to both firm ownership and management quality that bias the management score. The other partners all agree, and ask you to estimate the real effect of firm ownership type on management quality.\nSo you start by interviewing the partners and other to identify other factors, particularly those that might be related to variations in either ownership structure or management quality.\n\nPotential variables\nOne source of variation in ownership is how a firm starts, whether they were started by a founder or perhaps they were spin-offs, joint ventures, or affiliates of other companies. You don’t have this kind of data, but you do have data on the production technology the firm uses. Some technologies are very capital intensive, so they are unlikely to be used by start-ups, even those that become successful. So the technology a firm uses is a source of variation in ownership.\nWhether firms start as founder-owned or involve outside owners also depend on cultural or institutional factors in society. This may be important in data collected from firms in many countries, and even within countries. Similar factors may affect management quality.\nSome founder/family businesses are sold to investors, so variation may depend on supply and demand (i.e. the level of M&A business). Firm size and age may also be a factor in whether a firm is acquired.\nSimilarly the competition in the industry may be a factor in both ownership and management quality, as highly competitive industries may have fewer founder owned firms and better management quality.\nYou build a DAG to represent these assumptions, as follows:\n\n\nbusiness DAG\nset.seed(3534)\nset.seed(534)\n\nfq_dag &lt;- ggdag::dagify(\n  mq ~ ff + m + c + ci + ct,\n  ff ~ c + ct + ci + fsa + fc,\n  m ~ ff,\n  es ~ mq + ff,\n  #fsa ~ mq,\n  exposure = \"ff\",\n  outcome = \"mq\",\n  labels = c(\n    mq = \"management_quality\",\n    ff = \"founder_family\",\n    m = \"managers\",\n    fsa = \"firm_size_age\",\n    c = \"competition\",\n    ci = \"culture_institutions\",\n    ct = \"complex_technology\",\n    fc = \"family_circumstances\",\n    es = \"export share\"\n  )\n)\n\nfq_dag |&gt;\n  ggdag::tidy_dagitty() |&gt; \n  ggdag::node_status() |&gt;\n  ggplot(\n    aes(x, y, xend = xend, yend = yend, color = status)\n  ) +\n  ggdag::geom_dag_edges() +\n  ggdag::geom_dag_point() +\n  ggdag::geom_dag_label_repel(aes(label = label)) +\n  ggokabeito::scale_color_okabe_ito(na.value = \"darkgrey\") +\n  ggdag::theme_dag() +\n  theme(legend.position = \"none\") +\n  coord_cartesian(clip = \"off\")\n\n\n\n\n\n\n\n\n\n\n\nData\nNext you look for data that can measure the causal factors in your DAG. You have the following data:\n\nemployment count\nage of firm\nproportion of employees with a college education (except management)\nlevel of competition\nindustry classification\ncountry of origin\nshare of exports in sales\n\nOf these:\n\nindustry can be used as a measure of technology complexity, as is the share of college educated employees.\nnumber of competitors is a measure of competition strength\nthe country of origin is a measure of cultural and institutional factors\nthe number of employees is a measure of firm size\nthe age of the firm is missing for about 14% of the observations\n\nYou have data for the share of exports in sales, but as it is a collider you decide not to condition on this variable in your analysis.\n\n\nQ3(1)\n\ndat_portfolio &lt;- readr::read_csv('data/portfolio_companies.csv', show_col_types = FALSE)\n\n\ndata |&gt; \n  group_by(foundfam_owned) |&gt;  \n  summarise (mean(management))  |&gt; \n  dplyr::mutate(delta = `mean(management)` - dplyr::lag(`mean(management)`))\n\n# A tibble: 2 × 3\n  foundfam_owned `mean(management)`  delta\n           &lt;dbl&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1              0               3.08 NA    \n2              1               2.68 -0.396\n\n\nHere are the variables you have decided to use in your analysis:\n\nY &lt;- \"management\"\nD &lt;- \"foundfam_owned\"\n\ncontrol_vars &lt;- c(\"degree_nm\", \"degree_nm_sq\", \"compet_moder\", \"compet_strong\", \n                  \"lnemp\", \"age_young\", \"age_old\", \"age_unknown\")\ncontrol_vars_to_interact &lt;- c(\"industry\", \"countrycode\")\nX &lt;- c(control_vars, control_vars_to_interact)\n\nAnd here are the formulas for the models you have decided to test:\n\n# basic model without confounders\nformula1 &lt;- as.formula(paste0(Y, \" ~ \",D))\n\n# basic model with confounders\nformula2 &lt;- as.formula(paste0(Y, \" ~ \",D,\" + \", \n                  paste(X, collapse = \" + \")))\n\n# model with interactions between variables\nformula3 &lt;- as.formula(paste(Y, \" ~ \",D,\" + \", \n    paste(control_vars_to_interact, collapse = \":\"), \n    \" + (\", paste(control_vars, collapse = \"+\"),\")*(\",\n    paste(control_vars_to_interact, collapse = \"+\"),\")\",sep=\"\"))\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# fit each formula\nols1 &lt;- lm(as.formula(formula1), data = dat_portfolio) \n\nols2 &lt;- lm(as.formula(formula2), data = dat_portfolio) \n\nols3 &lt;- lm(as.formula(formula3), data = dat_portfolio) \n\n\nlist(ols1, ols2, ols3) |&gt; \n  purrr::map(\n    (\\(x){\n      x |&gt; broom::tidy() |&gt; \n        dplyr::slice(2) |&gt; \n        dplyr::select(1:3) |&gt; \n        dplyr::bind_cols(\n          x |&gt; broom::glance() |&gt; \n            dplyr::select(c(ends_with(\"r.squared\"), AIC, BIC))\n        )\n    })\n  ) |&gt; dplyr::bind_rows()\n\n# A tibble: 3 × 7\n  term           estimate std.error r.squared adj.r.squared    AIC    BIC\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 foundfam_owned   -0.396    0.0117    0.0897        0.0896 22244. 22267.\n2 foundfam_owned   -0.204    0.0114    0.295         0.292  19356. 19747.\n3 foundfam_owned   -0.193    0.0118    0.372         0.327  19465. 25232.\n\n\nConsidering the trade-off between model fit and model complexity, the most suitable model is ols2\n\n\n\n\nQ3(2)\nHaving settled on a formula/model, use the doubly-robust effect estimation function from class to\n\nestimate the causal effect using the best model.\ncalculate the confidence intervals on the effect\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ndoubly_robust &lt;- function(df, X, D, Y){\n  ps &lt;- # propensity score\n    as.formula(paste(D, \" ~ \", paste(X, collapse= \"+\"))) |&gt;\n    stats::glm( data = df, family = binomial() ) |&gt;\n    broom::augment(type.predict = \"response\", data = df) |&gt;\n    dplyr::pull(.fitted)\n  \n  lin_frml &lt;- formula(paste(Y, \" ~ \", paste(X, collapse= \"+\")))\n  \n  idx &lt;- df[,D] |&gt; dplyr::pull(1) == 0\n  mu0 &lt;- # mean response D == 0\n    lm(lin_frml, data = df[idx,]) |&gt; \n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt;\n    dplyr::pull(.fitted)\n  \n  idx &lt;- df[,D] |&gt; dplyr::pull(1) == 1\n  mu1 &lt;- # mean response D == 1\n    lm(lin_frml, data = df[idx,]) |&gt;  \n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt; \n    dplyr::pull(.fitted)\n  \n  # convert treatment factor to integer | recast as vectors\n  d &lt;- df[,D] |&gt; dplyr::pull(1) |&gt; as.character() |&gt; as.numeric()\n  y &lt;- df[,Y] |&gt; dplyr::pull(1)\n  \n  mean( d*(y - mu1)/ps + mu1 ) -\n    mean(( 1-d)*(y - mu0)/(1-ps) + mu0 )\n}\n\n# estimate the effect\ndat_portfolio |&gt; \n  doubly_robust(X, D, Y)\n\n[1] -0.191813\n\n\n\nbootstrapped_dat_portfolio &lt;- rsample::bootstraps(\n  dat_portfolio,\n  times = 100,\n  #strata = industry,\n  # required to calculate CIs later\n  apparent = TRUE\n)\n\n\n# NOTE: this will take a while\nresults_dat_portfolio &lt;- bootstrapped_dat_portfolio |&gt;\n  dplyr::mutate(\n    dre_estimate = \n      purrr::map_dbl(\n        splits\n        , (\\(x){\n            rsample::analysis(x) |&gt; \n              doubly_robust(X, D, Y)\n        })\n      )\n  )\n\n# summarize as bootstrap CI estimates\nresults_dat_portfolio |&gt; \n  dplyr::summarise(\n  mean = mean(dre_estimate)\n  , lower_ci = quantile(dre_estimate, 0.025)\n  , upper_ci = quantile(dre_estimate, 0.975)\n)\n\n# A tibble: 1 × 3\n    mean lower_ci upper_ci\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.191   -0.215   -0.165\n\n\n\n\n\n\nOmitted variables and bias\nIf all potential confounders are captured and included in the models, then we can put the expected effect of changing ownership within our 95% confidence intervals we calculated.\nHowever, we suspect that we do not have a full set of confounders in the data set, either because our measurements don’t capture all the variation due to a variable or because we just don’t have the data at all.\nFor example, we don’t have information on the the city the business is located in; being in a large city may make a business more attractive to outside investors, reducing the number of family-owned business while making the quality of family-owned businesses better. If this assumption is correct, then without this variable/data in the model, our estimate will be negatively biased and the effect is probably weaker than suggested by our estimates.\nIf other omitted variables behaved the same way we would see an even smaller effect.\nGiven your estimated effect and considering omitted variable bias, please provide two bullet points about your analysis for the partners:\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nbullet #1: Given the data available, the estimated causal effect of ownership is roughly half of the average management score difference between founder/family-owned firms and other firms.\nbullet #2: Not all potential confounders are included in the data, which we think biases the estimate negatively. Thus the true effect is likely smaller than the estimated effect (in absolute value)."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_8_solutions.html#q4",
    "href": "labs/solutions/BSMM_8740_lab_8_solutions.html#q4",
    "title": "Lab 8 - Causality: Methods",
    "section": "Q4",
    "text": "Q4\nYour firm has a customer intervention that management would like to evaluate. The data from a test of the intervention is as follows (where DD is the (binary intervention, YY is the outcome (in units of $1,000 dollars), and X1,…,X5X_1,\\ldots,X_5 are variables in the adjustment set). Each row in the data represents measurements for a distinct customer:\n\nset.seed(8740)\nn &lt;- 2000; p &lt;- 5;\n\ntest_dat &lt;- matrix(rnorm(n * p), n, p, dimnames = list(NULL, paste0(\"X\",1:p)) ) |&gt;\n  tibble::as_tibble() |&gt;\n  dplyr::mutate(\n    D = rbinom(n, 1, 0.5) * as.numeric(abs(X3) &lt; 2)\n    , Y = pmax(X1, 0) * D + X2 + pmin(X3, 0) + rnorm(n)\n  )\n\nEstimate the ATE as follows (hint: it is &gt;0):\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# (1) using regression adjustment, with formula Y ~ D*X1 + X2 + X3 + X4 + X5\nols &lt;- lm(Y~D*X1 + X2 + X3 + X4 + X5, data = test_dat)\n\n# (2) using doubly robust estimation\ndre &lt;- doubly_robust(test_dat, paste0(\"X\",1:p), \"D\", \"Y\")\n\n\n# summarize results\ntibble::tibble(\n  dre_ATE = dre\n  , ols_ATE = \n    ols |&gt; broom::tidy() |&gt; \n    dplyr::filter(term == \"D\") |&gt; \n    dplyr::pull(estimate)\n)\n\n# A tibble: 1 × 2\n  dre_ATE ols_ATE\n    &lt;dbl&gt;   &lt;dbl&gt;\n1   0.374   0.389\n\n\n\n\nGiven that the ATE is &gt;0, the firm would like to roll out the intervention with 1,000 new customers, and the project is budgeted as follows\n\ncost of the intervention is $100 (cost = 0.1 in the scale of the data).\nthe per-customer average incremental revenue on making the intervention is (ATE−0.1)×1000(\\textrm{ATE}-0.1)\\times 1000.\n\nGiven the substantial return on the investment, the budget is approved and the firm decides to implement the intervention with the 1,000 new customers.\nHowever, the firm’s good fortune is that you are in the room with management, and you suggest an alternative strategy, based on predicting the individual treatment effects for each new customer using the customer data.\nThe new customer data is:\n\n# new customer data\nnew_dat &lt;- \n  matrix(rnorm(n * p), n, p, dimnames = list(NULL, paste0(\"X\",1:p)) ) |&gt;\n  tibble::as_tibble() \n\nYou analyse your strategy for management as follows:\n\nmake two copies of the new data; mutate one to add a column D=1 and mutate the other to add a column D=0, and\ntake the regression model used to estimate the ATE, and predict the treatment effects for each customer, using the two data sets from steps 1 (i.e. use broom::augment with each dataset),\nselect the columns with the predicted responses from each dataset and bind them columnwise. Name the columns r1 (response when D=1) and r0 (response when D=0). Then\n\nmutate to compute the contrast r1-r0 and subtract the cost of the intervention. Call this new column ‘lift’ (standing for your new strategy based on estimating individual treatment effects)\nmutate to add a new column with the ATE and subtract the cost of the intervention. Call this new column ‘baseline’ (standing for baseline strategy)\nsort the rows in descending order by lift\nadd two new columns: one for the cumulative sum of the lifts and the other for the cumulative sum of the baseline.\n\nPlot the cumulative results of the baseline and the lift strategies\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# make two copies of new_dat and,\n\n# mutate one setting the intervention D=1\nnew_dat1 &lt;- new_dat |&gt; dplyr::mutate(D = 1)\n\n# mutate the other setting the intervention D=0  \nnew_dat0 &lt;- new_dat |&gt; dplyr::mutate(D = 0)\n\n\n# use the linear model (ols) to predict the responses under the two treatments\npredicted1 &lt;- ols |&gt;\n      broom::augment(newdata = new_dat1) \n  \npredicted0 &lt;- ols |&gt;\n      broom::augment(newdata = new_dat0)  \n\n\n# combine the columns containing the predicted responses to create a two column dataframe\nresult &lt;- \n  predicted1 |&gt; dplyr::select(r1 = .fitted) |&gt; \n  dplyr::bind_cols(\n    predicted0 |&gt; dplyr::select(r0 = .fitted)\n  ) |&gt; \n  # - add columns for the lift net of costs and the baseline net of costs\n  dplyr::mutate(\n    lift = r1 - r0 - 0.1\n    , baseline = ols$coefficients[\"D\"] - 0.1\n  ) |&gt; \n  # - sort in descending order by lift\n  dplyr::arrange( desc(lift) ) |&gt;\n  # - add an ID column, using tibble::rowid_to_column(\"ID\")\n  tibble::rowid_to_column(\"ID\") |&gt; \n  # - accumulate results\n  dplyr::mutate(\n    # add a column for the cumulative % of budget\n    cumulative_pct = ID / n(),\n    # and columns for the cumulative sums of lift and baseline \n    cumulative_baseline = cumsum(baseline),\n    cumulative_lift = cumsum(lift) \n  ) \n\n\n# plot the cumulative results to compare strategies \nresult |&gt;\n  tidyr::pivot_longer(c(cumulative_baseline, cumulative_lift)) |&gt;\n  ggplot(aes(x=cumulative_pct, y=value, color = name)) + geom_line() +\n  theme_minimal(base_size = 16) + \n  theme(legend.title = element_blank(),legend.position = \"top\") +\n  labs(title = \" Stategy Comparison\", x = \"cumulative budget spend (%)\", y = \"net revenue (000's)\")\n\n\n\n\n\n\n\n\n\n# compute the additional columns required\ndat &lt;- result |&gt; \n  dplyr::mutate(\n    revenue_diff = (cumulative_lift - cumulative_baseline) * 1000\n    , cumulative_budget = cumulative_pct) \n\n dat |&gt; \n  dplyr::filter(revenue_diff == max(revenue_diff)) |&gt; \n  dplyr::select(revenue = revenue_diff, cumulative_budget) |&gt; \n  tidyr::pivot_longer(everything(), names_to = \"measure\", values_to = \"max strategy difference\") |&gt;   \n  dplyr::left_join(\n    dat |&gt; \n      dplyr::mutate(cumulative_lift = cumulative_lift* 1000) |&gt; \n      dplyr::filter(cumulative_lift == max(cumulative_lift)) |&gt; \n      dplyr::select(revenue = cumulative_lift, cumulative_budget) |&gt; \n      tidyr::pivot_longer(everything(), names_to = \"measure\", values_to = \"max absolute difference\")\n  ) |&gt; \n   gt::gt(\"measure\") |&gt; \n   gt::fmt_currency(columns = -measure, rows = c(1), decimals = 0) |&gt; \n   gt::fmt_percent(columns = -measure, rows = c(2), decimals = 1)\n\n\n\n\n\n\n\n\nmax strategy difference\nmax absolute difference\n\n\n\n\nrevenue\n$406,524\n$755,184\n\n\ncumulative_budget\n49.9%\n71.0%"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_8_solutions.html#q5",
    "href": "labs/solutions/BSMM_8740_lab_8_solutions.html#q5",
    "title": "Lab 8 - Causality: Methods",
    "section": "Q5",
    "text": "Q5\nEstimate the causal effect of smoking cessation on weight, using the dataset data(nhefs) where the treatment variable is quit_smoking, the outcome variable is wt82_71, and the covariates of interest are age, wt71, smokeintensity, exercise, education, sex, and race.\nEstimate the causal effect using the matching estimator described in the lecture.\n\n# load data\nnhefs &lt;- \n  readr::read_csv('data/nhefs_data.csv', show_col_types = FALSE) |&gt; \n  dplyr::select(wt82_71, quit_smoking, age, wt71, smokeintensity, exercise, education, sex, race)\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# (1)\n# create a table to calculate the mean difference in effects for the two treatment groups in the raw data\nnhefs |&gt;\n  dplyr::group_by(quit_smoking) |&gt;\n  dplyr::summarize(mean_effect = mean(wt82_71)) |&gt;\n  dplyr::mutate(ATE = mean_effect - dplyr::lag( mean_effect) ) \n\n# A tibble: 2 × 3\n  quit_smoking mean_effect    ATE\n         &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1            0        2.83 NA    \n2            1        2.04 -0.786\n\n\n\n#(2)\n# create recipe to normalize the numerical covariates of interest (note - some covariates are factors)\nnhefs_data &lt;- nhefs |&gt; recipes::recipe(wt82_71 ~ .) |&gt;\n  recipes::update_role(quit_smoking, new_role = 'treatment') |&gt;\n  recipes::step_normalize(age, wt71, smokeintensity) |&gt;\n  recipes::prep() |&gt;\n  recipes::bake(new_data=NULL)\n\n\n#(3)\n# using nhefs_data calculate the un-corrected effect estimate, per the matching method we used in class\n# NOTE: this takes some time to run\n\ncovars &lt;- c('age', 'wt71', 'smokeintensity', 'exercise', 'education', 'sex', 'race')\n\ntreated   &lt;- nhefs_data |&gt; dplyr::filter(quit_smoking==1)\nuntreated &lt;- nhefs_data |&gt; dplyr::filter(quit_smoking==0)\n\nmt0 &lt;- # untreated knn model predicting recovery\n  caret::knnreg(x = untreated |&gt; dplyr::select(dplyr::all_of(covars)), y = untreated$wt82_71, k=1)\nmt1 &lt;- # treated knn model predicting recovery\n  caret::knnreg(x = treated |&gt; dplyr::select(dplyr::all_of(covars)), y = treated$wt82_71, k=1)\n\npredicted &lt;-\n  # combine the treated and untreated matches\n  c(\n    # find matches for the treated looking at the untreated knn model\n    treated |&gt;\n      tibble::rowid_to_column(\"ID\") |&gt;\n      {\\(y)split(y,y$ID)}() |&gt; # hack for native pipe\n      # split(.$ID) |&gt;         # this vesion works with magrittr\n      purrr::map(\n        (\\(x){\n          x |&gt;\n            dplyr::mutate(\n              match = predict( mt0, x[1,covars] )\n            )\n        })\n      )\n    # find matches for the untreated looking at the treated knn model\n    , untreated |&gt;\n      tibble::rowid_to_column(\"ID\") |&gt;\n      {\\(y)split(y,y$ID)}() |&gt;\n      # split(.$ID) |&gt;\n      purrr::map(\n        (\\(x){\n          x |&gt;\n            dplyr::mutate(\n              match = predict( mt1, x[1,covars] )\n            )\n        })\n      )\n  ) |&gt;\n  # bind the treated and untreated data\n  dplyr::bind_rows()\n\npredicted |&gt;\n  dplyr::summarize(\"ATE (est)\" = mean( (2*quit_smoking - 1) * (wt82_71 - match) ))\n\n# A tibble: 1 × 1\n  `ATE (est)`\n        &lt;dbl&gt;\n1      -0.928\n\n\nThe un-corrected treatment effect is: - 0.928\n\n#(4)\n# use the method from class to calculate the correction terms, and compute the revised estimate \n# NOTE: this takes some time to run\nfrmla &lt;- wt82_71 ~ age + wt71 + smokeintensity + exercise + education + sex + race\nols0 &lt;- lm(frmla, data = untreated)\nols1 &lt;- lm(frmla, data = treated)\n\n# find the units that match to the treated\ntreated_match_index &lt;- # RANN::nn2 does Nearest Neighbour Search\n  (RANN::nn2(mt0$learn$X, treated |&gt; dplyr::select(dplyr::all_of(covars)), k=1))$nn.idx |&gt;\n  as.vector()\n\n# find the units that match to the untreated\nuntreated_match_index &lt;- # RANN::nn2 does Nearest Neighbour Search\n  (RANN::nn2(mt1$learn$X, untreated |&gt; dplyr::select(dplyr::all_of(covars)), k=1))$nn.idx |&gt;\n  as.vector()\n\npredicted &lt;-\n  c(\n    purrr::map2(\n      .x =\n        treated |&gt; tibble::rowid_to_column(\"ID\") |&gt; {\\(y)split(y,y$ID)}() # split(.$ID)\n      , .y = treated_match_index\n      , .f = (\\(x,y){\n        x |&gt;\n          dplyr::mutate(\n            match = predict( mt0, x[1,covars] )\n            , bias_correct =\n              predict( ols0, x[1,covars] ) -\n              predict( ols0, untreated[y,covars] )\n          )\n      })\n    )\n    , purrr::map2(\n      .x =\n        untreated |&gt; tibble::rowid_to_column(\"ID\") |&gt; {\\(y)split(y,y$ID)}() # split(.$ID)\n      , .y = untreated_match_index\n      , .f = (\\(x,y){\n        x |&gt;\n          dplyr::mutate(\n            match = predict( mt1, x[1,covars] )\n            , bias_correct =\n              predict( ols1, x[1,covars] ) -\n              predict( ols1, treated[y,covars] )\n          )\n      })\n    )\n  ) |&gt;\n  # bind the treated and untreated data\n  dplyr::bind_rows()\n\n\n#(5)\n# calculate the corrected causal estimate for the effect of smoking cessation on weight\npredicted |&gt;\n  dplyr::summarize(\n    \"ATE (est)\" =\n      mean( (2*quit_smoking - 1) * (wt82_71 - match - bias_correct) ))\n\n# A tibble: 1 × 1\n  `ATE (est)`\n        &lt;dbl&gt;\n1      -0.943\n\n\nThe un-corrected treatment effect is: - 0.943"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_8_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_8_solutions.html#grading",
    "title": "Lab 8 - Causality: Methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 5\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_1_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_1_solutions.html",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "",
    "text": "Today’s data is all baseball statistics. The data is in the Lahman package.\n\n\nBefore doing any analysis, you will want to get quick view of the data. This is useful as part of the EDA process.\n\ndim(Teams)\n\n[1] 3045   48"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_1_solutions.html#data-yearly-statistics-and-standings-for-baseball-teams",
    "href": "labs/solutions/BSMM_8740_lab_1_solutions.html#data-yearly-statistics-and-standings-for-baseball-teams",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "",
    "text": "Today’s data is all baseball statistics. The data is in the Lahman package.\n\n\nBefore doing any analysis, you will want to get quick view of the data. This is useful as part of the EDA process.\n\ndim(Teams)\n\n[1] 3045   48"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_1_solutions.html#data-dictionary",
    "href": "labs/solutions/BSMM_8740_lab_1_solutions.html#data-dictionary",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nThe variable definitions are found in the help for Teams, and are listed below.\n\n?Teams\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nyearID\nYear\n\n\nlgID\nLeague; a factor with levels AA AL FL NL PL UA\n\n\nteamID\nTeam; a factor\n\n\nfranchID\nFranchise (links to TeamsFranchises table)\n\n\ndivID\nTeam’s division; a factor with levels C E W\n\n\nRank\nPosition in final standings\n\n\nG\nGames played\n\n\nGhome\nGames played at home\n\n\nW\nWins\n\n\nL\nLosses\n\n\nDivWin\nDivision Winner (Y or N)\n\n\nWCWin\nWild Card Winner (Y or N)\n\n\nLgWin\nLeague Champion(Y or N)\n\n\nWSWin\nWorld Series Winner (Y or N)\n\n\nR\nRuns scored\n\n\nAB\nAt bats\n\n\nH\nHits by batters\n\n\nX2B\nDoubles\n\n\nX3B\nTriples\n\n\nHR\nHomeruns by batters\n\n\nBB\nWalks by batters\n\n\nSO\nStrikeouts by batters\n\n\nSB\nStolen bases\n\n\nCS\nCaught stealing\n\n\nHBP\nBatters hit by pitch\n\n\nSF\nSacrifice flies\n\n\nRA\nOpponents runs scored\n\n\nER\nEarned runs allowed\n\n\nERA\nEarned run average\n\n\nCG\nComplete games\n\n\nSHO\nShutouts\n\n\nSV\nSaves\n\n\nIPouts\nOuts Pitched (innings pitched x 3)\n\n\nHA\nHits allowed\n\n\nHRA\nHomeruns allowed\n\n\nBBA\nWalks allowed\n\n\nSOA\nStrikeouts by pitchers\n\n\nE\nErrors\n\n\nDP\nDouble Plays\n\n\nFP\nFielding percentage\n\n\nname\nTeam’s full name\n\n\npark\nName of team’s home ballpark\n\n\nattendance\nHome attendance total\n\n\nBPF\nThree-year park factor for batters\n\n\nPPF\nThree-year park factor for pitchers\n\n\nteamIDBR\nTeam ID used by Baseball Reference website\n\n\nteamIDlahman45\nTeam ID used in Lahman database version 4.5\n\n\nteamIDretro\nTeam ID used by Retrosheet\n\n\n\n\n\nExercises\n\n\nExercise 1\nHow many observations are in the Teams dataset? How many variables?\n\n# take the first three rows and glimpse the data\nTeams |&gt; dplyr::slice_head(n=3) |&gt; dplyr::glimpse()\n\nRows: 3\nColumns: 48\n$ yearID         &lt;int&gt; 1871, 1871, 1871\n$ lgID           &lt;fct&gt; NA, NA, NA\n$ teamID         &lt;fct&gt; BS1, CH1, CL1\n$ franchID       &lt;fct&gt; BNA, CNA, CFC\n$ divID          &lt;chr&gt; NA, NA, NA\n$ Rank           &lt;int&gt; 3, 2, 8\n$ G              &lt;int&gt; 31, 28, 29\n$ Ghome          &lt;int&gt; NA, NA, NA\n$ W              &lt;int&gt; 20, 19, 10\n$ L              &lt;int&gt; 10, 9, 19\n$ DivWin         &lt;chr&gt; NA, NA, NA\n$ WCWin          &lt;chr&gt; NA, NA, NA\n$ LgWin          &lt;chr&gt; \"N\", \"N\", \"N\"\n$ WSWin          &lt;chr&gt; NA, NA, NA\n$ R              &lt;int&gt; 401, 302, 249\n$ AB             &lt;int&gt; 1372, 1196, 1186\n$ H              &lt;int&gt; 426, 323, 328\n$ X2B            &lt;int&gt; 70, 52, 35\n$ X3B            &lt;int&gt; 37, 21, 40\n$ HR             &lt;int&gt; 3, 10, 7\n$ BB             &lt;int&gt; 60, 60, 26\n$ SO             &lt;int&gt; 19, 22, 25\n$ SB             &lt;int&gt; 73, 69, 18\n$ CS             &lt;int&gt; 16, 21, 8\n$ HBP            &lt;int&gt; NA, NA, NA\n$ SF             &lt;int&gt; NA, NA, NA\n$ RA             &lt;int&gt; 303, 241, 341\n$ ER             &lt;int&gt; 109, 77, 116\n$ ERA            &lt;dbl&gt; 3.55, 2.76, 4.11\n$ CG             &lt;int&gt; 22, 25, 23\n$ SHO            &lt;int&gt; 1, 0, 0\n$ SV             &lt;int&gt; 3, 1, 0\n$ IPouts         &lt;int&gt; 828, 753, 762\n$ HA             &lt;int&gt; 367, 308, 346\n$ HRA            &lt;int&gt; 2, 6, 13\n$ BBA            &lt;int&gt; 42, 28, 53\n$ SOA            &lt;int&gt; 23, 22, 34\n$ E              &lt;int&gt; 243, 229, 234\n$ DP             &lt;int&gt; 24, 16, 15\n$ FP             &lt;dbl&gt; 0.834, 0.829, 0.818\n$ name           &lt;chr&gt; \"Boston Red Stockings\", \"Chicago White Stockings\", \"Cle…\n$ park           &lt;chr&gt; \"South End Grounds I\", \"Union Base-Ball Grounds\", \"Nati…\n$ attendance     &lt;int&gt; NA, NA, NA\n$ BPF            &lt;int&gt; 103, 104, 96\n$ PPF            &lt;int&gt; 98, 102, 100\n$ teamIDBR       &lt;chr&gt; \"BOS\", \"CHI\", \"CLE\"\n$ teamIDlahman45 &lt;chr&gt; \"BS1\", \"CH1\", \"CL1\"\n$ teamIDretro    &lt;chr&gt; \"BS1\", \"CH1\", \"CL1\"\n\n\nHow many character columns/measurements have missing variables?\n\nTeams |&gt; skimr::skim()\n\n\nData summary\n\n\nName\nTeams\n\n\nNumber of rows\n3045\n\n\nNumber of columns\n48\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\nfactor\n3\n\n\nnumeric\n35\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndivID\n1517\n0.50\n1\n1\n0\n3\n0\n\n\nDivWin\n1545\n0.49\n1\n1\n0\n2\n0\n\n\nWCWin\n2181\n0.28\n1\n1\n0\n2\n0\n\n\nLgWin\n28\n0.99\n1\n1\n0\n2\n0\n\n\nWSWin\n357\n0.88\n1\n1\n0\n2\n0\n\n\nname\n0\n1.00\n11\n33\n0\n140\n0\n\n\npark\n34\n0.99\n7\n70\n0\n224\n0\n\n\nteamIDBR\n0\n1.00\n3\n3\n0\n101\n0\n\n\nteamIDlahman45\n0\n1.00\n3\n3\n0\n148\n0\n\n\nteamIDretro\n0\n1.00\n3\n3\n0\n151\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nlgID\n0\n1\nFALSE\n7\nNL: 1549, AL: 1325, AA: 85, NA: 50\n\n\nteamID\n0\n1\nFALSE\n149\nCHN: 148, PHI: 141, PIT: 137, CIN: 134\n\n\nfranchID\n0\n1\nFALSE\n120\nATL: 148, CHC: 148, CIN: 142, PIT: 142\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyearID\n0\n1.00\n1960.12\n43.48\n1871.00\n1923.00\n1969.00\n1998.00\n2023.00\n▃▅▅▆▇\n\n\nRank\n0\n1.00\n4.02\n2.29\n1.00\n2.00\n4.00\n5.00\n13.00\n▇▅▃▁▁\n\n\nG\n0\n1.00\n150.24\n24.24\n6.00\n154.00\n160.00\n162.00\n165.00\n▁▁▁▁▇\n\n\nGhome\n399\n0.87\n78.12\n6.87\n24.00\n77.00\n81.00\n81.00\n84.00\n▁▁▁▁▇\n\n\nW\n0\n1.00\n74.74\n17.94\n0.00\n66.00\n77.00\n87.00\n116.00\n▁▁▃▇▂\n\n\nL\n0\n1.00\n74.74\n17.70\n4.00\n65.00\n76.00\n87.00\n134.00\n▁▂▇▅▁\n\n\nR\n0\n1.00\n681.81\n138.73\n24.00\n615.00\n692.00\n765.00\n1220.00\n▁▁▇▅▁\n\n\nAB\n0\n1.00\n5135.61\n791.75\n211.00\n5146.00\n5407.00\n5519.00\n5781.00\n▁▁▁▁▇\n\n\nH\n0\n1.00\n1339.47\n228.86\n33.00\n1297.00\n1389.00\n1463.00\n1783.00\n▁▁▁▇▅\n\n\nX2B\n0\n1.00\n229.47\n59.65\n1.00\n195.00\n235.00\n273.00\n376.00\n▁▂▆▇▂\n\n\nX3B\n0\n1.00\n45.22\n22.53\n0.00\n29.00\n39.00\n58.00\n150.00\n▅▇▃▁▁\n\n\nHR\n0\n1.00\n107.42\n64.50\n0.00\n47.00\n112.00\n157.00\n307.00\n▇▆▇▃▁\n\n\nBB\n0\n1.00\n474.14\n131.68\n0.00\n427.00\n494.00\n555.00\n835.00\n▁▁▇▇▁\n\n\nSO\n16\n0.99\n774.32\n327.97\n3.00\n518.00\n775.00\n1010.00\n1654.00\n▂▇▇▅▂\n\n\nSB\n125\n0.96\n109.17\n69.19\n0.00\n63.00\n93.00\n137.00\n581.00\n▇▃▁▁▁\n\n\nCS\n831\n0.73\n46.02\n21.87\n0.00\n32.00\n43.00\n56.00\n191.00\n▆▇▁▁▁\n\n\nHBP\n1158\n0.62\n46.57\n18.54\n7.00\n33.00\n44.00\n58.00\n160.00\n▆▇▂▁▁\n\n\nSF\n1541\n0.49\n43.98\n10.14\n7.00\n38.00\n44.00\n50.00\n77.00\n▁▂▇▃▁\n\n\nRA\n0\n1.00\n681.81\n138.51\n34.00\n611.00\n690.00\n766.00\n1252.00\n▁▁▇▃▁\n\n\nER\n0\n1.00\n575.14\n149.40\n23.00\n505.00\n597.00\n672.00\n1023.00\n▁▂▇▆▁\n\n\nERA\n0\n1.00\n3.85\n0.76\n1.22\n3.38\n3.85\n4.33\n8.00\n▁▇▇▁▁\n\n\nCG\n0\n1.00\n46.63\n39.46\n0.00\n9.00\n40.00\n75.00\n148.00\n▇▅▃▂▁\n\n\nSHO\n0\n1.00\n9.55\n5.08\n0.00\n6.00\n9.00\n12.00\n32.00\n▅▇▃▁▁\n\n\nSV\n0\n1.00\n24.75\n16.36\n0.00\n10.00\n26.00\n39.00\n68.00\n▇▅▆▅▁\n\n\nIPouts\n0\n1.00\n4019.05\n658.05\n162.00\n4083.00\n4261.00\n4341.00\n4518.00\n▁▁▁▁▇\n\n\nHA\n0\n1.00\n1339.25\n229.09\n49.00\n1287.00\n1388.00\n1467.00\n1993.00\n▁▁▁▇▁\n\n\nHRA\n0\n1.00\n107.42\n61.35\n0.00\n52.00\n115.00\n155.00\n305.00\n▆▆▇▂▁\n\n\nBBA\n0\n1.00\n474.49\n130.87\n1.00\n430.00\n496.00\n554.00\n827.00\n▁▁▇▇▁\n\n\nSOA\n0\n1.00\n773.72\n329.03\n0.00\n515.00\n772.00\n1012.00\n1687.00\n▂▇▇▅▁\n\n\nE\n0\n1.00\n178.89\n108.14\n20.00\n110.00\n140.00\n204.00\n639.00\n▇▅▁▁▁\n\n\nDP\n0\n1.00\n134.84\n42.22\n0.00\n116.00\n141.00\n157.00\n460.00\n▁▇▁▁▁\n\n\nFP\n0\n1.00\n0.97\n0.03\n0.76\n0.97\n0.98\n0.98\n0.99\n▁▁▁▁▇\n\n\nattendance\n279\n0.91\n1395654.54\n967031.81\n0.00\n553808.00\n1215447.50\n2091859.75\n4483350.00\n▇▆▅▂▁\n\n\nBPF\n0\n1.00\n100.19\n4.92\n60.00\n97.00\n100.00\n103.00\n129.00\n▁▁▇▅▁\n\n\nPPF\n0\n1.00\n100.21\n4.85\n60.00\n97.00\n100.00\n103.00\n141.00\n▁▁▇▁▁\n\n\n\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\nFrom the dim(Teams) statement used after library(Lahman), there are 3015 observations and 48 variables.\nFrom Teams |&gt; skimr::skim() 6 of 10 character variables have missing values\n\n\n\n\nExercise 2\nBen Baumer worked for the New York Mets from 2004 to 2012. What was the team W/L record during those years? Use filter() and select() to quickly identify only those pieces of information that we care about.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# filter to use only rows where teamID equals \"NYN\"\nmets &lt;- Teams  %&gt;% \n  dplyr::filter(teamID == \"NYN\")\n# filter to use only rows where yearID is &gt;= 2004 and &lt;= 2012\n# you could also write dplyr::filter(yearID  %in% 2004:2012)\nmy_mets &lt;- mets %&gt;% \n  dplyr::filter(yearID &gt;= 2004 & yearID &lt;= 2012)\n# the dataset needs to have at least the year and the won (W) loss (L) record for that year\nmy_mets %&gt;% \n  dplyr::select(teamID,yearID,W,L)\n\n  teamID yearID  W  L\n1    NYN   2004 71 91\n2    NYN   2005 83 79\n3    NYN   2006 97 65\n4    NYN   2007 88 74\n5    NYN   2008 89 73\n6    NYN   2009 70 92\n7    NYN   2010 79 83\n8    NYN   2011 77 85\n9    NYN   2012 74 88\n\n\nOverall, the won-loss record was as follows:\n\nmy_mets %&gt;% \n  dplyr::select(teamID,yearID,W,L) %&gt;% \n  dplyr::summarize(\n    \"2004-2012 wins\" = sum(W)\n    , \"2004-2012 losses\" = sum(L)\n  )\n\n  2004-2012 wins 2004-2012 losses\n1            728              730\n\n\n\n\n\n\nExercise 3\nThe model estimates the expected winning percentage as follows:\nŴpct=11+(RARS)2\n\\hat{\\text{W}}_{\\text{pct}}=\\frac{1}{1+\\left(\\frac{\\text{RA}}{\\text{RS}}\\right)^{2}}\n\nwhere RA\\text{RA} is the number of runs the team allows to be scored, RS\\text{RS} is the number of runs that the team scores, and Ŵpct\\hat{\\text{W}}_{\\text{pct}} is the team’s expected winning percentage. The runs scored and allowed are present in the Teams table, so we start by selecting them.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben &lt;- Teams |&gt;\n  # select to get the columns you want\n  dplyr::select(teamID, yearID, W, L, R, RA) |&gt;\n  # filter to get the rows you want\n  dplyr::filter(teamID == \"NYN\" & yearID %in% 2004:2012)\n\nThe column name can be changed with the dplyr::rename function (Use new_name = old_name to rename selected variables). Alternatively, you can rename the column directly in the select statement above, like this:\ndplyr::select(teamID,yearID,W,L,RS = R,RA)\n\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::rename(RS = R)    # new name = old name\nmets_ben\n\n  teamID yearID  W  L  RS  RA\n1    NYN   2004 71 91 684 731\n2    NYN   2005 83 79 722 648\n3    NYN   2006 97 65 834 731\n4    NYN   2007 88 74 804 750\n5    NYN   2008 89 73 799 715\n6    NYN   2009 70 92 671 757\n7    NYN   2010 79 83 656 652\n8    NYN   2011 77 85 718 742\n9    NYN   2012 74 88 650 709\n\n\n\n\n\n\nExercise 4\nNext, we need to compute the team’s actual winning percentage in each of these seasons. Thus, we need to add a new column to our data frame, and we do this with the mutate() command.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben &lt;- mets_ben |&gt;\n  # once we have the data, we mutate to add a new value (column), using the formula\n  dplyr::mutate( WPct = 1/(1 + (RA/RS)^2 ) )\nmets_ben\n\n  teamID yearID  W  L  RS  RA      WPct\n1    NYN   2004 71 91 684 731 0.4668211\n2    NYN   2005 83 79 722 648 0.5538575\n3    NYN   2006 97 65 834 731 0.5655308\n4    NYN   2007 88 74 804 750 0.5347071\n5    NYN   2008 89 73 799 715 0.5553119\n6    NYN   2009 70 92 671 757 0.4399936\n7    NYN   2010 79 83 656 652 0.5030581\n8    NYN   2011 77 85 718 742 0.4835661\n9    NYN   2012 74 88 650 709 0.4566674\n\n\n\n\nThe expected number of wins is then equal to the product of the expected winning percentage times the number of games.\n\nmets_ben &lt;- mets_ben |&gt;\n  # once we have calculated the expected winning percentage,\n  # the expected number of wins is the percentage times the total number of games played\n  dplyr::mutate( W_hat = WPct * (W+L) )\nmets_ben\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat\n1    NYN   2004 71 91 684 731 0.4668211 75.62501\n2    NYN   2005 83 79 722 648 0.5538575 89.72491\n3    NYN   2006 97 65 834 731 0.5655308 91.61600\n4    NYN   2007 88 74 804 750 0.5347071 86.62255\n5    NYN   2008 89 73 799 715 0.5553119 89.96053\n6    NYN   2009 70 92 671 757 0.4399936 71.27896\n7    NYN   2010 79 83 656 652 0.5030581 81.49541\n8    NYN   2011 77 85 718 742 0.4835661 78.33771\n9    NYN   2012 74 88 650 709 0.4566674 73.98012\n\n\n\n\nExercise 5\nIn this case, the Mets’ fortunes were better than expected in three of these seasons, and worse than expected in the other six.\nWe can confirm this as follows:\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben %&gt;% \n  # first check that the assertion above is correct\n  dplyr::summarize('better then expected' = sum(W &gt;= W_hat), 'worse than expected' = sum(W &lt; W_hat))\n\n  better then expected worse than expected\n1                    3                   6\n\n\nTo see how the Mets did over all seasons we can repeat our calculation\n\nTeams |&gt;\n  # here we repeat our prior calculation (all steps combined) for all the years in the dataset\n  dplyr::select(teamID, yearID, W, L, RS = R, RA) |&gt;\n  dplyr::filter(teamID == \"NYN\") |&gt;\n  dplyr::mutate( \n    WPct = 1/(1 + (RA/RS)^2 )\n    , W_hat = WPct * (W+L)\n  )  |&gt; \ndplyr::summarize( \n  \"better then expected\" = sum(W &gt;= W_hat)\n  , 'worse than expected' = sum(W &lt; W_hat) \n)\n\n  better then expected worse than expected\n1                   22                  40\n\n\n\n\n\n\nExercise 6\nNaturally, the Mets experienced ups and downs during Ben’s time with the team. Which seasons were best? To figure this out, we can simply sort the rows of the data frame by number of wins.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# for this we just need to sort the number of wins in descending order\nmets_ben |&gt; dplyr::arrange(desc(W))\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat\n1    NYN   2006 97 65 834 731 0.5655308 91.61600\n2    NYN   2008 89 73 799 715 0.5553119 89.96053\n3    NYN   2007 88 74 804 750 0.5347071 86.62255\n4    NYN   2005 83 79 722 648 0.5538575 89.72491\n5    NYN   2010 79 83 656 652 0.5030581 81.49541\n6    NYN   2011 77 85 718 742 0.4835661 78.33771\n7    NYN   2012 74 88 650 709 0.4566674 73.98012\n8    NYN   2004 71 91 684 731 0.4668211 75.62501\n9    NYN   2009 70 92 671 757 0.4399936 71.27896\n\n\n\n\n\n\nExercise 7\nIn 2006, the Mets had the best record in baseball during the regular season and nearly made the World Series. How do these seasons rank in terms of the team’s performance relative to our model?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben %&gt;% \n  # add a column with the difference between wins (W) and expected wins (W_hat)\n  dplyr::mutate(Diff = W - W_hat) |&gt;\n  # then sort the result\n  dplyr::arrange(desc(Diff))\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat        Diff\n1    NYN   2006 97 65 834 731 0.5655308 91.61600  5.38400315\n2    NYN   2007 88 74 804 750 0.5347071 86.62255  1.37744558\n3    NYN   2012 74 88 650 709 0.4566674 73.98012  0.01988152\n4    NYN   2008 89 73 799 715 0.5553119 89.96053 -0.96052803\n5    NYN   2009 70 92 671 757 0.4399936 71.27896 -1.27895513\n6    NYN   2011 77 85 718 742 0.4835661 78.33771 -1.33770571\n7    NYN   2010 79 83 656 652 0.5030581 81.49541 -2.49540821\n8    NYN   2004 71 91 684 731 0.4668211 75.62501 -4.62501135\n9    NYN   2005 83 79 722 648 0.5538575 89.72491 -6.72490937\n\n\nIn the years 2006, 2007 and 2012, the Mets had more wins than expected by the model. In all other seasons they performed worse than predicted by the model.\nWe can summarize the Mets performance as follows:\n\nmets_ben |&gt;\n  dplyr::summarize(\n    num_years = dplyr::n(),  # number of years\n    total_W = sum(W),        # total number of wins\n    total_L = sum(L),        # total number of losses\n    total_WPct = total_W / (total_W + total_L) # win percentage\n  )\n\n  num_years total_W total_L total_WPct\n1         9     728     730  0.4993141\n\n\nIn these nine years, the Mets had a combined record of 728 wins and 730 losses, for an overall winning percentage of 49.93%.\n\n\n\n\nExercise 8\nDiscretize the years into three chunks: one for each of the three general managers under whom Ben worked. Jim Duquette was the Mets’ general manager in 2004, Omar Minaya from 2005 to 2010, and Sandy Alderson from 2011 to 2012.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben %&gt;% \n  # this questions requires a logic for deciding \n  # which years each general manager worked\n  dplyr::mutate(\n    # nested ifelse statements are OK for this logic, \n    # but are only practical for about three cases\n    gm = ifelse(\n      yearID == 2004, \n      'Jim Duquette', \n      ifelse(\n        yearID &gt;= 2011, \n        'Sandy Alderson', \n        'Omar Minaya')\n    )\n  )\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat             gm\n1    NYN   2004 71 91 684 731 0.4668211 75.62501   Jim Duquette\n2    NYN   2005 83 79 722 648 0.5538575 89.72491    Omar Minaya\n3    NYN   2006 97 65 834 731 0.5655308 91.61600    Omar Minaya\n4    NYN   2007 88 74 804 750 0.5347071 86.62255    Omar Minaya\n5    NYN   2008 89 73 799 715 0.5553119 89.96053    Omar Minaya\n6    NYN   2009 70 92 671 757 0.4399936 71.27896    Omar Minaya\n7    NYN   2010 79 83 656 652 0.5030581 81.49541    Omar Minaya\n8    NYN   2011 77 85 718 742 0.4835661 78.33771 Sandy Alderson\n9    NYN   2012 74 88 650 709 0.4566674 73.98012 Sandy Alderson\n\n\nAlternatively, we can use the case_when function\n\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::mutate(\n    # same problem, but case_when is easier to work with\n    gm = dplyr::case_when(\n      yearID == 2004 ~ 'Jim Duquette', \n      yearID &gt;= 2011 ~ 'Sandy Alderson', \n      TRUE ~ 'Omar Minaya' # this is the default case\n    )\n  )\nmets_ben\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat             gm\n1    NYN   2004 71 91 684 731 0.4668211 75.62501   Jim Duquette\n2    NYN   2005 83 79 722 648 0.5538575 89.72491    Omar Minaya\n3    NYN   2006 97 65 834 731 0.5655308 91.61600    Omar Minaya\n4    NYN   2007 88 74 804 750 0.5347071 86.62255    Omar Minaya\n5    NYN   2008 89 73 799 715 0.5553119 89.96053    Omar Minaya\n6    NYN   2009 70 92 671 757 0.4399936 71.27896    Omar Minaya\n7    NYN   2010 79 83 656 652 0.5030581 81.49541    Omar Minaya\n8    NYN   2011 77 85 718 742 0.4835661 78.33771 Sandy Alderson\n9    NYN   2012 74 88 650 709 0.4566674 73.98012 Sandy Alderson\n\n\n\n\n\n\nExercise 9\nThe raw churn data can be transformed into a tidy dataset as follows:\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nset.seed(42)\n\n# read data and drop column 1 (it contains row numbers and doesn't have a column name)\ndf &lt;- readr::read_csv(\"data/monthly_data.csv\", show_col_types = FALSE, col_select = -1)\n\ndf |&gt;\n  # take date columns and pivot to longer table\n  tidyr::pivot_longer(starts_with(\"20\"), names_to = \"date\", values_to = \"quantity\") |&gt; \n  # split the 'date' column into two measurements\n  tidyr::separate_wider_delim(cols = date, delim = \"_\", names = c(\"date\",\"paymentMandate\")) |&gt; \n  # pivot the two columns paymentMandate and quantity to two columns called payment and mandate\n  tidyr::pivot_wider(names_from = paymentMandate, values_from = quantity) |&gt; \n  # finally, mutate the date columns from strings to Dates\n  dplyr::mutate(\n    incorporation_date = as.Date(incorporation_date)\n    , date = as.Date(date)\n  )\n\n# A tibble: 10,824 × 6\n   company_id vertical    incorporation_date date       payments mandates\n        &lt;dbl&gt; &lt;chr&gt;       &lt;date&gt;             &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1          1 gym/fitness 2013-05-30         2014-01-01        0        1\n 2          1 gym/fitness 2013-05-30         2014-02-01        0        2\n 3          1 gym/fitness 2013-05-30         2014-03-01        0        2\n 4          1 gym/fitness 2013-05-30         2014-04-01        1        1\n 5          1 gym/fitness 2013-05-30         2014-05-01        0        0\n 6          1 gym/fitness 2013-05-30         2014-06-01        1        0\n 7          1 gym/fitness 2013-05-30         2014-07-01        0        0\n 8          1 gym/fitness 2013-05-30         2014-08-01        0        0\n 9          1 gym/fitness 2013-05-30         2014-09-01        0        0\n10          1 gym/fitness 2013-05-30         2014-10-01        0        0\n# ℹ 10,814 more rows\n\n\n\n\n\n\nExercise 10\nUse the gm function to define the manager groups with the group_by() operator, and run the summaries again, this time across the manager groups.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n#| label: read the data\ndata &lt;- readr::read_csv(\"data/sales_dag.csv\", show_col_types = FALSE)\n\ndata |&gt; dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"sample marketing data\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nsample marketing data\n\n\nvisits\ndiscount\nis_loyal\nsales\nsales_per_visit\n\n\n\n\n12\n0\n0\n13.34830\n1.1123585\n\n\n26\n1\n1\n21.70125\n0.8346635\n\n\n13\n0\n0\n14.70040\n1.1308004\n\n\n24\n0\n0\n20.37734\n0.8490557\n\n\n14\n0\n0\n12.63372\n0.9024089\n\n\n\n\n\n\n\n\ndata |&gt; skimr::skim()\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n700\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nvisits\n0\n1\n21.19\n5.19\n3.00\n17.00\n21.00\n25.00\n40.00\n▁▃▇▃▁\n\n\ndiscount\n0\n1\n0.74\n0.44\n0.00\n0.00\n1.00\n1.00\n1.00\n▃▁▁▁▇\n\n\nis_loyal\n0\n1\n0.75\n0.43\n0.00\n0.75\n1.00\n1.00\n1.00\n▂▁▁▁▇\n\n\nsales\n0\n1\n19.40\n4.88\n2.09\n16.44\n19.50\n22.50\n36.14\n▁▃▇▃▁\n\n\nsales_per_visit\n0\n1\n0.92\n0.11\n0.42\n0.85\n0.91\n0.98\n1.41\n▁▂▇▂▁\n\n\n\n\n\nThe mean of the sales_per_visit columns/measurement is 0.9183 and there are no grouped observations.\n\n# calculate the % share of customers receiving a discount vs the % not receiving a discount\ndata$discount |&gt; table() / length(data$discount)\n\n\n   0    1 \n0.26 0.74 \n\n\nSimilarly for the share of customers which are loyal:\n\n# calculate the % share of customers that are 'loyal' vs not 'loyal'\ndata$is_loyal |&gt; table() / length(data$is_loyal)\n\n\n   0    1 \n0.25 0.75 \n\n\nTo understand these features better, they also looked at a cross-tab table:\n\n# build a cross-tab table of 'loyal' customers vs customers getting a discount\ndata |&gt; xtabs(~discount + is_loyal, data = _)\n\n        is_loyal\ndiscount   0   1\n       0 175   7\n       1   0 518\n\n\n\n\nAlternatively:\n\ndata |&gt; \n  dplyr::group_by(discount,is_loyal) |&gt; \n  dplyr::summarize(n = dplyr::n(), .groups='drop') |&gt; \n  tidyr::pivot_wider(\n    names_from = is_loyal\n    , values_from = n\n    , names_prefix = 'is_loyal '\n  ) |&gt; gt::gt() |&gt; \n  gt::tab_header(title = \"Cross-tabs\", subtitle = \"discount vs is_loyal\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nCross-tabs\n\n\ndiscount vs is_loyal\n\n\ndiscount\nis_loyal 0\nis_loyal 1\n\n\n\n\n0\n175\n7\n\n\n1\nNA\n518\n\n\n\n\n\n\ndata |&gt; dplyr::mutate(id = dplyr::row_number(), .before = 1) |&gt; \n  dplyr::filter(discount == 0) |&gt; \n  dplyr::arrange( desc(sales) ) |&gt; \n  dplyr::slice_head(n=10) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"Sales: loyal customers vs others\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nSales: loyal customers vs others\n\n\nid\nvisits\ndiscount\nis_loyal\nsales\nsales_per_visit\n\n\n\n\n567\n33\n0\n1\n31.72169\n0.9612633\n\n\n205\n33\n0\n1\n28.31111\n0.8579125\n\n\n366\n33\n0\n1\n28.08163\n0.8509586\n\n\n50\n33\n0\n1\n27.79064\n0.8421407\n\n\n281\n29\n0\n1\n27.58115\n0.9510740\n\n\n546\n27\n0\n1\n26.25533\n0.9724196\n\n\n105\n29\n0\n1\n26.21752\n0.9040526\n\n\n362\n28\n0\n0\n24.09410\n0.8605037\n\n\n652\n28\n0\n0\n24.06459\n0.8594495\n\n\n494\n27\n0\n0\n24.00630\n0.8891220\n\n\n\n\n\n\n\nThe loyal customers are the top ones in terms of sales. This is good news. It means that the definition of loyal customers is consistent with the data.\nIn order to have orders of magnitude for the sales, the data scientist provided some summary statistics table:\n\ngtExtras::gt_plt_summary(data, title = \"Sales data\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales data\n\n\n700 rows x 5 cols\n\n\n\nColumn\nPlot Overview\nMissing\nMean\nMedian\nSD\n\n\n\n\n\n\n\nvisits\n\n\n\n      340\n\n0.0%\n21.2\n21.0\n5.2\n\n\n\n\n\ndiscount\n\n\n\n      01\n\n0.0%\n0.7\n1.0\n0.4\n\n\n\n\n\nis_loyal\n\n\n\n      01\n\n0.0%\n0.8\n1.0\n0.4\n\n\n\n\n\nsales\n\n\n\n      236\n\n0.0%\n19.4\n19.5\n4.9\n\n\n\n\n\nsales_per_visit\n\n\n\n      0.421.41\n\n0.0%\n0.9\n0.9\n0.1\n\n\n\n\n\n\n\nTo have a better glimpse of the data, the data scientist also provided a histogram of the sales:\n\ndata |&gt; \n  ggplot(aes(x=sales)) +\n  geom_histogram(aes(y = ..density..), bins = 30, colour = 1, fill = \"white\") +\n  geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25) +\n  labs(title = \"Sales Distribution\") +\n  theme_minimal()"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_1_solutions.html#resources-for-additional-practice-optional",
    "href": "labs/solutions/BSMM_8740_lab_1_solutions.html#resources-for-additional-practice-optional",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Resources for additional practice (optional)",
    "text": "Resources for additional practice (optional)\n\nChapter 2: Get Started Data Visualization by Kieran Healy\nChapter 3: Data visualization in R for Data Science by Hadley Wickham\nRStudio Cloud Primers\n\nVisualization Basics: https://rstudio.cloud/learn/primers/1.1\nWork with Data: https://rstudio.cloud/learn/primers/2\nVisualize Data: https://rstudio.cloud/learn/primers/3"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html",
    "title": "Lab 2 - The Recipes package",
    "section": "",
    "text": "We will use the following package in this lab.\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2\n)\n\ntheme_set(theme_bw(base_size = 12))\nboston_cocktails &lt;- readr::read_csv('data/boston_cocktails.csv', show_col_types = FALSE)\n\n\n\n\n\n\n\nTip\n\n\n\nI like to use the gt:: and gtExtras:: packages to format my tables. The results can be saved as an image and are especially useful for inserting into PowerPoint decks.\nYou’ll see examples thoughout the course and I encourage you to examine the examples and use gt:: and gtExtras:: in your own work."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html#packages",
    "title": "Lab 2 - The Recipes package",
    "section": "",
    "text": "We will use the following package in this lab.\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2\n)\n\ntheme_set(theme_bw(base_size = 12))\nboston_cocktails &lt;- readr::read_csv('data/boston_cocktails.csv', show_col_types = FALSE)\n\n\n\n\n\n\n\nTip\n\n\n\nI like to use the gt:: and gtExtras:: packages to format my tables. The results can be saved as an image and are especially useful for inserting into PowerPoint decks.\nYou’ll see examples thoughout the course and I encourage you to examine the examples and use gt:: and gtExtras:: in your own work."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html#data-the-boston-cocktail-recipes",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html#data-the-boston-cocktail-recipes",
    "title": "Lab 2 - The Recipes package",
    "section": "Data: The Boston Cocktail Recipes",
    "text": "Data: The Boston Cocktail Recipes\nThe Boston Cocktail Recipes dataset appeared in a TidyTuesday posting. TidyTuesday is a weekly data project in R.\nThe dataset is derived from the Mr. Boston Bartender’s Guide, together with a dataset that was web-scraped as part of a hackathon.\nThis dataset contains the following information for each cocktail:\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of cocktail\n\n\ncategory\ncharacter\nCategory of cocktail\n\n\nrow_id\ninteger\nDrink identifier\n\n\ningredient_number\ninteger\nIngredient number\n\n\ningredient\ncharacter\nIngredient\n\n\nmeasure\ncharacter\nMeasurement/volume of ingredient\n\n\nmeasure_number\nreal\nmeasure as a number"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html#exercises",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html#exercises",
    "title": "Lab 2 - The Recipes package",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nFirst use skimr::skim and DataExplorer::introduce to assess the quality of the data set.\nNext prepare a summary. What is the median measure number across cocktail recipes?\n\n\n\n\n\n\nSOLUTION:\n\n\n\nboston_cocktails %&gt;% skimr::skim()\n\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2542\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n4\n36\n0\n937\n0\n\n\ncategory\n0\n1\n3\n21\n0\n11\n0\n\n\ningredient\n0\n1\n3\n23\n0\n40\n0\n\n\nmeasure\n0\n1\n1\n8\n0\n28\n0\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrow_id\n0\n1\n495.37\n284.75\n1.00\n261.25\n497\n730.75\n990\n▇▇▇▇▇\n\n\ningredient_number\n0\n1\n2.54\n1.29\n1.00\n1.00\n2\n3.00\n6\n▇▃▂▁▁\n\n\nmeasure_number\n0\n1\n0.98\n0.72\n0.02\n0.50\n1\n1.50\n16\n▇▁▁▁▁\n\n\n\n\n\n\n\nboston_cocktails %&gt;% summary()\n\n     name             category             row_id      ingredient_number\n Length:2542        Length:2542        Min.   :  1.0   Min.   :1.000    \n Class :character   Class :character   1st Qu.:261.2   1st Qu.:1.000    \n Mode  :character   Mode  :character   Median :497.0   Median :2.000    \n                                       Mean   :495.4   Mean   :2.545    \n                                       3rd Qu.:730.8   3rd Qu.:3.000    \n                                       Max.   :990.0   Max.   :6.000    \n  ingredient          measure          measure_number   \n Length:2542        Length:2542        Min.   : 0.0200  \n Class :character   Class :character   1st Qu.: 0.5000  \n Mode  :character   Mode  :character   Median : 1.0000  \n                                       Mean   : 0.9797  \n                                       3rd Qu.: 1.5000  \n                                       Max.   :16.0000  \n\n\nThe median measure is 1.0. Note that the dimensions are identified as ounces (oz) in the measure column.\n\n\n\n\nExercise 2\nFrom the boston_cocktails dataset select the name, category, ingredient, and measure_number columns and then pivot the table to create a column for each ingredient. Fill any missing values with the number zero.\nSince the names of the new columns may contain spaces, clean them using the janitor::clean_names(). Finally drop any rows with NA values and save this new dataset in a variable.\nHow much gin is in the cocktail called Leap Frog Highball?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ncocktails_df &lt;- boston_cocktails %&gt;%\n  # select the columns (by de-selecting the ones we don't want)\n  dplyr::select(-ingredient_number, -row_id, -measure) %&gt;%\n  # pivot wider (make more columns); use zeros in place of NA values\n  tidyr::pivot_wider(\n    names_from = ingredient\n    , values_from = measure_number\n    , values_fill = 0\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  tidyr::drop_na()\n# show the table in the document\ncocktails_df\n\n# A tibble: 937 × 42\n   name    category light_rum lemon_juice lime_juice sweet_vermouth orange_juice\n   &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1 Gauguin Cocktai…      2           1          1               0           0   \n 2 Fort L… Cocktai…      1.5         0          0.25            0.5         0.25\n 3 Cuban … Cocktai…      2           0          0.5             0           0   \n 4 Cool C… Cocktai…      0           0          0               0           1   \n 5 John C… Whiskies      0           1          0               0           0   \n 6 Cherry… Cocktai…      1.25        0          0               0           0   \n 7 Casa B… Cocktai…      2           0          1.5             0           0   \n 8 Caribb… Cocktai…      0.5         0          0               0           0   \n 9 Amber … Cordial…      0           0.25       0               0           0   \n10 The Jo… Whiskies      0           0.5        0               0           0   \n# ℹ 927 more rows\n# ℹ 35 more variables: powdered_sugar &lt;dbl&gt;, dark_rum &lt;dbl&gt;,\n#   cranberry_juice &lt;dbl&gt;, pineapple_juice &lt;dbl&gt;, bourbon_whiskey &lt;dbl&gt;,\n#   simple_syrup &lt;dbl&gt;, cherry_flavored_brandy &lt;dbl&gt;, light_cream &lt;dbl&gt;,\n#   triple_sec &lt;dbl&gt;, maraschino &lt;dbl&gt;, amaretto &lt;dbl&gt;, grenadine &lt;dbl&gt;,\n#   apple_brandy &lt;dbl&gt;, brandy &lt;dbl&gt;, gin &lt;dbl&gt;, anisette &lt;dbl&gt;,\n#   dry_vermouth &lt;dbl&gt;, apricot_flavored_brandy &lt;dbl&gt;, bitters &lt;dbl&gt;, …\n\n\n\ncocktails_df %&gt;% \n  # filter for the desired cocktail\n  dplyr::filter(name == 'Leap Frog Highball') %&gt;% \n  dplyr::pull(gin)\n\n[1] 2\n\n\nTwo ounces (oz) of gin are in the Leap Frog Highball.\n\n\n\n\nExercise 3\nPrepare a recipes::recipe object without a target but give name and category as ‘id’ roles. Add steps to normalize the predictors and perform PCA. Finally prep the data and save it in a variable.\nHow many predictor variables are prepped by the recipe?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create a recipe: y~. with an outcome/target, but here we just use ~.\npca_rec &lt;- recipes::recipe(~., data = cocktails_df) \npca_rec %&gt;% summary()\n\n# A tibble: 42 × 4\n   variable        type      role      source  \n   &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 name            &lt;chr [3]&gt; predictor original\n 2 category        &lt;chr [3]&gt; predictor original\n 3 light_rum       &lt;chr [2]&gt; predictor original\n 4 lemon_juice     &lt;chr [2]&gt; predictor original\n 5 lime_juice      &lt;chr [2]&gt; predictor original\n 6 sweet_vermouth  &lt;chr [2]&gt; predictor original\n 7 orange_juice    &lt;chr [2]&gt; predictor original\n 8 powdered_sugar  &lt;chr [2]&gt; predictor original\n 9 dark_rum        &lt;chr [2]&gt; predictor original\n10 cranberry_juice &lt;chr [2]&gt; predictor original\n# ℹ 32 more rows\n\n\n\npca_rec &lt;- pca_rec %&gt;% \n  # change the roles of name and category to 'id' from 'predictor'\n  recipes::update_role(name, category, new_role = \"id\") %&gt;%\n  # normalize the remaining predictors\n  recipes::step_normalize(all_predictors()) %&gt;%\n  # convert the predictors to principle components\n  recipes::step_pca(all_predictors())\n\n# note there are 40 predictors, but that nothing has been calculated yet\npca_rec %&gt;% summary()\n\n# A tibble: 42 × 4\n   variable        type      role      source  \n   &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 name            &lt;chr [3]&gt; id        original\n 2 category        &lt;chr [3]&gt; id        original\n 3 light_rum       &lt;chr [2]&gt; predictor original\n 4 lemon_juice     &lt;chr [2]&gt; predictor original\n 5 lime_juice      &lt;chr [2]&gt; predictor original\n 6 sweet_vermouth  &lt;chr [2]&gt; predictor original\n 7 orange_juice    &lt;chr [2]&gt; predictor original\n 8 powdered_sugar  &lt;chr [2]&gt; predictor original\n 9 dark_rum        &lt;chr [2]&gt; predictor original\n10 cranberry_juice &lt;chr [2]&gt; predictor original\n# ℹ 32 more rows\n\n\n\n# calculate prepare the data per the steps in the recipe\npca_prep &lt;- recipes::prep(pca_rec)\npca_prep %&gt;% summary\n\n# A tibble: 7 × 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 name     &lt;chr [3]&gt; id        original\n2 category &lt;chr [3]&gt; id        original\n3 PC1      &lt;chr [2]&gt; predictor derived \n4 PC2      &lt;chr [2]&gt; predictor derived \n5 PC3      &lt;chr [2]&gt; predictor derived \n6 PC4      &lt;chr [2]&gt; predictor derived \n7 PC5      &lt;chr [2]&gt; predictor derived \n\n\n\nThere are 40 predictors and 2 id variables before the data is prepped.\nOnce prepped, the PCA returns just 5 components by default, so we have (post-prep) 5 predictors and 2 id variables.\n\n\n\n\n\nExercise 4\nApply the recipes::tidy verb to the prepped recipe in the last exercise. The result is a table identifying the information generated and stored by each step in the recipe from the input data.\nTo see the values calculated for normalization, apply the recipes::tidy verb as before, but with second argument = 1.\nWhat ingredient is the most used, on average?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# tidy returns a tibble with the calculations performed by prep\npca_prep %&gt;% recipes::tidy()\n\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_h2FSo\n2      2 step      pca       TRUE    FALSE pca_J9YsH      \n\n\n\n# if we select the first (normalization) step we get the values calculated:\n# - the mean and standard deviation for each variable\nfoo &lt;- pca_prep %&gt;% recipes::tidy(1)\nfoo\n\n# A tibble: 80 × 4\n   terms           statistic  value id             \n   &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;          \n 1 light_rum       mean      0.161  normalize_h2FSo\n 2 lemon_juice     mean      0.229  normalize_h2FSo\n 3 lime_juice      mean      0.138  normalize_h2FSo\n 4 sweet_vermouth  mean      0.0691 normalize_h2FSo\n 5 orange_juice    mean      0.185  normalize_h2FSo\n 6 powdered_sugar  mean      0.0891 normalize_h2FSo\n 7 dark_rum        mean      0.0454 normalize_h2FSo\n 8 cranberry_juice mean      0.0363 normalize_h2FSo\n 9 pineapple_juice mean      0.0608 normalize_h2FSo\n10 bourbon_whiskey mean      0.0768 normalize_h2FSo\n# ℹ 70 more rows\n\n\n\n# we can just filter to find the largest mean value\n# first, isolate the mean values\nfoo %&gt;% dplyr::filter(statistic == 'mean') %&gt;% \n  # once we have just the mean values, filter out the row with the max value \n  dplyr::filter(value == max(value))\n\n# A tibble: 1 × 4\n  terms statistic value id             \n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;          \n1 gin   mean      0.252 normalize_h2FSo\n\n\nOn average, it is gin that is the largest component of the cocktails, with just over 1/4 oz per cocktail.\n\n\n\n\nExercise 5\nNow look at the result of the PCA, applying the recipes::tidy verb as before, but with second argument = 2. Save the result in a variable and filter for the components PC1 to PC5. Mutate the resulting component column so that the values are factors, ordering them in the order they appear using the forcats::fct_inorder verb.\nPlot this data using ggplot2 and the code below\n\nggplot(aes(value, terms, fill = terms)) +\ngeom_col(show.legend = FALSE) +\nfacet_wrap(~component, nrow = 1) +\nlabs(y = NULL) +\ntheme(axis.text=element_text(size=7),\n      axis.title=element_text(size=14,face=\"bold\"))\n\nHow would you describe the drinks represented by PC1?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# the tidy operation shows the weights of each ingredient, \n# for each principal component.\n# - i.e. PC1 (along with PC2 - PC5) is a weighted sum of ingredients\nbar &lt;- pca_prep %&gt;% recipes::tidy(2)\nbar\n\n# A tibble: 1,600 × 4\n   terms             value component id       \n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 light_rum        0.163  PC1       pca_J9YsH\n 2 lemon_juice     -0.0140 PC1       pca_J9YsH\n 3 lime_juice       0.224  PC1       pca_J9YsH\n 4 sweet_vermouth  -0.0661 PC1       pca_J9YsH\n 5 orange_juice     0.0308 PC1       pca_J9YsH\n 6 powdered_sugar  -0.476  PC1       pca_J9YsH\n 7 dark_rum         0.124  PC1       pca_J9YsH\n 8 cranberry_juice  0.0954 PC1       pca_J9YsH\n 9 pineapple_juice  0.119  PC1       pca_J9YsH\n10 bourbon_whiskey  0.0963 PC1       pca_J9YsH\n# ℹ 1,590 more rows\n\n\n\n# plot to show the ingredient weights\nbar %&gt;%\n  # since there are only 5 components, this is redundant\n  dplyr::filter(component %in% paste0(\"PC\", 1:5)) %&gt;%\n  # change component from a character to a factor, and give them an order\n  dplyr::mutate(component = forcats::fct_inorder(component)) %&gt;%\n  # plot\n  ggplot(aes(value, terms, fill = terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~component, nrow = 1) +\n  labs(y = NULL) +\n  theme(axis.text=element_text(size=7),\n        axis.title=element_text(size=14,face=\"bold\"))\n\n\n\n\n\n\n\n\nBased on the its largest components (in absolute value) PC1 is a syrupy (not sugary) drink, containing tequila and lime juice but without egg products.\n\n\n\n\nExercise 6\nAs in the last exercise, use the variable with the tidied PCA data and use only PCA components PC1 to PC4. Take/slice the top 8 ingedients by component, ordered by their absolute value using the verb dplyr::slice_max. Next, generate a grouped table using gt::gt, colouring the cell backgrounds (i.e. fill) with green for values ≥0\\ge0 and red for values &lt;0&lt;0.\nWhat is the characteristic alcoholic beverage of each of the first 4 principle components.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nbar %&gt;%\n  # filter our the rows for PC1 - PC4\n  dplyr::filter(component %in% paste0(\"PC\", 1:4)) %&gt;%\n  # group by component (i.e. principal component) \n  dplyr::group_by(component) %&gt;%\n  # for each group, take the top 8 ingredients by absolute value\n  dplyr::slice_max(n = 8, order_by = abs(value)) %&gt;% \n  # now make a nicely formatted table\n  gt::gt() %&gt;% \n  # make/apply a table style: this one for values &lt; 0\n  gt::tab_style(\n    style = list(\n      gt::cell_fill(color = \"red\"),\n      gt::cell_text(weight = \"bold\")\n      ),\n    locations = gt::cells_body(\n      columns = value,\n      rows = value &lt; 0\n    )\n  ) %&gt;% \n  # make/apply another table style: this one for values &gt;= 0\n    gt::tab_style(\n    style = list(\n      gt::cell_fill(color = \"green\"),\n      gt::cell_text(weight = \"bold\")\n      ),\n    locations = gt::cells_body(\n      columns = value,\n      rows = value &gt;= 0\n    )\n  ) %&gt;% \n  # apply a theme; any theme will do\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nterms\nvalue\nid\n\n\n\n\nPC1\n\n\npowdered_sugar\n-0.4764442\npca_J9YsH\n\n\nsimple_syrup\n0.3125978\npca_J9YsH\n\n\nwhole_egg\n-0.2903794\npca_J9YsH\n\n\negg_white\n-0.2643614\npca_J9YsH\n\n\ngin\n-0.2588481\npca_J9YsH\n\n\nport\n-0.2354010\npca_J9YsH\n\n\nlime_juice\n0.2240295\npca_J9YsH\n\n\nblanco_tequila\n0.2025436\npca_J9YsH\n\n\nPC2\n\n\ndry_vermouth\n0.4332412\npca_J9YsH\n\n\nsweet_vermouth\n0.3754366\npca_J9YsH\n\n\npowdered_sugar\n-0.3273723\npca_J9YsH\n\n\nlemon_juice\n-0.2651836\npca_J9YsH\n\n\nsimple_syrup\n-0.2512150\npca_J9YsH\n\n\ngin\n0.2437374\npca_J9YsH\n\n\nwhole_egg\n-0.2191547\npca_J9YsH\n\n\nport\n-0.1763370\npca_J9YsH\n\n\nPC3\n\n\ngin\n0.3780348\npca_J9YsH\n\n\negg_white\n0.3094631\npca_J9YsH\n\n\nbenedictine\n-0.2864463\npca_J9YsH\n\n\nwhole_egg\n-0.2825812\npca_J9YsH\n\n\nblended_scotch_whiskey\n-0.2352922\npca_J9YsH\n\n\nlemon_juice\n0.2318927\npca_J9YsH\n\n\nvodka\n-0.2167814\npca_J9YsH\n\n\napricot_flavored_brandy\n0.2134899\npca_J9YsH\n\n\nPC4\n\n\ngrenadine\n0.4068737\npca_J9YsH\n\n\norange_juice\n0.3625622\npca_J9YsH\n\n\nvodka\n0.3008406\npca_J9YsH\n\n\nsimple_syrup\n-0.2533007\npca_J9YsH\n\n\nhalf_and_half\n0.2493552\npca_J9YsH\n\n\negg_white\n0.2465323\npca_J9YsH\n\n\ncranberry_juice\n0.2228099\npca_J9YsH\n\n\nwhite_creme_de_cacao\n0.2089063\npca_J9YsH\n\n\n\n\n\n\n\nPrincipal components and similar methods are very useful in reducing the complexity of our models. In this case we reduced the 40 original predictors to just 5 predictors.\nThe challenge with using these methods is attaching meaning to the revised predictors, and this is important when we need to explain our models. The computer can compute the new predictors but they can’t tell us what they represent. For this we need to look at the structure of the new predictors and see if we can attach some meaning to them; often the solution is to give them names that capture the underlying structure.\nIn this case, looking at the ingredients that make up the PCA predictors\n\nPC1 represents a drink with:\n\nlittle or no sugar, egg, gin or port; some or a lot of syrup and citrus juice\nmost often / mainly tequila\n\nPC2 represents a drink with:\n\nlittle or no sugar, syrup, or citrus juice\nmost often / mainly vermouth\n\nPC3 represents a drink with:\n\nlittle or no egg, whiskey or vodka\nmost often / mainly gin\n\nPC4 represents a drink with\n\nlittle or no syrup; some or a lot of juice and dairy product\nmost often / mainly grenadine and vodka\n\n\n\n\n\n\nExercise 7\nFor this exercise, bake the prepped PCA recipe using recipes::bake on the original data and plot each cocktail by its PC1, PC2 component, using\n\nggplot(aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  geom_text(check_overlap = TRUE, hjust = \"inward\") + \n  labs(color = NULL)\n\nCan you create an interpretation of the PCA analysis?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# bake the dataset cocktails_df, creating a new dataset\nrecipes::bake(pca_prep, new_data = cocktails_df) %&gt;%\n  ggplot(aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  geom_text(check_overlap = TRUE, hjust = \"inward\") + \n  labs(color = NULL)\n\n\n\n\n\n\n\n\nIn this exercise we are plotting the cocktails against the first two principal components and trying to interpret the results.\nThe interpretation is more difficult now than it was in the last exercise where we found an interpretation for each principal component. Now we are looking at each cocktail in terms of combinations of PC1 and PC2, both positive and negative.\nIt appears that the lower left quadrant (negative PC1 and PC2 values) are mainly cocktail classics. If you look at the components of PC1 and PC2 from the last exercise and negate them, you can describe the cocktails in this quadrant: they have egg /egg-white, port, sugar, no vermouth or gin or tequila.\nThe lower right quadrant is positive PC1 and negative PC2. PC1 contains juice and syrup, while PC2 is negative juice and syrup - so in this quadrant we have cocktails that are like PC1 (juice and syrup) and unlike PC2 (juice and syrup & sugar). The cocktails in this quadrant have citrus juice, are sweet from the use of syrup & sugar and likely have tequila.\nThe top half of the plot shows cocktails clustered along the PC1=0 axis, so those are mainly tequila cocktails, very much like PC2.\nIs there an opportunity to create a new cocktail in the upper left or upper right quadrants?\n\n\n\n\nExercise 8\nIn the following exercise, we’ll use the recipes package to prepare time series data. The starting dataset contains monthly house price data for each of the four countries/regions in the UK\n\nuk_prices &lt;- readr::read_csv('data/UK_house_prices.csv', show_col_types = FALSE)\n\nWrite code to clean the names in the uk_prices dataset using janitor::clean_names(), and then using skimr::skim confirm that the region names are correct and that there are no missing values. Call the resulting analytic data set df.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# PLEASE SHOW YOUR WORK\ndf &lt;- uk_prices |&gt; janitor::clean_names()\nunique(df$region_name)\n\n[1] \"England\"          \"Northern Ireland\" \"Scotland\"         \"Wales\"           \n\n\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n888\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nregion_name\n0\n1\n5\n16\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2005-01-01\n2023-06-01\n2014-03-16\n222\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales_volume\n8\n0.99\n20490.35\n29647.56\n665\n2517.5\n4984.5\n18286.5\n145089\n▇▁▂▁▁\n\n\n\n\n\n\n\n\n\nExercise 9\nWe want to use the monthly house price data to predict house prices one month ahead for each region. The basic model will be:\nSalesVolume ~ .\nwhere the date and region-name are id variables, not predictor variables. Instead we will use the prior-month lagged prices as the only predictor.\nThe recipe uses the following 5 steps from the recipes package: update_role(region_name, new_role = “id”) | recipe(sales_volume ~ ., data = df |&gt; janitor::clean_names()) | step_naomit(lag_1_sales_volume, skip=FALSE) | step_lag(sales_volume, lag=1) | step_arrange(region_name, date)\nUse these 5 steps in the proper order to create a recipe to pre-process the time series data for each region. Prep and then bake your recipe using df.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# PLEASE SHOW YOUR WORK\ndf_rec &lt;- df |&gt; recipes::recipe(sales_volume ~ .) |&gt; \n  recipes::update_role(region_name, new_role = \"id\") |&gt; \n  recipes::step_lag(sales_volume, lag=1) |&gt; \n  recipes::step_naomit(lag_1_sales_volume, skip=FALSE) |&gt; \n  recipes::step_arrange(region_name, date)\n\ndf_rec |&gt; \n  recipes::prep() |&gt; \n  recipes::bake(new_data = NULL) |&gt; \n  dplyr::filter(date &lt;= lubridate::ymd(20050301)) \n\n# A tibble: 8 × 4\n  date       region_name      sales_volume lag_1_sales_volume\n  &lt;date&gt;     &lt;fct&gt;                   &lt;dbl&gt;              &lt;dbl&gt;\n1 2005-02-01 England                56044              53464 \n2 2005-03-01 England                67322              56044 \n3 2005-02-01 Northern Ireland         978.               978.\n4 2005-03-01 Northern Ireland         978.               978.\n5 2005-02-01 Scotland                7631               8876 \n6 2005-03-01 Scotland                9661               7631 \n7 2005-02-01 Wales                   2572               2516 \n8 2005-03-01 Wales                   3336               2572 \n\n\n\n\nThe result of the bake step, after filtering for dates ≤\\le 2005-03-01, should look like this:\n\nreadRDS(\"data/baked_uk_house_dat.rds\") |&gt; \n  dplyr::filter(date &lt;= lubridate::ymd(20050301)) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_currency(columns = -c(date,region_name), decimals = 0) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\ndate\nregion_name\nsales_volume\nlag_1_sales_volume\n\n\n\n\n2005-02-01\nEngland\n$56,044\n$53,464\n\n\n2005-03-01\nEngland\n$67,322\n$56,044\n\n\n2005-02-01\nNorthern Ireland\n$978\n$978\n\n\n2005-03-01\nNorthern Ireland\n$978\n$978\n\n\n2005-02-01\nScotland\n$7,631\n$8,876\n\n\n2005-03-01\nScotland\n$9,661\n$7,631\n\n\n2005-02-01\nWales\n$2,572\n$2,516\n\n\n2005-03-01\nWales\n$3,336\n$2,572\n\n\n\n\n\n\n\n\n\nExercise 10\n\nRecall The Business Problem\nWe’re at a fast paced startup. The company is growing fast and the marketing team is looking for ways to increase the sales from existing customers by making them buy more. The main idea is to unlock the potential of the customer base through incentives, in this case a discount. We of course want to measure the effect of the discount on the customer’s behavior. Still, they do not want to waste money giving discounts to users which are not valuable. As always, it is about return on investment (ROI).\nWithout going into specifics about the nature of the discount, it has been designed to provide a positive return on investment if the customer buys more than $1\\$ 1 as a result of the discount. How can we measure the effect of the discount and make sure our experiment has a positive ROI? The marketing team came up with the following strategy:\n\nSelect a sample of existing customers from the same cohort.\nSet a test window of 1 month.\nLook into the historical data of web visits from the last month. The hypothesis is that web visits are a good proxy for the customer’s interest in the product.\nFor customers with a high number of web visits, send them a discount. There will be a hold out group which will not receive the discount within the potential valuable customers based on the number of web visits. For customers with a low number of web visits, do not send them a discount (the marketing team wants to report a positive ROI, so they do not want to waste money on customers which are not valuable). Still, they want to use them to measure the effect of the discount.\nWe also want to use the results of the test to tag loyal customers. These are customers which got a discount (since they showed potential interest in the product) and customers with exceptional sales numbers even if they did not get a discount. The idea is to use this information to target them in the future if the discount strategy is positive.\n\nIn the last lab we did some exploratory data analysis. The next step is to prepare some descriptive statistics.\n\nDescriptive Statistics\nThe first thing the data analytics team did was to split the sales distribution by discount group:\n\ndata &lt;- readr::read_csv('data/sales_dag.csv', show_col_types = FALSE)\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x = sales, after_stat(count), fill = discount)) +\n  geom_histogram(alpha = 0.30, position = 'identity', color=\"#e9ecef\", bins = 30)+\n  geom_density(alpha = 0.30) +\n  xlab(\"Sales\") +\n  ylab(\"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks customers with a discount have higher sales. Data scientist A is optimistic with this initial result. To quantify this, they computed the difference in means:\n\n\n\n\n\n\nSOLUTION:\n\n\n\ndifference in means:\n\nmean_sales &lt;- data |&gt; dplyr::group_by(discount) |&gt; \n  dplyr::summarize(\"mean sales\" = mean(sales)) |&gt; \n  dplyr::mutate(\"mean sales difference\" = `mean sales` - lag(`mean sales`))\nmean_sales\n\n# A tibble: 2 × 3\n  discount `mean sales` `mean sales difference`\n     &lt;dbl&gt;        &lt;dbl&gt;                   &lt;dbl&gt;\n1        0         15.8                   NA   \n2        1         20.6                    4.80\n\n\n\n\nOur calculation gives a $4.8\\$ 4.8 mean uplift! This is great news. The discount strategy seems to be working. Data scientist A is happy with the results and decides to get feedback from the rest of the data science team.\nData scientist B is not so happy with the results. They think that the uplift is too good to be true (based on domain knowledge and the sales distributions 🤔). When thinking about reasons for such a high uplift, they realized the discount assignment was not at random. It was based on the number of web visits (remember the marketing plan?). This means that the discount group is not comparable to the control group completely! They decide to plot sales against web visits per discount group:\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x=visits, y = sales, color = discount)) +\n  geom_point() + \n  facet_grid(cols = vars(discount))\n\n\n\n\n\n\n\n\nIndeed, they realize they should probably adjust for the number of web visits. A natural metric is sales per web visit. Let’s compute it:\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmean_sales_pv &lt;- data |&gt; dplyr::group_by(discount) |&gt; \n  dplyr::summarize(\"sales_per_visit\" = mean(sales_per_visit)) \nmean_sales_pv\n\n# A tibble: 2 × 2\n  discount sales_per_visit\n     &lt;dbl&gt;           &lt;dbl&gt;\n1        0           0.861\n2        1           0.938\n\n\n\n\nThe mean value is higher for the discount group. As always, they also looked at the distributions:\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x = sales_per_visit, after_stat(count), fill = discount)) +\n  geom_histogram(alpha = 0.30, position = 'identity', color=\"#e9ecef\", bins = 30)+\n  # geom_density(alpha = 0.30) +\n  xlab(\"Sales per Visit\") +\n  ylab(\"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor both data scientists A & B the results look much better, but they were unsure about which uplift to report. They thought about the difference in means:\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmean_sales_per_visit &lt;- data |&gt; dplyr::group_by(discount) |&gt; \n  dplyr::summarize(\"mean sales per visit\" = mean(sales_per_visit)) |&gt; \n  dplyr::mutate(\"mean sales difference\" = `mean sales per visit` - dplyr::lag(`mean sales per visit`))\n\nmean_sales_per_visit |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"Mean sales per visit\") |&gt; \n  gt::fmt_number(columns = `mean sales difference`, decimals = 5) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nMean sales per visit\n\n\ndiscount\nmean sales per visit\nmean sales difference\n\n\n\n\n0\n0.8612426\nNA\n\n\n1\n0.9382929\n0.07705\n\n\n\n\n\n\n\n\n\nHowever, how to interpret this value in terms of dollars? To be continued …"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html#grading",
    "title": "Lab 2 - The Recipes package",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html",
    "href": "labs/BSMM_8740_lab_4.html",
    "title": "Lab 4 - The TidyModels Package",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison.\n\n\nBy the end of the lab you will…\n\nBe able to build workflows to evaluate different models and feature sets."
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#introduction",
    "href": "labs/BSMM_8740_lab_4.html#introduction",
    "title": "Lab 4 - The TidyModels Package",
    "section": "",
    "text": "In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison.\n\n\nBy the end of the lab you will…\n\nBe able to build workflows to evaluate different models and feature sets."
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#getting-started",
    "href": "labs/BSMM_8740_lab_4.html#getting-started",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to your github account and then go to the GitHub organization for the course and find the 2024-lab-4-[your github username] repository to complete the lab.\nCreate an R project using your 2024-lab-4-[your github username] repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the 2024-lab-4.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#packages",
    "href": "labs/BSMM_8740_lab_4.html#packages",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, tidymodels, modeldata, ranger, rsample, broom, recipes, parsnip\n)\n\n# set the efault theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#the-data",
    "href": "labs/BSMM_8740_lab_4.html#the-data",
    "title": "Lab 4 - The TidyModels Package",
    "section": "The Data",
    "text": "The Data\nToday we will be using the Ames Housing Data.\nThis is a data set from De Cock (2011) has 82 fields were recorded for 2,930 properties in Ames Iowa in the US. The version in the modeldata package is copied from the AmesHousing package but does not include a few quality columns that appear to be outcomes rather than predictors.\n\ndat &lt;- modeldata::ames\n\nThe data dictionary can be found on the internet:\n\ncat(readr::read_file(\"http://jse.amstat.org/v19n3/decock/DataDocumentation.txt\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-1-eda",
    "href": "labs/BSMM_8740_lab_4.html#exercise-1-eda",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nWrite and execute the code to perform summary EDA on the Ames Housing data using the package skimr.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-2-train-test-splits",
    "href": "labs/BSMM_8740_lab_4.html#exercise-2-train-test-splits",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 2: Train / Test Splits",
    "text": "Exercise 2: Train / Test Splits\nWrite and execute code to create training and test datasets. Have the training dataset represent 75% of the total data.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nset.seed(8740)\ndata_split &lt;- rsample::__\n\names_train &lt;- rsample::__\names_test  &lt;- rsample::__"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-3-data-preprocessing",
    "href": "labs/BSMM_8740_lab_4.html#exercise-3-data-preprocessing",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 3: Data Preprocessing",
    "text": "Exercise 3: Data Preprocessing\ncreate a recipe based on the formula Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold with the following steps:\n\ntransform the variable Sale_Price to log(Sale_Price)\ncenter and scale all predictors\ncreate dummy variables for all nominal variables\ntransform the variable Neighborhood to pool infrequent values (see recipes::step_other)\n\nFinally prep the recipe.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\nnorm_recipe &lt;- \n  recipes::recipe( ___ ) %&gt;% \n  ...\n  recipes::prep( ___ ) %&gt;% broom::tidy()"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-4-modeling",
    "href": "labs/BSMM_8740_lab_4.html#exercise-4-modeling",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 4 Modeling",
    "text": "Exercise 4 Modeling\nCreate three regression models\n\na base regression model using lm\na regression model using glmnet; set the model parameters penalty and mixture for tuning\na tree model using the ranger engine; set the model parameters min_n and trees for tuning\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\nlm_mod_base &lt;- \n  parsnip::linear_reg( ___ ) %&gt;% ___\n\nlm_mod_glmnet &lt;- \n  parsnip::linear_reg( ___ ) %&gt;% ___\n\nlm_mod_rforest &lt;- \n  parsnip::rand_forest( ___ ) %&gt;% ___"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-5",
    "href": "labs/BSMM_8740_lab_4.html#exercise-5",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse parsnip::translate() on each model to see the code object that is specific to a particular engine\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-6-bootstrap",
    "href": "labs/BSMM_8740_lab_4.html#exercise-6-bootstrap",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 6 Bootstrap",
    "text": "Exercise 6 Bootstrap\nCreate bootstrap samples for the training dataset. You can leave the parameters set to their defaults\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\nset.seed(8740)\ntrain_resamples &lt;- ___ %&gt;% rsample::bootstraps()\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-7",
    "href": "labs/BSMM_8740_lab_4.html#exercise-7",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 7",
    "text": "Exercise 7\nCreate workflows with workflowsets::workflow_set using your recipe and models.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nall_workflows &lt;- \n  workflowsets::workflow_set(\n    preproc = list(base = norm_recipe),\n    models = list(base = lm_mod_base, glmnet = lm_mod_glmnet, forest = lm_mod_rforest)\n  )"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-8",
    "href": "labs/BSMM_8740_lab_4.html#exercise-8",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 8",
    "text": "Exercise 8\nMap the default function (tune::tune_grid()) across the workflows in the workflowset you just created and update the variable all_workflows with the result\n\nall_workflows &lt;- all_workflows %&gt;% \n  workflowsets::workflow_map(\n    verbose = TRUE                # enable logging\n    , resamples = train_resamples # a parameter passed to tune::tune_grid()\n    , grid = 5                    # a parameter passed to tune::tune_grid()\n  )\n\nThe updated variable all_workflows contains a nested column named result, and each cell of the column result is a tibble containing a nested column named .metrics. Write code to\n\nun-nest the metrics in the column .metrics\nfilter out the rows for the metric rmse\ngroup by wflow_id, order the .estimate column from highest to lowest, and pick out the first row of each group\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nall_workflows %&gt;% \n  dplyr::select(wflow_id,__) %&gt;% \n  tidyr::unnest(__) %&gt;% \n  dplyr::select(wflow_id,__) %&gt;% \n  tidyr::unnest(__) %&gt;% \n  dplyr::filter(.metric == '___') %&gt;% \n  dplyr::group_by(wflow_id) %&gt;% \n  dplyr::arrange(desc(__) ) %&gt;% \n  dplyr::slice(1)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-9",
    "href": "labs/BSMM_8740_lab_4.html#exercise-9",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 9",
    "text": "Exercise 9\nRun the code below and compare to your results from exercise 8.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nworkflowsets::rank_results(all_workflows, rank_metric = \"rmse\", select_best = TRUE)\n\n# compare to your results from exercise 8."
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-10",
    "href": "labs/BSMM_8740_lab_4.html#exercise-10",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 10",
    "text": "Exercise 10\nSelect the best model per the rsme metric using its id.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nbest_model_workflow &lt;- \n  all_workflows %&gt;% \n  workflowsets::extract_workflow(\"__\")\n\n\n\nFinalize the workflow by setting the parameters for the best model\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nbest_model_workflow &lt;- \n  best_model_workflow %&gt;% \n  tune::finalize_workflow(\n    tibble::tibble(__ = __, __ = __) # enter the name and value of the best-fit parameters\n  ) \n\n\n\nNow compare the fits\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\ntraining_fit &lt;- best_model_workflow %&gt;% \n  fit(data = ames_train)\n\ntesting_fit &lt;- best_model_workflow %&gt;% \n  fit(data = ames_test)\n\n\n# What is the ratio of the OOB prediction errors (MSE): test/train?\n\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 4!”, and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyone’s submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work by 5:00pm on Sunday!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#grading",
    "href": "labs/BSMM_8740_lab_4.html#grading",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html",
    "href": "labs/BSMM_8740_lab_7.html",
    "title": "Lab 7 - Causality: DAGs",
    "section": "",
    "text": "In today’s lab, you’ll practice working with DAGs and building a causal workflow.\nBy the end of the lab you will…\n\nBe able to build DAGs to model causal assumptions and use the causal model to extract implications for answering causal questions.\nBe able to build a causal workflow to answer causal questions."
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#introduction",
    "href": "labs/BSMM_8740_lab_7.html#introduction",
    "title": "Lab 7 - Causality: DAGs",
    "section": "",
    "text": "In today’s lab, you’ll practice working with DAGs and building a causal workflow.\nBy the end of the lab you will…\n\nBe able to build DAGs to model causal assumptions and use the causal model to extract implications for answering causal questions.\nBe able to build a causal workflow to answer causal questions."
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#getting-started",
    "href": "labs/BSMM_8740_lab_7.html#getting-started",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-7-[your github username] repository .\nCreate an R project using your 2024-lab-7-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-7.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#packages",
    "href": "labs/BSMM_8740_lab_7.html#packages",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, broom, rsample, ggdag, causaldata, halfmoon, ggokabeito, malcolmbarrett/causalworkshop\n  , magrittr, ggplot2, estimatr, Formula, r-causal/propensity, gt, gtExtras)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#exercise-1-dags-and-open-paths",
    "href": "labs/BSMM_8740_lab_7.html#exercise-1-dags-and-open-paths",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 1: DAGs and open paths",
    "text": "Exercise 1: DAGs and open paths\nFind the open paths from D (treatment) to Y (outcome) in the four DAGs below.\nYou can examine the DAGS to identify the open paths (this is the recommended first step) and then use the code for the DAGs (below), along with the function dagitty::paths to confirm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nDAG_1\n\nopen paths from D (treatment) to Y (outcome):\n\nDAG_2\n\nopen paths from D (treatment) to Y (outcome):\n\nDAG_3\n\nopen paths from D (treatment) to Y (outcome):\n\nDAG_4\n\nopen paths from D (treatment) to Y (outcome):"
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#exercise-2-building-a-dag",
    "href": "labs/BSMM_8740_lab_7.html#exercise-2-building-a-dag",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 2: Building a DAG",
    "text": "Exercise 2: Building a DAG\nYou work for a company that sells a commodity to retail customers, and your management is interested in the relationship between your price and the demand for the commodity at your outlets. You have one competitor and your pricing tactic is to set your price at slightly less that your competitor’s. Your company surveys the competitor’s prices several times per day and once you know the competitor’s price, the pricing team resets your prices according to the pricing tactic. The public is well informed of both prices when they make their choice to buy.\nYou and your competitor buy from the wholesaler at a price that is set by the global market, and the wholesaler’s price is reset at the beginning of the each day according to the market price at the end of the day before. As the market is traded globally it reflects global demand for the commodity as well as other global and local economic shocks that you customers might be exposed to (interest rates, general business conditions, wages, etc.).\nYour company has panel data (i.e time-stamped) on local economic conditions, its own sales, its own prices, competitor prices and wholesale prices, and has asked you to do an analysis of the pricing tactics to increase demand.\n\nTo confirm your understanding of the business, perhaps identify missing data, and to inform your analysis, create a DAG describing the assumed relationships between the driving factors for this problem.\nWhat data might be missing from dataset provided by the company?\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\nFinish constructing the DAG and use this code to display it\n\nset.seed(8740)\n# fill in the missing node(s)\nggdag::dagify(\n  demand ~ ?           # demand for the commodity at your business\n  , price ~ ?          # price for the commodity at your business\n  , c_price ~ ?        # competitor price for the commodity \n  , wholesale ~ ?      # wholesale price for all businesses\n  , economy ~ ?        # a measure of general economic activity\n  , exposure = \"price\"\n  , outcome = \"demand\"\n  , labels = c(\n      demand = \"demand\",\n      price = \"price\",\n      c_price = \"c_price\",\n      economy = \"economy\",\n      wholesale = \"wholesale\"\n    )\n) %&gt;%\n  ggdag::ggdag(use_labels = \"label\", text = FALSE) +\n  ggdag::theme_dag()\n\nWhat data might be missing from the company data / this DAG, if any:"
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#exercise-3-inverse-probability-weights-ipws",
    "href": "labs/BSMM_8740_lab_7.html#exercise-3-inverse-probability-weights-ipws",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 3: Inverse Probability Weights (IPWs)",
    "text": "Exercise 3: Inverse Probability Weights (IPWs)\nin class we used the function propensity::wt_ate to calculate inverse probability weights, starting from the propensity scores, as in the code below:\n\npropensity_model &lt;- glm(\n  net ~ income + health + temperature,\n  data = causalworkshop::net_data,\n  family = binomial()\n)\n\nRepeat the calculation of the IPWs, using the definition of the weight as the inverse probability:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# calculate inverse probability weights\nnet_data_wts &lt;- propensity_model |&gt;\n  broom::augment(newdata = causalworkshop::net_data, type.predict = \"response\") |&gt;\n  dplyr::mutate(ip_wts = ??)\n\nand show that your calculated weights are the same as those computed by propensity::wt_ate\n\n# show that ip_wts = wts"
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#exercise-4-randomized-controlled-trials",
    "href": "labs/BSMM_8740_lab_7.html#exercise-4-randomized-controlled-trials",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 4: Randomized Controlled Trials",
    "text": "Exercise 4: Randomized Controlled Trials\nThe essence of exchangeability is that the treated and untreated groups are very similar with respect to values of potential confounders. Randomization of treatment makes outcomes independent of treatment, and also makes the treated and untreated groups very similar with respect to values of potential confounders.\nShow that this is the case for our mosquito net data by simulating random treatment assignment as follows:\n\n# use this data - mosquito net data plus a row id numer\nsmpl_dat &lt;- causalworkshop::net_data |&gt;\n  tibble::rowid_to_column()\n\n\nuse tidysmd::tidy_smd with smpl_dat and group=net to calculate the standardized mean differences (SMDs) for the confounders income, health and temperature.\nuse dplyr::slice_sample to randomly sample from smpl_dat, with proportion 0.5. Give this sample data a name.\nmutate the sample to add a column with smpl = 1.\ntake the data not in the first sample and form a second sample (start with the original data (smpl_dat) and remove the rows that appear in the sample of step 1. This is why we added a row id. Give this second sample data a name.\nmutate the second sample to add a column with smpl = 0.\nbind the two samples together by rows (e.g. dplyr::bind_rows).\nuse tidysmd::tidy_smd with the combined samples from step 6 and group=smpl to calculate the standardized mean differences (SMDs) for the confounders income, health and temperature.\n\nDid randomization make the treatment groups more alike with respect to income, health and temperature?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# show the smd table for the original data and for the randomized data"
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#exercise-5-frisch-waugh-lovell-theorem",
    "href": "labs/BSMM_8740_lab_7.html#exercise-5-frisch-waugh-lovell-theorem",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 5: Frisch-Waugh-Lovell Theorem",
    "text": "Exercise 5: Frisch-Waugh-Lovell Theorem\nHere we’ll look at credit and default-risk data. First we’ll load the data:\n\n# load data\nrisk_data = readr::read_csv(\"data/risk_data.csv\", show_col_types = FALSE)\n\nThe FWL Theorem states that a multivariate linear regression can be estimated all at once or in three separate steps. For example, you can regress default on the financial variables credit_limit, wage, credit_score1, and credit_score2 as follows:\n\n# regress default on financila variabbles in the dataset\nmodel &lt;- lm(default ~ credit_limit + wage +  credit_score1 + credit_score2, data = risk_data)\n\nmodel |&gt; broom::tidy(conf.int = TRUE)\n\n# A tibble: 5 × 7\n  term             estimate  std.error statistic  p.value     conf.low conf.high\n  &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.404      0.00860        46.9  0             3.87e-1   4.21e-1\n2 credit_limit   0.00000306 0.00000154      1.99 4.69e- 2      4.16e-8   6.08e-6\n3 wage          -0.0000882  0.00000607    -14.5  8.33e-48     -1.00e-4  -7.63e-5\n4 credit_score1 -0.0000417  0.0000183      -2.28 2.28e- 2     -7.77e-5  -5.82e-6\n5 credit_score2 -0.000304   0.0000152     -20.1  4.10e-89     -3.34e-4  -2.74e-4\n\n\nPer FWL you can also break this down into\n\na de-biasing step, where you regress the treatment (credit_limit) on the financial confounders wage, credit_score1, and credit_score2 , obtaining the residuals\na de-noising step, where you regress the outcome (default) on the financial confounders, obtaining the residuals\nan outcome model, where you regress the outcome residuals from step 2 on the treatment residuals of step 1.\n\nDue to confounding, the data looks like this, with default percentage trending down by credit limit.\n\nrisk_data |&gt; \n  dplyr::group_by(credit_limit) |&gt; \n  # add columns for number of measurements in the group, and the mean of the group\n  dplyr::mutate(size = n(), default = mean(default), ) |&gt; \n  # pull ot the distict values\n  dplyr::distinct(default,credit_limit, size) |&gt; \n  ggplot(aes(x = credit_limit, y = default, size = size)) +\n  geom_point() +\n  labs(title = \"Default Rate by Credit Limit\", x = \"credit_limit\", y = \"default\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nStep 1:\n\nCreate the debiasing model, and\nadd the residuals to the risk_data, saving the result in risk_data_deb in a column credit_limit_res.\nplot the de-biased data\nregress the outcome (default) on credit_limit_res\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# create debiasing model\ndebiasing_model &lt;- lm(?, data = risk_data)\n\ndebiasing_model |&gt; broom::tidy(conf.int = TRUE)\n\n# add a column with the residuals of the debiasing model \n# (add the residuals to the mean credit limit to give a nice interpretation to a zero residual)\nrisk_data_deb &lt;- risk_data |&gt; \n  dplyr::mutate(credit_limit_res = mean(credit_limit) + ? )\n\nrisk_data_deb |&gt; \n  # round the residuals prior to grouping\n  dplyr::mutate(credit_limit_res = round(credit_limit_res,digits=-2)) |&gt; \n  dplyr::group_by(credit_limit_res) |&gt; \n  # add columns for number of measurements in the group, and the mean of the group\n  dplyr::mutate(size = n(), default = mean(default), ) |&gt; \n  # only plot the residual groups with 'large' numbers of cases\n  dplyr::filter(size&gt;30) |&gt; \n  # pull ot the distict values\n  dplyr::distinct(default,credit_limit_res, size) |&gt; \n  ggplot(aes(x = credit_limit_res, y = default, size = size)) +\n  geom_point() +\n  labs(title = \"Default Rate by Debiased Credit Limit\", x = \"credit_limit\", y = \"default\")+\n  theme_minimal()\n\nlm(default ~ credit_limit_res, data = risk_data_deb) |&gt; \n    broom::tidy(conf.int = TRUE)\n\nIn this last regression, the coefficient estimated for credit_limit_res should be the same as the coefficient estimated for credit_limit in the initial regression.\nNote the difference in the confidence intervals though.\n\n\nThe de-biasing step is crucial for estimating the correct causal effect, while the de-noising step is nice to have, since it reduces the variance of the coefficient estimate and narrows the confidence interval.\nStep 2:\n\ncreate the de-noising model,\nadd the residuals to the de-biased data (risk_data_deb), saving the result in risk_data_denoise in a column default_res.\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# create the de-noising model\ndenoising_model &lt;- \n    lm(default ~ ?, data = risk_data_deb)\n\nrisk_data_denoise &lt;-  risk_data_deb |&gt; \n  # add a column with the residuals of the de-noised model \n  # (add the residuals to the mean default to give a nice interpretation to the zero residual)\n  dplyr::mutate(default_res = denoising_model$residuals + mean(default))\n\n\n\nStep 3:\n\nregress the default residuals (default_res) on the credit limit residuals (credit_limit_res)\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nlm(?, data = risk_data_denoise) |&gt; \n    broom::tidy(conf.int = TRUE)\n\nHow do the coefficients and confidence intervals from the FWL steps above compare to the original multivariate regression?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#exercise-6-causal-modeling",
    "href": "labs/BSMM_8740_lab_7.html#exercise-6-causal-modeling",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Exercise 6: Causal Modeling",
    "text": "Exercise 6: Causal Modeling\n\nQuestions at the end\nIn this guided exercise, we’ll attempt to answer a causal question: does quitting smoking make you gain weight? Causal modeling has a special place in the history of smoking research: the studies that demonstrated that smoking causes lung cancer were observational. Thanks to other studies, we also know that, if you’re already a smoker, quitting smoking reduces your risk of lung cancer. However, some have observed that former smokers tend to gain weight. Is this the result of quitting smoking, or does something else explain this effect? In the book Causal Inference: What If by Hernán and Robins, the authors analyze this question using several causal inference techniques.\nTo answer this question, we’ll use causal inference methods to examine the relationship between quitting smoking and gaining weight. First, we’ll draw our assumptions with a causal diagram (a directed acyclic graph, or DAG), which will guide our model. Then, we’ll use a modeling approach called inverse probability weighting–one of many causal modeling techniques–to estimate the causal effect we’re interested in.\nWe’ll use data from NHEFS to try to estimate the causal effect of quitting smoking on weight game. NHEFS is a longitudinal, observational study that has many of the variables we’ll need. Take a look at causaldata::nhefs_codebook if you want to know more about the variables in this data set. These data are included in the {causaldata} package. We’ll use the causaldata::nhefs_complete data set, but we’ll remove people who were lost to follow-up.\n\nnhefs_complete_uc &lt;- causaldata::nhefs_complete |&gt;\n  dplyr::filter(censored == 0)\nnhefs_complete_uc\n\n# A tibble: 1,566 × 67\n    seqn  qsmk death yrdth modth dadth   sbp   dbp sex     age race  income\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1   233     0     0    NA    NA    NA   175    96 0        42 1         19\n 2   235     0     0    NA    NA    NA   123    80 0        36 0         18\n 3   244     0     0    NA    NA    NA   115    75 1        56 1         15\n 4   245     0     1    85     2    14   148    78 0        68 1         15\n 5   252     0     0    NA    NA    NA   118    77 0        40 0         18\n 6   257     0     0    NA    NA    NA   141    83 1        43 1         11\n 7   262     0     0    NA    NA    NA   132    69 1        56 0         19\n 8   266     0     0    NA    NA    NA   100    53 1        29 0         22\n 9   419     0     1    84    10    13   163    79 0        51 0         18\n10   420     0     1    86    10    17   184   106 0        43 0         16\n# ℹ 1,556 more rows\n# ℹ 55 more variables: marital &lt;dbl&gt;, school &lt;dbl&gt;, education &lt;fct&gt;, ht &lt;dbl&gt;,\n#   wt71 &lt;dbl&gt;, wt82 &lt;dbl&gt;, wt82_71 &lt;dbl&gt;, birthplace &lt;dbl&gt;,\n#   smokeintensity &lt;dbl&gt;, smkintensity82_71 &lt;dbl&gt;, smokeyrs &lt;dbl&gt;,\n#   asthma &lt;dbl&gt;, bronch &lt;dbl&gt;, tb &lt;dbl&gt;, hf &lt;dbl&gt;, hbp &lt;dbl&gt;,\n#   pepticulcer &lt;dbl&gt;, colitis &lt;dbl&gt;, hepatitis &lt;dbl&gt;, chroniccough &lt;dbl&gt;,\n#   hayfever &lt;dbl&gt;, diabetes &lt;dbl&gt;, polio &lt;dbl&gt;, tumor &lt;dbl&gt;, …\n\n\nLet’s look at the distribution of weight gain between the two groups.\n\nnhefs_complete_uc |&gt;\n  ggplot(aes(wt82_71, fill = factor(qsmk))) + \n  geom_vline(xintercept = 0, color = \"grey60\", linewidth = 1) +\n  geom_density(color = \"white\", alpha = .75, linewidth = .5) +\n  ggokabeito::scale_color_okabe_ito(order = c(1, 5)) + \n  theme_minimal() +\n  theme(legend.position = \"bottom\") + \n  labs(\n    x = \"change in weight (kg)\",\n    fill = \"quit smoking (1 = yes)\"\n  )\n\n\n\n\n\n\n\n\nThere’s a difference–former smokers do seemed to have gained a bit more weight–but there’s also a lot of variation. Let’s look at the numeric summaries.\n\n# ~2.5 kg gained for quit vs. not quit\nnhefs_complete_uc |&gt;\n  dplyr::group_by(qsmk) |&gt;\n  dplyr::summarize(\n    mean_weight_change = mean(wt82_71), \n    sd = sd(wt82_71),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 3\n   qsmk mean_weight_change    sd\n  &lt;dbl&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1     0               1.98  7.45\n2     1               4.53  8.75\n\n\nHere, it looks like those who quit smoking gained, on average, 4.5 kg. But is there something else that could explain these results? There are many factors associated with both quitting smoking and gaining weight; could one of those factors explain away the results we’re seeing here?\nTo truly answer this question, we need to specify a causal diagram based on domain knowledge. For most circumstances, there is no data-driven approach that consistently identify confounders. Only our causal assumptions can help us identify them. Causal diagrams are a visual expression of those assumptions linked to rigorous mathematics that allow us to understand what we need to account for in our model.\nIn R, we can visualize and analyze our DAGs with the ggdag package. ggdag uses ggplot2 and ggraph to visualize diagrams and dagitty to analyze them. Let’s set up our assumptions. The dagify() function takes formulas, much like lm() and friends, to express assumptions. We have two basic causal structures: the causes of quitting smoking and the causes of gaining weight. Here, we’re assuming that the set of variables here affect both. Additionally, we’re adding qsmk as a cause of wt82_71, which is our causal question; we also identify these as our outcome and exposure. Finally, we’ll add some labels so the diagram is easier to understand. The result is a dagitty object, and we can transform it to a tidy_dagitty data set with tidy_dagitty().\n\n# set up DAG\nsmk_wt_dag &lt;- ggdag::dagify(\n  # specify causes of quitting smoking and weight gain:\n  qsmk ~ sex + race + age + education + \n    smokeintensity + smokeyrs + exercise + active + wt71,\n  wt82_71 ~ qsmk + sex + race + age + education + \n    smokeintensity + smokeyrs + exercise + active + wt71,\n  # specify causal question:\n  exposure = \"qsmk\", \n  outcome = \"wt82_71\",\n  coords = ggdag::time_ordered_coords(),\n  # set up labels:\n  # here, I'll use the same variable names as the data set, but I'll label them\n  # with clearer names\n  labels = c(\n    # causal question\n    \"qsmk\" = \"quit\\nsmoking\",\n    \"wt82_71\" = \"change in\\nweight\",\n    \n    # demographics\n    \"age\" = \"age\",\n    \"sex\" = \"sex\",\n    \"race\" = \"race\",\n    \"education\" = \"education\",\n    \n    # health\n    \"wt71\" = \"baseline\\nweight\",\n    \"active\" = \"daily\\nactivity\\nlevel\",\n    \"exercise\" = \"exercise\",\n    \n    # smoking history\n    \"smokeintensity\" = \"smoking\\nintensity\",\n    \"smokeyrs\" = \"yrs of\\nsmoking\"\n  )\n) |&gt;\n  ggdag::tidy_dagitty()\n\nsmk_wt_dag\n\n# A DAG with 11 nodes and 19 edges\n#\n# Exposure: qsmk\n# Outcome: wt82_71\n#\n# A tibble: 20 × 9\n   name               x     y direction to       xend  yend circular label      \n   &lt;chr&gt;          &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    &lt;chr&gt;      \n 1 active             1    -4 -&gt;        qsmk        2     0 FALSE    \"daily\\nac…\n 2 active             1    -4 -&gt;        wt82_71     3     0 FALSE    \"daily\\nac…\n 3 age                1    -3 -&gt;        qsmk        2     0 FALSE    \"age\"      \n 4 age                1    -3 -&gt;        wt82_71     3     0 FALSE    \"age\"      \n 5 education          1    -2 -&gt;        qsmk        2     0 FALSE    \"education\"\n 6 education          1    -2 -&gt;        wt82_71     3     0 FALSE    \"education\"\n 7 exercise           1    -1 -&gt;        qsmk        2     0 FALSE    \"exercise\" \n 8 exercise           1    -1 -&gt;        wt82_71     3     0 FALSE    \"exercise\" \n 9 qsmk               2     0 -&gt;        wt82_71     3     0 FALSE    \"quit\\nsmo…\n10 race               1     0 -&gt;        qsmk        2     0 FALSE    \"race\"     \n11 race               1     0 -&gt;        wt82_71     3     0 FALSE    \"race\"     \n12 sex                1     1 -&gt;        qsmk        2     0 FALSE    \"sex\"      \n13 sex                1     1 -&gt;        wt82_71     3     0 FALSE    \"sex\"      \n14 smokeintensity     1     2 -&gt;        qsmk        2     0 FALSE    \"smoking\\n…\n15 smokeintensity     1     2 -&gt;        wt82_71     3     0 FALSE    \"smoking\\n…\n16 smokeyrs           1     3 -&gt;        qsmk        2     0 FALSE    \"yrs of\\ns…\n17 smokeyrs           1     3 -&gt;        wt82_71     3     0 FALSE    \"yrs of\\ns…\n18 wt71               1     4 -&gt;        qsmk        2     0 FALSE    \"baseline\\…\n19 wt71               1     4 -&gt;        wt82_71     3     0 FALSE    \"baseline\\…\n20 wt82_71            3     0 &lt;NA&gt;      &lt;NA&gt;       NA    NA FALSE    \"change in…\n\n\nLet’s visualize our assumptions with ggdag().\n\nsmk_wt_dag |&gt;\n  ggdag::ggdag(text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\n\nWhat do we need to control for to estimate an unbiased effect of quitting smoking on weight gain? In many DAGs, there will be many sets of variables–called adjustment sets–that will give us the right effect (assuming our DAG is correct–a big, unverifiable assumption!). ggdag_adjustment_set() can help you visualize them. Here, there’s only one adjustment set: we need to control for everything! While we’re add it, since a {ggdag} plot is just a {ggplot2} plot, let’s clean it up a bit, too.\n\nsmk_wt_dag |&gt;\n  ggdag::ggdag_adjustment_set(text = FALSE, use_labels = \"label\") +\n  ggdag::theme_dag() +\n  ggokabeito::scale_color_okabe_ito(order = c(1, 5)) + \n  ggokabeito::scale_fill_okabe_ito(order = c(1, 5))\n\n\n\n\n\n\n\n\nLet’s fit a model with these variables. Note that we’ll fit all continuous variables with squared terms, as well, to allow them a bit of flexibility.\n\nlm(\n  wt82_71~ qsmk + sex + \n    race + age + I(age^2) + education + \n    smokeintensity + I(smokeintensity^2) + \n    smokeyrs + I(smokeyrs^2) + exercise + active + \n    wt71 + I(wt71^2), \n  data = nhefs_complete_uc\n) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"qsmk\")\n\n# A tibble: 1 × 7\n  term  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 qsmk      3.46     0.438      7.90 5.36e-15     2.60      4.32\n\n\nWhen we adjust for the variables in our DAG, we get an estimate of about 3.5 kg–people who quit smoking gained about this amount of weight. However, we are trying to answer a specific causal question: how much weight would a person gain if the quit smoking vs. if the same person did not quit smoking? Let’s use an inverse probability weighting model to try to estimate that effect at the population level (what if everyone quit smoking vs what if no one quit smoking).\nFor a simple IPW model, we have two modeling steps. First, we fit a propensity score model, which predicts the probability that you received a treatment or exposure (here, that a participant quit smoking). We use this model to calculate inverse probability weights–1 / your probability of treatment. Then, in the second step, we use this weights in the outcome model, which estimates the effect of exposure on the outcome (here, the effect of quitting smoking on gaining weight).\nFor the propensity score model, we’ll use logistic regression (since quitting smoking is a binary variable). The outcome is quitting smoking, and the variables in the model are all those included in our adjustment set. Then, we’ll use augment() from {broom} (which calls predict() on the inside) to calculate our weights using propensity::wt_ate() and save it back into our data set.\n\npropensity_model &lt;- glm(\n  qsmk ~ sex + \n    race + age + I(age^2) + education + \n    smokeintensity + I(smokeintensity^2) + \n    smokeyrs + I(smokeyrs^2) + exercise + active + \n    wt71 + I(wt71^2), \n  family = binomial(), \n  data = nhefs_complete_uc\n)\n\nnhefs_complete_uc &lt;- propensity_model |&gt;\n  # predict whether quit smoking\n  broom::augment(type.predict = \"response\", data = nhefs_complete_uc) |&gt;\n  # calculate inverse probability\n  dplyr::mutate(wts = propensity::wt_ate(.fitted, qsmk))\n\nℹ Treating `.exposure` as binary\n\nnhefs_complete_uc |&gt;\n  dplyr::select(qsmk, .fitted, wts)\n\n# A tibble: 1,566 × 3\n    qsmk .fitted   wts\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1     0  0.0987  1.11\n 2     0  0.140   1.16\n 3     0  0.126   1.14\n 4     0  0.400   1.67\n 5     0  0.294   1.42\n 6     0  0.170   1.20\n 7     0  0.220   1.28\n 8     0  0.345   1.53\n 9     0  0.283   1.40\n10     0  0.265   1.36\n# ℹ 1,556 more rows\n\n\nLet’s look at the distribution of the weights.\n\nggplot(nhefs_complete_uc, aes(wts)) +\n  geom_histogram(color = \"white\", fill = \"#E69F00\", bins = 50) + \n  #  use a log scale for the x axis\n  scale_x_log10() + \n  theme_minimal(base_size = 20) + \n  xlab(\"Weights\")\n\n\n\n\n\n\n\n\nIt looks a little skewed, in particular there are some participants with much higher weights. There are a few techniques for dealing with this–trimming weights and stabilizing weights–but we’ll keep it simple for now and just use them as is.\nThe main goal here is to break the non-causal associations between quitting smoking and gaining weight–the other paths that might distort our results. In other words, if we succeed, there should be no differences in these variables between our two groups, those who quit smoking and those who didn’t. This is where randomized trials shine; you can often assume that there is no baseline differences among potential confounders between your treatment groups (of course, no study is perfect, and there’s a whole set of literature on dealing with this problem in randomized trials).\nStandardized mean differences (SMD) are a simple measurement of differences that work across variable types. In general, the closer to 0 we are, the better job we have done eliminating the non-causal relationships we drew in our DAG. Note that low SMDs for everything we adjust for does not mean that there is not something else that might confound our study. Unmeasured confounders or misspecified DAGs can still distort our effects, even if our SMDs look great!\nWe’ll use the {halfmoon} package to calculate the SMDs, then visualize them.\n\nvars &lt;- c(\n  \"sex\", \"race\", \"age\", \"education\", \n  \"smokeintensity\", \"smokeyrs\", \n  \"exercise\", \"active\", \"wt71\"\n)\n\nplot_df &lt;- halfmoon::tidy_smd(\n    nhefs_complete_uc,\n    all_of(vars),\n    qsmk,\n    wts\n)\n\nggplot(\n    data = plot_df,\n    mapping = aes(x = abs(smd), y = variable, group = method, color = method)\n) +\n    halfmoon::geom_love()\n\n\n\n\n\n\n\n\nThese look pretty good! Some variables are better than others, but weighting appears to have done a much better job eliminating these differences than an unadjusted analysis.\nWe can also use halfmoon’s geom_mirror_histogram() to visualize the impact that the weights are having on our population.\n\nnhefs_complete_uc |&gt;\n  dplyr::mutate(qsmk = factor(qsmk)) |&gt;\n  ggplot(aes(.fitted)) +\n  halfmoon::geom_mirror_histogram(\n    aes(group = qsmk),\n    bins = 50\n  ) +\n  halfmoon::geom_mirror_histogram(\n    aes(fill = qsmk, weight = wts),\n    bins = 50,\n    alpha = .5\n  ) +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\") + \n  theme_minimal(base_size = 20)\n\n\n\n\n\n\n\n\nBoth groups are being upweighted so that their distributions of propensity scores are much more similar.\nWe could do more here to analyze our assumptions, but let’s move on to our second step: fitting the outcome model weighted by our inverse probabilities. Some researchers call these Marginal Structural Models, in part because the model is marginal; we only need to include our outcome (wt82_71) and exposure (qsmk). The other variables aren’t in the model; they are accounted for with the IPWs!\n\nipw_model &lt;- lm(\n  wt82_71 ~ qsmk, \n  data = nhefs_complete_uc, \n  weights = wts # inverse probability weights\n) \n\nipw_estimate &lt;- ipw_model |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"qsmk\")\n\nipw_estimate\n\n# A tibble: 1 × 7\n  term  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 qsmk      3.44     0.408      8.43 7.47e-17     2.64      4.24\n\n\nThis estimate is pretty similar to what we saw before, if a little smaller. In fact, for simple causal questions, this is often the case: adjusting for confounders directly in your regression model sometimes estimates the same effect as IPWs and other causal techniques. Causal techniques are special, though, in that the use counterfactual modeling, which allows you to deal with many circumstances, such as when you have selection bias or time-dependendent confounding. They also often have variance properties.\nBut we have other problem that we need to address. While we’re just using lm() to estimate our IPW model, it doesn’t properly account for the weights. That means our standard error is too small, which will artificially narrow confidence intervals and artificially shrink p-values. There are many ways to address this, including robust estimators. We’ll focus on using the bootstrap via the {rsamples} package in this workshop, but here’s one way to do it with robust standard errors:\n\n# also see robustbase, survey, gee, and others\nlibrary(estimatr)\nipw_model_robust &lt;- lm_robust( \n  wt82_71 ~ qsmk, \n  data = nhefs_complete_uc, \n  weights = wts \n) \n\nipw_estimate_robust &lt;- ipw_model_robust |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"qsmk\")\n\nipw_estimate_robust\n\n  term estimate std.error statistic      p.value conf.low conf.high   df\n1 qsmk 3.440535 0.5264638  6.535179 8.573524e-11 2.407886  4.473185 1564\n  outcome\n1 wt82_71\n\n\nNow let’s try the bootstrap. First, we need to wrap our model in a function so we can call it many times on our bootstrapped data. A function like this might be your instinct; however, it’s not quite right.\n\n# fit ipw model for a single bootstrap sample\nfit_ipw_not_quite_rightly &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  .df &lt;- rsample::analysis(split)\n  \n  # fit ipw model\n  lm(wt82_71 ~ qsmk, data = .df, weights = wts) |&gt;\n    tidy()\n}\n\nThe problem is that we need to account for the entire modeling process, so we need to include the first step of our analysis – fitting the inverse probability weights.\n\nfit_ipw &lt;- function(split, ...) {\n  .df &lt;- rsample::analysis(split)\n  \n  # fit propensity score model\n  propensity_model &lt;- glm(\n    qsmk ~ sex + \n      race + age + I(age^2) + education + \n      smokeintensity + I(smokeintensity^2) + \n      smokeyrs + I(smokeyrs^2) + exercise + active + \n      wt71 + I(wt71^2), \n    family = binomial(), \n    data = .df\n  )\n  \n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(\n      .fitted,\n      qsmk, \n      exposure_type = \"binary\"\n    ))\n  \n  # fit correctly bootstrapped ipw model\n  lm(wt82_71 ~ qsmk, data = .df, weights = wts) |&gt;\n    tidy()\n}\n\nrsample makes the rest easy for us: bootstraps() resamples our data 1000 times, then we can use purrr::map() to apply our function to each resampled set (splits). rsample’s int_*() functions help us get confidence intervals for our estimate.\n\n# fit ipw model to bootstrapped samples | THIS MAY TAKE SOME TIME\nipw_results &lt;- rsample::bootstraps(causaldata::nhefs_complete, 1000, apparent = TRUE) |&gt;\n  dplyr::mutate(results = purrr::map(splits, fit_ipw))\n\n# get t-statistic-based CIs\nboot_estimate &lt;- rsample::int_t(ipw_results, results) |&gt;\n  dplyr::filter(term == \"qsmk\")\n\nboot_estimate\n\n# A tibble: 1 × 6\n  term  .lower .estimate .upper .alpha .method  \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n1 qsmk    2.54      3.42   4.40   0.05 student-t\n\n\nLet’s compare to our naive weighted model that just used a single estimate from lm()\n\ndplyr::bind_rows(\n  ipw_estimate |&gt;\n    dplyr::select(estimate, conf.low, conf.high) |&gt;\n    dplyr::mutate(type = \"ols\"),\n  ipw_estimate_robust |&gt;\n    dplyr::select(estimate, conf.low, conf.high) |&gt;\n    dplyr::mutate(type = \"robust\"),\n  boot_estimate |&gt;\n    dplyr::select(estimate = .estimate, conf.low = .lower, conf.high = .upper) |&gt;\n    dplyr::mutate(type = \"bootstrap\")\n) |&gt;\n  #  calculate CI width to sort by it\n  dplyr::mutate(width = conf.high - conf.low) |&gt;\n  dplyr::arrange(width) |&gt;\n  #  fix the order of the model types for the plot  \n  dplyr::mutate(type = forcats::fct_inorder(type)) |&gt;\n  ggplot(aes(x = type, y = estimate, ymin = conf.low, ymax = conf.high)) + \n    geom_pointrange(color = \"#0172B1\", size = 1, fatten = 3) +\n    coord_flip() +\n    theme_minimal(base_size = 20) +\n    theme(axis.title.y = element_blank())\n\n\n\n\n\n\n\n\nOur bootstrapped confidence intervals are wider, which is expected; remember that they were artificially narrow in the naive OLS model!\nSo, we have a final estimate for our causal effect: on average, a person who quits smoking will gain 3.5 kg (95% CI 2.4 kg, 4.4 kg) versus if they had not quit smoking.\n\n\n\nQuestions:\n\nPlease enumerate the steps in the causal analysis workflow\nWhat do you think? Is this estimate reliable? Did we do a good job addressing the assumptions we need to make for a causal effect, particularly that there is no confounding? How might you criticize this model, and what would you do differently?\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\ncausal workflow steps:\nyour critique of the results of the exercise\n\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 7!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyone’s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_7.html#grading",
    "href": "labs/BSMM_8740_lab_7.html#grading",
    "title": "Lab 7 - Causality: DAGs",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 6\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html",
    "href": "labs/BSMM_8740_lab_2.html",
    "title": "Lab 2 - The Recipes package",
    "section": "",
    "text": "In today’s lab, you’ll explore several data sets and practice pre-processing and feature engineering with recipes. The main goal is to increase your understanding of the recipes workflow, including the data structures underlying recipes, what they contain and how to access them.\n\n\nBy the end of the lab you will…\n\nBe able to use the recipes package to prepare and train & test datasets for analysis/modeling.\nBe able to access recipes data post-processing to both extract & validate the results of your recipes steps."
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#introduction",
    "href": "labs/BSMM_8740_lab_2.html#introduction",
    "title": "Lab 2 - The Recipes package",
    "section": "",
    "text": "In today’s lab, you’ll explore several data sets and practice pre-processing and feature engineering with recipes. The main goal is to increase your understanding of the recipes workflow, including the data structures underlying recipes, what they contain and how to access them.\n\n\nBy the end of the lab you will…\n\nBe able to use the recipes package to prepare and train & test datasets for analysis/modeling.\nBe able to access recipes data post-processing to both extract & validate the results of your recipes steps."
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#getting-started",
    "href": "labs/BSMM_8740_lab_2.html#getting-started",
    "title": "Lab 2 - The Recipes package",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2025-lab-2-[your github username] repository .\nCreate an R project using your 2025-lab-2-[your github username] repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the 2025-lab-2.qmd file in your personal repository.\nWhen you are done, be sure to save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#packages",
    "href": "labs/BSMM_8740_lab_2.html#packages",
    "title": "Lab 2 - The Recipes package",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in today’s lab.\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, forcats\n)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#data-the-boston-cocktail-recipes",
    "href": "labs/BSMM_8740_lab_2.html#data-the-boston-cocktail-recipes",
    "title": "Lab 2 - The Recipes package",
    "section": "Data: The Boston Cocktail Recipes",
    "text": "Data: The Boston Cocktail Recipes\nThe Boston Cocktail Recipes dataset appeared in a TidyTuesday posting. TidyTuesday is a weekly data project in R.\nThe dataset is derived from the Mr. Boston Bartender’s Guide, together with a dataset that was web-scraped as part of a hackathon.\nThis dataset contains the following information for each cocktail:\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of cocktail\n\n\ncategory\ncharacter\nCategory of cocktail\n\n\nrow_id\ninteger\nDrink identifier\n\n\ningredient_number\ninteger\nIngredient number\n\n\ningredient\ncharacter\nIngredient\n\n\nmeasure\ncharacter\nMeasurement/volume of ingredient\n\n\nmeasure_number\nreal\nmeasure as a number\n\n\n\nUse the code below to load the Boston Cocktail Recipes data set.\n\nboston_cocktails &lt;- readr::read_csv('data/boston_cocktails.csv', show_col_types = FALSE)\n\n\n\n\n\n\n\nNote\n\n\n\nExercises 1-7 use the recipes package to preprocess data, producing normalized data and principle components whose key parameters can be accessed from the prepped recipe object. As an unsupervised ‘learning’ method the results of PCA often need to interpreted for management. The majority of exercises 1-7 ask you to do that."
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#exercises",
    "href": "labs/BSMM_8740_lab_2.html#exercises",
    "title": "Lab 2 - The Recipes package",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nFirst use skimr::skim to assess the quality of the data set.\nNext prepare a summary. What is the median measure number across cocktail recipes?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 2\nFrom the boston_cocktails dataset select the name, category, ingredient, and measure_number columns and then pivot the table to create a column for each ingredient. Fill any missing values with the number zero.\nSince the names of the new columns may contain spaces, clean then using the janitor::clean_names(). Finally drop any rows with NA values and save this new dataset in a variable.\nHow much gin is in the cocktail called Leap Frog Highball?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 3\nPrepare a recipes::recipe object without a target but give name and category as ‘id’ roles. Add steps to normalize the predictors and perform PCA. Finally prep the data and save it in a variable.\nHow many predictor variables are prepped by the recipe?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nApply the recipes::tidy verb to the prepped recipe in the last exercise. The result is a table identifying the information generated and stored by each step in the recipe from the input data.\nTo see the values calculated for normalization, apply the recipes::tidy verb as before, but with second argument = 1.\nWhat ingredient is the most used, on average?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 5\nNow look at the result of the PCA, applying the recipes::tidy verb as before, but with second argument = 2. Save the result in a variable and filter for the components PC1 to PC5. Mutate the resulting component column so that the values are factors, ordering them in the order they appear using the forcats::fct_inorder verb.\nPlot this data using ggplot2 and the code below\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK \n\n\n\n\nggplot(aes(value, terms, fill = terms)) +\ngeom_col(show.legend = FALSE) +\nfacet_wrap(~component, nrow = 1) +\nlabs(y = NULL) +\ntheme(axis.text=element_text(size=7),\n      axis.title=element_text(size=14,face=\"bold\"))\n\nHow would you describe the drinks represented by PC1?\n\n\nExercise 6\nAs in the last exercise, use the variable with the tidied PCA data and use only PCA components PC1 to PC4. Take/slice the top 8 ingedients by component, ordered by their absolute value using the verb dplyr::slice_max. Next, generate a grouped table using gt::gt, colouring the cell backgrounds (i.e. fill) with green for values ≥0\\ge0 and red for values &lt;0&lt;0.\nWhat is the characteristic alcoholic beverage of each of the first 4 principle components.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 7\nFor this exercise, bake the prepped PCA recipe using recipes::bake on the original data and plot each cocktail by its PC1, PC2 component, using\n\nggplot(aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  geom_text(check_overlap = TRUE, hjust = \"inward\") + \n  labs(color = NULL)\n\nCan you create an interpretation of the PCA analysis?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 8\nIn the following exercise, we’ll use the recipes package to prepare time series data. The starting dataset contains monthly house price data for each of the four countries/regions in the UK\n\nuk_prices &lt;- readr::read_csv('data/UK_house_prices.csv', show_col_types = FALSE)\n\nWrite code to clean the names in the uk_prices dataset using janitor::clean_names(), and then using skimr::skim confirm that the region names are correct and that there are no missing values. Call the resulting analytic data set df.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 9\nWe want to use the monthly house price data to predict house prices one month ahead for each region. The basic model will be:\nSalesVolume ~ .\nwhere the date and region-name are id variables, not predictor variables. Instead we will use the prior-month lagged prices as the only predictor.\nThe recipe uses the following 5 steps from the recipes package: update_role(date, region_name, new_role = “id”) | recipe(sales_volume ~ ., data = df |&gt; janitor::clean_names()) | step_naomit(lag_1_sales_volume, skip=FALSE) | step_lag(sales_volume, lag=1) | step_arrange(region_name, date)\nUse these 5 steps in the proper order to create a recipe to pre-process the time series data for each region. Prep and then bake your recipe using df.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\nThe result of the bake step, after filtering for dates ≤\\le 2005-03-01, should look like this:\n\nreadRDS(\"data/baked_uk_house_dat.rds\") |&gt; \n  dplyr::filter(date &lt;= lubridate::ymd(20050301)) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_currency(columns = -c(date,region_name), decimals = 0) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\ndate\nregion_name\nsales_volume\nlag_1_sales_volume\n\n\n\n\n2005-02-01\nEngland\n$56,044\n$53,464\n\n\n2005-03-01\nEngland\n$67,322\n$56,044\n\n\n2005-02-01\nNorthern Ireland\n$978\n$978\n\n\n2005-03-01\nNorthern Ireland\n$978\n$978\n\n\n2005-02-01\nScotland\n$7,631\n$8,876\n\n\n2005-03-01\nScotland\n$9,661\n$7,631\n\n\n2005-02-01\nWales\n$2,572\n$2,516\n\n\n2005-03-01\nWales\n$3,336\n$2,572\n\n\n\n\n\n\n\n\n\nExercise 10\n\nRecall The Business Problem\nWe’re at a fast paced startup. The company is growing fast and the marketing team is looking for ways to increase the sales from existing customers by making them buy more. The main idea is to unlock the potential of the customer base through incentives, in this case a discount. We of course want to measure the effect of the discount on the customer’s behavior. Still, they do not want to waste money giving discounts to users which are not valuable. As always, it is about return on investment (ROI).\nWithout going into specifics about the nature of the discount, it has been designed to provide a positive return on investment if the customer buys more than $1\\$ 1 as a result of the discount. How can we measure the effect of the discount and make sure our experiment has a positive ROI? The marketing team came up with the following strategy:\n\nSelect a sample of existing customers from the same cohort.\nSet a test window of 1 month.\nLook into the historical data of web visits from the last month. The hypothesis is that web visits are a good proxy for the customer’s interest in the product.\nFor customers with a high number of web visits, send them a discount. There will be a hold out group which will not receive the discount within the potential valuable customers based on the number of web visits. For customers with a low number of web visits, do not send them a discount (the marketing team wants to report a positive ROI, so they do not want to waste money on customers which are not valuable). Still, they want to use them to measure the effect of the discount.\nWe also want to use the results of the test to tag loyal customers. These are customers which got a discount (since they showed potential interest in the product) and customers with exceptional sales numbers even if they did not get a discount. The idea is to use this information to target them in the future if the discount strategy is positive.\n\nIn the last lab we did some exploratory data analysis. The next step is to prepare some descriptive statistics.\n\nDescriptive Statistics\nThe first thing the data analytics team did was to split the sales distribution by discount group:\n\ndata &lt;- readr::read_csv('data/sales_dag.csv', show_col_types = FALSE)\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x = sales, after_stat(count), fill = discount)) +\n  geom_histogram(alpha = 0.30, position = 'identity', color=\"#e9ecef\", bins = 30)+\n  geom_density(alpha = 0.30) +\n  xlab(\"Sales\") +\n  ylab(\"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks customers with a discount have higher sales. Data scientist A is optimistic with this initial result. To quantify this, compute the difference in means by discount:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\ngroup by discount and find the difference in means:\nWhat is the difference in means?\n\n\nThe discount strategy seems to be working. Data scientist A is happy with the results and decides to get feedback from the rest of the data science team.\nData scientist B is not so happy with the results. They think that the uplift is too good to be true (based on domain knowledge and the sales distributions 🤔). When thinking about reasons for such a high uplift, they realized the discount assignment was not at random. It was based on the number of web visits (remember the marketing plan?). This means that the discount group is not comparable to the control group completely! They decide to plot sales against web visits per discount group:\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x=visits, y = sales, color = discount)) +\n  geom_point() + \n  facet_grid(cols = vars(discount))\n\n\n\n\n\n\n\n\nIndeed, they realize they should probably adjust for the number of web visits. A natural metric is sales per web visit. Compute the sales per visit for each discount group:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n\n\nThe mean value is higher for the discount group. As always, they also looked at the distributions:\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x = sales_per_visit, after_stat(count), fill = discount)) +\n  geom_histogram(alpha = 0.30, position = 'identity', color=\"#e9ecef\", bins = 30)+\n  # geom_density(alpha = 0.30) +\n  xlab(\"Sales per Visit\") +\n  ylab(\"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor both data scientists A & B the results look much better, but they were unsure about which uplift to report. They thought about the difference in means:\nCompute the difference in mean sales_per_visit by discount:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\nCan you see a way to interpret this in terms of dollars? yes/no\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 2!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub."
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#submission",
    "href": "labs/BSMM_8740_lab_2.html#submission",
    "title": "Lab 2 - The Recipes package",
    "section": "Submission",
    "text": "Submission\nI will pull (copy) everyone’s submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work by 5:00pm on Sunday!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#grading",
    "href": "labs/BSMM_8740_lab_2.html#grading",
    "title": "Lab 2 - The Recipes package",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html",
    "href": "labs/BSMM_8740_lab_3.html",
    "title": "Lab 3 - Regression",
    "section": "",
    "text": "In today’s lab, you’ll explore several data sets and practice building and evaluating regression models.\n\n\nBy the end of the lab you will…\n\nBe able to use different regression models to predict a response/target/outcome as a function of a set of variates."
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#introduction",
    "href": "labs/BSMM_8740_lab_3.html#introduction",
    "title": "Lab 3 - Regression",
    "section": "",
    "text": "In today’s lab, you’ll explore several data sets and practice building and evaluating regression models.\n\n\nBy the end of the lab you will…\n\nBe able to use different regression models to predict a response/target/outcome as a function of a set of variates."
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#getting-started",
    "href": "labs/BSMM_8740_lab_3.html#getting-started",
    "title": "Lab 3 - Regression",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-3-[your github username] repository to complete the lab.\nCreate an R project using your 2024-lab-3-[your github username] repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the 2024-lab-3.qmd file in your repository.\nWhen you are done, be sure to save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#packages",
    "href": "labs/BSMM_8740_lab_3.html#packages",
    "title": "Lab 3 - Regression",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in today’s lab.\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, knitr,\n  ISLR2, stats, xgboost\n)\ntheme_set(theme_bw(base_size = 12))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#data-boston-house-values",
    "href": "labs/BSMM_8740_lab_3.html#data-boston-house-values",
    "title": "Lab 3 - Regression",
    "section": "Data: Boston House Values",
    "text": "Data: Boston House Values\nThe Boston House Values dataset (usually referred to as the Boston dataset) appears in several R packages in different versions and is based on economic studies published in the late 1970’s.\nThis dataset contains the following information for each cocktail:\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ncrim\nper capita crime rate by town.\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus\nproportion of non-retail business acres per town.\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox\nnitrogen oxides concentration (parts per 10 million).\n\n\nrm\naverage number of rooms per dwelling.\n\n\nage\nproportion of owner-occupied units built prior to 1940.\n\n\ndis\nweighted mean of distances to five Boston employment centres.\n\n\nrad\nindex of accessibility to radial highways.\n\n\ntax\nfull-value property-tax rate per $10,000.\n\n\nptratio\npupil-teacher ratio by town.\n\n\nlstat\nlower status of the population (percent).\n\n\nmedv\nmedian value of owner-occupied homes in $1000s.\n\n\n\nUse the code below to load the Boston Cocktail Recipes data set.\n\nboston &lt;- ISLR2::Boston"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#exercises",
    "href": "labs/BSMM_8740_lab_3.html#exercises",
    "title": "Lab 3 - Regression",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nPlot the median value of owner-occupied homes (medv) vs the percentage of houses with lower socioeconomic status (lstat) then use lm to model medv ~ lstat and save the result in a variable for use later.\nNext prepare a summary of the model. What is the intercept and the coefficient of lstat in this model?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n# plot medv vs lstat\n\n\n# create a linear model of medv vs lstat and save the model\n\n\n\n\n\nExercise 2\nUsing the result from Exercise 1, and the data below, use the predict function (stats::predict.lm or just predict) with the argument interval = “confidence” to prepare a summary table with columns lstat, fit, lwr, upr.\nYou can use stats::predict.lm directly with the data below.\nOr consider creating a nested column using dplyr::mutate along with purrr::map with first argument lstat and second argument a function you create. The last operation is to unnest the nested column with tidyr::unnest(__).\nOr\n\ntibble(lstat = c(5, 10, 15, 20))\n\nFinally, use your model to plot some performance checks using the performance::check_model function with arguments check=c(\"linearity\",\"qq\",\"homogeneity\", \"outliers\").\nAre there any overly influential observations in this dataset?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# Are there any overly influential observations in this dataset?\n\n\n\n\n\nExercise 3\nFit the variable medv (median value of owner-occupied homes) to all predictors in the dataset and use the performance::check_collinearity function on the resulting model to check if any predictors are redundant.\nThe variance inflation factor is a measure of the magnitude of multicollinearity of model terms. A VIF less than 5 indicates a low correlation of that predictor with other predictors. A value between 5 and 10 indicates a moderate correlation, while VIF values larger than 10 are a sign for high, not tolerable correlation of model predictors.\nWhich predictors in this dataset might be redundant for predicting medv?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# Which predictors in this dataset might be redundant for predicting `medv`?\n\n\n\n\nThis is a good place to save, commit, and push changes to your remote lab repo. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nIn this exercise you will compare and interpret the results of linear regression on two similar datasets.\nThe first dataset (dat0 - generated below) has demand0 and price0 variables along with an unobserved variable (unobserved0 - so not in our dataset) that doesn’t change the values of demand0 and price0. Use lm to build a model to predict demand0 from price0 . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset dat0?\n\n\n\n\n\n\nTip\n\n\n\nplot with something like:\ndat0 %&gt;% ggplot(aes(x=price0,y=demand0)) + \n         # plot the points\n         geom_point() +\n         # add a straight line to the plot\n         geom_abline(\n          data = ?? a table with the coefficient estimates ??\n            , aes(intercept = `(Intercept)`, slope = price0)\n            , colour = \"red\"\n         )\n\n\n\nN &lt;- 500\nset.seed(1966)\n\ndat0 &lt;- tibble::tibble(\n  price0 = 10+rnorm(500)\n  , demand0 = 30-(price0 + rnorm(500))\n  , unobserved0 = 0.45*price0 + 0.77*demand0 + rnorm(500)\n)\n\nThe second dataset (dat1 - generated below) has demand1 and price1 variables, along with a variable unobserved1 that is completely random and is not observed, so it isn’t in our dataset. Use lm to build a model to predict demand1 from price1 . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset dat1?\n\nset.seed(1966)\n\ndat1 &lt;- tibble::tibble(\n  unobserved1 = rnorm(500)\n  , price1 = 10 + unobserved1 + rnorm(500)\n  , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))\n)\n\nWhich linear model returns the (approximately) correct dependence of demand on price, as given in the data generation process?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# Which linear model returns the (approximately) correct dependence of demand on price, as given in the data generation process?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#exercise-5",
    "href": "labs/BSMM_8740_lab_3.html#exercise-5",
    "title": "Lab 3 - Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNow repeat the modeling of exercise 4, but assuming that the formerly unobservable variables are now observable, and so can be included in the linear regression models.\nWhich model returns the (approximately) correct dependence of demand on price, as given in the data generation process?\nWhat can you conclude from these two exercises?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# What can you conclude from these two exercises?\n\n\n\n\nExercise 6\nFor the next several exercises, we’ll work with a new dataset. This dataset is taken from an EPA site on fuel economy, in particular the fuel economy dataset for 2023.\nUse the code below to load the FE Guide data set.\n\ndat &lt;- \n  readxl::read_xlsx(\"data/2023 FE Guide for DOE-release dates before 7-28-2023.xlsx\")\n\nFrom the raw data in dat, we’ll make a smaller dataset, and we’ll need to do some cleaning to make it useable.\nFirst select the columns “Comb FE (Guide) - Conventional Fuel”, “Eng Displ”,‘# Cyl’, Transmission , “# Gears”, “Air Aspiration Method Desc”, “Regen Braking Type Desc”, “Batt Energy Capacity (Amp-hrs)” , “Drive Desc”, “Fuel Usage Desc - Conventional Fuel”, “Cyl Deact?”, and “Var Valve Lift?” and then clean the column names using janitor::janitor::clean_names(). Assign the revised data to the variable cars_23.\nPerform a quick check of the data using DataExplorer::introduce() and DataExplorer::plot_missing() and modify the data as follows\n\nmutate the columns comb_fe_guide_conventional_fuel, number_cyl, and number_gears to ensure that they contain integers values, not doubles.\nuse tidyr::replace_na to replace any missing values in batt_energy_capacity_amp_hrs column with zeros, and replace and missing values in regen_braking_type_desc with empty strings (““).\nfinally, mutate the columns ‘transmission’,‘air_aspiration_method_desc’,‘regen_braking_type_desc’,‘drive_desc’ ,‘fuel_usage_desc_conventional_fuel’,‘cyl_deact’,‘var_valve_lift’ so their values are factors.\n\nPrepare a recipe to pre-process cars_23 ahead of modelling, using comb_fe_guide_conventional_fuel as the outcome, with the following steps.\n\nCentering for: recipes::all_numeric()\nScaling for: recipes::all_numeric()\nDummy variables from: recipes::all_factor()\n\nHow many predictor variables are there in cars_23 ?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# How many predictor variables are there in `cars_23` ?\n\n\n\n\n\nExercise 7\nFor this exercise, set a sample size equal to 75% of the observations of cars_23 and split the data as follows:\n\nset.seed(1966)\n\n# sample 75% of the rows of the cars_23 dataset to make the training set\ntrain &lt;- cars_23 %&gt;% \n  # make an ID column for use as a key\n  tibble::rowid_to_column(\"ID\") %&gt;% \n  # sample the rows\n  dplyr::sample_frac(0.75)\n\n# remove the training dataset from the original dataset to make the training set\ntest  &lt;- \n  dplyr::anti_join(\n    cars_23 %&gt;% tibble::rowid_to_column(\"ID\") # add a key column to the original data\n    , train\n    , by = 'ID'\n  )\n\n# drop the ID column from training and test datasets\ntrain %&lt;&gt;% dplyr::select(-ID); test %&lt;&gt;% dplyr::select(-ID)\n\nNext prep the recipe created in the last exercise using recipes::prep on the training data, and then use the result of the prep step to recipes::bake with the training and test data. Save the baked data in separate variables for use later.\nAfter these two steps how many columns are in the data? Why does this differ from the last step?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# After these two steps how many columns are in the data? Why does this differ from the last step?\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 8\nIn this exercise we will run xgboost::xgboost to evaluate the regression.\nFirst run fit the model with default meta-parameters for max_depth and eta, using the training data per the code below:\n\nuntuned_xgb &lt;-\n  xgboost::xgboost(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    max_depth = 6,\n    eta = .25\n    , verbose = FALSE\n  )\n\nNext use the fitted model to predict the outcome using the test data:\n\n# create predictions using the test data and the fitted model\nyhat &lt;- predict(\n  untuned_xgb\n  , cars_23_test %&gt;% \n    dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% \n    as.matrix() \n)\n\nFinally, pull out the comb_fe_guide_conventional_fuel column from the test data, assign it to the variable y and then use caret::postResample with arguments yhat and y to evaluate how well the model fits.\nWhat is the RMSE for the un-tuned model?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# What is the RMSE for the un-tuned model?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#exercise-9",
    "href": "labs/BSMM_8740_lab_3.html#exercise-9",
    "title": "Lab 3 - Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nIn this exercise we are going to tune the model using cross validation. First we create a tuning grid for the parameters and then fit the model for all the values in the grid, saving the results.\nFinally, we select the best parameters by least RMSE. This code will take a while to run\n\n#create hyperparameter grid\nhyper_grid &lt;- expand.grid(max_depth = seq(3, 6, 1), eta = seq(.2, .35, .01))  \n\n# initialize our metric variables\nxgb_train_rmse &lt;- NULL\nxgb_test_rmse  &lt;- NULL\n\nfor (j in 1:nrow(hyper_grid)) {\n  set.seed(123)\n  m_xgb_untuned &lt;- xgboost::xgb.cv(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    nfold = 5,\n    max_depth = hyper_grid$max_depth[j],\n    eta = hyper_grid$eta[j],\n    verbose = FALSE\n  )\n  \n  xgb_train_rmse[j] &lt;- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]\n  xgb_test_rmse[j] &lt;- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]\n}    \n\nbest &lt;- hyper_grid[which(xgb_test_rmse == min(xgb_test_rmse)),]; best # there may be ties\n\nre-run the code from the last exercise and evaluate the fit using one of the best tuning parameters (i.e. when re-running the regression, set max_depth and eta to one pair the best-fit parameters [there may be ties]).\nIs the tuned model better than the un-tuned model? If better, how much has the RMSE improved (in %).\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# Is the tuned model better than the un-tuned model? If better, how much has the RMSE improved (in %)."
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#exercise-10",
    "href": "labs/BSMM_8740_lab_3.html#exercise-10",
    "title": "Lab 3 - Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nUsing xgboost::xgb.importance rank the importance of each predictor in the model. Finally, take the top 10 predictors by importance and plot them using xgboost::xgb.plot.importance.\nPer this model, what is the most important feature for predicting fuel efficiency?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# What is the most important feature for predicting fuel efficiency?\n\n\n\n\nYou’re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message “Done with Lab 3!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyone’s submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (don’t forget to commit and then push your work by 5:00pm on Sunday!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#grading",
    "href": "labs/BSMM_8740_lab_3.html#grading",
    "title": "Lab 3 - Regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "GitHub.html",
    "href": "GitHub.html",
    "title": "GitHub",
    "section": "",
    "text": "GitHub data\n\n&lt;p&gt;Loading…&lt;/p&gt;",
    "crumbs": [
      "Course information",
      "GitHub username survey"
    ]
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html",
    "href": "weeks/BSMM_8740_week_9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\nDue date: Exam 1 released on Fri, Feb 35, due Mon, Feb 28 at 11:59pm",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html#prepare",
    "href": "weeks/BSMM_8740_week_9.html#prepare",
    "title": "Week 9",
    "section": "Prepare",
    "text": "Prepare\nNo readings this week.",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html#participate",
    "href": "weeks/BSMM_8740_week_9.html#participate",
    "title": "Week 9",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 09 - Causality: effects",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html#practice",
    "href": "weeks/BSMM_8740_week_9.html#practice",
    "title": "Week 9",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 8 - Rail Trail",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html#perform",
    "href": "weeks/BSMM_8740_week_9.html#perform",
    "title": "Week 9",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 9 - Causality: effects\n✅ Exam 2\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html",
    "href": "weeks/BSMM_8740_week_11.html",
    "title": "Week 11",
    "section": "",
    "text": "No additional readings this week. Catch up with previously assigned readings if you’ve fallen behind.",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html#prepare",
    "href": "weeks/BSMM_8740_week_11.html#prepare",
    "title": "Week 11",
    "section": "",
    "text": "No additional readings this week. Catch up with previously assigned readings if you’ve fallen behind.",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html#participate",
    "href": "weeks/BSMM_8740_week_11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 11 - Bayesian methods",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html#practice",
    "href": "weeks/BSMM_8740_week_11.html#practice",
    "title": "Week 11",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 10 - Flight delays",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html#perform",
    "href": "weeks/BSMM_8740_week_11.html#perform",
    "title": "Week 11",
    "section": "Perform",
    "text": "Perform\n✍️ HW 3 - Logistic regression and log transformation\n⌨️ Lab 11 - Bayesian methods\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html",
    "href": "weeks/BSMM_8740_week_5.html",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 5 - Sunday, Oct 13, 5pm ET\nQuiz 1 will be in class on October 09 (est. 30 minutes)",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#prepare",
    "href": "weeks/BSMM_8740_week_5.html#prepare",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Sec 9: Logistic regression\n📖 Read Classification with the Tidymodels Framework in R\n📖 Absorb Understanding Precision, Sensitivity, and Specificity In Classification Modeling and How To Calculate Them With A Confusion Matrix\n📖 Check out Tidymodels Tutorial - Classification\n📖 Look at Classification: ROC and AUC",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#participate",
    "href": "weeks/BSMM_8740_week_5.html#participate",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 5 - Classification & Clustering Methods",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#perform",
    "href": "weeks/BSMM_8740_week_5.html#perform",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 5 - Classification & Clustering Methods",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#podcast",
    "href": "weeks/BSMM_8740_week_5.html#podcast",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Podcast",
    "text": "Podcast\nListen here",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#study",
    "href": "weeks/BSMM_8740_week_5.html#study",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Study",
    "text": "Study\n\nShort Answer Questions\nInstructions: Answer the following questions in 2-3 sentences each.\n\nWhat is the key difference between eager learners and lazy learners in classification?\nProvide an example of a situation where accuracy might be a misleading metric for evaluating a binary classifier.\nExplain how adjusting the classification threshold in a binary classifier affects the trade-off between sensitivity and specificity.\nWhat is the main assumption made by Naive Bayes classification, and why is it considered “naive”?\nHow does the choice of ‘k’ affect the performance of a k-Nearest Neighbors (k-NN) classifier?\nWhat is the concept of a “margin” in Support Vector Machine (SVM) classification, and why is maximizing it desirable?\nDescribe two common kernel functions used in SVM classification and their applications.\nWhat is the curse of dimensionality, and how does it impact k-NN classification?\nWhat is the fundamental difference between classification and clustering in machine learning?\nBriefly explain the steps involved in the k-means clustering algorithm.\n\nShort Answer Key\n\nEager learners (e.g., logistic regression, decision trees) learn a model from the training data before making predictions, while lazy learners (e.g., k-NN) memorize the training data and classify new instances based on similarity to stored instances.\nIn a highly imbalanced dataset (e.g., fraud detection), a model that always predicts the majority class will have high accuracy but fail to identify the minority class, which is often of greater interest.\nLowering the classification threshold increases sensitivity (recall) but decreases specificity, leading to more true positives but also more false positives. Raising the threshold has the opposite effect.\nNaive Bayes assumes that all features are conditionally independent, given the class label. This assumption is often unrealistic in real-world data, hence the term “naive.”\nA small ‘k’ can make k-NN sensitive to noise (overfitting), while a large ‘k’ can oversmooth the decision boundary (underfitting). The optimal ‘k’ depends on the dataset and is typically found through cross-validation.\nThe margin in SVM is the distance between the separating hyperplane and the nearest data points (support vectors) of each class. Maximizing the margin aims to improve the classifier’s generalization ability and reduce overfitting.\nThe Radial Basis Function (RBF) kernel is commonly used for non-linear classification problems. The polynomial kernel maps data into a higher-dimensional space and can model non-linear relationships between features.\nThe curse of dimensionality refers to the phenomenon where distances between points become less meaningful as the number of dimensions increases. In high-dimensional spaces, k-NN can struggle to find meaningful nearest neighbors.\nClassification is a supervised learning task where the goal is to predict predefined labels for data points. Clustering is an unsupervised learning task where the goal is to discover natural groupings or structures in data without predefined labels.\nThe k-means algorithm initializes ‘k’ centroids randomly. Then, it iteratively assigns each data point to the nearest centroid and recalculates the centroids until convergence (centroids no longer change significantly).\n\nEssay Questions\n\nCompare and contrast eager learning and lazy learning algorithms for classification. Discuss their strengths, weaknesses, and situations where one might be preferred over the other.\nExplain the concepts of precision and recall in the context of binary classification. Discuss how these metrics are affected by changes in the classification threshold and provide examples of applications where each metric might be prioritized.\nDescribe the Naive Bayes classification algorithm in detail, including its underlying assumptions and the calculation of conditional probabilities. Discuss its advantages, limitations, and potential applications.\nExplain the workings of Support Vector Machines (SVMs) for classification. Describe the concepts of support vectors, margins, and kernel functions. Compare and contrast linear, polynomial, and radial basis function (RBF) kernels in SVM.\n\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html",
    "href": "weeks/BSMM_8740_week_6.html",
    "title": "Week 6 - Time Series Methods",
    "section": "",
    "text": "Important\n\n\n\nDue date: Lab 6 - Sunday, Oct 27, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html#prepare",
    "href": "weeks/BSMM_8740_week_6.html#prepare",
    "title": "Week 6 - Time Series Methods",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Time Series Analysis with R, Chapter 7: Structural Decomposition\n📖 Read Modeling time series with tidy resampling\n📖 Read Introducing Modeltime: Tidy Time Series Forecasting using Tidymodels\n📖 Read Forecasting: Principles and Practice, Chapter 7: Time series regression models\n📖 Read Forecasting: Principles and Practice, Chapter 9: ARIMA Models",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html#participate",
    "href": "weeks/BSMM_8740_week_6.html#participate",
    "title": "Week 6 - Time Series Methods",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 6 - Time Series Methods",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html#perform",
    "href": "weeks/BSMM_8740_week_6.html#perform",
    "title": "Week 6 - Time Series Methods",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 6 - Time Series Methods",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html#podcast",
    "href": "weeks/BSMM_8740_week_6.html#podcast",
    "title": "Week 6 - Time Series Methods",
    "section": "Podcast",
    "text": "Podcast\nListen here",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html#study",
    "href": "weeks/BSMM_8740_week_6.html#study",
    "title": "Week 6 - Time Series Methods",
    "section": "Study",
    "text": "Study\n\nShort Answer Questions\nInstructions: Answer the following questions in 2-3 sentences each.\n\nWhat is a time series and why is it a separate area of study?\nWhat are the key characteristics of economic and financial time series?\nDescribe the difference between deterministic and stochastic processes.\nWhat are the defining characteristics of a white noise process?\nExplain the concept of a random walk and how it differs from a random walk with drift.\nDefine a Markov chain and state its key properties.\nDescribe the key features of an autoregressive (AR) process.\nWhat are the conditions for stationarity in an AR process?\nExplain the difference between the autocorrelation function (ACF) and the partial autocorrelation function (PACF).\nWhat are the components of an ARIMA model, and what does each component represent?\n\n\n\nShort-Answer Answer Key\n\nA time series is a collection of data points indexed in time order. It is a separate area of study because it often exhibits serial correlation, violating assumptions of simple linear regression.\nEconomic and financial time series typically exhibit serial correlation, meaning past values influence present and future values. Changes in one period can impact future periods, and shocks can affect variables over successive quarters.\nDeterministic processes produce the same output from a given starting point, while stochastic processes involve randomness and are described by statistical distributions.\nA white noise process is characterized by serially uncorrelated random variables with zero mean and constant variance. They are often assumed to be independent and identically distributed.\nA random walk is a stochastic process where the current value is equal to the previous value plus a random shock. A random walk with drift includes a constant term, introducing a trend component.\nA Markov chain is a stochastic process where the probability of transitioning to the next state depends only on the current state (Markov Property). It’s characterized by a state space and a transition matrix governing probabilities between states.\nAn AR process models the current value as a linear function of its past values. The order of the AR process (AR(p)) indicates the number of lagged values used. The errors are typically assumed to be white noise.\nFor an AR process to be stationary, its statistical properties must remain constant over time. This typically requires the roots of the characteristic equation, derived from the autoregressive parameters, to lie outside the unit circle.\nThe ACF measures the correlation between a time series and its lagged values. The PACF measures the correlation between a time series and its lagged values after removing the influence of intermediate lags.\nAn ARIMA model combines autoregression (AR), differencing (I), and moving average (MA) components. AR captures the relationship with lagged values, I makes the series stationary, and MA models the relationship with past forecast errors.\n\n\n\nEssay Questions\n\nDiscuss the importance of stationarity in time series analysis. Explain how to identify and address non-stationarity in a time series.\nCompare and contrast AR, MA, and ARMA processes. Provide examples of real-world phenomena that can be modeled by each process.\nExplain the concept of exponential smoothing and its different variations. Discuss the strengths and limitations of exponential smoothing models.\nDescribe the steps involved in building a time series forecasting model using the tidymodels and timetk frameworks in R. Provide an example using a real-world dataset.\nDiscuss the challenges and considerations in evaluating the performance of time series forecasting models. Explain different performance metrics and their interpretations.\n\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_3.html",
    "href": "weeks/BSMM_8740_week_3.html",
    "title": "Week 3 - Regression Methods",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 3 - Sunday, Sept 29, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_3.html#prepare",
    "href": "weeks/BSMM_8740_week_3.html#prepare",
    "title": "Week 3 - Regression Methods",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Chapter 2 - General Aspects of Fitting Regression Models in: Regression Modeling Strategies\n📖 Read Chapter (8.1-8.5) - Regression Models in: Modern Statistics in R\n📖 Follow along with the R code in Linear Regression in R: Linear Regression Hands on Tutorial\n📖 Follow along with the R code from R code for Regression Analysis in An R companion\n📖 Check out Regression and Other Stories - Examples: Regression and Other Stories Examples",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_3.html#participate",
    "href": "weeks/BSMM_8740_week_3.html#participate",
    "title": "Week 3 - Regression Methods",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 3 - Regression Methods",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_3.html#perform",
    "href": "weeks/BSMM_8740_week_3.html#perform",
    "title": "Week 3 - Regression Methods",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 3 - Regression Methods\n⌨️ Example 1: grouped data & weighted regression\nFrom Section 10.8 of Regression and Other Stories:\n\nThree models leading to weighted regression\nWeighted least squares can be derived from three different models:\n\nUsing observed data to represent a larger population. This is the most common way that regression weights are used in practice. A weighted regression is fit to sample data in order to estimate the (unweighted) linear model that would be obtained if it could be fit to the entire population. For example, suppose our data come from a survey that oversamples older white women, and we are interested in estimating the population regression. Then we would assign to survey respondent a weight that is proportional to the number of people of that type in the population represented by that person in the sample. In this example, men, younger people, and members of ethnic minorities would have higher weights. Including these weights in the regression is a way to approximately minimize the sum of squared errors with respect to the population rather than the sample.\nDuplicate observations. More directly, suppose each data point can represent one or more actual observations, so that i represents a collection of w_i data points, all of which happen to have x_i as their vector of predictors, and where y_i is the average of the corresponding wi outcome variables. Then weighted regression on the compressed dataset, (x, y, w), is equivalent to unweighted regression on the original data.\nUnequal variances. From a completely different direction, weighted least squares is the maximum likelihood estimate for the regression model with independent normally distributed errors with unequal variances, where sd(ε_i) is proportional to 1/√w_i . That is, measurements with higher variance get lower weight when fitting the model. As discussed further in Section 11.1, unequal variances are not typically a major issue for the goal of estimating regression coefficients, but they become more important when making predictions about individual cases.\n\nWe will use weighted regression later in the course (Lectures 7 & 8), using observed data to represent a larger population - case 1 above.\nHere’s an example of the second case:\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(dplyr, broom)\n\n\n  The 'cran_repo' argument in shelf() was not set, so it will use\n  cran_repo = 'https://cran.r-project.org' by default.\n\n  To avoid this message, set the 'cran_repo' argument to a CRAN\n  mirror URL (see https://cran.r-project.org/mirrors.html) or set\n  'quiet = TRUE'.\n\nset.seed(1024)\n\n# individual (true) dataset, with 100,000 rows\nx &lt;- round(rnorm(1e5))\ny &lt;- round(x + x^2 + rnorm(1e5))\nind &lt;- data.frame(x, y)\n\n# aggregated dataset: grouped\nagg &lt;- ind %&gt;%\n  dplyr::group_by(x, y) |&gt; \n  dplyr::summarize(freq = dplyr::n(), .groups = 'drop') \n\nmodels &lt;- list( \n  \"True\"                = lm(y ~ x, data = ind),\n  \"Aggregated\"          = lm(y ~ x, data = agg),\n  \"Aggregated & W\"      = lm(y ~ x, data = agg, weights=freq)\n)\n\nmodels[['True']] |&gt; broom::tidy(conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1.08   0.00580      187.       0    1.07       1.10\n2 x               1.01   0.00558      181.       0    0.998      1.02\n\nmodels[['Aggregated']] |&gt; broom::tidy(conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    5.51      0.717      7.69 8.74e-11    4.08       6.95\n2 x              0.910     0.302      3.01 3.69e- 3    0.306      1.51\n\nmodels[['Aggregated & W']] |&gt; broom::tidy(conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic    p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1.08     0.224      4.84 0.00000795    0.637      1.53\n2 x               1.01     0.216      4.68 0.0000145     0.579      1.44\n\n\nNote the differences in the coefficient estimate for xx and the corresponding standard errors.\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "hw/hw-5.html",
    "href": "hw/hw-5.html",
    "title": "HW 5 - Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a statistics experience\n2️⃣ Make a slide summarizing on your experience\nYou must complete both parts to receive credit."
  },
  {
    "objectID": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you’d like to do but you’re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::global 2021 talks\nrstudio::conf 2020 talks\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team. One option is DataFest, which will take place over the April 1-3, 2022 weekend. More information to follow here.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nHow Charts Lie by Alberto Cairo\nList of books about data science ethics\n\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n✅ Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The visualization should include features or customization that are beyond what we’ve done in class .\n✅ Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: Coding out loud\nWatch an episode of Coding out loud (either live or pre-recorded) and work through the project.\nA few guidelines:\n✅ Create a GitHub repo for your Coding out loud submission. Your repo should include\n\nThe Quarto file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The final product (visualuzation, table, etc.) should include features or customization that are beyond what was achieved in the Coding out loud episode.\n✅ Include the link to your GitHub repo in the slide summarizing your experience."
  },
  {
    "objectID": "hw/hw-5.html#part-2-summarize-your-experience",
    "href": "hw/hw-5.html#part-2-summarize-your-experience",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we’ve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e. use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "hw/hw-5.html#submission",
    "href": "hw/hw-5.html#submission",
    "title": "HW 5 - Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the HW 5 - Statistics Experience assignment on Gradescope by Fri, Apr 15 at 5 pm ET. It must be submitted by the deadline on Gradescope to be considered for grading."
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret logistic regression models.\nFit and interpret multiple linear regression models with log transformed outcomes.\nReason around log transformations of various types.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-3. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-3.html#introduction",
    "href": "hw/hw-3.html#introduction",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret logistic regression models.\nFit and interpret multiple linear regression models with log transformed outcomes.\nReason around log transformations of various types.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-3. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-3.html#part-1---palmer-penguins",
    "href": "hw/hw-3.html#part-1---palmer-penguins",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 1 - Palmer penguins",
    "text": "Part 1 - Palmer penguins\nIn this part we’ll go back to the Palmer penguins dataset from HW 2.\nWe will use the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ninteger\nPenguin species (Adelie, Gentoo, Chinstrap)\n\n\nisland\ninteger\nIsland where recorded (Biscoe, Dream, Torgersen)\n\n\nflipper_length_mm\ninteger\nFlipper length in mm\n\n\n\nThe goal of this analysis is to use logistic regression to understand the relationship between flipper length, island, and whether a penguin is from the Adelie species. First, we need to create a new response variable to identify whether a penguin is from the Adelie species.\n\npenguins &lt;- penguins %&gt;%\n  mutate(adelie = factor(if_else(species == \"Adelie\", 1, 0)))\n\nAnd let’s check to make sure the new variable looks right before we continue with the analysis.\n\npenguins %&gt;%\n  count(adelie, species)\n\n# A tibble: 3 × 3\n  adelie species       n\n  &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt;\n1 0      Chinstrap    68\n2 0      Gentoo      124\n3 1      Adelie      152\n\n\nLet’s start by looking at the relationship between island and whether a penguin is from the Adelie species.\n\nWhat does the values_fill argument do in the following chunk? The documentation for the function will be helpful in answering this question.\n\npenguins %&gt;%\n  count(island, adelie) %&gt;%\n  pivot_wider(names_from = adelie, values_from = n, values_fill = 0)\n\n# A tibble: 3 × 3\n  island      `0`   `1`\n  &lt;fct&gt;     &lt;int&gt; &lt;int&gt;\n1 Biscoe      124    44\n2 Dream        68    56\n3 Torgersen     0    52\n\n\nCalculate the odds ratio of a penguin being from the Adelie species for those recorded on Dream compared to those recorded on Biscoe.\nYou want to fit a model using island to predict the odds of being from the Adelie species. Let π\\pi be the probability a penguin is from the Adelie species. The model has the following form. What do you expect the value of β̂1\\hat{\\beta}_1, the estimated coefficient for Dream, to be? Explain your reasoning.\n\nlog(π1−π)=β0+β1Dream+β2Torgersen\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1 ~ Dream + \\beta_2 ~ Torgersen\n\n\nFit a model predicting adelie from island and display the model output. For the following exercise, use this model.\nBased on this model, what are the odds of a penguin being from the Adelie species if it was recorded on Biscoe island? on Dream island?\nNext, add flipper length to the model so that there are two predictors. Display the model output. For the following exercises, use this model.\nWrite the regression equation for the model.\nInterpret the coefficient of flipper_length_mm in terms of the log-odds of being from the Adelie species.\nInterpret the coefficient of flipper_length_mm in terms of the odds of being from the Adelie species.\nInterpret the coefficient of Dream in terms of the odds of being from the Adelie species.\nHow do you expect the log-odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island.\nHow do you expect the odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island."
  },
  {
    "objectID": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "href": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 2 - GDP and Urban population",
    "text": "Part 2 - GDP and Urban population\nData on countries’ Gross Domestic Product (GDP) and percentage of urban population was collected and made available by The World Bank in 2020. A description of the variables as defined by The World Bank are provided below.\n\nGDP: “GDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.”\nUrban Population (% of total): “Urban population refers to people living in urban areas as defined by national statistical offices. It is calculated using World Bank population estimates and urban ratios from the United Nations World Urbanization Prospects.”\n\nThe data can be found in the data folder of your repository. Read the data and name it gdp_2020.\n\nFit a model predicting GDP from urban population. Then make a plot of residuals vs. fitted for this model. Does the linear model seem appropriate for modeling this relationship? Explain your reasoning.\nAdd a new column to the gdp_2020 dataset called gdp_log which is the (natural) log of gdp.\nFit a new model, predicting the log of GDP from urban population. Then make a plot of residuals vs. fitted for this model. Does the model predicting logged GDP or original GDP appear to be a better fit? Explain your reasoning.\n\nThe model output for predicting logged GDP.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.107\n0.202\n30.291\n0\n\n\nurban\n0.042\n0.003\n13.769\n0\n\n\n\n\n\nThe linear model for predicting log of GDP can be expressed as follows:\nlog(GDP)̂=6.11+0.042×urban\n\\widehat{\\log(GDP)} = 6.11 + 0.042 \\times urban\n\nTherefore, the coefficient of urban (0.042) can be interpreted as the change in logged GDP associated with 1 percentage point increase in urban population. The problem is, logged GDP is not a very informative value to talk about. So we need to undo the transformation we’ve done.\nTo do so, let’s do a quick review of some properties of logs.\n\nSubtraction and logs: log(a)−log(b)=log(ab)log(a) − log(b) = log(\\frac{a}{b})\nNatural logarithm: elog(x)=xe^{log(x)} = x\n\nBased on the interpretation of the slope above, the difference between the predicted values of logged GDP for a given value of urban and a value that is 1 percentage point higher is 0.0425. Let’s write this out mathematically, and then use the properties we’ve listed above to work through the equation.\nlog(GDP for urban x+1)−log(GDP for urban x)=0.042log(GDP for urban x+1GDP for urban x)=0.042elog(GDP for urban x+1GDP for urban x)=e0.042GDP for urban x+1GDP for urban x=e0.042\n\\begin{aligned}\nlog(\\text{GDP for urban } x + 1) - log(\\text{GDP for urban } x) &= 0.042 \\\\\nlog\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big) &= 0.042 \\\\\ne^{log\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big)} &= e^{0.042}\\\\\n\\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} &= e^{0.042}\n\\end{aligned}\n\n\nBased on the derivation above, fill in the blanks in the following sentence for an alternative (and more useful interpretation) of the slope of urban.\n\nFor each additional percentage point the urban population is higher, the GDP of a country is expected to be ___, on average, by a factor of ___."
  },
  {
    "objectID": "hw/hw-3.html#submission",
    "href": "hw/hw-3.html#submission",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-3.html#grading",
    "href": "hw/hw-3.html#grading",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-3.html#footnotes",
    "href": "hw/hw-3.html#footnotes",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "HW 1 - In-person voting trends",
    "section": "",
    "text": "In this assignment, you’ll use simple linear regression to explore the percent of votes cast in-person in the 2020 U.S. election based on the county’s political leanings.\n\n\nIn this assignment, you will…\n\nFit and interpret simple linear regression models\nAssess the conditions for simple linear regression.\nCreate and interpret spatial data visualizations using R.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-1.html#introduction",
    "href": "hw/hw-1.html#introduction",
    "title": "HW 1 - In-person voting trends",
    "section": "",
    "text": "In this assignment, you’ll use simple linear regression to explore the percent of votes cast in-person in the 2020 U.S. election based on the county’s political leanings.\n\n\nIn this assignment, you will…\n\nFit and interpret simple linear regression models\nAssess the conditions for simple linear regression.\nCreate and interpret spatial data visualizations using R.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-1.html#getting-started",
    "href": "hw/hw-1.html#getting-started",
    "title": "HW 1 - In-person voting trends",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\n\nGo to https://vm-manage.oit.duke.edu/containers and login with your Duke NetID and Password.\nClick STA210 to log into the Docker container. You should now see the RStudio environment.\n\n\n\nClone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-s22 organization on GitHub. Click on the repo with the prefix hw-1. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ➛ New Project ➛Version Control ➛ Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick hw-1-voting.qmd to open the template R Markdown file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "hw/hw-1.html#packages",
    "href": "hw/hw-1.html#packages",
    "title": "HW 1 - In-person voting trends",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(scales)"
  },
  {
    "objectID": "hw/hw-1.html#data-2020-election",
    "href": "hw/hw-1.html#data-2020-election",
    "title": "HW 1 - In-person voting trends",
    "section": "Data: 2020 Election",
    "text": "Data: 2020 Election\nThere are multiple data sets for this assignment. Use the code below to load the data.\n\nelection_nc &lt;- read_csv(\"data/nc-election-2020.csv\") %&gt;%\n  mutate(fips = as.integer(FIPS))\ncounty_map_data &lt;-  read_csv(\"data/nc-county-map-data.csv\")\nelection_sample &lt;- read_csv(\"data/us-election-2020-sample.csv\")\n\nThe county-level election data in election_nc and election_sample are from The Economist GitHub repo. The data were originally analyzed in the July 2021 article In-person voting really did accelerate covid-19’s spread in America. For this analysis, we will focus on the following variables:\n\ninperson_pct: The proportion of a county’s votes cast in-person in the 2020 election\npctTrump_2016: The proportion of a county’s votes cast for Donald Trump in the 2016 election\n\nThe data in county_map_data were obtained from the maps package in R. We will not analyze any of the variables in this data set but will use it to help create maps in the assignment. Click here to see the documentation for the maps package. Click here for code examples."
  },
  {
    "objectID": "hw/hw-1.html#exercises",
    "href": "hw/hw-1.html#exercises",
    "title": "HW 1 - In-person voting trends",
    "section": "Exercises",
    "text": "Exercises\nDue to COVID-19 pandemic, many states made alternatives in-person voting, such as voting by mail, more widely available for the 2020 U.S. election. The general consensus was that voters who were more Democratic leaning would be more likely to vote by mail, while more Republican leaning voters would largely vote in-person. This was supported by multiple surveys, including this survey conducted by Pew Research.\nThe goal of this analysis is to use regression analysis to explore the relationship between a county’s political leanings and the proportion of votes cast in-person in 2020. The ultimate question we want to answer is “Did counties with more Republican leanings have a larger proportion of votes cast in-person in the 2020 election?”\nWe will use the proportion of votes cast for Donald Trump in 2016 (pctTrump_2016) as a measure of a county’s political leaning. Counties with a higher proportion of votes for Trump in 2016 are considered to have more Republican leanings.\n\n\n\n\n\n\nNote\n\n\n\nAll narrative should be written in complete sentences, and all visualizations should have informative titles and axis labels.\n\n\n\nPart 1: Counties in North Carolina\nFor this part of the analysis, we will focus on counties in North Carolina. We will use the data sets election_nc and county_map_data.\n\nVisualize the distribution of the response variable inperson_pct and calculate appropriate summary statistics. Use the visualization and summary statistics to describe the distribution. Include an informative title and axis labels on the plot.\nLet’s view the data in another way. Use the code below to make a map of North Carolina with the color of each county filled in based on the percentage of votes cast in-person in the 2020 election. Fill in title and axis labels.\nThen use the plot answer the following:\n\nWhat are 2 - 3 observations you have from the plot?\nWhat is a feature that is apparent in the map that wasn’t apparent from the histogram in the previous exercise? What is a feature that is apparent in the histogram that is not apparent in the map?\n\n\n\nelection_map_data &lt;- left_join(election_nc, county_map_data)\n\nggplot() +\n  geom_polygon(data = county_map_data,\n    mapping = aes(x = long, y = lat, group = group),\n    fill = \"lightgray\", color = \"white\"\n    ) +\n  geom_polygon(data = election_map_data, \n    mapping = aes(x = long, y = lat, group = group,\n    fill = inperson_pct)\n    ) +\n  labs(\n    x = \"___\",\n    y = \"___\",\n    fill = \"___\",\n    title = \"___\"\n  ) +\n  scale_fill_viridis_c(labels = label_percent(scale = 1)) +\n  coord_quickmap()\n\n\nCreate a visualization of the relationship between inperson_pct and pctTrump_2016. Use the visualization to describe the relationship between the two variables.\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you haven’t yet done so, now is a good time to render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe can use a linear regression model to better quantify the relationship between the variables.\n\nFit the linear model to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nWrite the regression equation using mathematical notation.\n\nNow let’s use the model coefficients to describe the relationship.\n\nInterpret the slope. The interpretation should be written in a way that is meaningful in the context of the data.\nDoes it make sense to interpret the intercept? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\n\nIf the linear model is a good fit to these data, there should be no structure left in the residuals and the residuals should have constant variance. Augment the data with the model to obtain the residuals and predicted values for each observation, and call the augmented data frame nc_election_aug (You will use this name in Exercise 8). Then, make a plot of the residuals vs. the fitted values, and based on this plot, and provide a brief explanation for whether these two conditions are met. Hint: Zoom out on the plot by extending the limits of the y-axis.\n\n\n\n\n\n\n\nWarning\n\n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe might also be interested in our observations being independent, particularly if we are to use these data for inference. To evaluate whether the independence condition is met, we will examine a map of the counties in North Carolina with the color filled based on the value of the residuals.\n\nBriefly explain why we may want to view the residuals on a map to assess independence.\nBriefly explain what pattern (if any) we would expect to observe on the map if the independence condition is satisfied.\n\nFill in the name of your model in the code below to calculate the residuals and add them to election_map_data. Then, a map with the color of each county filled in based on the value of the residual. Hint: Start with the code from Exercise 2.\nIs the independence condition satisfied? Briefly explain based on what you observe from the plot.\n\nnc_election_aug &lt;- nc_election_aug %&gt;% \n  bind_cols(fips = election_nc$fips)\n\nelection_map_data &lt;- left_join(election_map_data, nc_election_aug)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore moving on to the next part, make sure you render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\n\nPart 2: Inference for the U.S.\nTo get a better understanding of the trend across the entire United States, we analyze data from a random sample of 200 counties. This data is in the election_sample data frame. Because these counties were randomly selected out of the 3,006 counties in the United States, we can reasonably treat the counties as independent observations.\n\nFit the linear model to these sample data to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nConduct a hypothesis test for the slope using a permutation test. In your response, state the null and alternative hypotheses in words, and state the conclusion in the context of the data.\nNext, construct a 95% confidence interval for the slope using bootstrapping. Interpret the confidence interval in the context of the data.\nComment on whether the hypothesis test and confidence interval support the general consensus that Republican voters were more likely to vote in-person in the 2020 election? A brief explanation is sufficient but it should be based on your conclusions from Exercises 10 and 11.\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-1.html#submission",
    "href": "hw/hw-1.html#submission",
    "title": "HW 1 - In-person voting trends",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-1.html#grading",
    "href": "hw/hw-1.html#grading",
    "title": "HW 1 - In-person voting trends",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-1.html#footnotes",
    "href": "hw/hw-1.html#footnotes",
    "title": "HW 1 - In-person voting trends",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html",
    "href": "ae/ae-11-volcanoes.html",
    "title": "AE 11: Multinomial classification",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-11-volcanoes-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#packages",
    "href": "ae/ae-11-volcanoes.html#packages",
    "title": "AE 11: Multinomial classification",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#data",
    "href": "ae/ae-11-volcanoes.html#data",
    "title": "AE 11: Multinomial classification",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of on volcanoes. The data come from The Smithsonian Institution via TidyTuesday.\n\nvolcano &lt;- read_csv(here::here(\"ae\", \"data/volcano.csv\"))\n\nRows: 958 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): volcano_name, primary_volcano_type, last_eruption_year, country, r...\ndbl  (8): volcano_number, latitude, longitude, elevation, population_within_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst, a bit of data prep:\n\nvolcano &lt;- volcano %&gt;%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  ) %&gt;%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %&gt;%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "href": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "title": "AE 11: Multinomial classification",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nCreate a map of volcanoes that is faceted by volcano_type.\n\n\nworld &lt;- map_data(\"world\")\n\nworld_map &lt;- ggplot() +\n  geom_polygon(\n    data = world, \n    aes(\n      x = long, y = lat, group = group),\n      color = \"white\", fill = \"gray50\", \n      size = 0.05, alpha = 0.2\n    ) +\n  theme_minimal() +\n  coord_quickmap() +\n  labs(x = NULL, y = NULL)\n\nworld_map +\n  geom_point(\n    data = volcano,\n    aes(x = longitude, y = latitude,\n        color = volcano_type, \n        shape = volcano_type),\n    alpha = 0.5\n  ) +\n  facet_wrap(~volcano_type) +\n  scale_color_OkabeIto()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#build-a-new-model",
    "href": "ae/ae-11-volcanoes.html#build-a-new-model",
    "title": "AE 11: Multinomial classification",
    "section": "Build a new model",
    "text": "Build a new model\n\nBuild a new model that uses a recipe that includes geographic information (latitude and longitude). How does this model compare to the original? Note:\nUse the same test/train split as well as same cross validation folds. Code for these is provided below.\n\n\n# test/train split\nset.seed(1234)\n\nvolcano_split &lt;- initial_split(volcano)\nvolcano_train &lt;- training(volcano_split)\nvolcano_test  &lt;- testing(volcano_split)\n\n# cv folds\nset.seed(9876)\n\nvolcano_folds &lt;- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [574/144]&gt; Fold1\n2 &lt;split [574/144]&gt; Fold2\n3 &lt;split [574/144]&gt; Fold3\n4 &lt;split [575/143]&gt; Fold4\n5 &lt;split [575/143]&gt; Fold5\n\n\nNew recipe, including geographic information:\n\nvolcano_rec2 &lt;- recipe(volcano_type ~ ., data = volcano_train) %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())\n\nOriginal model specification and new workflow:\n\nvolcano_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\")\n\nvolcano_wflow2 &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec2) %&gt;%\n  add_model(volcano_spec)\n\nvolcano_wflow2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_other()\n• step_other()\n• step_dummy()\n• step_zv()\n• step_center()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\n\nFit resamples:\n\nvolcano_fit_rs2 &lt;- volcano_wflow2 %&gt;%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs2\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits            id    .metrics         .notes           .predictions      \n  &lt;list&gt;            &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n1 &lt;split [574/144]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n2 &lt;split [574/144]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n3 &lt;split [574/144]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n4 &lt;split [575/143]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;\n5 &lt;split [575/143]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;\n\n\nCollect metrics:\n\ncollect_metrics(volcano_fit_rs2)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.606     5  0.0138 Preprocessor1_Model1\n2 roc_auc  hand_till  0.695     5  0.0245 Preprocessor1_Model1\n\n\nROC curves:\n\nvolcano_fit_rs2 %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#roc-curves",
    "href": "ae/ae-11-volcanoes.html#roc-curves",
    "title": "AE 11: Multinomial classification",
    "section": "ROC curves",
    "text": "ROC curves\n\nRecreate the ROC curve from the slides.\n\n\nfinal_fit &lt;- last_fit(\n  volcano_wflow2, \n  split = volcano_split\n  )\n\ncollect_predictions(final_fit) %&gt;%\n  roc_curve(truth = volcano_type, .pred_Stratovolcano:.pred_Other) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +\n  geom_path(size = 1) +\n  scale_color_OkabeIto() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  theme_minimal() +\n  labs(color = NULL)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#acknowledgement",
    "href": "ae/ae-11-volcanoes.html#acknowledgement",
    "title": "AE 11: Multinomial classification",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://juliasilge.com/blog/multinomial-volcano-eruptions."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html",
    "href": "ae/ae-6-the-office-cv.html",
    "title": "AE 6: The Office",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-6-the-office-cv-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#packages",
    "href": "ae/ae-6-the-office-cv.html#packages",
    "title": "AE 6: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#load-data",
    "href": "ae/ae-6-the-office-cv.html#load-data",
    "title": "AE 6: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_episodes &lt;- read_csv(\"data/office_episodes.csv\")"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "href": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "title": "AE 6: The Office",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#specify-model",
    "href": "ae/ae-6-the-office-cv.html#specify-model",
    "title": "AE 6: The Office",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-recipe",
    "href": "ae/ae-6-the-office-cv.html#create-recipe",
    "title": "AE 6: The Office",
    "section": "Create recipe",
    "text": "Create recipe\nCreate the recipe from class. Call it office_rec1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-workflow",
    "href": "ae/ae-6-the-office-cv.html#create-workflow",
    "title": "AE 6: The Office",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe. Call it office_wflow1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#cross-validation",
    "href": "ae/ae-6-the-office-cv.html#cross-validation",
    "title": "AE 6: The Office",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "href": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "title": "AE 6: The Office",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "href": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "title": "AE 6: The Office",
    "section": "Another model - Model 2",
    "text": "Another model - Model 2\nCreate a different (simpler, involving fewer variables) recipe and call it office_rec2. Conduct 10-fold cross validation and summarize metrics. Describe how the two models compare to each other based on cross validation metrics."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html",
    "href": "ae/ae-1-dcbikeshare.html",
    "title": "AE 02: Bike rentals in DC",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-1-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 02: Bike rentals in DC",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#data",
    "href": "ae/ae-1-dcbikeshare.html#data",
    "title": "AE 02: Bike rentals in DC",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- readr::read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts and temperature",
    "text": "Daily counts and temperature\n\nExercise 1\nVisualize the distribution of daily bike rentals and temperature as well as the relationship between these two variables.\n\nggplot(bikeshare, aes(x = count)) +\n  geom_histogram(binwidth = 250)\n\n\n\n\n\n\n\nggplot(bikeshare, aes(y = count, x = temp_orig)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nExercise 2\nDescribe the distribution of daily bike rentals and the distribution of temperature based on the visualizations created in Exercise 1. Include the shape, center, spread, and presence of any potential outliers.\n[Add your answer here]\n\n\nExercise 3\nThere appears to be one day with a very small number of bike rentals. What was the day? Why were the number of bike rentals so low on that day? Hint: You can Google the date to figure out what was going on that day.\n[Add your answer here]\n\n\nExercise 4\nDescribe the relationship between daily bike rentals and temperature based on the visualization created in Exercise 1. Comment on how we expect the number of bike rentals to change as the temperature increases.\n[Add your answer here]\n\n\nExercise 5\nSuppose you want to fit a model so you can use the temperature to predict the number of bike rentals. Would a model of the form\ncount=β0+β1temp_orig+ϵ\\text{count} = \\beta_0 + \\beta_1 ~ \\text{temp_orig} + \\epsilon\nbe the best fit for the data? Why or why not?\nNo."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 6\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 7\nNext, let’s look at how the daily bike rentals differ by season. Let’s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a “smoothed out histogram”. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 8\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 9\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#modeling",
    "href": "ae/ae-1-dcbikeshare.html#modeling",
    "title": "AE 02: Bike rentals in DC",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 10\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 11\nUsing the data you filtered in Exercise 10, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here\n\n\n\nExercise 12\nUse the output to write out the estimated regression equation.\n[Add your answer here]\n\n\nExercise 13\nInterpret the slope in the context of the data.\n[Add your answer here]\n\n\nExercise 14\nInterpret the intercept in the context of the data.\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#synthesis",
    "href": "ae/ae-1-dcbikeshare.html#synthesis",
    "title": "AE 02: Bike rentals in DC",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 15\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]\n\nThe following exercises will be completed only if time permits.\n\n\nExercise 16\nPick another season. Based on the visualization in Exercise 8, would you expect the slope of the relationship between temperature and daily bike rentals to be smaller or larger than the slope of the model you’ve been working with so far? Explain your reasoning.\n[Add your answer here]\n\n\nExercise 17\nFor this season you picked in Exercise 16, fit a linear model for predicting daily bike rentals from temperature. Note, you will need to filter your data for this season first. Use the output to write out the estimated regression equation and interpret the slope and the intercept of this model.\n\n# add your code here\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-9-odds.html",
    "href": "ae/ae-9-odds.html",
    "title": "AE 9: Odds",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-9-odds-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-9-odds.html#packages",
    "href": "ae/ae-9-odds.html#packages",
    "title": "AE 9: Odds",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(here::here(\"ae\", \"data/framingham.csv\")) %&gt;%\n  select(totChol, TenYearCHD) %&gt;%\n  drop_na() %&gt;%\n  mutate(high_risk = as.factor(TenYearCHD)) %&gt;%\n  select(totChol, high_risk)"
  },
  {
    "objectID": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "href": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "title": "AE 9: Odds",
    "section": "Linear regression vs. logistic regression",
    "text": "Linear regression vs. logistic regression\nState whether a linear regression model or logistic regression model is more appropriate for each scenario:\n\nUse age and education to predict if a randomly selected person will vote in the next election.\nUse budget and run time (in minutes) to predict a movie’s total revenue.\nUse age and sex to calculate the probability a randomly selected adult will visit Duke Health in the next year."
  },
  {
    "objectID": "ae/ae-9-odds.html#heart-disease",
    "href": "ae/ae-9-odds.html#heart-disease",
    "title": "AE 9: Odds",
    "section": "Heart disease",
    "text": "Heart disease\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the total cholesterol to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\ntotChol: total cholesterol (mg/dL)\n\n\n\nOutcome: high_risk\n\nggplot(data = heart_disease, aes(x = high_risk)) + \n  geom_bar() + \n  scale_x_discrete(labels = c(\"1\" = \"High risk\", \"0\" = \"Low risk\")) +\n  labs(\n    title = \"Distribution of 10-year risk of heart disease\", \n    x = NULL)\n\n\n\n\n\n\n\n\n\nheart_disease %&gt;%\n  count(high_risk)\n\n# A tibble: 2 × 2\n  high_risk     n\n  &lt;fct&gt;     &lt;int&gt;\n1 0          3555\n2 1           635\n\n\n\n\nCalculating probability and odds\n\nWhat is the probability a randomly selected person in the study is not high risk for heart disease?\nWhat are the odds a randomly selected person in the study is not high risk for heart disease?\n\n\n\nLogistic regression model\nFit a logistic regression model to understand the relationship between total cholesterol and risk for heart disease.\nLet pipi be the probability an adult is high risk. The statistical model is\nlog(πi1−πi)=β0+β1TotCholi\\log\\Big(\\frac{\\pi_i}{1-\\pi_i}\\Big) = \\beta_0 + \\beta_1 TotChol_i\n\nheart_disease_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ totChol, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) %&gt;% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.894\n0.230\n-12.607\n0\n\n\ntotChol\n0.005\n0.001\n5.268\n0\n\n\n\n\n\n\nWrite the regression equation. Round to 3 digits.\n\n\n\nCalculating log-odds, odds and probabilities\nBased on the model, if a randomly selected person has a total cholesterol of 250 mg/dL,\n\nWhat are the log-odds they are high risk for heart disease?\nWhat are the odds they are high risk for heart disease?\nWhat is the probability they are high risk for heart disease? Use the odds to calculate your answer.\n\n\n\nComparing observations\nSuppose a person’s cholesterol changes from 250 mg/dL to 200 mg/dL.\n\nHow do you expect the log-odds that this person is high risk for heart disease to change?\nHow do you expect the odds that this person is high risk for heart disease to change?"
  },
  {
    "objectID": "ae/ae-0-movies.html",
    "href": "ae/ae-0-movies.html",
    "title": "Movie budgets and revenues",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is a demo only. You do not have a corresponding repository for it and you’re not expected to turn in anything for it.\nWe will look at the relationship between budget and revenue for movies made in the United States in 1986 to 2020. The dataset is created based on data from the Internet Movie Database (IMDB).\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse  # for data analysis and visualisation\n  , magrittr # for piping data between operations\n  , scales   # for pretty axis labels\n  , DT       # for interactive table\n)"
  },
  {
    "objectID": "ae/ae-0-movies.html#data",
    "href": "ae/ae-0-movies.html#data",
    "title": "Movie budgets and revenues",
    "section": "Data",
    "text": "Data\nThe movies data set includes basic information about each movie including budget, genre, movie studio, director, etc. A full list of the variables may be found here.\n\nmovies &lt;- \n  readr::read_csv(\n    \"https://raw.githubusercontent.com/danielgrijalva/movie-stats/master/movies.csv\"\n    , show_col_types = FALSE\n    )\n\nView the first 10 rows of data.\n\nmovies\n\n# A tibble: 7,668 × 15\n   name   rating genre  year released score  votes director writer star  country\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n 1 The S… R      Drama  1980 June 13…   8.4 9.27e5 Stanley… Steph… Jack… United…\n 2 The B… R      Adve…  1980 July 2,…   5.8 6.5 e4 Randal … Henry… Broo… United…\n 3 Star … PG     Acti…  1980 June 20…   8.7 1.20e6 Irvin K… Leigh… Mark… United…\n 4 Airpl… PG     Come…  1980 July 2,…   7.7 2.21e5 Jim Abr… Jim A… Robe… United…\n 5 Caddy… R      Come…  1980 July 25…   7.3 1.08e5 Harold … Brian… Chev… United…\n 6 Frida… R      Horr…  1980 May 9, …   6.4 1.23e5 Sean S.… Victo… Bets… United…\n 7 The B… R      Acti…  1980 June 20…   7.9 1.88e5 John La… Dan A… John… United…\n 8 Ragin… R      Biog…  1980 Decembe…   8.2 3.30e5 Martin … Jake … Robe… United…\n 9 Super… PG     Acti…  1980 June 19…   6.8 1.01e5 Richard… Jerry… Gene… United…\n10 The L… R      Biog…  1980 May 16,…   7   1   e4 Walter … Bill … Davi… United…\n# ℹ 7,658 more rows\n# ℹ 4 more variables: budget &lt;dbl&gt;, gross &lt;dbl&gt;, company &lt;chr&gt;, runtime &lt;dbl&gt;\n\n\nThe ___ dataset has ___ observations and ___ variables."
  },
  {
    "objectID": "ae/ae-0-movies.html#analysis",
    "href": "ae/ae-0-movies.html#analysis",
    "title": "Movie budgets and revenues",
    "section": "Analysis",
    "text": "Analysis\n\nGross over time\nWe begin by looking at how the average gross revenue (gross) has changed over time. Since we want to visualize the results, we will choose a few genres of interest for the analysis.\n\ngenre_list &lt;- c(\"Comedy\", \"Action\", \"Animation\", \"Horror\")\n\nThen, we will filter for these genres and visualize the average gross revenue over time.\n\nmovies %&gt;%\n  dplyr::filter(genre %in% genre_list) %&gt;% \n  dplyr::group_by(genre,year) %&gt;%\n  dplyr::summarise(avg_gross = mean(gross), .groups = \"keep\") %&gt;%\n  ggplot(mapping = aes(x = year, y = avg_gross, color= genre)) +\n    geom_point() + \n    geom_line() +\n    scale_color_viridis_d() +\n    scale_y_continuous(labels = label_dollar()) +\n    labs(\n      x = \"Year\",\n      y = \"Average Gross Revenue (US Dollars)\",\n      color = \"Genre\",\n      title = \"Gross Revenue Over Time\"\n    )\n\n\n\n\n\n\n\n\nThe plot suggests …\n\n\nBudget and gross\nNext, let’s see the relationship between a movie’s budget and its gross revenue.\n\nmovies %&gt;%\n  dplyr::filter(genre %in% genre_list, budget &gt; 0) %&gt;% \n  ggplot(mapping = aes(x=log(budget), y = log(gross), color=genre)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ genre) + \n  scale_color_viridis_d() +\n  labs(\n    x = \"Log-transformed Budget\",\n    y = \"Log-transformed Gross Revenue\"\n  )"
  },
  {
    "objectID": "ae/ae-0-movies.html#exercises",
    "href": "ae/ae-0-movies.html#exercises",
    "title": "Movie budgets and revenues",
    "section": "Exercises",
    "text": "Exercises\n\nSuppose we fit a regression model for each genre that uses budget to predict gross revenue. What are the signs of the correlation between budget and gross and the slope in each regression equation?\nSuppose we fit the regression model from the previous question. Which genre would you expect to have the smallest residuals, on average (residual = observed revenue - predicted revenue)?\nIn the remaining time, discuss the following: Notice in the graph above that budget and gross are log-transformed. Why are the log-transformed values of the variables displayed rather than the original values (in U.S. dollars)?"
  },
  {
    "objectID": "ae/ae-0-movies.html#appendix",
    "href": "ae/ae-0-movies.html#appendix",
    "title": "Movie budgets and revenues",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of genres in the data set:\n\nmovies %&gt;% \n  distinct(genre) %&gt;%\n  arrange(genre) %&gt;% \n  datatable()"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html",
    "href": "ae/ae-2-dcbikeshare.html",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-2-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#data",
    "href": "ae/ae-2-dcbikeshare.html#data",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- readr::read_csv(\"data/dcbikeshare.csv\")\n\nSee AE 1 for the first part of this analysis."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 1\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 2\nNext, let’s look at how the daily bike rentals differ by season. Let’s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a “smoothed out histogram”. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 3\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 4\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#modeling",
    "href": "ae/ae-2-dcbikeshare.html#modeling",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 5\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 6\nUsing the data you filtered in Exercise 5, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#synthesis",
    "href": "ae/ae-2-dcbikeshare.html#synthesis",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 7\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]"
  },
  {
    "objectID": "old_index.html",
    "href": "old_index.html",
    "title": "BSMM-8740: Data Analytic Methods & Algorithms",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\nAE\nLab\nLab Solution\nExam\nProject\n\n\n\n\n1\nMon, Sep 11\nTidyverse & Git\n📖\n🖥️\n📋\n\n\n\n\n\n\n\n\nLab 1\n\n\n\n\n\n\n\n\n\n2\nMon, Sep 18\nEDA and Feature engineering\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 2\n\n\n\n\n\n\n\n\n\n3\nMon, Sep 25\nThe Recipes package\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 3\n\n\n\n\n\n\n\n\n\n4\nMon, Oct 02\nRegression methods\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 4\n\n\n\n\n\n\n\n\n\n5\nMon, Oct 16\nThe Models Package [DRAFT STAGE]\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 5\n\n\n\n\n\n\n\n\n\n6\nMon, Oct 23\nClassification methods [DRAFT STAGE]\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 6\n\n\n\n\n\n\n\n\n\n6\nMon, Oct 30\nTime Series methods [DRAFT STAGE]\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 7\n\n\n\n\n\n\n\n\n\n6\nMon, Oct 06\nCausality: DAGs [DRAFT STAGE]\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 8\n\n\n\n\n\n\n\n\n\n6\nMon, Nov 13\nCausality: Effects [DRAFT STAGE]\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 9\n\n\n\n\n\n\n\n\n\n6\nMon, Nov 20\nMonte Carlo methods [DRAFT STAGE]\n\n\n\n\n\n\n\n\n\n\n\nLab 10\n\n\n\n💻\n\n\n\n\n\n6\nMon, Nov 27\nBayesian methods [DRAFT STAGE]\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 11\n\n\n\n💻\n\n\n\n\n\n6\nMon, Dec 04\nAdvanced Topics [DRAFT STAGE]\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nLab 12\n\n\n\n💻"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "BSMM-8740: Data Analytic Methods & Algorithms",
    "section": "",
    "text": "This course is the exploration of an analytical framework for method selection and model building to help students develop professional capability in data-based techniques of data analytics. A focus will be placed on comparing and selecting appropriate methodology to conduct advanced statistical analysis and on building predictive modeling in order to create a competitive advantage in business operations with efficient analytical methods and data modeling.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Louis (Lou) L. Odette was most recently Head of Advanced Analytics at Greenery Retail Canada. Prior to joining Greenery, Lou was Vice President of Quantitative Modeling & Research at AIG in Boston with responsibilities for review and validation of existing risk models and risk metrics, in addition to modeling and assessment of alternative asset-class risks. He holds a PhD in Electrical Engineering from MIT and an MSc in Mathematics from Oxford University. Spoiler alert: the picture to the right was taken many years ago.\n\n\n\n\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesdays 2:30pm - 3:30 pm\n(on Teams by request)\nMeeting ID: 230 892 400 109\nPasscode: eXFjwj\n\n\nOr call in (audio only)\n+1 226-782-3511,,488681491# Windsor\n(866) 603-5721,,488681491# (Toll-free)\nPhone Conference ID: 488 681 491#\n\n\n\nIf this time doesn’t work for you or you’d like to schedule a one-on-one meeting, you can email me at lodette@uwindsor.ca.",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Louis (Lou) L. Odette was most recently Head of Advanced Analytics at Greenery Retail Canada. Prior to joining Greenery, Lou was Vice President of Quantitative Modeling & Research at AIG in Boston with responsibilities for review and validation of existing risk models and risk metrics, in addition to modeling and assessment of alternative asset-class risks. He holds a PhD in Electrical Engineering from MIT and an MSc in Mathematics from Oxford University. Spoiler alert: the picture to the right was taken many years ago.\n\n\n\n\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesdays 2:30pm - 3:30 pm\n(on Teams by request)\nMeeting ID: 230 892 400 109\nPasscode: eXFjwj\n\n\nOr call in (audio only)\n+1 226-782-3511,,488681491# Windsor\n(866) 603-5721,,488681491# (Toll-free)\nPhone Conference ID: 488 681 491#\n\n\n\nIf this time doesn’t work for you or you’d like to schedule a one-on-one meeting, you can email me at lodette@uwindsor.ca.",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-team.html#teaching-assistants",
    "href": "course-team.html#teaching-assistants",
    "title": "Teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\n\n\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\n\nBinny Kaur\nTBD: starting Sept 18, 2024\nTBD",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_10.html#recap-of-last-week",
    "title": "Bayesian Methods",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced Markov Chain methods for integration and sampling from probability distributions.\nWe also built a basic understanding of the tools for sampling in Bayesian analysis."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#this-week",
    "href": "slides/BSMM_8740_lec_10.html#this-week",
    "title": "Bayesian Methods",
    "section": "This week",
    "text": "This week\n\nWe will explore Bayesian methods in greater detail, including Bayesian workflow and model comparison.\nWe will use BRMS (Bayesian regression models with Stan) one of the popular R packages for Bayesian analysis."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#brms",
    "href": "slides/BSMM_8740_lec_10.html#brms",
    "title": "Bayesian Methods",
    "section": "BRMS",
    "text": "BRMS\n\nReference materials for BRMS can be found here and here.\nInstructions for installing BRMS can be found here and here. The basic steps (in order) are\n\nConfigure the C++ toolchain (use RTools in Windows)\nInstall Stan and verify the Stan installation\nInstall BRMS"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#bayesian-estimation-1",
    "href": "slides/BSMM_8740_lec_10.html#bayesian-estimation-1",
    "title": "Bayesian Methods",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\n\nA statistical model \\(M\\) is a model of a random process that could have generated our observable data. The observable data \\(\\mathcal{D}\\) contains both dependent variables \\(\\mathcal{D}_\\mathrm{DV}\\) and independent variables \\(\\mathcal{D}_\\mathrm{IV}\\)\nA model \\(M\\) for data \\(\\mathcal{D}\\) fixes a likelihood function for \\(\\mathcal{D}_\\mathrm{DV}\\). The likelihood function often has parameters, represented by a parameter vector \\(\\theta\\).\nUsing Bayes rule we can estimate the model parameters \\(\\theta\\) from the data\n\\[\n\\underbrace{P_M(\\theta \\vert \\mathcal{D})}_{\\text{Posterior}} =\n    \\frac{1}{\\underbrace{P(\\mathcal{D},\\theta)}_{\\text{Normalization}}}\n\\overbrace{P_M(\\mathcal{D}_\\mathrm{DV} \\vert\\mathcal{D}_\\mathrm{IV}, \\theta)}^{\\text{Likelihood}}\\overbrace{P_M(\\theta)}^{\\text{Prior}}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#maximum-likelihood-mle",
    "href": "slides/BSMM_8740_lec_10.html#maximum-likelihood-mle",
    "title": "Bayesian Methods",
    "section": "Maximum likelihood (MLE)",
    "text": "Maximum likelihood (MLE)\nRecall that for a linear regression \\(y\\sim \\mathcal{N}(\\beta x,\\sigma^2)\\) the likelihood of any one observation \\(y_i\\) is (with \\(\\theta\\) representing the set of parameters)\n\\[\n\\pi\\left(\\left.y_{i}\\right|x_{i},\\beta,\\sigma^{2}\\right)=\\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(y_{i}-\\beta x_{i})^{2}}{2\\sigma^{2}}}\n\\] and the log-likelihood of \\(N\\) observations \\(\\{y_i\\}_{i=1}^N\\) is\n\\[\n\\log\\prod_{i=1}^{N}\\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right) = \\sum_{i=1}^{N}\\log \\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#maximum-likelihood-mle-1",
    "href": "slides/BSMM_8740_lec_10.html#maximum-likelihood-mle-1",
    "title": "Bayesian Methods",
    "section": "Maximum likelihood (MLE)",
    "text": "Maximum likelihood (MLE)\nThe maximum likelihood estimate of \\(\\beta\\) is\n\\[\n\\hat{\\theta}_{\\text{MLE}}=\\arg\\max_{\\theta} -\\sum_{i=1}^{N}\\log \\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)\n\\]\n\\[\n\\log\\prod_{i=1}^{N}\\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right) = \\sum_{i=1}^{N}\\log \\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)\n\\]\nthis is equivalent to minimizing the sum of the squared errors, and is also called the a priori estimate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#bayesian-model",
    "href": "slides/BSMM_8740_lec_10.html#bayesian-model",
    "title": "Bayesian Methods",
    "section": "Bayesian model",
    "text": "Bayesian model\nThe Bayesian model for linear regression is (to within a scaling constant)\n\\[\n\\begin{align*}\n\\pi_{\\theta\\vert\\mathcal{D}}\\left(\\left.\\theta\\right|y_i,x_i\\right)\\sim\\pi_{\\mathcal{D}}\\left(\\left.y_i\\right|x_i,\\theta\\right)\\times\\pi_\\theta\\left(\\theta\\right)\n\\end{align*}\n\\]\nwhere the parameters are \\(\\theta=\\{\\beta,\\sigma^2\\}\\).\n\nIn words: the joint probability of the parameters given the observed volume data is equal to (to within a scaling constant) the probability of the observed volume data given the parameters, times the prior probabilities of the parameters. In practice we refer to the probabilities as likelihoods, and use log-likelihoods to avoid numerical problems arising from the product of small probabilities."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#max-a-posteriori-estimate-mape",
    "href": "slides/BSMM_8740_lec_10.html#max-a-posteriori-estimate-mape",
    "title": "Bayesian Methods",
    "section": "Max a posteriori estimate (MAPE)",
    "text": "Max a posteriori estimate (MAPE)\nThe maximum a posteriori estimate of the parameters is\n\\[\n\\begin{align*}\n\\hat{\\theta}_{\\text{MAP}} & =\\arg\\max_{\\theta}\\log\\prod_{i=1}^{N}\\pi_{\\theta\\vert\\mathcal{D}}\\left(\\left. \\theta \\right|y_{i},x_{i}\\right)\\\\\n& =\\arg\\max_{\\theta}\\sum_{i=1}^{N} \\left(\\log \\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)+\\log\\pi_\\theta\\left(\\theta\\right)\\right)\\\\\n& =\\arg\\min_{\\theta}-\\sum_{i=1}^{N} \\left(\\log \\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)+\\log\\pi_\\theta\\left(\\theta\\right)\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#max-a-posteriori-estimate-mape-1",
    "href": "slides/BSMM_8740_lec_10.html#max-a-posteriori-estimate-mape-1",
    "title": "Bayesian Methods",
    "section": "Max a posteriori estimate (MAPE)",
    "text": "Max a posteriori estimate (MAPE)\n\nIf \\(\\theta\\) is not uncertain/random, then \\(\\pi(\\theta)=1\\rightarrow\\log\\pi(\\theta)=0\\) and the MAPE is equal to the MLE.\nIn linear regression we assume \\(\\pi(\\sigma^2) = 1\\), so If \\(\\theta\\) is uncertain/random it remains to give a prior distribution to \\(\\beta\\) (as a vector of dimension \\(D\\), in general). Assume \\(\\beta=\\mathscr{N}\\left(0,\\lambda^{-1}I\\right)\\) (with a single scale constant \\(\\lambda\\)), then\n\\[\n\\pi_\\theta(\\beta) =  \\frac{1}{\\sqrt{(2\\pi)^D \\frac{1}{\\lambda^D}}}exp(-\\frac{1}{2}(\\beta - 0)^\\top (\\frac{1}{\\lambda} I)^{-1} (\\beta - 0)) =\n\\frac{\\lambda^{\\frac{D}{2}}}{(2\\pi)^{\\frac{D}{2}}}exp(-\\frac{\\lambda}{2} \\beta^\\top \\beta)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#max-a-posteriori-estimate-mape-2",
    "href": "slides/BSMM_8740_lec_10.html#max-a-posteriori-estimate-mape-2",
    "title": "Bayesian Methods",
    "section": "Max a posteriori estimate (MAPE)",
    "text": "Max a posteriori estimate (MAPE)\n\nWith Gaussian \\(\\pi(\\beta)\\) as in the last slide, and likelihood\n\\[\n\\pi(y_i \\vert x_i, \\beta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y_i- x_i^\\top\\beta)^2)\n\\]\nwe have, for linear regression\n\\[\n\\begin{align*}\n\\hat{\\theta}_{\\text{MAP}} & =\\arg\\min_{\\theta}-\\sum_{i=1}^{N}\\left(\\log\\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)+\\log\\pi_\\theta\\left(\\theta\\right)\\right)\\\\\n& =\\arg\\min_{\\theta}\\left(\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}(y_i-x_i^{T}\\beta)^{2}+\\frac{\\lambda}{2}\\beta^\\top\\beta\\right)\n\\end{align*}\n\\]\nwhich turns out to be a linear interpolation between the prior mean and the sample mean weighted by their respective covariances."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#max-a-posteriori-estimate-mape-3",
    "href": "slides/BSMM_8740_lec_10.html#max-a-posteriori-estimate-mape-3",
    "title": "Bayesian Methods",
    "section": "Max a posteriori estimate (MAPE)",
    "text": "Max a posteriori estimate (MAPE)\nIn this MAPE for linear regression, with Gaussian priors, the posterior is also a Gaussian, since the product of Gaussian distributions is proportional to a Gaussian distribution, and the denominator in Bayes rule reflects the proportionality.\nHowever, this a special case where the likelihood and prior distributions are conjugate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\nIf the posterior distribution \\(\\pi_{\\theta\\vert\\mathcal{D}}\\) is in the same probability distribution family as the prior probability distribution \\(\\pi_\\theta\\) (generally this means they are the same to within a normalizing constant), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function \\(\\pi_{\\mathcal{D}}\\).\nA conjugate prior is an algebraic convenience, giving a closed-form expression for the posterior."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors-1",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors-1",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\nexample 1\nConsider a random variable which consists of the number of successes \\(s\\) in \\(n\\) Bernoulli trials with unknown probability of success \\(p\\in[0,1]\\). This random variable will follow the binomial distribution, with a probability mass function of the form\n\\[\n\\pi(s\\vert p)={n \\choose s}p^s(1-p)^{n-s}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors-2",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors-2",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\nexample 1\nThe usual conjugate prior for the Bernoulli is the beta distribution with parameters (\\(\\alpha, \\beta\\)):\n\\[\n\\pi_\\theta(p;\\alpha,\\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{\\mathrm{B}(\\alpha,\\beta)}\n\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are chosen to reflect any existing belief or information (\\(\\alpha = 1\\) and \\(\\beta = 1\\) would give a uniform distribution) and \\(\\mathrm{B}(\\alpha,\\beta)\\) is the Beta function acting as a normalising constant."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors-3",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors-3",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\nexample 1\nIf we sample this random variable and get \\(s'\\) successes and \\(f=n-s'\\) failures, then we have\n\\[\n\\begin{align*}\n\\pi_{\\theta\\vert\\mathcal{D}}(p=x) & \\sim x^{s'}(1-x)^{n-s'}\\times x^{\\alpha-1}(1-x)^{\\beta-1}\\\\\n& \\sim x^{s'+\\alpha-1}(1-x)^{(n-s')+\\beta-1}\\\\\n& \\sim\\pi_{\\theta\\vert\\mathcal{D}}(x;s'+\\alpha-1,(n-s')+\\beta-1)\n\\end{align*}\n\\]\nAnd the posterior distribution \\(\\pi_{\\theta\\vert\\mathcal{D}}\\) is in the same probability distribution family as the prior probability distribution \\(\\pi_\\theta\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors-4",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors-4",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\nexample 2\n\nSuppose you’ve been asked to find the probability that you have exactly 5 outages at your website during any hour of the day. Your client has limited data, in fact they have just three data points \\(y=[3,4,1]\\)\nIf you assume that the data are generated by a Poisson distribution (which has a single parameter, the rate \\(\\lambda\\)), then the maximum likelihood estimate of \\(\\lambda\\) is \\(\\lambda=\\frac{3+4+1}{3}\\approx 2.67\\), and you would estimate the probability as:\n\\[\n\\pi(n=5\\vert\\lambda\\approx 2.67) = \\frac{\\lambda^n e^{-\\lambda}}{n!}=\\frac{2.67^5 e^{-2.67}}{5!}=0.078\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors-5",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors-5",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\nexample 2\n\nWe’ve assumed that the observed data \\(y\\) is most likely to have been generated by a Poisson distribution with MLE for \\(\\lambda= 2.67\\).\nBut the data could also have come from another Poisson distribution, e.g., one with \\(\\lambda =3\\), or \\(\\lambda =2\\), etc. In fact, there is an infinite number of Poisson distributions that could have generated the observed data.\nWith relatively few data points, we should be quite uncertain about which exact Poisson distribution generated this data. Intuitively we should instead take a weighted average of the probability of \\(\\pi(y\\ge 0|\\lambda )\\) for each of those Poisson distributions, weighted by how likely they each are, given the data we’ve observed.\nThis is exactly what Bayes’ Rule does."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors-6",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors-6",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\nexample 2\n\nLuckily, the Poisson distribution has a conjugate, the Gamma distribution:\n\\[\n\\pi_\\theta(x\\lambda;\\alpha,\\beta)=\\frac{x^{\\alpha-1}e^{-\\beta \\lambda}\\beta^\\alpha}{\\Gamma(\\alpha)}\n\\]\nand\n\\[\n\\begin{align*}\n\\pi\\left(y\\vert\\lambda\\right) & =\\prod_{i=1}^{n}\\frac{\\lambda^{y_{i}}e^{-\\lambda}}{y_{i}!}\\\\\n& =\\lambda^{n\\bar{y}}e^{-n\\lambda}\\prod_{i=1}^{n}\\frac{1}{y_{i}!}\n\\end{align*}\n\\]\nso \\(\\pi\\left(y\\vert\\lambda\\right)\\times \\pi_\\theta(x;\\alpha,\\beta)\\sim \\lambda^{n\\bar{y}+\\alpha-1}e^{-(n+\\beta)\\lambda}\\sim\\pi_\\theta(\\lambda;n\\bar{y}+\\alpha,(n+\\beta))\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors-7",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors-7",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\nexample 2\n\n\n\nGiven our observations \\(\\lambda=\\frac{3+4+1}{3}\\approx 2.67\\), we might arbitrarily take the prior as a Gamma with \\(\\alpha=9;\\;\\beta = 2\\) (mean \\(\\alpha/\\beta\\)) so that the prior and posterior look like this:\n\n\nCode\n.shape &lt;- 9 + 3*2.67  ; .rate &lt;- 3+2\n\ntibble::tibble(lambda = seq(0.04,15,0.02), plambda = dgamma(seq(0.04,15,0.02), shape=9, rate = 2), measure = \"prior\") |&gt; \n  dplyr::bind_rows(\n    tibble::tibble(lambda = seq(0.04,15,0.02), plambda = dgamma(seq(0.04,15,0.02), shape=.shape, rate = .rate), measure = \"posterior\")\n  ) |&gt; \n  ggplot(aes(x=lambda, y = plambda, color = measure)) + geom_line() + \n  labs(title = \"Probability distributions for Lambda\", subtitle = \" prior and posterior predictive\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n.shape &lt;- 9 + 3*2.67  ; .rate &lt;- 3+2\nci &lt;- qgamma(c(0.05,0.95), shape=.shape, rate = .rate) |&gt; \n  purrr::map_dbl(function(x){round( x^5 * exp(-x)/factorial(5),digits=3)})\n\n\nGiven the posterior hyperparameters, we can finally compute the posterior predictive distribution (\\(\\alpha=9+3\\times2.67\\), \\(\\beta=3\\times2\\), \\(\\mu=2.835\\)) and estimate the 90% confidence intervals for the probability as 0.046, 0.175. This much more conservative estimate reflects the uncertainty in the model parameters, which the posterior predictive takes into account."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors-8",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors-8",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\n\nConjugate priors offer ease of computation, efficiency in updates, and clear interpretation, making them suitable for simpler models or real-time applications.\nHowever, they are often too restrictive for complex or non-standard models, where flexibility in capturing prior beliefs is crucial."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#conjugate-priors-9",
    "href": "slides/BSMM_8740_lec_10.html#conjugate-priors-9",
    "title": "Bayesian Methods",
    "section": "Conjugate priors",
    "text": "Conjugate priors\n\nLimitations of Conjugate Priors\n\nRestrictive Choice of Priors: Conjugate priors limit the choice of prior distributions to a specific family. This restricts flexibility, especially if real-world data suggests a prior belief outside the conjugate family, which may not accurately capture prior knowledge or uncertainty.\nLack of Flexibility with Complex Models: Conjugate priors are often insufficient for complex models, such as hierarchical or multi-level models, where dependencies between variables require more flexible priors. Non-conjugate priors, despite being more computationally intensive, can better accommodate the complexity of these models.\nPotential for Over-Simplification: Choosing a conjugate prior for convenience can sometimes lead to oversimplification, especially if it does not match the true prior knowledge. This can introduce bias and reduce the model’s accuracy in reflecting genuine prior beliefs.\nLess Suitable for Non-Standard Likelihoods: Conjugate priors work best with specific likelihood functions, and many real-world problems don’t have standard likelihoods that match conjugate prior forms. In these cases, using a conjugate prior may be infeasible, forcing the use of non-conjugate methods."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-1",
    "href": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-1",
    "title": "Bayesian Methods",
    "section": "Generative (bayesian) modelling",
    "text": "Generative (bayesian) modelling\n\nGenerative Bayesian modeling is an approach in Bayesian statistics where we create models that describe how data is generated, often by specifying probability distributions for both observed and latent (unobserved) variables/parameters.\nThis process involves defining a generative process — a step-by-step probabilistic framework that models how data could have arisen."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-2",
    "href": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-2",
    "title": "Bayesian Methods",
    "section": "Generative (bayesian) modelling",
    "text": "Generative (bayesian) modelling\nNote the similarity to DAGs:\nGiven a DAG, we next assign probability distributions to each node/variable, relate the nodes through the parameters of the distributions and finally assign priors for any remaining/undetermined parameters, including parameters used to define the relationships between variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-3",
    "href": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-3",
    "title": "Bayesian Methods",
    "section": "Generative (bayesian) modelling",
    "text": "Generative (bayesian) modelling\n\nKey Components of Generative Bayesian Modeling\n\nDefining Priors: Start by assigning prior distributions to parameters, reflecting prior beliefs about these parameters before observing data. Priors incorporate domain knowledge and regularize the model.\nLikelihood Function: Specify the likelihood, which represents the probability of observing the data given the parameters. It describes how data is assumed to be generated given specific values of the model parameters.\nPosterior Inference: Using Bayes’ theorem, combine priors and the likelihood to calculate the posterior distribution of the parameters. This posterior reflects updated beliefs after seeing the data.\nLatent Variables and Hierarchies: Generative models can include latent variables, which represent unobserved or hidden factors, and hierarchical structures, which model data with multiple levels of variation (e.g., nested or grouped data)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-4",
    "href": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-4",
    "title": "Bayesian Methods",
    "section": "Generative (bayesian) modelling",
    "text": "Generative (bayesian) modelling\n\nAdvantages of Generative Bayesian Models\n\nInterpretability: Generative models explicitly describe the data-generating process, making them interpretable and suitable for understanding complex systems.\nPredictive Power: By learning the underlying structure of the data, generative models can predict unseen outcomes and infer missing or latent data.\nUncertainty Quantification: Bayesian models naturally quantify uncertainty in parameter estimates and predictions, enhancing decision-making with probabilistic insights."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-5",
    "href": "slides/BSMM_8740_lec_10.html#generative-bayesian-modelling-5",
    "title": "Bayesian Methods",
    "section": "Generative (bayesian) modelling",
    "text": "Generative (bayesian) modelling\n\nLimitations\n\nComputational Complexity: Generative Bayesian models, especially those with complex hierarchical structures or latent variables, can be computationally demanding, often requiring methods like MCMC.\nModel Specification: The accuracy of generative Bayesian models heavily depends on correctly specifying the generative process, which can be challenging with limited domain knowledge or complex data.\n\nApplications\nGenerative Bayesian modeling is widely used in areas requiring a deep understanding of data-generating processes, such as healthcare, natural language processing, finance, and other business applications. It enables tasks like anomaly detection, missing data imputation, and causal inference by modeling the probability structure of observed and unobserved variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBRMS (Bayesian Regression Models using Stan) is an R package for fitting complex Bayesian regression models using Stan, a powerful probabilistic programming language. BRMS provides a high-level, formula-based interface in R, making it easy to specify and fit Bayesian models.\nBRMS is useful for performing complex Bayesian analyses in R without diving into raw Stan code.\nWe’ll start with a very simple example."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-1",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-1",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBasic model: manufacturing failures per \\(N\\) units\n\n\n\nWe express the likelihood for our coin toss example as\n\\[y_{i} \\sim \\operatorname{Bernoulli}(\\theta)\\]\nand our prior will be\n\\[\\theta \\sim \\operatorname{Beta}(\\alpha, \\beta)\\]\n\n\n\nCode\ndat &lt;- readr::read_csv(\"data/z15N50.csv\", show_col_types = FALSE)\n\ndat |&gt; \n  dplyr::mutate(y = y |&gt; as.character()) |&gt; \n  ggplot(aes(x = y)) +\n  geom_bar() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal(base_size = 36)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-2",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-2",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBasic model: Prior\n\n\nuninformative : \\(\\theta_c\\sim\\mathrm{Beta}(1,1)\\)\nweakly informative : \\(\\theta_c\\sim\\mathrm{Beta}(5,2)\\)\nstrongly informative : \\(\\theta_c\\sim\\mathrm{Beta}(50,20)\\)\npoint-valued : \\(\\theta_c\\sim\\mathrm{Beta}(\\alpha,\\beta)\\) with \\(\\alpha,\\beta\\rightarrow\\infty\\) and \\(\\alpha,\\beta=52\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-3",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-3",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBasic model: manufacturing failures per \\(N\\) units\n\n\nfit the model\nfit8.1 &lt;-\n  brms::brm(data = dat, \n      family = brms::bernoulli(link = identity),\n      formula = y ~ 1,\n      brms::prior(beta(2, 2), class = Intercept, lb = 0, ub = 1),\n      iter = 500 + 3334, warmup = 500, chains = 3,\n      seed = 8,\n      file = \"fits/fit08.01\")\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 3834 [  0%]  (Warmup)\nChain 1: Iteration:  383 / 3834 [  9%]  (Warmup)\nChain 1: Iteration:  501 / 3834 [ 13%]  (Sampling)\nChain 1: Iteration:  883 / 3834 [ 23%]  (Sampling)\nChain 1: Iteration: 1266 / 3834 [ 33%]  (Sampling)\nChain 1: Iteration: 1649 / 3834 [ 43%]  (Sampling)\nChain 1: Iteration: 2032 / 3834 [ 52%]  (Sampling)\nChain 1: Iteration: 2415 / 3834 [ 62%]  (Sampling)\nChain 1: Iteration: 2798 / 3834 [ 72%]  (Sampling)\nChain 1: Iteration: 3181 / 3834 [ 82%]  (Sampling)\nChain 1: Iteration: 3564 / 3834 [ 92%]  (Sampling)\nChain 1: Iteration: 3834 / 3834 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.002 seconds (Warm-up)\nChain 1:                0.016 seconds (Sampling)\nChain 1:                0.018 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 3834 [  0%]  (Warmup)\nChain 2: Iteration:  383 / 3834 [  9%]  (Warmup)\nChain 2: Iteration:  501 / 3834 [ 13%]  (Sampling)\nChain 2: Iteration:  883 / 3834 [ 23%]  (Sampling)\nChain 2: Iteration: 1266 / 3834 [ 33%]  (Sampling)\nChain 2: Iteration: 1649 / 3834 [ 43%]  (Sampling)\nChain 2: Iteration: 2032 / 3834 [ 52%]  (Sampling)\nChain 2: Iteration: 2415 / 3834 [ 62%]  (Sampling)\nChain 2: Iteration: 2798 / 3834 [ 72%]  (Sampling)\nChain 2: Iteration: 3181 / 3834 [ 82%]  (Sampling)\nChain 2: Iteration: 3564 / 3834 [ 92%]  (Sampling)\nChain 2: Iteration: 3834 / 3834 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.003 seconds (Warm-up)\nChain 2:                0.018 seconds (Sampling)\nChain 2:                0.021 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 3834 [  0%]  (Warmup)\nChain 3: Iteration:  383 / 3834 [  9%]  (Warmup)\nChain 3: Iteration:  501 / 3834 [ 13%]  (Sampling)\nChain 3: Iteration:  883 / 3834 [ 23%]  (Sampling)\nChain 3: Iteration: 1266 / 3834 [ 33%]  (Sampling)\nChain 3: Iteration: 1649 / 3834 [ 43%]  (Sampling)\nChain 3: Iteration: 2032 / 3834 [ 52%]  (Sampling)\nChain 3: Iteration: 2415 / 3834 [ 62%]  (Sampling)\nChain 3: Iteration: 2798 / 3834 [ 72%]  (Sampling)\nChain 3: Iteration: 3181 / 3834 [ 82%]  (Sampling)\nChain 3: Iteration: 3564 / 3834 [ 92%]  (Sampling)\nChain 3: Iteration: 3834 / 3834 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.003 seconds (Warm-up)\nChain 3:                0.017 seconds (Sampling)\nChain 3:                0.02 seconds (Total)\nChain 3: \n\n\nfit the model\nplot(fit8.1)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-4",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-4",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBasic model: manufacturing failures per \\(N\\) units\n\n\nfitposterior summary\n\n\n\nprint(fit8.1)\n\n Family: bernoulli \n  Links: mu = identity \nFormula: y ~ 1 \n   Data: dat (Number of observations: 50) \n  Draws: 3 chains, each with iter = 3834; warmup = 500; thin = 1;\n         total post-warmup draws = 10002\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept     0.32      0.06     0.20     0.44 1.00\n          Bulk_ESS Tail_ESS\nIntercept     3338     4646\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nbrms::posterior_summary(fit8.1, robust = T)\n\n            Estimate Est.Error      Q2.5    Q97.5\nb_Intercept   0.3128   0.06276   0.20028   0.4444\nIntercept     0.3128   0.06276   0.20028   0.4444\nlprior        0.2544   0.10549  -0.03978   0.3929\nlp__        -32.0707   0.30671 -34.35061 -31.8453"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-5",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-5",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBasic model: manufacturing failures per \\(N\\) units\n\n\ndrawsdensityACF\n\n\n\n\nextract the draws\ndraws &lt;- brms::as_draws_df(fit8.1) \ndraws\n\n\n# A draws_df: 3334 iterations, 3 chains, and 4 variables\n   b_Intercept Intercept lprior lp__\n1         0.26      0.26  0.133  -32\n2         0.19      0.19 -0.080  -34\n3         0.21      0.21  0.011  -33\n4         0.27      0.27  0.165  -32\n5         0.28      0.28  0.193  -32\n6         0.31      0.31  0.244  -32\n7         0.31      0.31  0.254  -32\n8         0.31      0.31  0.250  -32\n9         0.29      0.29  0.219  -32\n10        0.38      0.38  0.344  -32\n# ... with 9992 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\n\n\n\n\nplot the draws by chain\ndraws |&gt; \n  dplyr::mutate(chain = .chain) |&gt; \n  bayesplot::mcmc_dens_overlay(pars = vars(b_Intercept)) \n\n\n\n\n\n\n\n\n\n\n\n\n\nplot acf by chain\ndraws |&gt; \n  dplyr::mutate(chain = .chain) |&gt; \n  bayesplot::mcmc_acf(pars = vars(b_Intercept), lags = 35) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-6",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-6",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBasic model: manufacturing failures per \\(N\\) units\n\ndat |&gt; \n  dplyr::mutate(y = y |&gt; as.character()) |&gt; \n  ggplot(aes(x = y, fill = s)) +\n  geom_bar(show.legend = F) +\n  ggthemes::scale_fill_colorblind() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal() +\n  facet_wrap(~ s)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-7",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-7",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBasic model: manufacturing failures per \\(N\\) units\n\n\nfit the two-site model\nfit8.2 &lt;-\n  brms::brm(data = dat, \n      family = brms::bernoulli(identity),\n      y ~ 0 + s,\n      brms::prior(beta(2, 2), class = b, lb = 0, ub = 1),\n      iter = 2000, warmup = 500, cores = 4, chains = 4,\n      seed = 8,\n      file = \"fits/fit08.02\")\n\n\n\n\nplot chains for both sites\nplot(fit8.2, widths = c(2, 3))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-8",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-8",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBasic model: manufacturing failures per \\(N\\) units\n\n\nfitpairsdrawstable\n\n\n\nsummary(fit8.2)\n\n Family: bernoulli \n  Links: mu = identity \nFormula: y ~ 0 + s \n   Data: dat (Number of observations: 15) \n  Draws: 4 chains, each with iter = 2000; warmup = 500; thin = 1;\n         total post-warmup draws = 6000\n\nRegression Coefficients:\n         Estimate Est.Error l-95% CI u-95% CI Rhat\nsLondon      0.37      0.14     0.12     0.65 1.00\nsWindsor     0.67      0.13     0.39     0.90 1.00\n         Bulk_ESS Tail_ESS\nsLondon      4942     4036\nsWindsor     5003     3408\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\nCode\npairs(fit8.2,\n      off_diag_args = list(size = 1/3, alpha = 1/3))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndraws &lt;- brms::as_draws_df(fit8.2)\n\ndraws &lt;-\n  draws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor, theta_London  = b_sLondon) |&gt; \n  dplyr::mutate(`theta_Windsor - theta_London` = theta_Windsor - theta_London)\n\nlong_draws &lt;-\n  draws |&gt; \n  dplyr::select(starts_with(\"theta\")) |&gt; \n  tidyr::pivot_longer(everything()) |&gt; \n  dplyr::mutate(name = factor(name, levels = c(\"theta_Windsor\", \"theta_London\", \"theta_Windsor - theta_London\"))) \n\n\nWarning: Dropping 'draws_df' class as required\nmetadata was removed.\n\n\nCode\nlong_draws |&gt; \n  ggplot(aes(x = value, y = 0, fill = name)) +\n  tidybayes::stat_histinterval(point_interval = tidybayes::mode_hdi, .width = .95,\n                    slab_color = \"white\", outline_bars = T,\n                    normalize = \"panels\") +\n  scale_fill_manual(values = ggthemes::colorblind_pal()(8)[2:4], breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlong_draws |&gt; \n  dplyr::group_by(name) |&gt; \n  tidybayes::mode_hdi()\n\n\n# A tibble: 3 × 7\n  name     value  .lower .upper .width .point .interval\n  &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 theta_W… 0.695  0.420   0.917   0.95 mode   hdi      \n2 theta_L… 0.317  0.105   0.626   0.95 mode   hdi      \n3 theta_W… 0.330 -0.0689  0.664   0.95 mode   hdi"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-9",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-9",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nBasic model: manufacturing failures per \\(N\\) units\n\n\nfitplotsindependence\n\n\n\n\nseparate theta estimates\nfit8.3 &lt;-\n  brms::brm(data = dat, \n      family = brms::bernoulli(identity),\n      y ~ 0 + s,\n      prior =\n        c(brms::prior(beta(2, 2), class = b, coef = sWindsor),\n          brms::prior(beta(2, 2), class = b, coef = sLondon),\n          # this just sets the lower and upper bounds\n          brms::prior(beta(2, 2), class = b, lb = 0, ub = 1)),\n      iter = 2000, warmup = 500, cores = 4, chains = 4,\n      sample_prior = \"only\",\n      seed = 8,\n      file = \"fits/fit08.03\")\n\n\n\n\n\n\nseparate theta estimates\ndraws &lt;- brms::as_draws_df(fit8.3) |&gt; \n  dplyr::select(starts_with(\"b_\"))\n\n\nWarning: Dropping 'draws_df' class as required\nmetadata was removed.\n\n\nseparate theta estimates\n# dat |&gt; \n#   dplyr::group_by(s) |&gt; \n#   dplyr::summarise(z = sum(y), N = dplyr::n()) |&gt; \n#   dplyr::mutate(`z/N` = z / N)\n\nlevels &lt;- c(\"theta_Windsor\", \"theta_London\", \"theta_Windsor - theta_London\")\nd_line &lt;-\n  tibble::tibble(value = c(.75, .286, .75 - .286),\n         name  =  factor(c(\"theta_Windsor\", \"theta_London\", \"theta_Windsor - theta_London\"), \n                         levels = levels))\n\ndraws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  dplyr::mutate(\"theta_Windsor - theta_London\" = theta_Windsor - theta_London) |&gt; \n  tidyr::pivot_longer(contains(\"theta\")) |&gt; \n  dplyr::mutate(name = factor(name, levels = levels)) |&gt;\n  \n  ggplot(aes(x = value, y = 0)) +\n  tidybayes::stat_histinterval(point_interval = tidybayes::mode_hdi, .width = .95,\n                    fill = ggthemes::colorblind_pal()(8)[5], normalize = \"panels\") +\n  geom_vline(data = d_line, \n             aes(xintercept = value), \n             linetype = 2) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(\"The dashed vertical lines mark off \"*italic(z[s])/italic(N[s]))) +\n  cowplot::theme_cowplot() +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod for separate theta estimates\ndraws |&gt; \n  dplyr::rename(theta_Windsor = b_sWindsor,\n         theta_London  = b_sLondon) |&gt; \n  \n  ggplot(aes(x = theta_Windsor, y = theta_London)) +\n  geom_point(alpha = 1/4, color = ggthemes::colorblind_pal()(8)[6]) +\n  coord_equal() +\n  cowplot::theme_minimal_grid()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-10",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-10",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nRecall from the last chapter that our likelihood is the Bernoulli distribution,\n\\[y_i \\sim \\operatorname{Bernoulli}(\\theta).\\]\nWe’ll use the beta density for our prior distribution for \\(\\theta\\),\n\\[\\theta \\sim \\operatorname{Beta}(\\alpha, \\beta).\\]\nAnd we can re-express \\(\\alpha\\) and \\(\\beta\\) in terms of the mode \\(\\omega\\) and concentration \\(\\kappa\\), such that\n\\[\\alpha = \\omega(\\kappa - 2) + 1 \\;\\;\\; \\textrm{and} \\;\\;\\; \\beta = (1 - \\omega)(\\kappa - 2) + 1.\\]\nAs a consequence, we can re-express \\(\\theta\\) as\n\\[\\theta \\sim \\operatorname{Beta}(\\omega(\\kappa - 2) + 1, (1 - \\omega)(\\kappa - 2) + 1).\\]\nThe value of \\(\\kappa\\) governs how near \\(\\theta\\) is to \\(\\omega\\), with larger values of \\(\\kappa\\) generating values of \\(\\theta\\) more concentrated near \\(\\omega\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-11",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-11",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nUsing \\(s\\) for shape and \\(r\\) for rate are as follows:\n\\[\n\\begin{align*}\ns & =\\frac{\\mu^{2}}{\\sigma^{2}}\\;\\;\\;\\text{and}\\;\\;\\;r=\\frac{\\mu}{\\sigma^{2}}\\;\\;\\;\\text{for mean}\\;\\;\\;\\mu&gt;0\\\\\ns & =1+\\omega r\\;\\;\\;\\text{where}\\;\\;\\;r=\\frac{\\omega+\\sqrt{\\omega^{2}+4\\sigma^{2}}}{2\\sigma^{2}}\\;\\;\\;\\text{for mode}\\;\\;\\;\\omega&gt;0\n\\end{align*}\n\\]\nThe value of \\(\\kappa\\) governs how near \\(\\theta\\) is to \\(\\omega\\), with larger values of \\(\\kappa\\) generating values of \\(\\theta\\) more concentrated near \\(\\omega\\).\n\n\nre-parameterization functions\ngamma_s_and_r_from_mean_sd &lt;- function(mean, sd) {\n  if (mean &lt;= 0) stop(\"mean must be &gt; 0\")\n  if (sd   &lt;= 0) stop(\"sd must be &gt; 0\")\n  shape &lt;- mean^2 / sd^2\n  rate  &lt;- mean   / sd^2\n  return(list(shape = shape, rate = rate))\n}\n\ngamma_s_and_r_from_mode_sd &lt;- function(mode, sd) {\n  if (mode &lt;= 0) stop(\"mode must be &gt; 0\")\n  if (sd   &lt;= 0) stop(\"sd must be &gt; 0\")\n  rate  &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)\n  shape &lt;- 1 + mode * rate\n  return(list(shape = shape, rate = rate))\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-12",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-12",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nmultiple sites\n\n\ndataby siteoverall\n\n\n\ndat &lt;- readr::read_csv(\"data/TherapeuticTouchData.csv\", show_col_types = FALSE)\n\n\n\n\n\nCode\ndat |&gt; \n  dplyr::mutate(y = y |&gt; as.character()) |&gt; \n  \n  ggplot(aes(y = y)) +\n  geom_bar(aes(fill = after_stat(count))) +\n  scale_fill_viridis_c(option = \"A\", end = .7, breaks = NULL) +\n  scale_x_continuous(breaks = 0:4 * 2, expand = c(0, NA), limits = c(0, 9)) +\n  cowplot::theme_minimal_vgrid() +\n  cowplot::panel_border() +\n  facet_wrap(~ s, ncol = 7)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\na_purple &lt;- viridis::viridis_pal(option = \"A\")(9)[4]\ndat |&gt; \n  dplyr::group_by(s) |&gt; \n  dplyr::summarize(mean = mean(y)) |&gt;\n  \n  ggplot(aes(x = mean)) +\n  geom_histogram(color = \"white\", fill = a_purple,\n                 linewidth = .2, binwidth = .1) +\n  scale_x_continuous(\"Proportion Not Failing\", limits = c(0, 1)) +\n  scale_y_continuous(\"# Practitioners\", expand = c(0, NA)) +\n  cowplot::theme_minimal_hgrid()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-13",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-13",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nfit multiple sites | hierarchical\n\n\nfitplotacfneffdrawsdraws tablepairs\n\n\n\n\n\nCode\nfit9.1 &lt;-\n  brms::brm(data = dat,\n      family = brms::bernoulli(link = logit),\n      y ~ 1 + (1 | s),\n      prior = c(brms::prior(normal(0, 1.5), class = Intercept),\n                brms::prior(normal(0, 1), class = sd)),\n      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4,\n      seed = 9,\n      file = \"fits/fit09.01\")\nprint(fit9.1)\n\n\n\n\n\n\n\nCode\nplot(fit9.1, widths = c(2, 3))\n\n\n\n\n\npopulation parameter estimates (log-odds)\n\n\n\n\n\n\n\n\nCode\ndraws &lt;- brms::as_draws_df(fit9.1)\ndraws |&gt; \n  dplyr::mutate(chain = .chain) |&gt; \n  bayesplot::mcmc_acf(pars = vars(b_Intercept, sd_s__Intercept), lags = 10) +\n  cowplot::theme_cowplot()\n\n\n\n\n\nacf by chain; population parameters\n\n\n\n\n\n\n\n\nCode\nbayesplot::neff_ratio(fit9.1) |&gt; \n  bayesplot::mcmc_neff() +\n  cowplot::theme_cowplot(font_size = 12)\n\n\n\n\n\neffective samples by variable\n\n\n\n\n\n\n\n\nCode\ndraws_small &lt;-\n  draws |&gt; \n  # convert the linear model parameters to the probability space with `inv_logit_scaled()`\n  dplyr::mutate(`theta[1]`  = (b_Intercept + `r_s[S01,Intercept]`) |&gt; brms::inv_logit_scaled(),\n         `theta[14]` = (b_Intercept + `r_s[S14,Intercept]`) |&gt; brms::inv_logit_scaled(),\n         `theta[28]` = (b_Intercept + `r_s[S28,Intercept]`) |&gt; brms::inv_logit_scaled()) |&gt; \n  # make the difference distributions\n  dplyr::mutate(`theta[1] - theta[14]`  = `theta[1]`  - `theta[14]`,\n         `theta[1] - theta[28]`  = `theta[1]`  - `theta[28]`,\n         `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) |&gt; \n  dplyr::select(starts_with(\"theta\"))\n\ndraws_small |&gt; \n  tidyr::pivot_longer(everything()) |&gt; \n  # this line is unnecessary, but will help order the plots \n  dplyr::mutate(name = factor(name, levels = c(\"theta[1]\", \"theta[14]\", \"theta[28]\", \n                                        \"theta[1] - theta[14]\", \"theta[1] - theta[28]\", \"theta[14] - theta[28]\"))) |&gt; \n\n  ggplot(aes(x = value, y = 0)) +\n  tidybayes::stat_histinterval(point_interval = ggdist::mode_hdi, .width = .95,\n                    fill = a_purple, breaks = 40, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(NULL) +\n  cowplot::theme_minimal_hgrid() +\n  facet_wrap(~ name, scales = \"free\", ncol = 3)\n\n\n\n\n\nselected parameters and contrasts on probability scale\n\n\n\n\n\n\n\n\nCode\ndraws_small |&gt; \n  tidyr::pivot_longer(everything()) |&gt;\n  dplyr::group_by(name) |&gt; \n  tidybayes::mode_hdi(value) |&gt; \n  gt::gt(\"name\") |&gt; \n  gt::fmt_number(columns=value:.width, decimals=3) |&gt; \n  gt::tab_header(title = \"Parameter values and contrasts (logit, aka log-odds)\", subtitle = \"sites 1,14,28\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nParameter values and contrasts (logit, aka log-odds)\n\n\nsites 1,14,28\n\n\n\nvalue\n.lower\n.upper\n.width\n.point\n.interval\n\n\n\n\ntheta[1]\n0.421\n0.206\n0.521\n0.950\nmode\nhdi\n\n\ntheta[1] - theta[14]\n0.000\n−0.274\n0.120\n0.950\nmode\nhdi\n\n\ntheta[1] - theta[28]\n−0.002\n−0.428\n0.068\n0.950\nmode\nhdi\n\n\ntheta[14]\n0.437\n0.289\n0.579\n0.950\nmode\nhdi\n\n\ntheta[14] - theta[28]\n−0.001\n−0.328\n0.105\n0.950\nmode\nhdi\n\n\ntheta[28]\n0.463\n0.363\n0.710\n0.950\nmode\nhdi\n\n\n\n\n\n\n\n\n\n\n\nCode\ncolor_scheme_set(\"purple\")\nbayesplot::bayesplot_theme_set(theme_default() + cowplot::theme_minimal_grid())\n\nstats::coef(fit9.1, summary = F)$s |&gt; \n  brms::inv_logit_scaled() |&gt; \n  data.frame() |&gt; \n  rename(`theta[1]`  = S01.Intercept, \n         `theta[14]` = S14.Intercept, \n         `theta[28]` = S28.Intercept) |&gt; \n  dplyr::select(`theta[1]`, `theta[14]`, `theta[28]`) |&gt; \n  bayesplot::mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8)) \n\n\n\n\n\npairs plot on probability scale"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-14",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-14",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nShrinkage in hierarchical models\n\n“In typical hierarchical models, the estimates of low-level parameters are pulled closer together than they would be if there were not a higher-level distribution. This pulling together is called shrinkage of the estimates”\nFurther,\n“shrinkage is a rational implication of hierarchical model structure, and is (usually) desired by the analyst because the shrunken parameter estimates are less affected by random sampling noise than estimates derived without hierarchical structure. Intuitively, shrinkage occurs because the estimate of each low-level parameter is influenced from two sources: (1) the subset of data that are directly dependent on the low-level parameter, and (2) the higher-level parameters on which the low-level parameter depends. The higher- level parameters are affected by all the data, and therefore the estimate of a low-level parameter is affected indirectly by all the data, via their influence on the higher-level parameters.”1\n\nDoing Bayesian Data Analysis, J.K. Kruschke (2015)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-15",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-15",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nShrinkage in hierarchical models\n\n\nmultilevel shrinkage\ndat |&gt; \n  group_by(s) |&gt; \n  summarise(p = mean(y)) |&gt; \n  mutate(theta = coef(fit9.1)$s[, 1, \"Intercept\"] |&gt; inv_logit_scaled()) |&gt; \n  pivot_longer(-s) |&gt; \n  # add a little jitter to reduce the overplotting\n  mutate(value = value + runif(n = n(), min = -0.02, max = 0.02),\n         name  = if_else(name == \"p\", \"italic(z/N)\", \"theta\")) |&gt; \n\n  ggplot(aes(x = value, y = name, group = s)) +\n  geom_point(color = alpha(a_purple, 1/2)) +\n  geom_line(linewidth = 1/3, alpha = 1/3) +\n  scale_x_continuous(breaks = 0:5 / 5, expand = c(0.01, 0.01), limits = 0:1) +\n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  labs(title = \"Multilevel shrinkage in model\",\n       x = \"data proportion or theta value\") +\n  cowplot::theme_minimal_hgrid() +\n  cowplot::panel_border()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-16",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-16",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nspeeding up model fits\n\nFull Bayesian inference is a computationally very demanding task and often we need to run our models faster in shorter walltime (elapsed real time). With modern computers we have multiple processors available on a given machine such that the use of running the inference in parallel will shorten the overall walltime.\nWhile between-chain parallelization is straightforward by merely launching multiple chains at the same time, the use of within-chain parallelization is more complicated in various ways. This vignette is an introduction to within-chain parallelization with brms."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-17",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-17",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nregression models\n\n\n\n\n\nCode\n# load data\ndata(\"epilepsy\", package = \"brms\")\n\nepilepsy\n\n\n    Age Base Trt patient visit count obs     zAge\n1    31   11   0       1     1     5   1  0.42500\n2    30   11   0       2     1     3   2  0.26528\n3    25    6   0       3     1     2   3 -0.53327\n4    36    8   0       4     1     4   4  1.22355\n5    22   66   0       5     1     7   5 -1.01241\n6    29   27   0       6     1     5   6  0.10557\n7    31   12   0       7     1     6   7  0.42500\n8    42   52   0       8     1    40   8  2.18182\n9    37   23   0       9     1     5   9  1.38326\n10   28   10   0      10     1    14  10 -0.05414\n11   36   52   0      11     1    26  11  1.22355\n12   24   33   0      12     1    12  12 -0.69299\n13   23   18   0      13     1     4  13 -0.85270\n14   36   42   0      14     1     7  14  1.22355\n15   26   87   0      15     1    16  15 -0.37356\n16   26   50   0      16     1    11  16 -0.37356\n17   28   18   0      17     1     0  17 -0.05414\n18   31  111   0      18     1    37  18  0.42500\n19   32   18   0      19     1     3  19  0.58471\n20   21   20   0      20     1     3  20 -1.17212\n21   29   12   0      21     1     3  21  0.10557\n22   21    9   0      22     1     3  22 -1.17212\n23   32   17   0      23     1     2  23  0.58471\n24   25   28   0      24     1     8  24 -0.53327\n25   30   55   0      25     1    18  25  0.26528\n26   40    9   0      26     1     2  26  1.86240\n27   19   10   0      27     1     3  27 -1.49154\n28   22   47   0      28     1    13  28 -1.01241\n29   18   76   1      29     1    11  29 -1.65125\n30   32   38   1      30     1     8  30  0.58471\n31   20   19   1      31     1     0  31 -1.33183\n32   30   10   1      32     1     3  32  0.26528\n33   18   19   1      33     1     2  33 -1.65125\n34   24   24   1      34     1     4  34 -0.69299\n35   30   31   1      35     1    22  35  0.26528\n36   35   14   1      36     1     5  36  1.06384\n37   27   11   1      37     1     2  37 -0.21385\n38   20   67   1      38     1     3  38 -1.33183\n39   22   41   1      39     1     4  39 -1.01241\n40   28    7   1      40     1     2  40 -0.05414\n41   23   22   1      41     1     0  41 -0.85270\n42   40   13   1      42     1     5  42  1.86240\n43   33   46   1      43     1    11  43  0.74442\n44   21   36   1      44     1    10  44 -1.17212\n45   35   38   1      45     1    19  45  1.06384\n46   25    7   1      46     1     1  46 -0.53327\n47   26   36   1      47     1     6  47 -0.37356\n48   25   11   1      48     1     2  48 -0.53327\n49   22  151   1      49     1   102  49 -1.01241\n50   32   22   1      50     1     4  50  0.58471\n51   25   41   1      51     1     8  51 -0.53327\n52   35   32   1      52     1     1  52  1.06384\n53   21   56   1      53     1    18  53 -1.17212\n54   41   24   1      54     1     6  54  2.02211\n55   32   16   1      55     1     3  55  0.58471\n56   26   22   1      56     1     1  56 -0.37356\n57   21   25   1      57     1     2  57 -1.17212\n58   36   13   1      58     1     0  58  1.22355\n59   37   12   1      59     1     1  59  1.38326\n60   31   11   0       1     2     3  60  0.42500\n61   30   11   0       2     2     5  61  0.26528\n62   25    6   0       3     2     4  62 -0.53327\n63   36    8   0       4     2     4  63  1.22355\n64   22   66   0       5     2    18  64 -1.01241\n65   29   27   0       6     2     2  65  0.10557\n66   31   12   0       7     2     4  66  0.42500\n67   42   52   0       8     2    20  67  2.18182\n68   37   23   0       9     2     6  68  1.38326\n69   28   10   0      10     2    13  69 -0.05414\n70   36   52   0      11     2    12  70  1.22355\n71   24   33   0      12     2     6  71 -0.69299\n72   23   18   0      13     2     4  72 -0.85270\n73   36   42   0      14     2     9  73  1.22355\n74   26   87   0      15     2    24  74 -0.37356\n75   26   50   0      16     2     0  75 -0.37356\n76   28   18   0      17     2     0  76 -0.05414\n77   31  111   0      18     2    29  77  0.42500\n78   32   18   0      19     2     5  78  0.58471\n79   21   20   0      20     2     0  79 -1.17212\n80   29   12   0      21     2     4  80  0.10557\n81   21    9   0      22     2     4  81 -1.17212\n82   32   17   0      23     2     3  82  0.58471\n83   25   28   0      24     2    12  83 -0.53327\n84   30   55   0      25     2    24  84  0.26528\n85   40    9   0      26     2     1  85  1.86240\n86   19   10   0      27     2     1  86 -1.49154\n87   22   47   0      28     2    15  87 -1.01241\n88   18   76   1      29     2    14  88 -1.65125\n89   32   38   1      30     2     7  89  0.58471\n90   20   19   1      31     2     4  90 -1.33183\n91   30   10   1      32     2     6  91  0.26528\n92   18   19   1      33     2     6  92 -1.65125\n93   24   24   1      34     2     3  93 -0.69299\n94   30   31   1      35     2    17  94  0.26528\n95   35   14   1      36     2     4  95  1.06384\n96   27   11   1      37     2     4  96 -0.21385\n97   20   67   1      38     2     7  97 -1.33183\n98   22   41   1      39     2    18  98 -1.01241\n99   28    7   1      40     2     1  99 -0.05414\n100  23   22   1      41     2     2 100 -0.85270\n101  40   13   1      42     2     4 101  1.86240\n102  33   46   1      43     2    14 102  0.74442\n103  21   36   1      44     2     5 103 -1.17212\n104  35   38   1      45     2     7 104  1.06384\n105  25    7   1      46     2     1 105 -0.53327\n106  26   36   1      47     2    10 106 -0.37356\n107  25   11   1      48     2     1 107 -0.53327\n108  22  151   1      49     2    65 108 -1.01241\n109  32   22   1      50     2     3 109  0.58471\n110  25   41   1      51     2     6 110 -0.53327\n111  35   32   1      52     2     3 111  1.06384\n112  21   56   1      53     2    11 112 -1.17212\n113  41   24   1      54     2     3 113  2.02211\n114  32   16   1      55     2     5 114  0.58471\n115  26   22   1      56     2    23 115 -0.37356\n116  21   25   1      57     2     3 116 -1.17212\n117  36   13   1      58     2     0 117  1.22355\n118  37   12   1      59     2     4 118  1.38326\n119  31   11   0       1     3     3 119  0.42500\n120  30   11   0       2     3     3 120  0.26528\n121  25    6   0       3     3     0 121 -0.53327\n122  36    8   0       4     3     1 122  1.22355\n123  22   66   0       5     3     9 123 -1.01241\n124  29   27   0       6     3     8 124  0.10557\n125  31   12   0       7     3     0 125  0.42500\n126  42   52   0       8     3    21 126  2.18182\n127  37   23   0       9     3     6 127  1.38326\n128  28   10   0      10     3     6 128 -0.05414\n129  36   52   0      11     3     6 129  1.22355\n130  24   33   0      12     3     8 130 -0.69299\n131  23   18   0      13     3     6 131 -0.85270\n132  36   42   0      14     3    12 132  1.22355\n133  26   87   0      15     3    10 133 -0.37356\n134  26   50   0      16     3     0 134 -0.37356\n135  28   18   0      17     3     3 135 -0.05414\n136  31  111   0      18     3    28 136  0.42500\n137  32   18   0      19     3     2 137  0.58471\n138  21   20   0      20     3     6 138 -1.17212\n139  29   12   0      21     3     3 139  0.10557\n140  21    9   0      22     3     3 140 -1.17212\n141  32   17   0      23     3     3 141  0.58471\n142  25   28   0      24     3     2 142 -0.53327\n143  30   55   0      25     3    76 143  0.26528\n144  40    9   0      26     3     2 144  1.86240\n145  19   10   0      27     3     4 145 -1.49154\n146  22   47   0      28     3    13 146 -1.01241\n147  18   76   1      29     3     9 147 -1.65125\n148  32   38   1      30     3     9 148  0.58471\n149  20   19   1      31     3     3 149 -1.33183\n150  30   10   1      32     3     1 150  0.26528\n151  18   19   1      33     3     7 151 -1.65125\n152  24   24   1      34     3     1 152 -0.69299\n153  30   31   1      35     3    19 153  0.26528\n154  35   14   1      36     3     7 154  1.06384\n155  27   11   1      37     3     0 155 -0.21385\n156  20   67   1      38     3     7 156 -1.33183\n157  22   41   1      39     3     2 157 -1.01241\n158  28    7   1      40     3     1 158 -0.05414\n159  23   22   1      41     3     4 159 -0.85270\n160  40   13   1      42     3     0 160  1.86240\n161  33   46   1      43     3    25 161  0.74442\n162  21   36   1      44     3     3 162 -1.17212\n163  35   38   1      45     3     6 163  1.06384\n164  25    7   1      46     3     2 164 -0.53327\n165  26   36   1      47     3     8 165 -0.37356\n166  25   11   1      48     3     0 166 -0.53327\n167  22  151   1      49     3    72 167 -1.01241\n168  32   22   1      50     3     2 168  0.58471\n169  25   41   1      51     3     5 169 -0.53327\n170  35   32   1      52     3     1 170  1.06384\n171  21   56   1      53     3    28 171 -1.17212\n172  41   24   1      54     3     4 172  2.02211\n173  32   16   1      55     3     4 173  0.58471\n174  26   22   1      56     3    19 174 -0.37356\n175  21   25   1      57     3     0 175 -1.17212\n176  36   13   1      58     3     0 176  1.22355\n177  37   12   1      59     3     3 177  1.38326\n178  31   11   0       1     4     3 178  0.42500\n179  30   11   0       2     4     3 179  0.26528\n180  25    6   0       3     4     5 180 -0.53327\n181  36    8   0       4     4     4 181  1.22355\n182  22   66   0       5     4    21 182 -1.01241\n183  29   27   0       6     4     7 183  0.10557\n184  31   12   0       7     4     2 184  0.42500\n185  42   52   0       8     4    12 185  2.18182\n186  37   23   0       9     4     5 186  1.38326\n187  28   10   0      10     4     0 187 -0.05414\n188  36   52   0      11     4    22 188  1.22355\n189  24   33   0      12     4     4 189 -0.69299\n190  23   18   0      13     4     2 190 -0.85270\n191  36   42   0      14     4    14 191  1.22355\n192  26   87   0      15     4     9 192 -0.37356\n193  26   50   0      16     4     5 193 -0.37356\n194  28   18   0      17     4     3 194 -0.05414\n195  31  111   0      18     4    29 195  0.42500\n196  32   18   0      19     4     5 196  0.58471\n197  21   20   0      20     4     7 197 -1.17212\n198  29   12   0      21     4     4 198  0.10557\n199  21    9   0      22     4     4 199 -1.17212\n200  32   17   0      23     4     5 200  0.58471\n201  25   28   0      24     4     8 201 -0.53327\n202  30   55   0      25     4    25 202  0.26528\n203  40    9   0      26     4     1 203  1.86240\n204  19   10   0      27     4     2 204 -1.49154\n205  22   47   0      28     4    12 205 -1.01241\n206  18   76   1      29     4     8 206 -1.65125\n207  32   38   1      30     4     4 207  0.58471\n208  20   19   1      31     4     0 208 -1.33183\n209  30   10   1      32     4     3 209  0.26528\n210  18   19   1      33     4     4 210 -1.65125\n211  24   24   1      34     4     3 211 -0.69299\n212  30   31   1      35     4    16 212  0.26528\n213  35   14   1      36     4     4 213  1.06384\n214  27   11   1      37     4     4 214 -0.21385\n215  20   67   1      38     4     7 215 -1.33183\n216  22   41   1      39     4     5 216 -1.01241\n217  28    7   1      40     4     0 217 -0.05414\n218  23   22   1      41     4     0 218 -0.85270\n219  40   13   1      42     4     3 219  1.86240\n220  33   46   1      43     4    15 220  0.74442\n221  21   36   1      44     4     8 221 -1.17212\n222  35   38   1      45     4     7 222  1.06384\n223  25    7   1      46     4     3 223 -0.53327\n224  26   36   1      47     4     8 224 -0.37356\n225  25   11   1      48     4     0 225 -0.53327\n226  22  151   1      49     4    63 226 -1.01241\n227  32   22   1      50     4     4 227  0.58471\n228  25   41   1      51     4     7 228 -0.53327\n229  35   32   1      52     4     5 229  1.06384\n230  21   56   1      53     4    13 230 -1.17212\n231  41   24   1      54     4     0 231  2.02211\n232  32   16   1      55     4     3 232  0.58471\n233  26   22   1      56     4     8 233 -0.37356\n234  21   25   1      57     4     1 234 -1.17212\n235  36   13   1      58     4     0 235  1.22355\n236  37   12   1      59     4     2 236  1.38326\n        zBase\n1   -0.757173\n2   -0.757173\n3   -0.944403\n4   -0.869511\n5    1.302363\n6   -0.158035\n7   -0.719727\n8    0.778117\n9   -0.307820\n10  -0.794619\n11   0.778117\n12   0.066641\n13  -0.495050\n14   0.403656\n15   2.088731\n16   0.703225\n17  -0.495050\n18   2.987437\n19  -0.495050\n20  -0.420158\n21  -0.719727\n22  -0.832065\n23  -0.532496\n24  -0.120589\n25   0.890456\n26  -0.832065\n27  -0.794619\n28   0.590887\n29   1.676824\n30   0.253872\n31  -0.457604\n32  -0.794619\n33  -0.457604\n34  -0.270374\n35  -0.008251\n36  -0.644835\n37  -0.757173\n38   1.339809\n39   0.366210\n40  -0.906957\n41  -0.345266\n42  -0.682281\n43   0.553441\n44   0.178980\n45   0.253872\n46  -0.906957\n47   0.178980\n48  -0.757173\n49   4.485281\n50  -0.345266\n51   0.366210\n52   0.029195\n53   0.927902\n54  -0.270374\n55  -0.569942\n56  -0.345266\n57  -0.232927\n58  -0.682281\n59  -0.719727\n60  -0.757173\n61  -0.757173\n62  -0.944403\n63  -0.869511\n64   1.302363\n65  -0.158035\n66  -0.719727\n67   0.778117\n68  -0.307820\n69  -0.794619\n70   0.778117\n71   0.066641\n72  -0.495050\n73   0.403656\n74   2.088731\n75   0.703225\n76  -0.495050\n77   2.987437\n78  -0.495050\n79  -0.420158\n80  -0.719727\n81  -0.832065\n82  -0.532496\n83  -0.120589\n84   0.890456\n85  -0.832065\n86  -0.794619\n87   0.590887\n88   1.676824\n89   0.253872\n90  -0.457604\n91  -0.794619\n92  -0.457604\n93  -0.270374\n94  -0.008251\n95  -0.644835\n96  -0.757173\n97   1.339809\n98   0.366210\n99  -0.906957\n100 -0.345266\n101 -0.682281\n102  0.553441\n103  0.178980\n104  0.253872\n105 -0.906957\n106  0.178980\n107 -0.757173\n108  4.485281\n109 -0.345266\n110  0.366210\n111  0.029195\n112  0.927902\n113 -0.270374\n114 -0.569942\n115 -0.345266\n116 -0.232927\n117 -0.682281\n118 -0.719727\n119 -0.757173\n120 -0.757173\n121 -0.944403\n122 -0.869511\n123  1.302363\n124 -0.158035\n125 -0.719727\n126  0.778117\n127 -0.307820\n128 -0.794619\n129  0.778117\n130  0.066641\n131 -0.495050\n132  0.403656\n133  2.088731\n134  0.703225\n135 -0.495050\n136  2.987437\n137 -0.495050\n138 -0.420158\n139 -0.719727\n140 -0.832065\n141 -0.532496\n142 -0.120589\n143  0.890456\n144 -0.832065\n145 -0.794619\n146  0.590887\n147  1.676824\n148  0.253872\n149 -0.457604\n150 -0.794619\n151 -0.457604\n152 -0.270374\n153 -0.008251\n154 -0.644835\n155 -0.757173\n156  1.339809\n157  0.366210\n158 -0.906957\n159 -0.345266\n160 -0.682281\n161  0.553441\n162  0.178980\n163  0.253872\n164 -0.906957\n165  0.178980\n166 -0.757173\n167  4.485281\n168 -0.345266\n169  0.366210\n170  0.029195\n171  0.927902\n172 -0.270374\n173 -0.569942\n174 -0.345266\n175 -0.232927\n176 -0.682281\n177 -0.719727\n178 -0.757173\n179 -0.757173\n180 -0.944403\n181 -0.869511\n182  1.302363\n183 -0.158035\n184 -0.719727\n185  0.778117\n186 -0.307820\n187 -0.794619\n188  0.778117\n189  0.066641\n190 -0.495050\n191  0.403656\n192  2.088731\n193  0.703225\n194 -0.495050\n195  2.987437\n196 -0.495050\n197 -0.420158\n198 -0.719727\n199 -0.832065\n200 -0.532496\n201 -0.120589\n202  0.890456\n203 -0.832065\n204 -0.794619\n205  0.590887\n206  1.676824\n207  0.253872\n208 -0.457604\n209 -0.794619\n210 -0.457604\n211 -0.270374\n212 -0.008251\n213 -0.644835\n214 -0.757173\n215  1.339809\n216  0.366210\n217 -0.906957\n218 -0.345266\n219 -0.682281\n220  0.553441\n221  0.178980\n222  0.253872\n223 -0.906957\n224  0.178980\n225 -0.757173\n226  4.485281\n227 -0.345266\n228  0.366210\n229  0.029195\n230  0.927902\n231 -0.270374\n232 -0.569942\n233 -0.345266\n234 -0.232927\n235 -0.682281\n236 -0.719727\n\n\n\n\n\nCode\noutput &lt;- \n  capture.output(\n    fit_epi_gaussian1 &lt;- \n      brms::brm(\n        count ~ 1 + Trt, data = epilepsy, \n        silent = 2, seed = 8740, file = \"fits/fit_epi_gaussian1\"\n      )\n  )\nfit_epi_gaussian1\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: count ~ 1 + Trt \n   Data: epilepsy (Number of observations: 236) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept     8.42      1.18     6.10    10.77 1.00\nTrt1         -0.59      1.61    -3.79     2.53 1.00\n          Bulk_ESS Tail_ESS\nIntercept     4159     2452\nTrt1          4159     2701\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat\nsigma    12.36      0.57    11.31    13.52 1.00\n      Bulk_ESS Tail_ESS\nsigma     4118     2864\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-18",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-18",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nHere we see, for each parameter separately, traces of the four MCMC chains in different colors with post-warmup iterations on the x-axis and parameter values on the y-axis. These trace plots are showing ideal convergence: All chains are overlaying each other nicely, are stationary (horizontal on average), and show little autocorrelation.\n\n\n\nCode\nbrms::mcmc_plot(fit_epi_gaussian1, type = \"trace\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-19",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-19",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nHaving convinced us of convergence, at least graphically for now, we can move on to inspecting the posteriors. e.g., we can plot histograms of the posterior samples per parameter.\n\n\n\nCode\nbrms::mcmc_plot(fit_epi_gaussian1, type = \"hist\", bins=30)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-20",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-20",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nIn R, we can easily perform data transformations by first extracting the posterior draws and then applying the transformation per draws in a vectorized manner. e.g., using the functionality from the posterior:: package\n\n\nCode\ndraws &lt;- \n  posterior::as_draws_df(fit_epi_gaussian1) |&gt; \n  posterior::mutate_variables(\n    variance = sigma^2, mu_Trt = b_Intercept + b_Trt1\n  )\n\ndraws\n\n\n# A draws_df: 1000 iterations, 4 chains, and 8 variables\n   b_Intercept b_Trt1 sigma Intercept lprior lp__\n1          8.4 -1.778    13       7.4   -7.3 -933\n2          8.1  0.053    12       8.2   -7.4 -932\n3          7.8  0.690    12       8.2   -7.2 -934\n4          9.2 -2.263    13       8.0   -7.5 -934\n5          8.0 -0.013    12       8.0   -7.4 -932\n6          7.9 -0.185    13       7.8   -7.5 -934\n7          8.1 -1.169    13       7.5   -7.3 -933\n8          8.1  0.509    12       8.4   -7.2 -933\n9          8.5 -1.988    12       7.5   -7.2 -933\n10         8.4 -2.017    13       7.4   -7.4 -934\n   variance mu_Trt\n1       163    6.6\n2       154    8.2\n3       134    8.5\n4       174    6.9\n5       155    8.0\n6       176    7.7\n7       162    7.0\n8       134    8.6\n9       153    6.6\n10      173    6.4\n# ... with 3990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-21",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-21",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nWith mu_Trt we computed the model-implied predictions of the mean for the treatment group. For the control group, it would just be mu_Ctrl = \\(\\beta_0\\) for this simple model.\n\n\n\nCode\nbayesplot::mcmc_hist(draws, c(\"variance\", \"mu_Trt\"), bins = 30)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-22",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-22",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nFor more complex models, computing the predictions for different groups or, more generally, different predictor values manually becomes quite cumbersome. For this reason brms provides you with a convenient method to provide quick graphical summarizes of the model-implied predictions per predictor:\n\n\n\nCode\nce &lt;- brms::conditional_effects(fit_epi_gaussian1)\nplot(ce)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-23",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-23",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nThe error bars in the last slide are representing 95% credible intervals by default, but we can change that value if we like via the prob argument. Comparing the conditional_effects plot with our manually computed posterior of mu_Trt, we see that they actually are the same.\nTo put the mean predictions into the context of the observed data, we can also show the data as points in the plot:\n\n\n\nCode\nplot(ce, points = TRUE)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-24",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-24",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nWhat we do in conditional_effects by default is visualize the expected value (mean parameter) of the likelihood distribution, conditional on certain predictor values. In brms, this is done via the posterior_epred (posterior expected predictions) method. For example, we can run the code below to create expected posterior predictions for both groups. The resulting object contains the posterior draws in the rows and the different conditions (here, treatment groups) in the columns.\n\n\nCode\nnewdata &lt;- data.frame(Trt = c(0, 1))\npe &lt;- brms::posterior_epred(fit_epi_gaussian1, newdata = newdata)\n\n\nWe also can summarize the draws, for example, via\n\nbrms::posterior_summary(pe)\n\n     Estimate Est.Error  Q2.5 Q97.5\n[1,]    8.422     1.183 6.098 10.77\n[2,]    7.828     1.096 5.754 10.01\n\n\nIn linear models, posterior_epred directly coincides with evaluating the linear predictor \\(\\mu\\) as exemplified above. What posterior_epred does not include is the residual uncertainty, which is represented by \\(\\sigma\\) in our linear models."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-25",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-25",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nConsider again the task of evaluating predictions for the treatment group. If we are only interested in (the posterior of) the likelihood’s mean parameter, we would compute \\(\\mu^{(s)}_{\\mathrm{Trt}}=\\beta_0^{(s)} + \\beta_1^{(s)}\\).\nIn contrast, if we are interested in prediction of hypothetical new data points \\(y^{(s)}_{\\mathrm{Trt}}\\) from the treatment group (i.e., actual posterior predictions), we would sample\n\\[\ny^{(s)}_{\\mathrm{Trt}}\\sim\\mathscr{N}\\left(\\mu^{(s)}_{\\mathrm{Trt}}, \\sigma^{(s)} \\right)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-26",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-26",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nThis is exactly what happens behind the scenes when we execute the code below:\n\n\nCode\noptions(brms.plot_points = TRUE)\nbrms::conditional_effects(fit_epi_gaussian1, method = \"posterior_predict\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-27",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-27",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nWe could have also done this more manually via\n\n\n\nCode\nnewdata &lt;- data.frame(Trt = c(0, 1))\npp &lt;- brms::posterior_predict(fit_epi_gaussian1, newdata = newdata)\n\nbrms::posterior_summary(pp)\n\n\n     Estimate Est.Error   Q2.5 Q97.5\n[1,]    8.256     12.32 -15.39 33.29\n[2,]    7.837     12.63 -16.83 32.61"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-28",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-28",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nWe are already aware that linear regression model is not ideal for the epilepsy data. But how bad is it? As quick graphical method, we can use posterior predictive (PP) checks, where we compare the observed outcome data with the model predicted outcome data, that is, with the posterior predictions. In brms, we can perform PP-checks via:\n\n\n\nCode\nbrms::pp_check(fit_epi_gaussian1)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-29",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-29",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nWe see that the model predictions can neither account for the strong spike of observed outcomes close to zero nor for their right-skewness. Instead, the the model also predicts a lot of negative outcomes, which is impossible in reality because we are predicting counts of epileptic seizures.\nIn the plot, it looks as if the observed data also had few negative values (the dark blue density going below zero) but this is just an artifact of estimating a continuous density from counts. While this PP-check type is definitely not ideal to illustrate count outcome data, it still very clearly points to the shortcomings of our linear model."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-30",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-30",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nWhile the default PP-check was already eye-opening, there are lot of types that can further our understanding of model appropriateness. For example, an often very useful check is obtained by comparing the residuals = observed outcomes - model predictions with the observed outcomes, also known as residual plot. In pp_check this check type is called error_scatter_avg:\n\n\n\nCode\nbrms::pp_check(fit_epi_gaussian1, type = \"error_scatter_avg\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-31",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-31",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\nHere there is a strongly almost perfectly linear relationship indicating strong problems with the independence assumption of the errors.\nEssentially, both PP-checks have told us that our initial model is a very bad for the data at hand.\nIf you don’t know which check types are available, you can simply pass an arbitrary non-supported type name to get a list of all currently supported types:\n\n\nbrms::pp_check(fit_epi_gaussian1, type = \"help_me\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-32",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-32",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n\n\n\n\n\n\nCode\nfit_epi_student1 &lt;- \n  brms::brm(\n    count ~ Trt * Base,\n    data = epilepsy,\n    family = brms::student()\n    , silent = 2, seed = 8740, file = \"fits/fit_epi_student1\"\n  )\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.161 seconds (Warm-up)\nChain 1:                0.081 seconds (Sampling)\nChain 1:                0.242 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.14 seconds (Warm-up)\nChain 2:                0.083 seconds (Sampling)\nChain 2:                0.223 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 9e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.147 seconds (Warm-up)\nChain 3:                0.078 seconds (Sampling)\nChain 3:                0.225 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.164 seconds (Warm-up)\nChain 4:                0.075 seconds (Sampling)\nChain 4:                0.239 seconds (Total)\nChain 4: \n\n\nCode\nsummary(fit_epi_student1)\n\n\n Family: student \n  Links: mu = identity; sigma = identity; nu = identity \nFormula: count ~ Trt * Base \n   Data: epilepsy (Number of observations: 236) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept    -0.01      0.36    -0.69     0.71 1.00\nTrt1          0.46      0.63    -0.82     1.65 1.00\nBase          0.26      0.01     0.24     0.28 1.00\nTrt1:Base    -0.12      0.02    -0.16    -0.07 1.00\n          Bulk_ESS Tail_ESS\nIntercept     3393     3262\nTrt1          2269     2221\nBase          3000     2641\nTrt1:Base     2176     2357\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat\nsigma     2.09      0.21     1.71     2.54 1.00\nnu        1.38      0.18     1.08     1.78 1.00\n      Bulk_ESS Tail_ESS\nsigma     2471     2533\nnu        1795     1289\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\nCode\nbrms::pp_check(fit_epi_student1) + xlim(-30, 30)\n\n\nWarning: Removed 91 rows containing non-finite outside the\nscale range (`stat_density()`).\n\n\nWarning: Removed 7 rows containing non-finite outside the scale\nrange (`stat_density()`)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-33",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-33",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nAbsolute predictive performance\n\n\n\n\n\nCode\n# fit with a covariate\nfit_epi_gaussian2 &lt;- brms::brm(\n  count ~ Trt + Base, data = epilepsy, silent = 2, seed = 8740, file = \"fits/fit_epi_gaussian2\"\n  )\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.016 seconds (Warm-up)\nChain 1:                0.009 seconds (Sampling)\nChain 1:                0.025 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.017 seconds (Warm-up)\nChain 2:                0.007 seconds (Sampling)\nChain 2:                0.024 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.016 seconds (Warm-up)\nChain 3:                0.008 seconds (Sampling)\nChain 3:                0.024 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.016 seconds (Warm-up)\nChain 4:                0.008 seconds (Sampling)\nChain 4:                0.024 seconds (Total)\nChain 4: \n\n\nCode\n# fit with a covariate and interaction\nfit_epi_gaussian3 &lt;- brms::brm(\n  count ~ Trt * Base, data = epilepsy, silent = 2, seed = 8740, file = \"fits/fit_epi_gaussian3\"\n  )\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 1:                0.015 seconds (Sampling)\nChain 1:                0.041 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 2:                0.014 seconds (Sampling)\nChain 2:                0.043 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 3:                0.016 seconds (Sampling)\nChain 3:                0.042 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 4:                0.014 seconds (Sampling)\nChain 4:                0.042 seconds (Total)\nChain 4: \n\n\nCode\nbrms::pp_check(fit_epi_gaussian2) + theme_minimal(base_size = 18) + theme(legend.position = \"side\")\n\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\nDefault posterior predictive check for model fit_epi_gaussian2\n\n\n\n\n\n\n\nCode\nbrms::pp_check(fit_epi_gaussian2, type = \"error_scatter_avg\") + theme_minimal(base_size = 18)\n\n\nUsing all posterior draws for ppc type 'error_scatter_avg' by default.\n\n\n\n\n\nPosterior predictive check comparing observed responses (y-axis) with the predictive residuals (x-axis) of model fit_epi_gaussian2"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-34",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-34",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nMeasures of explained variance: \\(R^2\\)\n\nThe standard \\(R^2_{\\mathrm{basic}}\\) measure is defined only for Gaussian models and is often referred to as the percentage of explained variance. Note that in the Gaussian context we compute draws from the posterior \\(R^2_{\\mathrm{basic}}\\)\n\n# compute draws of the predictive errors based on posterior_epred\nerrors &lt;- brms::predictive_error(fit_epi_gaussian2, method = \"posterior_epred\")\nstr(errors)\n\n num [1:4000, 1:236] 3.96 5.06 5.08 2.36 4.58 ...\n\n\n\n# sum errors over observations\nerror_variation &lt;- rowSums(errors^2)\nstr(error_variation)\n\n num [1:4000] 15111 15238 15311 15162 15109 ...\n\n\n\n# compute R2_basic\noverall_variation &lt;- sum((epilepsy$count - mean(epilepsy$count))^2)\nR2_basic_epi_gaussian2 &lt;- 1 - error_variation / overall_variation\nbrms::posterior_summary(R2_basic_epi_gaussian2)\n\n     Estimate Est.Error   Q2.5  Q97.5\n[1,]   0.5767  0.004474 0.5652 0.5817"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-35",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-35",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nMeasures of explained variance: \\(R^2\\)\n\nThe \\(R^2_{\\mathrm{basic}}\\)is a good starting point, but it doesn’t generalize to models that are more complicated than Gaussian linear models. In particular it doesn’t readily generalize to most other likelihood families.\nWe’ll use a more general form of \\(R^2\\) that we can apply to (almost) all brms models, regardless of what likelihood families they have. The measure is based on the ratio of explained variance and the sum of explained and error variance:\n\\[\nR^2_{\\mathrm{general}} = \\frac{\\mathrm{Var}(\\hat{y})}{\\mathrm{Var}(\\hat{y})+$\\mathrm{Var}(\\hat{e})}\n\\]\nWhere \\(\\mathrm{Var}(\\hat{y})\\)) is the variance of the posterior predicted mean over observations (again posterior_epred) and \\(\\mathrm{Var}(\\hat{e})\\))) is the variance of the model-implied errors over observations, where \\(\\hat{e}=y_n-\\hat{y}_n\\).\n\nbrms::bayes_R2(fit_epi_gaussian2)\n\n   Estimate Est.Error   Q2.5  Q97.5\nR2   0.5803   0.02672 0.5224 0.6275"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-36",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-36",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nMeasures of Squared Errors\n\n\\(\\mathrm{RMSE}_{\\mathrm{basic}}\\) computes a mean square error of observations \\(n\\) for each posterior draw \\(s\\), and thus yields a posterior distribution over RMSE values\n\n\n\n# compute draws of the predictive errors based on posterior_epred\nerrors_epi_gaussian3 &lt;-\nbrms::predictive_error(fit_epi_gaussian3, method = \"posterior_epred\")\nstr(errors_epi_gaussian3)\n\n num [1:4000, 1:236] 3.03 0.75 0.99 1.47 1.43 ...\n\n\n\n# root mean of squared errors over observations\nrmse_basic_epi_gaussian3 &lt;- sqrt(rowMeans(errors_epi_gaussian3^2))\nstr(rmse_basic_epi_gaussian3)\n\n num [1:4000] 7.77 7.73 7.83 7.73 7.73 ...\n\n\n\n\n\nCode\nlattice::histogram(rmse_basic_epi_gaussian3) \n\n\n\n\n\nPosterior histogram of RMSE_basic for model fit_epi_gaussian3."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-37",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-37",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nMeasures of Squared Errors\n\nWe can also exchange the use of \\(n\\) and \\(s\\) and compute a mean square error over draws \\(s\\) for each observation \\(n\\):\n\n\n\nrmse_alt_epi_gaussian3 &lt;- \n  sqrt(colMeans(errors_epi_gaussian3^2))\nstr(rmse_alt_epi_gaussian3)\n\n num [1:236] 1.959 0.969 1.028 1.812 10.942 ...\n\n\n\n\n\nCode\nlattice::histogram(rmse_alt_epi_gaussian3) \n\n\n\n\n\nPosterior histogram of RMSE_alt for model fit_epi_gaussian3.\n\n\n\n\n\nIn this case, we get a distribution of RMSE over observations, where each individual RMSE value would be computed over the posterior predictive distribution of a single observation. Both of the above RMSE measures are fully Bayesian as they take into account the uncertainty in the posterior distribution, but in different ways."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-38",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-38",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nMeasures of Squared Errors\n\nTypically see only a point estimate \\(\\hat{\\bar{y}}_n\\) being used to represent the model-implied predictions, instead of a (posterior) distribution over such predictions for each \\(n\\). For example, for a Bayesian model this point estimate could simply be the posterior mean.\nWhen using such a point prediction approach, our RMSE definition becomes:\n\n# extract a point estimate of the predictions per observation\nppmean_epi_gaussian3 &lt;- colMeans(brms::posterior_epred(fit_epi_gaussian3))\nstr(ppmean_epi_gaussian3)\n\n num [1:236] 3.28 3.28 1.95 2.48 17.87 ...\n\n# compute RMSE based on the responses and point predictions\nrmse_point_epi_gaussian3 &lt;- sqrt(mean((epilepsy$count - ppmean_epi_gaussian3)^2))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-39",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-39",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nRelative predictive performance\n\nIn general, it is more common to compare multiple models against each other and thus investigate their relative predictive performance.\n\nerrors_epi_student1 &lt;-\n  brms::predictive_error(fit_epi_student1, method = \"posterior_epred\")\nrmse_alt_epi_student1 &lt;- sqrt(colMeans(errors_epi_student1^2))\n\nWe can now even compute the pointwise (per-observation) difference in RMSE values:\n\nrmse_alt_diff &lt;- rmse_alt_epi_student1 - rmse_alt_epi_gaussian3\nstr(rmse_alt_diff)\n\n num [1:236] 0.232 -0.632 -0.467 0.158 -0.899 ...\n\nse_mean &lt;- function(x) {sd(x) / sqrt(length(x))}\n\nse_rmse_alt_diff &lt;- se_mean(rmse_alt_diff)\nse_rmse_alt_diff\n\n[1] 0.3678"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-40",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-40",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nLikelihood Density Scores\n\nWe have looked at variations of \\(R^2\\) and RMSE metrics. Next we use the log-likelihood of models as predictive metric more generally.\nThe log-likelihood plays a pivotal role not only in to derive the posterior in Bayesian statistics but also to obtain maximum likelihood estimates in a frequentist framework. Intuitively, the higher the likelihood of the data given the model’s parameters estimates (represented as either posterior draws or point estimates), the better the fit of the model to the data. Many important predictive metrics, Bayesian or otherwise, are based on log-likelihood scores."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-41",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-41",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nLikelihood Density Scores\n\nSince log is a strictly monotonic transformation, we are not changing anything fundamental by looking at log likelihoods instead of likelihoods. However, we are making the math much simpler by working with sums instead of products. In particular, this concerns computing gradients because the gradient of a sum is just the sum of the individual (pointwise) gradients. Much of the modern statistics and ML relies on this property.\nbrms comes with a dedicated log_lik method that does all the required math.\n\nll_epi_gaussian3 &lt;- brms::log_lik(fit_epi_gaussian3)\nstr(ll_epi_gaussian3)\n\n num [1:4000, 1:236] -3.03 -2.97 -2.97 -2.92 -2.86 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-42",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-42",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nLikelihood Density Scores\n\nThe output of log_lik has the same structure as posterior_predict and friends, that is, it has as many columns as we have observations and as many rows as we posterior draws.\n\n\nCode\nlattice::histogram(colMeans(ll_epi_gaussian3), nint=30, type = \"density\")\n\n\n\n\n\nPer-observation (pointwise) log-likelihoods of model fit_epi_gaussian3."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-43",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-43",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nLikelihood Density Scores\n\nSimilar to the RMSE_alt metric earlier, we average over posterior draws per observation such that we obtain one log-likelihood value per observation.\nThe mean of the log-likelihood differences is slightly positive which points to a slightly better fit of the Student-t model.\n\n\n\n\nCode\nllm_epi_student1  &lt;- \n  colMeans( brms::log_lik(fit_epi_student1) )\nllm_epi_gaussian3 &lt;- \n  colMeans( brms::log_lik(fit_epi_gaussian3) )\n\nllm_epi_diff &lt;- \n  llm_epi_student1 - llm_epi_gaussian3\nmean(llm_epi_diff)\n\n\n[1] 0.501\n\n\n\n\n\nCode\nllm_epi_gaussian3 &lt;- colMeans(ll_epi_gaussian3)\nlattice::histogram(~llm_epi_gaussian3 + llm_epi_student1 + llm_epi_diff, nint=50, type = \"density\")\n\n\n\n\n\nPointwise log-likelihood values of model fit_epi_gaussian3 and fit_epi_student1 as well as their pointwise log-likelihood differences."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-44",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-44",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nLikelihood Density Scores\n\nIt is more typical to work with sums instead of means of log-likelihood values over observations, a quantity that we call LPD1:\n\nlpd_epi_diff &lt;- sum(llm_epi_diff)\n\nThe corresponding standard error is also not particular difficult to obtain:\n\nse_sum &lt;- function(x) {sd(x) * sqrt(length(x))}\n\nse_lpd_epi_diff &lt;- se_sum(llm_epi_diff)\n\n\nLLM (Log-Likelihood Mean) and LPD (Log Posterior Density)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-45",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-45",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\n2.7 Out-of-sample predictions\n\n\n\nCode\nset.seed(8973)\nsplits &lt;- epilepsy |&gt; rsample::initial_split(prop = 0.8)\nepilepsy_train &lt;- rsample::training(splits)\nepilepsy_test  &lt;- rsample::testing(splits)\n\n\n\n\nCode\nfit_student1_train &lt;- \n  brms::brm(\n    count ~ Trt * Base, data = epilepsy_train, family = brms::student()\n    , silent = 2, seed = 8740, file = \"fits/fit_student1_train\")\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.136 seconds (Warm-up)\nChain 1:                0.059 seconds (Sampling)\nChain 1:                0.195 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.112 seconds (Warm-up)\nChain 2:                0.054 seconds (Sampling)\nChain 2:                0.166 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.3e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.13 seconds (Warm-up)\nChain 3:                0.064 seconds (Sampling)\nChain 3:                0.194 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.11 seconds (Warm-up)\nChain 4:                0.053 seconds (Sampling)\nChain 4:                0.163 seconds (Total)\nChain 4: \n\n\nCode\nllm_epi_student1_test &lt;-  brms::log_lik(fit_student1_train, newdata = epilepsy_test)\n\nfit_gaussian3_train &lt;- \n  brms::brm(\n    count ~ Trt * Base, data = epilepsy_train\n    , silent = 2, seed = 8740, file = \"fits/fit_gaussian3_train\")\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.025 seconds (Warm-up)\nChain 1:                0.014 seconds (Sampling)\nChain 1:                0.039 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.025 seconds (Warm-up)\nChain 2:                0.013 seconds (Sampling)\nChain 2:                0.038 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 3:                0.015 seconds (Sampling)\nChain 3:                0.042 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.025 seconds (Warm-up)\nChain 4:                0.014 seconds (Sampling)\nChain 4:                0.039 seconds (Total)\nChain 4: \n\n\nCode\nllm_epi_gaussian3_test &lt;- colMeans( brms::log_lik(fit_gaussian3_train, newdata = epilepsy_test) )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-46",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-46",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nOut-of-sample predictions\n\n\n\nCode\nllm_epi_diff_test &lt;- llm_epi_student1_test - llm_epi_gaussian3_test\nlattice::histogram(llm_epi_diff_test |&gt; matrix(), nint=50, type = \"density\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n(elpd_epi_diff_test &lt;- sum(llm_epi_diff_test))\n\n\n[1] 49608\n\n\n\n\nCode\n(se_elpd_epi_diff_test &lt;- se_sum(llm_epi_diff_test))\n\n\n[1] 742.8"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-47",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-47",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nOut-of-sample predictions\n\nIn leave-one-out cross-validation (LOO-CV), we perform \\(N\\) training-test splits, where each time we are leaving out a single observations, fitting the model on the remaining \\(N-1\\) observations before evaluating model fit on that single left-out observation.\n\nloo-gauss3loo-student1loo diffloo both\n\n\n\nloo_epi_gaussian3 &lt;- brms::loo(fit_epi_gaussian3)\nloo_epi_gaussian3\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -831.6 43.0\np_loo        21.2 12.0\nlooic      1663.3 86.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     234   99.2%   983     \n   (0.7, 1]   (bad)        0    0.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   2    0.8%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\n\n\n\nloo_epi_student1 &lt;- brms::loo(fit_epi_student1)\nloo_epi_student1\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -704.5 23.5\np_loo         7.2  0.6\nlooic      1408.9 46.9\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.1]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\n\n\n\nbrms::loo_compare(loo_epi_gaussian3, loo_epi_student1)\n\n                  elpd_diff se_diff\nfit_epi_student1     0.0       0.0 \nfit_epi_gaussian3 -127.2      37.0 \n\n\n\n\n\nbrms::loo(fit_epi_gaussian3, fit_epi_student1)\n\nWarning: Found 2 observations with a pareto_k &gt; 0.7 in\nmodel 'fit_epi_gaussian3'. We recommend to set\n'moment_match = TRUE' in order to perform moment\nmatching for problematic observations.\n\n\nOutput of model 'fit_epi_gaussian3':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -831.6 43.0\np_loo        21.2 12.0\nlooic      1663.3 86.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     234   99.2%   983     \n   (0.7, 1]   (bad)        0    0.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   2    0.8%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit_epi_student1':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -704.5 23.5\np_loo         7.2  0.6\nlooic      1408.9 46.9\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.1]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n                  elpd_diff se_diff\nfit_epi_student1     0.0       0.0 \nfit_epi_gaussian3 -127.2      37.0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-48",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-48",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nPrior predictive performance\n\nWe will now take a fundamentally different approach by evaluating prior predictive distribution, that is, by looking at what predictions a model implies before seeing any data. The starting point to investigating prior predictive performance is to perform graphical prior predictive checks.\n\n\nCode\nprior_epi_gaussian6 &lt;-\n  brms::prior(normal(6, 3), class = \"Intercept\") +\n  brms::prior(normal(0, 5), class = \"b\", coef = \"Trt1\") +\n  brms::prior(normal(0, 1), class = \"b\", coef = \"Base\") +\n  brms::prior(normal(0, 1), class = \"b\", coef = \"Trt1:Base\") +\n  brms::prior(normal(0, 15), class = \"sigma\")\nprior_epi_gaussian6 \n\n\n         prior     class      coef group resp dpar\n  normal(6, 3) Intercept                          \n  normal(0, 5)         b      Trt1                \n  normal(0, 1)         b      Base                \n  normal(0, 1)         b Trt1:Base                \n normal(0, 15)     sigma                          \n nlpar   lb   ub source\n       &lt;NA&gt; &lt;NA&gt;   user\n       &lt;NA&gt; &lt;NA&gt;   user\n       &lt;NA&gt; &lt;NA&gt;   user\n       &lt;NA&gt; &lt;NA&gt;   user\n       &lt;NA&gt; &lt;NA&gt;   user"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-49",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-49",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nPrior predictive performance\n\nHere we are “fitting” the model with the option sample_prior = “only”, which ensures that Stan ignores the likelihood contribution to the posterior, such that the posterior directly resembles the prior.\n\n\nCode\nfit_prior_epi_gaussian6 &lt;- \n  brms::brm(\n    count ~ 1 + Trt * Base,\n    data = epilepsy,\n    prior = prior_epi_gaussian6,\n    sample_prior = \"only\",\n    file = \"fits/fit_prior_epi_gaussian6\"\n  )\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 1:                0.009 seconds (Sampling)\nChain 1:                0.019 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 2:                0.007 seconds (Sampling)\nChain 2:                0.016 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 3:                0.007 seconds (Sampling)\nChain 3:                0.016 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.008 seconds (Warm-up)\nChain 4:                0.007 seconds (Sampling)\nChain 4:                0.015 seconds (Total)\nChain 4: \n\n\nCode\nsummary(fit_prior_epi_gaussian6)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: count ~ 1 + Trt * Base \n   Data: epilepsy (Number of observations: 236) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept     6.08     36.12   -63.26    77.29 1.00\nTrt1         -0.07      5.08   -10.32     9.73 1.00\nBase          0.01      1.01    -2.00     1.98 1.00\nTrt1:Base    -0.02      0.99    -1.94     1.92 1.00\n          Bulk_ESS Tail_ESS\nIntercept     3582     2704\nTrt1          4822     2711\nBase          3374     2770\nTrt1:Base     4030     2785\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat\nsigma    12.06      9.12     0.48    34.20 1.00\n      Bulk_ESS Tail_ESS\nsigma     2211     1172\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-50",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-50",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nPrior predictive performance\n\nFor all practical purposes, our “prior-only” brms model can be post-processed as any other brms model.\n\n\nbrms::pp_check(fit_prior_epi_gaussian6, ndraws = 100) + xlim(-150, 150)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-51",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-51",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nMarginal likelihood-based metrics\n\nTo mathematically formalize the prior predictive performance of a model, consider the marginal likelihood that we find in the denominator of Bayes theorem (aka evidence): \\[\np(y)=\\int p(y\\vert\\theta)p(\\theta)d\\theta\n\\] We can write this as the likelihood of the data given the model and we can do inference about the models based on the marginal likelihood. \\[\np(y\\vert M)=\\int p(y\\vert\\theta,M)p(\\theta\\vert M)d\\theta\n\\] The absolute marginal likelihood values \\(p(y\\vert M)\\)are very hard to interpret. We only know that higher is better."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-52",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-52",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nMarginal likelihood-based metrics\n\nThe most common such comparative metric is the Bayes factor, defined as the ratio of two models’ marginal likelihoods:\n\\[\n\\mathrm{BF}_{1,2}=\\frac{p(y\\vert M_{1})}{p(y\\vert M_{2})}\n\\] If the Bayes factor is greater than 1, the data \\(y\\) have a higher likelihood given model \\(M_1\\) compared to \\(M_2\\), and vice versa."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-53",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-53",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nMarginal likelihood-based metrics\n\nWe can say “Given the data \\(y\\), model \\(M_1\\) is more likely than model \\(M_2\\)” if we use the posterior odds:\n\\[\n\\frac{p(M_{1}\\vert y)}{p(M_{2}\\vert y)}=\\frac{p(y\\vert M_{1})}{p(y\\vert M_{2})}\\frac{p(M_{1})}{p(M_{2})}=\\mathrm{BF}_{1,2}\\times\\frac{p(M_{1})}{p(M_{2})}\n\\] We often set the prior odds to 1 not actually because we really believe in models being equally likely a priori, but simply out of convenience; just as we often set wide or even completely flat priors on parameters."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-54",
    "href": "slides/BSMM_8740_lec_10.html#generative-modelling-with-brms-54",
    "title": "Bayesian Methods",
    "section": "Generative modelling with BRMS",
    "text": "Generative modelling with BRMS\nMarginal likelihood-based metrics\n\nSince the Bayes factor is based on marginal likelihoods, the computational challenges are substantial. Fortunately, there is one class of algorithms that enables reliable computation of (log) marginal likelihood on the basis of posterior draws. This class of algorithms is called bridge sampling.\nMarginal likelihood estimation via bridge sampling usually requires several times more posterior draws than the estimation of posterior moments or quantiles (i.e., what we usually do with posterior draws).\n\n\nCode\nfit_epi_gaussian6 &lt;- \n  brms::brm(\n    count ~ 1 + Trt * Base, data = epilepsy,\n    prior = prior_epi_gaussian6,\n    save_pars = brms::save_pars(all = TRUE),\n    iter = 5000, warmup = 1000,\n    file = \"fits/fit_epi_gaussian6\"\n  )\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 5000 [  0%]  (Warmup)\nChain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)\nChain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)\nChain 1: Iteration: 1001 / 5000 [ 20%]  (Sampling)\nChain 1: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 1: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.023 seconds (Warm-up)\nChain 1:                0.051 seconds (Sampling)\nChain 1:                0.074 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 5000 [  0%]  (Warmup)\nChain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)\nChain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)\nChain 2: Iteration: 1001 / 5000 [ 20%]  (Sampling)\nChain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 2: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 2:                0.064 seconds (Sampling)\nChain 2:                0.09 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 5000 [  0%]  (Warmup)\nChain 3: Iteration:  500 / 5000 [ 10%]  (Warmup)\nChain 3: Iteration: 1000 / 5000 [ 20%]  (Warmup)\nChain 3: Iteration: 1001 / 5000 [ 20%]  (Sampling)\nChain 3: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 3: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 3: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 3: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 3: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 3: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 3: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 3: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.025 seconds (Warm-up)\nChain 3:                0.065 seconds (Sampling)\nChain 3:                0.09 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 5000 [  0%]  (Warmup)\nChain 4: Iteration:  500 / 5000 [ 10%]  (Warmup)\nChain 4: Iteration: 1000 / 5000 [ 20%]  (Warmup)\nChain 4: Iteration: 1001 / 5000 [ 20%]  (Sampling)\nChain 4: Iteration: 1500 / 5000 [ 30%]  (Sampling)\nChain 4: Iteration: 2000 / 5000 [ 40%]  (Sampling)\nChain 4: Iteration: 2500 / 5000 [ 50%]  (Sampling)\nChain 4: Iteration: 3000 / 5000 [ 60%]  (Sampling)\nChain 4: Iteration: 3500 / 5000 [ 70%]  (Sampling)\nChain 4: Iteration: 4000 / 5000 [ 80%]  (Sampling)\nChain 4: Iteration: 4500 / 5000 [ 90%]  (Sampling)\nChain 4: Iteration: 5000 / 5000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 4:                0.064 seconds (Sampling)\nChain 4:                0.09 seconds (Total)\nChain 4: \n\n\nCode\nlogml_epi_gaussian6 &lt;- brms::bridge_sampler(fit_epi_gaussian6, silent = TRUE);\nsummary(logml_epi_gaussian6)\n\n\n\nBridge sampling log marginal likelihood estimate \n(method = \"normal\", repetitions = 1):\n\n -831.3\n\nError Measures:\n\n Relative Mean-Squared Error: 6.236e-07\n Coefficient of Variation: 0.0007897\n Percentage Error: 0%\n\nNote:\nAll error measures are approximate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#more",
    "href": "slides/BSMM_8740_lec_10.html#more",
    "title": "Bayesian Methods",
    "section": "More",
    "text": "More\n\nRead Bayes Rules!\nRead Think Bayes\nRead Statistical Rethinking"
  },
  {
    "objectID": "slides/BSMM_8740_lec_10.html#recap",
    "href": "slides/BSMM_8740_lec_10.html#recap",
    "title": "Bayesian Methods",
    "section": "Recap",
    "text": "Recap\n\nWe’ve had the smallest possible taste of statistical programming using Bayes theorem and sampling methods, in the context of addressing the limitations of off-the-shelf implementations of statistical methods and algorithms."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_07.html#recap-of-last-week",
    "title": "Causality",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced the modeltime and timetk R packages.\nWe showed how these integrate into the tidymodels workflows for predicting timeseries."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#this-week",
    "href": "slides/BSMM_8740_lec_07.html#this-week",
    "title": "Causality",
    "section": "This week",
    "text": "This week\n\nIntroduction to the fundamental problems of causal inference and the biases of some intuitive estimators,\nDevelop a basic understanding of the tools used to state and then satisfy causality assumptions,\nUnderstanding of how econometric methods recover treatment effects,\nAbility to use these methods and estimate their precision using R."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#the-fundamental-problems-of-inference",
    "href": "slides/BSMM_8740_lec_07.html#the-fundamental-problems-of-inference",
    "title": "Causality",
    "section": "The fundamental problems of inference",
    "text": "The fundamental problems of inference\nWhen trying to estimate the effect of a treatment / intervention on an outcome, we face two very difficult problems: the Fundamental Problem of Causal Inference (FPCI) and the Fundamental Problem of Statistical Inference (FPSI).\nIn this lecture we’ll assume all interventions are binary, e.g. we offer a customer a discount or we don’t; we re-brand a branch office or we don’t."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#rubin-causal-model-rcm1",
    "href": "slides/BSMM_8740_lec_07.html#rubin-causal-model-rcm1",
    "title": "Causality",
    "section": "Rubin Causal Model (RCM1)",
    "text": "Rubin Causal Model (RCM1)\nRCM is made of three distinct building blocks:\n\na treatment allocation rule, that decides which unit receives the treatment / intervention;\na definition of potential outcomes that measure how each unit reacts to the treatment;\nthe switching equation that relates potential outcomes to observed outcomes through the allocation rule.\n\nThe Rubin Causal Model (RCM), also known as the potential outcomes framework, was developed by Donald Rubin in the 1970s and gained prominence in the 1980s."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#treatment-allocation",
    "href": "slides/BSMM_8740_lec_07.html#treatment-allocation",
    "title": "Causality",
    "section": "Treatment allocation",
    "text": "Treatment allocation\nTreatment allocation is represented by the variable \\(D_i\\), where \\(D_i=1\\) if unit \\(i\\) receives the treatment and \\(D_i=0\\) if unit \\(i\\) does not receive the treatment.\nThe treatment allocation rule is critical:\n\nit switches the treatment on or off for each unit, it is going to be at the source of the FPCI.\nthe specific properties of the treatment allocation rule are going to matter for the feasibility and bias of various effect estimation methods."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#potential-outcomes",
    "href": "slides/BSMM_8740_lec_07.html#potential-outcomes",
    "title": "Causality",
    "section": "Potential outcomes",
    "text": "Potential outcomes\nEach unit can be treated or untreated, and so there are two potential outcomes:\n\n\\(Y_i^1\\): the outcome unit \\(i\\) will have, if treated\n\\(Y_i^0\\): the outcome unit \\(i\\) will have, if not treated"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example",
    "href": "slides/BSMM_8740_lec_07.html#example",
    "title": "Causality",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\ncustomer\n\\(Y^1\\)\n\\(Y^0\\)\n\\(\\text{D}\\)\n\\(\\text{outcome Y}\\)\ntreatment effect\n\n\n\n\n1\n500\n450\n1\n500\n50\n\n\n2\n600\n600\n1\n600\n0\n\n\n3\n800\n600\n0\n600\n200\n\n\n4\n700\n750\n0\n750\n-50\n\n\n\n\n\n\n\n\nIn this example \\(\\text{ATE}=\\left(50+0+200-50\\right)/4=50\\) and \\(\\text{ATT}=\\left(50+0\\right)/2=25\\).\nBut this example imagines that we know the counterfactuals (i.e. \\(Y^0\\) for \\(D=1\\) and \\(Y^1\\) for \\(D=0\\)), when we never do.\nThe next example comparing the outcomes with and without treatment is more realistic."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-1",
    "href": "slides/BSMM_8740_lec_07.html#example-1",
    "title": "Causality",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\ncustomer\n\\(Y^1\\)\n\\(Y^0\\)\n\\(\\text{D}\\)\n\\(\\text{outcome Y}\\)\ntreatment effect\n\n\n\n\n1\n500\nNA\n1\n500\nNA\n\n\n2\n600\nNA\n1\n600\nNA\n\n\n3\nNA\n600\n0\n600\nNA\n\n\n4\nNA\n750\n0\n750\nNA\n\n\n\n\n\n\n\n\nGiven these observations, and comparing the mean of the treated outcomes to the mean of the untreated outcomes: \\(\\text{ATE}\\ne\\Delta_\\text{WW}=\\left(500+600\\right)/2-\\left(600+750\\right)/2=-125\\).\nThe \\(\\Delta_\\text{WW}\\) is a biased estimate of the \\(\\text{ATE}\\) because the untreated are bigger spenders than the treated on average."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#switching-equation",
    "href": "slides/BSMM_8740_lec_07.html#switching-equation",
    "title": "Causality",
    "section": "Switching equation",
    "text": "Switching equation\nHere is the switching equation. It links the observed outcome to the potential outcomes through the allocation rule:\n\\[\n\\begin{align*}\nY_{i} & =\\begin{cases}\nY_{i}^{1} & \\text{if}\\,D_{i}=1\\\\\nY_{i}^{0} & \\text{if}\\,D_{i}=0\n\\end{cases}\\\\\n& =Y_{i}^{1}D_{i}+Y_{i}^{0}\\left(1-D_{i}\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#switching-equation-1",
    "href": "slides/BSMM_8740_lec_07.html#switching-equation-1",
    "title": "Causality",
    "section": "Switching equation",
    "text": "Switching equation\nWhat the switching equation means is that, for each unit \\(i\\) we get to observe only one of the two potential outcomes. we can never see both potential outcomes for the same unit at the same time.\nFor each of the units, one of the two potential outcomes is unobserved - it is counterfactual. It can be conceived by an effort of reason: it is the consequence of what would have happened had some action not been taken."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#treatment-effects",
    "href": "slides/BSMM_8740_lec_07.html#treatment-effects",
    "title": "Causality",
    "section": "Treatment effects",
    "text": "Treatment effects\nNow we can defined the causal effect:\nThe unit level treatment effect1 is defined as\n\\[\n\\Delta_i^Y=Y_i^1-Y_i^0\n\\] (which cannot be observed).\nNote that the treatment effects can be heterogeneous (they can differ by unit)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#treatment-effects-1",
    "href": "slides/BSMM_8740_lec_07.html#treatment-effects-1",
    "title": "Causality",
    "section": "Treatment effects",
    "text": "Treatment effects\nAverage treatment effect on the treated (TT1) gives us summary stats (in the population [\\(TT\\)] or sample [\\(TT_s\\)]).\n\\[\n\\Delta_{TT_{s}}^{Y}=\\frac{1}{\\sum_{i=1}^{N}D_{i}}\\sum_{i=1}^{N}\\left(Y_{i}^{1}-Y_{i}^{0}\\right)D_{i}\n\\] with the corresponding expected value in the population\n\\[\n\\Delta_{TT}^{Y}=\\mathbb{E}\\left[\\left.Y_{i}^{1}-Y_{i}^{0}\\right|D_{i}=1\\right]\n\\]\nYou’ll sometimes see this written as ATT"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#fundamental-problem-of-causal-inference",
    "href": "slides/BSMM_8740_lec_07.html#fundamental-problem-of-causal-inference",
    "title": "Causality",
    "section": "Fundamental problem of causal inference",
    "text": "Fundamental problem of causal inference\nIt is impossible to observe TT, either in the population or in the sample.\n\\[\n\\begin{align*}\n\\Delta_{TT_{s}}^{Y} & =\\frac{1}{\\sum_{i=1}^{N}D_{i}}\\sum_{i=1}^{N}\\left(Y_{i}^{1}-Y_{i}^{0}\\right)D_{i}\\\\\n& =\\frac{1}{\\sum_{i=1}^{N}D_{i}}\\sum_{i=1}^{N}Y_{i}^1D_{i}-\\frac{1}{\\sum_{i=1}^{N}D_{i}}\\sum_{i=1}^{N}Y_{i}^{0}D_{i}\n\\end{align*}\n\\] but \\(Y_{i}^{0}D_{i}\\) is unobserved when \\(D_i=1\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#intuitive-casual-estimators",
    "href": "slides/BSMM_8740_lec_07.html#intuitive-casual-estimators",
    "title": "Causality",
    "section": "Intuitive casual estimators",
    "text": "Intuitive casual estimators\nIntuitive comparisons are often made in order to estimate causal effects, e.g. the with/without comparison (WW).\nWW compares the average outcomes of the treated units with those of the untreated units.\nIntuitive comparisons try to proxy for the expected counterfactual outcome in the treated group by using an observed quantity."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#intuitive-casual-estimators-1",
    "href": "slides/BSMM_8740_lec_07.html#intuitive-casual-estimators-1",
    "title": "Causality",
    "section": "Intuitive casual estimators",
    "text": "Intuitive casual estimators\nUnfortunately, these proxies are generally poor and provide biased estimates of TT.\nThe reason that these proxies are poor is that the treatment is not the only factor that differentiates the treated group from the groups used to form the proxy. The intuitive comparisons are biased because factors other than the treatment are correlated to its allocation."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#withwithout-comparison",
    "href": "slides/BSMM_8740_lec_07.html#withwithout-comparison",
    "title": "Causality",
    "section": "With/Without comparison",
    "text": "With/Without comparison\n\\[\n\\Delta_{WW}^{Y}=\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=1\\right] - \\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=0\\right]\n\\]\nNote that this is not the same as the treatment effect on the treated (TT):\n\\[\n\\Delta_{TT}^{Y}=\\mathbb{E}\\left[\\left.Y_{i}^{1}-Y_{i}^{0}\\right|D_{i}=1\\right]\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#withwithout-comparison-1",
    "href": "slides/BSMM_8740_lec_07.html#withwithout-comparison-1",
    "title": "Causality",
    "section": "With/Without comparison",
    "text": "With/Without comparison\nThe difference \\(\\Delta_{WW}^{Y}-\\Delta_{TT}^{Y}\\) is called the selection bias \\(\\Delta_{SB}^{Y}\\):\n\\[\n\\begin{align*}\n\\Delta_{SB}^{Y} & =\\Delta_{WW}^{Y}-\\Delta_{TT}^{Y}\\\\\n& =\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=0\\right]-\\mathbb{E}\\left[\\left.Y_{i}^{1}-Y_{i}^{0}\\right|D_{i}=1\\right]\\\\\n& =\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=0\\right]\n\\end{align*}\n\\]\nSo the selection bias is the difference in the untreated potential-outcomes between treatment groups"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#withwithout-comparison-2",
    "href": "slides/BSMM_8740_lec_07.html#withwithout-comparison-2",
    "title": "Causality",
    "section": "With/Without comparison",
    "text": "With/Without comparison\nUnder what conditions would we have\n\\[\n\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=0\\right]\\ne0\n\\]\nWith non-zero selection bias, selection into the treatment biases the effect of the treatment on outcomes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#withwithout-comparison-3",
    "href": "slides/BSMM_8740_lec_07.html#withwithout-comparison-3",
    "title": "Causality",
    "section": "With/Without comparison",
    "text": "With/Without comparison\n\n\nConfounding factors are the factors that generate differences between treated and untreated individuals even in the absence of the treatment.\nSuppose we wanted to test giving discounts to our smaller-value customers to induce them to buy more. The results of the test could be biased if small-value customer would just buy less anyway.\nThe mere fact of being selected for receiving the discount means that these customers have a host of characteristics that would differentiate them from the unselected customers."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#identification-assumption",
    "href": "slides/BSMM_8740_lec_07.html#identification-assumption",
    "title": "Causality",
    "section": "Identification assumption",
    "text": "Identification assumption\nThere is no selection bias if\n\\[\n\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=0\\right]=0\n\\]\ni.e. the expected counterfactual outcome of the treated is equal to the expected potential outcome of the untreated .\nThis is called the identification assumption."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#identification-assumption-1",
    "href": "slides/BSMM_8740_lec_07.html#identification-assumption-1",
    "title": "Causality",
    "section": "Identification assumption",
    "text": "Identification assumption\nFor the identification assumption to hold, it has to be that all the determinants of \\(D_i\\) are actually unrelated to \\(Y_i^0\\).\nOne way to enforce this assumption is to randomize treatment assignment.\nOne way to test for the validity of the identification assumption is to compare the values of observed covariates in the treated and untreated group, because differences in covariates can lead to confounding."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#identification-assumption-2",
    "href": "slides/BSMM_8740_lec_07.html#identification-assumption-2",
    "title": "Causality",
    "section": "Identification assumption",
    "text": "Identification assumption\nIf the identification assumption holds, and the treatment assignment is (effectively) independent of the outcome, then\n\\[\n\\begin{align*}\n\\Delta_{WW}^{Y} & =\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=0\\right]\\\\\n& =\\mathbb{E}\\left[Y_{i}\\right]-\\mathbb{E}\\left[Y_{i}\\right]\\\\\n& =\\Delta_{ATE}^{Y}\n\\end{align*}\n\\] where \\(\\Delta_{ATE}^{Y}\\) is the average treatment effect in the population."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#fundamental-problems",
    "href": "slides/BSMM_8740_lec_07.html#fundamental-problems",
    "title": "Causality",
    "section": "Fundamental problems",
    "text": "Fundamental problems\n\nThe FPCI states that our causal parameter of interest (\\(\\Delta_{TT}^{Y}\\) - the effect of treatment on the treated) is fundamentally unobservable, even when the sample size is infinite.\nThe FPSI states that, even if we have an estimator \\(E\\) that identifies \\(\\Delta_{TT}^{Y}\\) in the population, we cannot observe \\(E\\) because we only have access to a finite sample of the population."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#causal-inference-workflow",
    "href": "slides/BSMM_8740_lec_07.html#causal-inference-workflow",
    "title": "Causality",
    "section": "Causal Inference Workflow",
    "text": "Causal Inference Workflow\n\nSpecify a causal question\nDraw our assumptions using a causal diagram (DAG)\nModel our assumptions\nDiagnose our models\nEstimate the causal effect\nConduct sensitivity analysis on the effect estimate\n\n\n\n\n\n\n\n\nImportant\n\n\nThe following material is from Causal Inference in R by Barrett, D’Agostino McGowan & Gerke under a MIT + file license."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#directed-acyclic-graphs-dags",
    "href": "slides/BSMM_8740_lec_07.html#directed-acyclic-graphs-dags",
    "title": "Causality",
    "section": "Directed acyclic graphs (DAGs)",
    "text": "Directed acyclic graphs (DAGs)\n\n\nFigure 1: A causal directed acyclic graph (DAG). DAGs depict causal relationships. In this DAG, the assumption is that x causes y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags-1",
    "href": "slides/BSMM_8740_lec_07.html#dags-1",
    "title": "Causality",
    "section": "DAGs",
    "text": "DAGs\n\n\nFigure 2: Three types of causal relationships: forks, chains, and colliders. The direction of the arrows and the relationships of interest dictate which type of path a series of variables represents. Forks represent a mutual cause, chains represent direct causes, and colliders represent a mutual descendant."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags-2",
    "href": "slides/BSMM_8740_lec_07.html#dags-2",
    "title": "Causality",
    "section": "DAGs",
    "text": "DAGs\n\nForks represent a common cause of two variables. Here, we’re saying that q causes both x and y, the traditional definition of a confounder. They’re called forks because the arrows from x to y are in different directions.\nChains, on the other hand, represent a series of arrows going in the same direction. Here, q is called a mediator: it is along the causal path from x to y. In this diagram, the only path from x to y is mediated through q.\nFinally, a collider is a path where two arrowheads meet at a variable. Because causality always goes forward in time, this naturally means that the collider variable is caused by two other variables. Here, we’re saying that x and y both cause q."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags-3",
    "href": "slides/BSMM_8740_lec_07.html#dags-3",
    "title": "Causality",
    "section": "DAGs",
    "text": "DAGs\n\nThese three types of paths have different implications for the statistical relationship between x and y. If we only look at the correlation between the two variables under these assumptions:\n\nIn the fork, x and y will be associated, despite there being no arrow from x to y.\nIn the chain, x and y are related only through q.\nIn the collider, x and y will not be related.\n\nPaths that transmit association are called open paths. Paths that do not transmit association are called closed paths. Forks and chains are open, while colliders are closed."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---forks",
    "href": "slides/BSMM_8740_lec_07.html#dags---forks",
    "title": "Causality",
    "section": "DAGs - forks",
    "text": "DAGs - forks\n\nSo, should we adjust for q, i.e. include q in our model? That depends on the nature of the path.\nForks are confounding paths. Because q causes both x and y, x and y will have a spurious association. They both contain information from q, their mutual cause. That mutual causal relationship makes x and y associated statistically.\nAdjusting for q, i.e. fixing the value of q, will block the bias from confounding and give us the true relationship between x and y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---mediators",
    "href": "slides/BSMM_8740_lec_07.html#dags---mediators",
    "title": "Causality",
    "section": "DAGs - mediators",
    "text": "DAGs - mediators\n\nFor chains, whether or not we adjust for mediators depends on the research question.\nHere, adjusting for q would result in a null estimate of the effect of x on y. Because the only effect of x on y is via q, no other effect remains.\nThe effect of x on y mediated by q is called the indirect effect, while the effect of x on y directly is called the direct effect. If we’re only interested in the direct effect, controlling for q might be what we want. If we want to know about both effects, we shouldn’t try to adjust for q."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---colliders",
    "href": "slides/BSMM_8740_lec_07.html#dags---colliders",
    "title": "Causality",
    "section": "DAGs - colliders",
    "text": "DAGs - colliders\n\nColliders are different. In the collider DAG of Figure 2, x and y are not associated, but both cause q.\nAdjusting for q has the opposite effect than with confounding: it opens a biasing pathway.\nSometimes, people draw the path opened up by conditioning on a collider connecting x and y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---colliders-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---colliders-1",
    "title": "Causality",
    "section": "DAGs - colliders",
    "text": "DAGs - colliders\n\nHow can this be? Since x and y happen before q, q can’t impact them.\nLet’s turn the DAG on its side and consider Figure 3 on the next slide. If we break down the two time points, at time point 1, q hasn’t happened yet, and x and y are unrelated. At time point 2, q happens due to x and y. But causality only goes forward in time.\nq happening later can’t change the fact that x and y happened independently in the past."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---colliders-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---colliders-2",
    "title": "Causality",
    "section": "DAGs - colliders",
    "text": "DAGs - colliders\n\n\nFigure 3: A collider relationship over two points in time. At time point one, there is no relationship between x and y. Both cause q by time point two, but this does not change what already happened at time point one."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---colliders-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---colliders-3",
    "title": "Causality",
    "section": "DAGs - colliders",
    "text": "DAGs - colliders\n\nCausality only goes forward. Association, however, is time-agnostic. It’s just an observation about the numerical relationships between variables. When we control for the future, we risk introducing bias. It takes time to develop an intuition for this.\nConsider a case where x and y are the only causes of q, and all three variables are binary. When either x or y equals 1, then q happens. If we know q = 1 and x = 0 then logically it must be that y = 1. Thus, knowing about q gives us information about y via x.\nThis example is extreme, but it shows how this type of bias, sometimes called collider-stratification bias or selection bias, occurs: conditioning on q provides statistical information about x and y and distorts their relationship."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags-4",
    "href": "slides/BSMM_8740_lec_07.html#dags-4",
    "title": "Causality",
    "section": "DAGs",
    "text": "DAGs\n\nCorrectly identifying the causal structure between the exposure and outcome helps us\n\ncommunicate the assumptions we’re making about the relationships between variables, and\nidentify sources of bias.\n\nImportantly, in doing 2), we are also often able to identify ways to prevent bias based on the assumptions in 1). In the simple case of the three DAGs (Figure 2), we know whether or not to control for q depending on the nature of the causal structure. The set or sets of variables we need to adjust for is called the adjustment set. DAGs can help us identify adjustment sets even in complex settings."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---exchangeability",
    "href": "slides/BSMM_8740_lec_07.html#dags---exchangeability",
    "title": "Causality",
    "section": "DAGs - exchangeability",
    "text": "DAGs - exchangeability\n\nWe commonly refer to exchangeability as the assumption of no confounding. Actually, this isn’t quite right. It’s the assumption of no open, non-causal paths. Many times, these are confounding pathways. However, conditioning on a collider can also open paths. Even though these aren’t confounders, doing so creates non-exchangeability between the two groups: they are different in a way that matters to the exposure and outcome.\nOpen, non-causal paths are also called backdoor paths. We’ll use this terminology often because it captures the idea well: these are any open paths biasing the effect we’re interested in estimating."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---exchangeability-vs-identification",
    "href": "slides/BSMM_8740_lec_07.html#dags---exchangeability-vs-identification",
    "title": "Causality",
    "section": "DAGs - exchangeability vs identification",
    "text": "DAGs - exchangeability vs identification\n\n\nExchangeability is one of the assumptions that can help in achieving identification. If you can assume exchangeability, then you can often identify the causal effect using the observed data.\nExchangeability is more about the structure of the data and the treatment assignment mechanism, while identification is a broader concept that encompasses the entire process of linking causal models to statistical models and data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\nTo create a DAG object, we’ll use ggdag::dagify() which returns a dagitty object that works with both the dagitty and ggdag packages.\nThe dagify() function takes formulas, separated by commas, that specify causes and effects, with the left element of the formula defining the effect and the right all of the factors that cause it. This is just like the type of formula we specify for most regression models in R.\nggdag::dagify( \n  effect1 ~ cause1 + cause2 + cause3\n  , effect2 ~ cause1 + cause4\n  , ...\n)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization-1",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\nWhat are all of the factors that cause graduate students to listen to a podcast the morning before an exam? What are all of the factors that could cause a graduate student to do well on a test? Let’s posit some here.\n\nggdag::dagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared\n)\n\ndag {\nexam\nhumor\nmood\npodcast\nprepared\nhumor -&gt; podcast\nmood -&gt; exam\nmood -&gt; podcast\nprepared -&gt; exam\nprepared -&gt; podcast\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization-2",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\nIn the code, we assume that:\n\na graduate student’s mood, sense of humor, and how prepared they feel for the exam could influence whether they listened to a podcast the morning of the test, and\ntheir mood and how prepared they are also influence their exam score.\n\nNotice we do not see podcast in the exam equation; this means that we assume that there is no causal relationship between podcast and the exam score."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization-3",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\nSome other useful arguments for dagify():\n\nexposure and outcome: Telling ggdag the variables that are the exposure and outcome of your research question is required for many of the most valuable queries we can make of DAGs.\nlatent: This argument lets us tell ggdag that some variables in the DAG are unmeasured. latent helps identify valid adjustment sets with the data we actually have.\ncoords: Coordinates for the variables. You can choose between algorithmic or manual layouts, as discussed below. We’ll use time_ordered_coords() here.\nlabels: A character vector of labels for the variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization-4",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\n\n\nCode\npodcast_dag &lt;- ggdag::dagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared,\n  coords = ggdag::time_ordered_coords(\n    list(\n      c(\"prepared\", \"humor\", \"mood\"), # time point 1\n      \"podcast\",                      # time point 2\n      \"exam\"                          # time point 3\n    )\n  ),\n  exposure = \"podcast\",\n  outcome = \"exam\",\n  labels = c(\n    podcast = \"podcast\",\n    exam = \"exam score\",\n    mood = \"mood\",\n    humor = \"humor\",\n    prepared = \"prepared\"\n  )\n)\nggdag::ggdag(podcast_dag, use_labels = \"label\", text = FALSE) +\n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigure 4: Proposed DAG to answer the question: Does listening to a comedy podcast the morning before an exam improve graduate students’ test scores?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\nWe’ve specified the DAG for this question and told ggdag what the exposure and outcome of interest are. According to the DAG, there is no direct causal relationship between listening to a podcast and exam scores.\nAre there any other open paths? ggdag_paths() takes a DAG and visualizes the open paths.\nIn Figure 5 (next slide), we see two open paths:\n\npodcast &lt;- mood -&gt; exam and\npodcast &lt;- prepared -&gt; exam.\n\nThese are both forks—confounding pathways. Since there is no causal relationship between listening to a podcast and exam scores, the only open paths are backdoor paths, these two confounding pathways."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-1",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\n\n\nCode\npodcast_dag |&gt; \n  # show the whole dag as a light gray shadow rather than just the paths\n  ggdag::ggdag_paths(shadow = TRUE, text = FALSE, use_labels = \"label\") +\n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigure 5: ggdag_paths() visualizes open paths in a DAG. There are two open paths in podcast_dag: the fork from mood and the fork from prepared."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-2",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\n\nDAGs are not pure data frames, but you can retrieve either the dataframe or dagitty object to work with them directly using pull_dag_data() or pull_dag(). pull_dag() can be useful when you want to work with dagitty functions:\nggdag::tidy_dagitty()\n\n\nCode\npodcast_dag_tidy &lt;- podcast_dag |&gt; \n  ggdag::tidy_dagitty()\n\npodcast_dag_tidy\n\n\n# A DAG with 5 nodes and 5 edges\n#\n# Exposure: podcast\n# Outcome: exam\n#\n# A tibble: 7 × 9\n  name         x     y direction to       xend  yend\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n1 exam         3     0 &lt;NA&gt;      &lt;NA&gt;       NA    NA\n2 humor        1     0 -&gt;        podcast     2     0\n3 mood         1     1 -&gt;        exam        3     0\n4 mood         1     1 -&gt;        podcast     2     0\n5 podcast      2     0 &lt;NA&gt;      &lt;NA&gt;       NA    NA\n6 prepared     1    -1 -&gt;        exam        3     0\n7 prepared     1    -1 -&gt;        podcast     2     0\n# ℹ 2 more variables: circular &lt;lgl&gt;, label &lt;chr&gt;\n\n\n\nggdag::dag_paths()\n\n\nCode\npodcast_dag_tidy |&gt; \n  ggdag::dag_paths() |&gt; \n  filter(set == 2, path == \"open path\")\n\n\n# A DAG with 3 nodes and 2 edges\n#\n# Exposure: podcast\n# Outcome: exam\n#\n# A tibble: 4 × 11\n  set   name        x     y direction to     xend  yend\n  &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 2     exam        3     0 &lt;NA&gt;      &lt;NA&gt;     NA    NA\n2 2     podcast     2     0 &lt;NA&gt;      &lt;NA&gt;     NA    NA\n3 2     prepar…     1    -1 -&gt;        exam      3     0\n4 2     prepar…     1    -1 -&gt;        podc…     2     0\n# ℹ 3 more variables: circular &lt;lgl&gt;, label &lt;chr&gt;,\n#   path &lt;chr&gt;\n\n\nggdag::pull_dag()\n\n\nCode\npodcast_dag_tidy |&gt; \n  ggdag::pull_dag() |&gt; \n  dagitty::paths()\n\n\n$paths\n[1] \"podcast &lt;- mood -&gt; exam\"    \n[2] \"podcast &lt;- prepared -&gt; exam\"\n\n$open\n[1] TRUE TRUE"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-3",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\n\nggdag::ggdag_adjustment_set() visualizes any valid adjustment sets implied by the DAG. Figure 5.11 shows adjusted variables as squares. Any arrows coming out of adjusted variables are removed from the DAG because the path is longer open at that variable.\n\nggdag::ggdag_adjustment_set(\n  podcast_dag, \n  text = FALSE, \n  use_labels = \"label\"\n) + ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\n\nFigure 6: A visualization of the minimal adjustment set for the podcast-exam DAG. If this DAG is correct, two variables are required to block the backdoor paths: mood and prepared."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-4",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\nFigure 6 shows the minimal adjustment set. By default, ggdag returns the set(s) that can close all backdoor paths with the fewest number of variables possible.\nIn this DAG, that’s just one set: mood and prepared. This set makes sense because there are two backdoor paths, and the only other variables on them besides the exposure and outcome are these two variables.\nSo, at minimum, we must account for both to get a valid estimate.\nMinimal adjustment sets are only one type of valid adjustment set. Sometimes, other combinations of variables can get us an unbiased effect estimate. Two other options available in ggdag are full adjustment sets and canonical adjustment sets. Full adjustment sets are every combination of variables that result in a valid set."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-5",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-5",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\n\nCode\nggdag::ggdag_adjustment_set(\n  podcast_dag, \n  text = FALSE, \n  use_labels = \"label\",\n  # get full adjustment sets\n  type = \"all\"\n) + ggdag::theme_dag()\n\n\n\n\nFigure 7: All valid adjustment sets for podcast_dag."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-example",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-example",
    "title": "Causality",
    "section": "DAGs - open paths: example",
    "text": "DAGs - open paths: example\n\n\nCode\nset.seed(8740)\nsim_data &lt;- podcast_dag |&gt;\n  ggdag::simulate_data()\nsim_data\n\n\n# A tibble: 500 × 5\n     exam  humor    mood podcast prepared\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.687  0.529  0.0451  0.0784  -0.477 \n 2  0.743  1.23  -0.235   2.28    -1.34  \n 3  0.906 -0.615 -0.778  -1.36    -0.839 \n 4  0.487  0.756 -0.187  -1.12     0.461 \n 5 -2.39   1.00  -0.0725 -1.02     1.81  \n 6  0.519 -1.42  -0.0496 -1.08    -0.429 \n 7 -0.178  0.321 -0.560   0.594    0.193 \n 8  0.424  0.196 -0.435  -0.396    0.618 \n 9  0.972  1.98  -2.06    0.990    1.40  \n10 -0.595 -1.66   0.215  -2.20    -0.0223\n# ℹ 490 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-example-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-example-1",
    "title": "Causality",
    "section": "DAGs - open paths: example",
    "text": "DAGs - open paths: example\n\n\nSince we have simulated this data, we know that this is a case where standard methods will succeed and, therefore, can estimate the causal effect using a basic linear regression model.\nFigure 8 (Estimates tab) shows a forest plot of the simulated data based on our DAG. Notice the model that only included the exposure resulted in a spurious effect (an estimate of &gt;0.1 when we know the truth is 0).\nBy contrast, the model that adjusted for the two variables as suggested by ggdag_adjustment_set() is not spurious (much closer to 0).\n\n\n\nCode\n## Model that does not close backdoor paths\nunadjusted_model &lt;- lm(exam ~ podcast, sim_data) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"podcast\") |&gt;\n  dplyr::mutate(formula = \"podcast\")\n\n## Model that closes backdoor paths\nadjusted_model &lt;- lm(exam ~ podcast + mood + prepared, sim_data) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"podcast\") |&gt;\n  dplyr::mutate(formula = \"podcast + mood + prepared\")\n\ndplyr::bind_rows(\n  unadjusted_model,\n  adjusted_model\n) |&gt;\n  ggplot(aes(x = estimate, y = formula, xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, linewidth = 1, color = \"grey80\") +\n  geom_pointrange(fatten = 3, size = 1) +\n  theme_minimal(18) +\n  labs(\n    y = NULL,\n    caption = \"correct effect size: 0\"\n  )\n\n\n\n\n\n\n\n\nFigure 8: Forest plot of simulated data based on the DAG described in Figure 4."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding",
    "href": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding",
    "title": "Causality",
    "section": "DAGs - advanced confounding",
    "text": "DAGs - advanced confounding\n\nIn podcast_dag, mood and prepared were direct confounders: an arrow was going directly from them to podcast and exam.\nOften, backdoor paths are more complex. For example, let’s add two new variables: alertness and skills_course. alertness represents the feeling of alertness from a good mood, thus an arrow from mood to alertness. skills_course represents whether the student took a College Skills Course and learned time management techniques.\nNow, skills_course is what frees up the time to listen to a podcast as well as being prepared for the exam. mood and prepared are no longer direct confounders: they are two variables along a more complex backdoor path. Additionally, we’ve added an arrow going from humor to mood."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-1",
    "title": "Causality",
    "section": "DAGs - advanced confounding",
    "text": "DAGs - advanced confounding\n\nLet’s take a look at Figure 9.\n\n\nCode\npodcast_dag2 &lt;- ggdag::dagify(\n    podcast ~ mood + humor + skills_course,\n    alertness ~ mood,\n    mood ~ humor,\n    prepared ~ skills_course,\n    exam ~ alertness + prepared,\n    coords = ggdag::time_ordered_coords(),\n    exposure = \"podcast\",\n    outcome = \"exam\",\n    labels = c(\n        podcast = \"podcast\",\n        exam = \"exam score\",\n        mood = \"mood\",\n        alertness = \"alertness\",\n        skills_course = \"college\\nskills course\",\n        humor = \"humor\",\n        prepared = \"prepared\"\n    )\n)\n\nggdag::ggdag(podcast_dag2, use_labels = \"label\", text = FALSE)  + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigure 9: An expanded version of podcast_dag that includes two additional variables: skills_course, representing a College Skills Course, and alertness."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-2",
    "title": "Causality",
    "section": "DAGs - advanced confounding",
    "text": "DAGs - advanced confounding\n\nNow there are three backdoor paths we need to close: podcast &lt;- humor -&gt; mood -&gt; alertness -&gt; exam, podcast &lt;- mood -&gt; alertness -&gt; exam, and podcast &lt;- skills_course -&gt; prepared -&gt; exam.\n\ncodepaths\n\n\n\nggdag::ggdag_paths(podcast_dag2, use_labels = \"label\", text = FALSE, shadow = TRUE) + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Three open paths in podcast_dag2. Since there is no effect of podcast on exam, all three are backdoor paths that must be closed to get the correct effect."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-3",
    "title": "Causality",
    "section": "DAGs - advanced confounding",
    "text": "DAGs - advanced confounding\n\nThere are four minimal adjustment sets to close all three paths (and eighteen full adjustment sets!). The minimal adjustment sets are alertness + prepared, alertness + skills_course, mood + prepared, mood + skills_course.\nWe can now block the open paths in several ways - mood and prepared still work, but we’ve got other options now.\nNotably, prepared and alertness could happen at the same time or even after podcast. skills_course and mood still happen before both podcast and exam, so the idea is still the same: the confounding pathway starts before the exposure and outcome.\nThe next slide shows the result of executing the following code:\nggdag::ggdag_adjustment_set(podcast_dag2, use_labels = \"label\", text = FALSE)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-4",
    "title": "Causality",
    "section": "DAGs - advanced confounding",
    "text": "DAGs - advanced confounding\n\n\n\n\n\n\n\n\n\nFigure 11: Valid minimal adjustment sets that will close the backdoor paths in Figure 10."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-5",
    "href": "slides/BSMM_8740_lec_07.html#dags---advanced-confounding-5",
    "title": "Causality",
    "section": "DAGs - advanced confounding",
    "text": "DAGs - advanced confounding\n\nDeciding between these adjustment sets is a matter of judgment: if all data are perfectly measured, the DAG is correct, and we’ve modeled them correctly, then it doesn’t matter which we use. Each adjustment set will result in an unbiased estimate.\nAll three of those assumptions are usually untrue to some degree. Let’s consider the path via skills_course and prepared.\nIt may be that we are better able to assess whether or not someone took the College Skills Course than how prepared for the exam they are. In that case, an adjustment set with skills_course is a better option.\nBut perhaps we better understand the relationship between preparedness and exam results. If we have it measured, controlling for that might be better. We could get the best of both worlds by including both variables: between the better measurement of skills_course and the better modeling of prepared, we might have a better chance of minimizing confounding from this path."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\nSelection bias is another name for the type of bias that is induced by adjusting for a collider. It’s called “selection bias” because a common form of collider-induced bias is a variable inherently stratified upon by the design of the study—selection into the study.\nLet’s consider a case based on the original podcast_dag but with one additional variable: whether or not the student showed up to the exam.\nNow, there is an indirect effect of podcast on exam: listening to a podcast influences whether or not the students attend the exam. The true result of exam is missing for those who didn’t show up; by studying the group of people who did show up, we are inherently stratifying on this variable."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias-1",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\n\n\nCode\npodcast_dag3 &lt;- ggdag::dagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared + showed_up,\n  showed_up ~ podcast + mood + prepared,\n  coords = ggdag::time_ordered_coords(\n    list(\n      # time point 1\n      c(\"prepared\", \"humor\", \"mood\"), \n      # time point 2\n      \"podcast\",  \n      \"showed_up\",  \n      # time point 3\n      \"exam\"\n    )\n  ),\n  exposure = \"podcast\",\n  outcome = \"exam\",\n  labels = c(\n    podcast = \"podcast\",\n    exam = \"exam score\",\n    mood = \"mood\",\n    humor = \"humor\",\n    prepared = \"prepared\",\n    showed_up = \"showed up\"\n  )\n)\nggdag::ggdag(podcast_dag3, use_labels = \"label\", text = FALSE) + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigure 12: Another variant of podcast_dag, this time including the inherent stratification on those who appear for the exam. There is still no direct effect of podcast on exam, but there is an indirect effect via showed_up."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias-2",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\nThe problem is that showed_up is both a collider and a mediator: stratifying on it induces a relationship between many of the variables in the DAG but blocks the indirect effect of podcast on exam.\nLuckily, the adjustment sets can handle the first problem; because showed_up happens before exam, we’re less at risk of collider bias between the exposure and outcome.\nUnfortunately, we cannot calculate the total effect of podcast on exam because part of the effect is missing: the indirect effect is closed at showed_up."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias-3",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\n\n\nCode\npodcast_dag3 |&gt; \n  ggdag::adjust_for(\"showed_up\") |&gt; \n  ggdag::ggdag_adjustment_set(text = FALSE, use_labels = \"label\") + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigure 13: The adjustment set for podcast_dag3 given that the data are inherently conditioned on showing up to the exam. In this case, there is no way to recover an unbiased estimate of the total effect of podcast on exam."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias-4",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\n\nSometimes, you can still estimate effects in this situation by changing the estimate you wish to calculate. We can’t calculate the total effect because we are missing the indirect effect, but we can still calculate the direct effect of podcast on exam.\n\npodcast_dag3 |&gt; \n  ggdag::adjust_for(\"showed_up\") |&gt; \n  ggdag::ggdag_adjustment_set(\n    effect = \"direct\"\n    , text = FALSE\n    , use_labels = \"label\"\n  )\n\n\n\n\n\n\n\n\n\n\nFigure 14: The adjustment set for podcast_dag3 when targeting a different effect. There is one minimal adjustment set that we can use to estimate the direct effect of podcast on exam."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias",
    "href": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias",
    "title": "Causality",
    "section": "DAGs - M-Bias and Butterfly Bias",
    "text": "DAGs - M-Bias and Butterfly Bias\n\n\nA particular case of selection bias that you’ll often see is M-bias. It’s called M-bias because it looks like an M when arranged top to bottom.\n\n\n\n\n\n\nTip\n\n\nggdag has several quick-DAGs for demonstrating basic causal structures, including confounder_triangle(), collider_triangle(), m_bias(), and butterfly_bias().\n\n\n\n\ndagitty::paths( ggdag::m_bias() )\n\n$paths\n[1] \"x &lt;- a -&gt; m &lt;- b -&gt; y\"\n\n$open\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: A DAG representing M-Bias, a situation where a collider predates the exposure and outcome."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-1",
    "title": "Causality",
    "section": "DAGs - M-Bias and Butterfly Bias",
    "text": "DAGs - M-Bias and Butterfly Bias\n\nLet’s focus on the mood path of the podcast-exam DAG.\nWhat if we were wrong about mood, and the actual relationship was M-shaped? Let’s say that, rather than causing podcast and exam, mood was itself caused by two mutual causes of podcast and exam, u1 and u2.\nWe don’t know what u1 and u2 are, and we don’t have them measured. As above, there are no open paths in this subset of the DAG."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-2",
    "title": "Causality",
    "section": "DAGs - M-Bias and Butterfly Bias",
    "text": "DAGs - M-Bias and Butterfly Bias\n\n\n\nCode\npodcast_dag4 &lt;- ggdag::dagify(\n  podcast ~ u1,\n  exam ~ u2,\n  mood ~ u1 + u2,\n  coords = time_ordered_coords(list(\n    c(\"u1\", \"u2\"),\n    \"mood\",\n    \"podcast\", \n    \"exam\"\n  )),\n  exposure = \"podcast\",\n  outcome = \"exam\",\n  labels = c(\n    podcast = \"podcast\",\n    exam = \"exam score\",\n    mood = \"mood\",\n    u1 = \"unmeasured_1\",\n    u2 = \"unmeasured_2\"\n  ),\n  # we don't have them measured\n  latent = c(\"u1\", \"u2\")\n)\n\nggdag::ggdag(podcast_dag4, use_labels = \"label\", text = FALSE)  + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigure 16: A reconfiguration of Figure 4 where mood is a collider on an M-shaped path."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-3",
    "title": "Causality",
    "section": "DAGs - M-Bias and Butterfly Bias",
    "text": "DAGs - M-Bias and Butterfly Bias\n\n\nThe problem arises when we think our original DAG is the right DAG: mood is in the adjustment set, so we control for it. But this induces bias!\nIt opens up a path between u1 and u2, thus creating a path from podcast to exam.\nIf we had either u1 or u2 measured, we could adjust for them to close this path, but we don’t. There is no way to close this open path.\n\n\n\n\n\n\n\n\n\nFigure 17: The adjustment set where mood is a collider. If we control for mood and don’t know about or have the unmeasured causes of mood, we have no means of closing the backdoor path opened by adjusting for a collider."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome",
    "href": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome",
    "title": "Causality",
    "section": "DAGs - One of exposure / outcome",
    "text": "DAGs - One of exposure / outcome\nLet’s consider one other type of causal structure that’s important: causes of the exposure and not the outcome, and their opposites, causes of the outcome and not the exposure.\nLet’s add a variable, grader_mood, to the original DAG."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-1",
    "title": "Causality",
    "section": "DAGs - One of exposure / outcome",
    "text": "DAGs - One of exposure / outcome\n\n\n\nCode\npodcast_dag5 &lt;- ggdag::dagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared + grader_mood,\n  coords = ggdag::time_ordered_coords(\n    list(\n      # time point 1\n      c(\"prepared\", \"humor\", \"mood\"), \n      # time point 2\n      c(\"podcast\", \"grader_mood\"),  \n      # time point 3\n      \"exam\"\n    )\n  ),\n  exposure = \"podcast\",\n  outcome = \"exam\",\n  labels = c(\n    podcast = \"podcast\",\n    exam = \"exam score\",\n    mood = \"student\\nmood\",\n    humor = \"humor\",\n    prepared = \"prepared\",\n    grader_mood = \"grader\\nmood\"\n  )\n)\nggdag::ggdag(podcast_dag5, use_labels = \"label\", text = FALSE)  + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigure 18: A DAG containing a cause of the exposure that is not the cause of the outcome (humor) and a cause of the outcome that is not a cause of the exposure (grader_mood)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-2",
    "title": "Causality",
    "section": "DAGs - One of exposure / outcome",
    "text": "DAGs - One of exposure / outcome\n\nStarting with humor:\nVariables that cause the exposure but not the outcome are also called instrumental variables (IVs). IVs are an unusual circumstance where, under certain conditions, controlling for them can make other types of bias worse.\nWhat’s unique about this is that IVs can also be used to conduct an entirely different approach to estimating an unbiased effect of the exposure on the outcome. IVs are commonly used this way in econometrics and are increasingly popular in other areas.\nIn short, IV analysis allows us to estimate the causal effect using a different set of assumptions than the approaches we’ve talked about thus far.\nIf you’re unsure if the variable is an IV or not, you should probably add it to your model: it’s more likely to be a confounder than an IV, and, it turns out, the bias from adding an IV is usually small in practice. So, like adjusting for a potential M-structure variable, the risk of bias is worse from confounding."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-3",
    "title": "Causality",
    "section": "DAGs - One of exposure / outcome",
    "text": "DAGs - One of exposure / outcome\n\nAbout variables that are a cause of the outcome but are not the cause of the exposure:\nWe’ll call them precision variables because we’re concerned about the relationship to the research question at hand, not to another research question where they are exposures.\nLike IVs, precision variables do not occur along paths from the exposure to the outcome. Thus, including them is not necessary.\nUnlike IVs, including precision variables is beneficial. Including other causes of the outcomes helps a statistical model capture some of its variation.\nThis doesn’t impact the point estimate of the effect, but it does reduce the variance, resulting in smaller standard errors and narrower confidence intervals. Thus, it’s recommended they be included when possible."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\n\nIn principle, using DAGs is easy: specify the causal relationships you think exist and then query the DAG for information like valid adjustment sets.\nIn practice, assembling DAGs takes considerable time and thought. Next to defining the research question itself, it’s one of the most challenging steps in making causal inferences.\nVery little guidance exists on best practices in assembling DAGs. The next few slides contain some approaches to consider:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-1",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nIterate early and often\n\nMake the DAG before you conduct the study, ideally before you even collect the data.\nIf you’re already working with your data, build your DAG before doing data analysis. Declaring your assumptions ahead of time can help clarify what you need to do, reduce the risk of overfitting (e.g., determining confounders incorrectly from the data), and give you time to get feedback on your DAG.\nThis last benefit is significant: you should ideally democratize your DAG. Share it early and often with others who are experts on the data, domain, and models.\nIf you have more than one candidate DAG, check their adjustment sets. If two DAGs have overlapping adjustment sets, focus on those sets; then, you can move forward in a way that satisfies the plausible assumptions you have."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-2",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nConsider your question\n\nSome questions can be challenging to answer with certain data, while others are more approachable. You should consider precisely what it is you want to estimate.\nAnother important detail about how your DAG relates to your question is the population and time. Many causal structures are not static over time and space.\nThe same is true for confounders. Even if something can cause the exposure and outcome, if the prevalence of that thing is zero in the population you’re analyzing, it’s irrelevant to the causal question.\nThe reverse is also true: something might be unique to the target population."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-3",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nOrder nodes by time\n\nIt is recommended to order your variables by time, either left-to-right or up-to-down. There are two reasons for this.\nFirst, time ordering is an integral part of your assumptions. After all, something happening before another thing is a requirement for it to be a cause. Thinking this through carefully will clarify your DAG and the variables you need to address.\nSecond, after a certain level of complexity, it’s easier to read a DAG when arranged by time because you have to think less about that dimension; it’s inherent to the layout.\nThe time ordering algorithm in ggdag automates much of this for you, although it’s sometimes helpful to give it more information about the order."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-4",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nOrder nodes by time - feedback loops\n\n\nWe might think about two things that mutually cause each other as happening in a circle, like global warming and A/C use (A/C use increases global warming, which makes it hotter, which increases A/C use, and so on).\nIt’s tempting to visualize that relationship like this:\n\n\nCode\nggdag::dagify(\n  ac_use ~ global_temp,\n  global_temp ~ ac_use,\n  labels = \n    c(ac_use = \"A/C use\", global_temp = \"Global\\ntemperature\")\n) |&gt; \n  ggdag::ggdag(\n    layout = \"circle\", edge_type = \"arc\", text = FALSE, use_labels = \"label\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\nFigure 19: A DAG representing the reciprocal relationship between A/C use and global temperature because of global warming. Feedback loops are useful mental shorthands to describe variables that impact each other over time compactly, but they are not true causal diagrams."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-5",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-5",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nOrder nodes by time - feedback loops\n\n\nFrom a DAG perspective, this is a problem because of the A part of DAG: it’s cyclic!\nFeedback loops are a shorthand for what really happens, which is that the two variables mutually affect each other over time. Causality only goes forward in time, so it doesn’t make sense to go back and forth like in Figure 19.\nThe real DAG looks something like this:\n\n\nCode\nggdag::dagify(\n  global_temp_2000 ~ ac_use_1990 + global_temp_1990,\n  ac_use_2000 ~ ac_use_1990 + global_temp_1990,\n  global_temp_2010 ~ ac_use_2000 + global_temp_2000,\n  ac_use_2010 ~ ac_use_2000 + global_temp_2000,\n  global_temp_2020 ~ ac_use_2010 + global_temp_2010,\n  ac_use_2020 ~ ac_use_2010 + global_temp_2010,\n  coords = ggdag::time_ordered_coords(),\n  labels = c(\n    ac_use_1990 = \"A/C use\\n(1990)\", \n    global_temp_1990 = \"Global\\ntemperature\\n(1990)\",\n    ac_use_2000 = \"A/C use\\n(2000)\", \n    global_temp_2000 = \"Global\\ntemperature\\n(2000)\",\n    ac_use_2010 = \"A/C use\\n(2010)\", \n    global_temp_2010 = \"Global\\ntemperature\\n(2010)\",\n    ac_use_2020 = \"A/C use\\n(2020)\", \n    global_temp_2020 = \"Global\\ntemperature\\n(2020)\"\n  )\n) |&gt; \n  ggdag::ggdag(text = FALSE, use_labels = \"label\") + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\n\n\nFigure 20: A DAG showing the relationship between A/C use and global temperature over time. The true causal relationship in a feedback loop goes forward."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-6",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-6",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nUse robustness checks\n\nFinally, check your DAG for robustness. It is unlikely the correctness of your DAG can be verified, but you can use the implications in your DAG to check the support for it. Some robustness checks:\n\nDAG-data consistency. There are many implications of your DAG. Because blocking a path removes statistical dependencies from that path, you can check those assumptions in several places in your DAG (see dagitty::impliedConditionalIndependencies).\nAlternate adjustment sets. Adjustment sets should give roughly the same answer because, outside of random and measurement errors, they are all sets that block backdoor paths. If more than one adjustment set seems reasonable, you can use that as a sensitivity analysis by checking multiple models.\n\nThe checks should be complementary to your initial DAG, not a way of replacing it. If you use more than one adjustment set during your analysis, you should report the results from all of them to avoid overfitting your results to your data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-steps",
    "href": "slides/BSMM_8740_lec_07.html#example-steps",
    "title": "Causality",
    "section": "Example: steps",
    "text": "Example: steps\nIn the remainder of today’s slides, we’ll analyze simulated data using a few key steps\n\nSpecify a causal question\nDraw our assumptions using a causal diagram\nModel our assumptions\nDiagnose our models\nEstimate the causal effect\nConduct sensitivity analysis on the effect estimate"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-causal-question",
    "href": "slides/BSMM_8740_lec_07.html#example-causal-question",
    "title": "Causality",
    "section": "Example: causal question",
    "text": "Example: causal question\n\n\nResearchers are interested in whether using mosquito nets decreases an individual’s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-causal-question-1",
    "href": "slides/BSMM_8740_lec_07.html#example-causal-question-1",
    "title": "Causality",
    "section": "Example: causal question",
    "text": "Example: causal question\n\n\nid - an ID variable\nnet and net_num - a binary variable indicating if the participant used a net (1) or didn’t use a net (0)\nmalaria_risk- risk of malaria scale ranging from 0-100\nincome - weekly income, measured in dollars\nhealth - a health score scale ranging from 0–100\nhousehold - number of people living in the household\neligible - a binary variable indicating if the household is eligible for the free net program.\ntemperature - the average temperature at night, in Celsius\nresistance - Insecticide resistance of local mosquitoes. A scale of 0–100, with higher values indicating higher resistance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-causal-question-2",
    "href": "slides/BSMM_8740_lec_07.html#example-causal-question-2",
    "title": "Causality",
    "section": "Example: causal question",
    "text": "Example: causal question\n\nThe distribution of malaria risk appears to be quite different by net usage.\n\n\n\n\n\n\n\n\nFigure 21: A density plot of malaria risk for those who did and did not use nets. The average risk of malaria is lower for those who use nets."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-causal-question-3",
    "href": "slides/BSMM_8740_lec_07.html#example-causal-question-3",
    "title": "Causality",
    "section": "Example: causal question",
    "text": "Example: causal question\n\nIn Figure 21, the density of those who used nets is to the left of those who did not use nets. The mean difference in malaria risk is about 16.4, suggesting net use might be protective against malaria.\n\n\nCode\ncausalworkshop::net_data |&gt;\n  dplyr::group_by(net) |&gt;\n  dplyr::summarize(malaria_risk = mean(malaria_risk)) |&gt; \n  dplyr::mutate(diff = malaria_risk - dplyr::lag(malaria_risk))\n\n\n# A tibble: 2 × 3\n  net   malaria_risk  diff\n  &lt;lgl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 FALSE         43.9  NA  \n2 TRUE          27.5 -16.4\n\n\nAnd that’s what we see with simple linear regression, as well, as we would expect.\n\n\nCode\ncausalworkshop::net_data |&gt;\n  lm(malaria_risk ~ net, data = _) |&gt;\n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     43.9     0.377     116.  0       \n2 netTRUE        -16.4     0.741     -22.1 1.10e-95"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag",
    "href": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag",
    "title": "Causality",
    "section": "Example: assumptions using a DAG",
    "text": "Example: assumptions using a DAG\n\nThe problem that we face is that other factors may be responsible for the effect we’re seeing.\nIn this example, we’ll focus on confounding: a common cause of net usage and malaria will bias the effect we see unless we account for it somehow.\nTo determine which variables we need to account we’ll use use a causal diagram or DAG, to visualize the assumptions that we’re making about the causal relationships between the exposure, outcome, and other variables we think might be related.\nThe proposed DAG for this question is one the next slide."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag-1",
    "href": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag-1",
    "title": "Causality",
    "section": "Example: assumptions using a DAG",
    "text": "Example: assumptions using a DAG\n\n\n\n\n\n\n\n\n\nFigure 22: A proposed causal diagram of the effect of bed net use on malaria. This directed acyclic graph (DAG) states our assumption that bed net use causes a reduction in malaria risk. It also says that we assume: malaria risk is impacted by net usage, income, health, temperature, and insecticide resistance; net usage is impacted by income, health, temperature, eligibility for the free net program, and the number of people in a household; eligibility for the free net programs is impacted by income and the number of people in a household; and health is impacted by income."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag-2",
    "href": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag-2",
    "title": "Causality",
    "section": "Example: assumptions using a DAG",
    "text": "Example: assumptions using a DAG\n\n\n\n\n\n\n\n\n\nFigure 23: In the proposed DAG, there are eight open pathways that contribute to the causal effect seen in the naive regression: the true effect (in green) of net usage on malaria risk and seven other confounding pathways (in orange). The naive estimate is wrong because it is a composite of all these effects."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions",
    "href": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions",
    "title": "Causality",
    "section": "Example: Model our assumptions",
    "text": "Example: Model our assumptions\n\nFor this DAG, we need to control for three variables: health, income, and temperature. These three variables are a minimal adjustment set, the minimum set (or sets) of variables you need to block all confounding pathways.\nWe could estimate the causal effect using Regression Adjustment: including the minimal set of covariates in our regression.\nInstead we’ll use a technique called Inverse Probability Weighting (IPW) to control for these variables:\nWe’ll use logistic regression to predict the probability of treatment—the propensity score. Then, we’ll calculate inverse probability weights to apply to the linear regression model we fit above.\nThe propensity score model includes the exposure—net use—as the dependent variable and the minimal adjustment set as the independent variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-1",
    "href": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-1",
    "title": "Causality",
    "section": "Example: Model our assumptions",
    "text": "Example: Model our assumptions\n\nThe propensity score model is a logistic regression model with the formula net ~ income + health + temperature, which predicts the probability of bed net usage based on the confounders income, health, and temperature.\n\n\nCode\npropensity_model &lt;- glm(\n  net ~ income + health + temperature,\n  data = causalworkshop::net_data,\n  family = binomial()\n)\n\n# the first six propensity scores\nhead(predict(propensity_model, type = \"response\"))\n\n\n     1      2      3      4      5      6 \n0.2464 0.2178 0.3230 0.2307 0.2789 0.3060 \n\n\nWe can use propensity scores to control for confounding in various ways. In this example, we’ll focus on weighting.\nIn particular, we’ll compute the inverse probability weight for the average treatment effect (ATE). The ATE represents a particular causal question: what if everyone in the study used bed nets vs. what if no one in the study used bed nets?\nTo calculate the ATE, we’ll use the broom and propensity packages. broom’s augment() function extracts prediction-related information from the model and joins it to the data. propensity’s wt_ate() function calculates the inverse probability weight given the propensity score and exposure."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-2",
    "href": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-2",
    "title": "Causality",
    "section": "Example: Model our assumptions",
    "text": "Example: Model our assumptions\n\nFor inverse probability weighting, the ATE weight is the probability of receiving the treatment you actually received. In other words, if you used a bed net, the ATE weight is the probability that you used a net, and if you did not use a net, it is the probability that you did not use a net.\n\n\nCode\nnet_data_wts &lt;- propensity_model |&gt;\n  broom::augment(newdata = causalworkshop::net_data, type.predict = \"response\") |&gt;\n  # .fitted is the value predicted by the model\n  # for a given observation\n  dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\nnet_data_wts |&gt;\n  dplyr::select(net, .fitted, wts) |&gt;\n  dplyr::slice_head(n=16)\n\n\n# A tibble: 16 × 3\n   net   .fitted   wts\n   &lt;lgl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 FALSE   0.246  1.33\n 2 FALSE   0.218  1.28\n 3 FALSE   0.323  1.48\n 4 FALSE   0.231  1.30\n 5 FALSE   0.279  1.39\n 6 FALSE   0.306  1.44\n 7 FALSE   0.332  1.50\n 8 FALSE   0.168  1.20\n 9 FALSE   0.222  1.29\n10 FALSE   0.255  1.34\n11 FALSE   0.215  1.27\n12 FALSE   0.220  1.28\n13 FALSE   0.195  1.24\n14 FALSE   0.167  1.20\n15 FALSE   0.200  1.25\n16 TRUE    0.413  2.42"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-3",
    "href": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-3",
    "title": "Causality",
    "section": "Example: Model our assumptions",
    "text": "Example: Model our assumptions\n\nwts represents the amount each observation will be up-weighted or down-weighted in the outcome model we will fit.\nFor instance, the 16th household used a bed net and had a predicted probability of 0.41. That’s a pretty low probability considering they did, in fact, use a net, so their weight is higher at 2.42. In other words, this household will be up-weighted compared to the naive linear model we fit above.\nThe first household did not use a bed net; they’re predicted probability of net use was 0.25 (or put differently, a predicted probability of not using a net of 0.75). That’s more in line with their observed value of net, but there’s still some predicted probability of using a net, so their weight is 1.28."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models",
    "href": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models",
    "title": "Causality",
    "section": "Example: Diagnose our models",
    "text": "Example: Diagnose our models\n\n\nThe goal of propensity score weighting is to weight the population of observations such that the distribution of confounders is balanced between the exposure groups.\nIn principle, we’re removing the arrows between the confounders and exposure in the DAG, so that the confounding paths no longer distort our estimates.\n\n\n\nCode\nlibrary(halfmoon)\nggplot(net_data_wts, aes(.fitted)) +\n  halfmoon::geom_mirror_histogram(\n    aes(fill = net),\n    bins = 50\n  ) +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\")\n\n\n\n\n\n\n\n\nFigure 24: A mirrored histogram of the propensity scores of those who used nets (top, blue) versus those who who did not use nets (bottom, orange). The range of propensity scores is similar between groups, with those who used nets slightly to the left of those who didn’t, but the shapes of the distribution are different."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-1",
    "href": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-1",
    "title": "Causality",
    "section": "Example: Diagnose our models",
    "text": "Example: Diagnose our models\n\n\nThe weighted propensity score creates a pseudo-population where the distributions are much more similar:\n\n\nCode\nggplot(net_data_wts, aes(.fitted)) +\n  halfmoon::geom_mirror_histogram(\n    aes(group = net),\n    bins = 50\n  ) +\n  halfmoon::geom_mirror_histogram(\n    aes(fill = net, weight = wts),\n    bins = 50,\n    alpha = .5\n  ) +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 25: A mirrored histogram of the propensity scores of those who used nets (top, blue) versus those who did not use nets (bottom, orange). The dark regions represent the unweighted distributions, and the lighter regions represent the weighted distributions. The ATE weights up-weight the groups to be similar in range and shape of the distribution of propensity scores."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-2",
    "href": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-2",
    "title": "Causality",
    "section": "Example: Diagnose our models",
    "text": "Example: Diagnose our models\n\n\nWe might also want to know how well-balanced the groups are by each confounder. One way to do this is to calculate the standardized mean differences (SMDs) for each confounder with and without weights. We’ll calculate the SMDs with tidy_smd() then plot them with geom_love() (see SMD).\n\n\nCode\nplot_df &lt;- tidysmd::tidy_smd(\n  net_data_wts,\n  c(income, health, temperature),\n  .group = net,\n  .wts = wts\n)\n\nplot_df %&gt;% ggplot(\n  aes(x = abs(smd), y = variable,\n    group = method, color = method\n  )\n) + tidysmd::geom_love()\n\n\n\n\n\n\n\n\n\n\n\nFigure 26: A love plot representing the standardized mean differences (SMD) between exposure groups of three confounders: temperature, income, and health. Before weighting, there are considerable differences in the groups. After weighting, the confounders are much more balanced between groups."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-3",
    "href": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-3",
    "title": "Causality",
    "section": "Example: Diagnose our models",
    "text": "Example: Diagnose our models\n\n\nBefore we apply the weights to the outcome model, let’s check their overall distribution for extreme weights.\nExtreme weights can destabilize the estimate and variance in the outcome model, so we want to be aware of it.\n\n\nCode\nnet_data_wts |&gt;\n  ggplot(aes(wts)) +\n  geom_density(\n    fill = \"#CC79A7\"\n    , color = NA\n    , alpha = 0.8\n  )\n\n\n\n\n\n\n\n\n\n\n\nFigure 27: A density plot of the average treatment effect (ATE) weights. The plot is skewed, with higher values towards 8. This may indicate a problem with the model, but the weights aren’t so extreme to destabilize the variance of the estimate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nWe’re now ready to use the ATE weights to (attempt to) account for confounding in the naive linear regression model. Fitting such a model is pleasantly simple in this case: we fit the same model as before but with weights = wts, which will incorporate the inverse probability weights.\n\n\nCode\nnet_data_wts |&gt;\n  lm(malaria_risk ~ net, data = _, weights = wts) |&gt;\n  broom::tidy(conf.int = TRUE)\n\n\n# A tibble: 2 × 7\n  term   estimate std.error statistic  p.value conf.low\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 (Inte…     42.7     0.442      96.7 0            41.9\n2 netTR…    -12.5     0.624     -20.1 5.50e-81    -13.8\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\n\n\nCode\nestimates &lt;- net_data_wts |&gt;\n  lm(malaria_risk ~ net, data = _, weights = wts) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"netTRUE\") |&gt;\n  dplyr::select(estimate, starts_with(\"conf\")) |&gt;\n  dplyr::mutate( \n    dplyr::across( everything(), \\(x)round(x, digits = 1) ) \n  )\nestimates\n\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    -12.5    -13.8     -11.3\n\n\nThe estimate for the average treatment effect is -12.5 (95% CI -13.8, -11.3)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-1",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-1",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nUnfortunately, the confidence intervals don’t account for the dependence within the weights! Generally, confidence intervals for propensity-score weighted models will be too narrow unless we correct for this dependence.\nThe nominal coverage of the confidence intervals will thus be wrong (they aren’t 95% CIs because their coverage is much lower than 95%) and may lead to misinterpretation.\nFor this example, we’ll use the bootstrap, a flexible tool that calculates distributions of parameters using re-sampling. We’ll use the rsample package from tidymodels to work with bootstrap samples."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-2",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-2",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nBecause the the inverse probability weights are not fixed values (they depend on the data), we need to account for this by bootstrapping the entire modeling process.\nFor every bootstrap sample, we need to fit the propensity score model, calculate the inverse probability weights, then fit the weighted outcome model, using the following function:\n\n\nCode\nfit_ipw &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  .df &lt;- rsample::analysis(split)\n\n  # fit propensity score model\n  propensity_model &lt;- glm(\n    net ~ income + health + temperature,\n    data = .df,\n    family = binomial()\n  )\n\n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\n  # fit correctly bootstrapped ipw model\n  lm(malaria_risk ~ net, data = .df, weights = wts) |&gt;\n    broom::tidy()\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-3",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-3",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nWe generate a distribution of estimates as follows:\n\n\nbootstrap - ipw - estimate\n# create bootstrap samples\nbootstrapped_net_data &lt;- rsample::bootstraps(\n  causalworkshop::net_data,\n  times = 1000,\n  # required to calculate CIs later\n  apparent = TRUE\n)\n\n# create ipw and fit each bootstrap sample\nipw_results &lt;- bootstrapped_net_data |&gt;\n  dplyr::mutate(\n    boot_fits = purrr::map(splits, fit_ipw))\n\n\n\n\ndistribution\nipw_results |&gt;\n  mutate(\n    estimate = map_dbl(\n      boot_fits,\n      # pull the `estimate` for `netTRUE` for each fit\n      \\(.fit) .fit |&gt;\n        filter(term == \"netTRUE\") |&gt;\n        pull(estimate)\n    )\n  ) |&gt;\n  ggplot(aes(estimate)) +\n  geom_histogram(fill = \"#D55E00FF\", color = \"white\", alpha = 0.8)\n\n\n\n\n\n\n\n\nFigure 28: “A histogram of 1,000 bootstrapped estimates of the effect of net use on malaria risk. The spread of these estimates accounts for the dependency and uncertainty in the use of IPW weights.”"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-4",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-4",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nFigure 28 gives a sense of the variation in estimate, but let’s calculate 95% confidence intervals from the bootstrapped distribution using rsample’s int_t() :\n\n\nCode\nboot_estimate &lt;- ipw_results |&gt;\n  # calculate T-statistic-based CIs\n  rsample::int_t(boot_fits) |&gt;\n  dplyr::filter(term == \"netTRUE\")\n\nboot_estimate\n\n\n# A tibble: 1 × 6\n  term    .lower .estimate .upper .alpha .method  \n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n1 netTRUE  -13.4     -12.5  -11.6   0.05 student-t\n\n\nNow we have a confounder-adjusted estimate with correct standard errors. The estimate of the effect of all households using bed nets versus no households using bed nets on malaria risk is -12.5 (95% CI -13.4, -11.6).\nBed nets do indeed seem to reduce malaria risk in this study."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis",
    "href": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis",
    "title": "Causality",
    "section": "Example: Conduct sensitivity analysis",
    "text": "Example: Conduct sensitivity analysis\n\nWhen conducting a causal analysis, it’s a good idea to use sensitivity analyses to test your assumptions. There are many potential sources of bias in any study and many sensitivity analyses to go along with them; we’ll focus on the assumption of no confounding.\nWhen we have less information about unmeasured confounders, we can use tipping point analysis to ask how much confounding it would take to tip my estimate to the null. In other words, what would the strength of the unmeasured confounder have to be to explain our results away? The tipr package is a toolkit for conducting sensitivity analyses."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis-1",
    "href": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis-1",
    "title": "Causality",
    "section": "Example: Conduct sensitivity analysis",
    "text": "Example: Conduct sensitivity analysis\n\nAssume an unknown, normally-distributed confounder.\nThe tip_coef() function takes an estimate (e.g. the upper or lower bound of the coefficient) and further requires either the\n\nthe scaled differences in means of the confounder between exposure groups, or\nthe effect of the confounder on the outcome.\n\nFor the estimate, we’ll use conf.high, which is closer to 0 (the null), and ask: how much would the confounder have to affect malaria risk to have an unbiased upper confidence interval of 0?\nWe’ll use tipr to calculate this answer for 5 scenarios, where the mean difference in the confounder between exposure groups is 1, 2, 3, 4, or 5."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-sensitivity-analysis",
    "href": "slides/BSMM_8740_lec_07.html#example-sensitivity-analysis",
    "title": "Causality",
    "section": "Example: Sensitivity analysis",
    "text": "Example: Sensitivity analysis\n\n\n\nCode\ntipping_points &lt;- tipr::tip_coef(boot_estimate$.upper, exposure_confounder_effect = 1:5)\n\ntipping_points |&gt;\n  ggplot(aes(confounder_outcome_effect, exposure_confounder_effect)) +\n  geom_line(color = \"#009E73\", linewidth = 1.1) +\n  geom_point(fill = \"#009E73\", color = \"white\", size = 2.5, shape = 21) +\n  labs(\n    x = \"Confounder-Outcome Effect\",\n    y = \"Scaled mean differences in\\n confounder between exposure groups\"\n  )\n\n\n\n\n\n\n\n\nFigure 29: A tipping point analysis under several confounding scenarios where the unmeasured confounder is a normally-distributed continuous variable. The line represents the strength of confounding necessary to tip the upper confidence interval of the causal effect estimate to 0. The x-axis represents the coefficient of the confounder-outcome relationship adjusted for the exposure and the set of measured confounders. The y-axis represents the scaled mean difference of the confounder between exposure groups."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#more",
    "href": "slides/BSMM_8740_lec_07.html#more",
    "title": "Causality",
    "section": "More",
    "text": "More\n\nRead Statistical tools for causal inference\nRead Causal inference in R"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#summary-assumptions",
    "href": "slides/BSMM_8740_lec_07.html#summary-assumptions",
    "title": "Causality",
    "section": "Summary: Assumptions",
    "text": "Summary: Assumptions\n\nIn order to estimate a causal effect when there is a confounder, we need the following assumptions\n\n\\((Y^1,Y^0)\\, \\bot\\, D\\mid X\\) (conditional independence)\n\\(0&lt;\\mathbb{P}(D=1 \\mid X) &lt;1\\) (common support)\n\nThese two assumptions yield the following identity\n\\[\n\\begin{align}   E\\big[Y^1-Y^0\\mid X\\big] & = E\\big[Y^1 - Y^0 \\mid X,D=1\\big]                        \\\\            & = E\\big[Y^1\\mid X,D=1\\big] - E\\big[Y^0\\mid X,D=0\\big]   \\\\            & = E\\big[Y\\mid X,D=1\\big] - E\\big[Y\\mid X,D=0\\big]     \\end{align}\n\\]\nwhere each value of \\(Y\\) is determined by the switching equation.\nWhereas we need treatment to be conditionally independent of both potential outcomes to identify the ATE, we need only treatment to be conditionally independent of \\(Y^0\\) to identify the ATT and the fact that there exist some units in the control group for each treatment strata. Note, the reason for the common support assumption is because we are weighting the data; without common support, we cannot calculate the relevant weights."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#recap",
    "href": "slides/BSMM_8740_lec_07.html#recap",
    "title": "Causality",
    "section": "Recap",
    "text": "Recap\n\nWe introduced the fundamental problems of inference and the biases of some intuitive estimators,\nWe developed a basic understanding of the tools used to state and then satisfy causality assumptions,\nWe looked at an example of how econometric methods recover treatment effects."
  },
  {
    "objectID": "slides/BSMM_8740_lec_12.html#congratulations-on-completing-the-course",
    "href": "slides/BSMM_8740_lec_12.html#congratulations-on-completing-the-course",
    "title": "Final Exam",
    "section": "Congratulations on Completing the Course",
    "text": "Congratulations on Completing the Course\n\nBefore we wrap up though, complete the Student Perceptions of Teaching,\nthen make sure you have BRMS installed, and\nfinally, the final exam."
  },
  {
    "objectID": "slides/BSMM_8740_lec_12.html#brms",
    "href": "slides/BSMM_8740_lec_12.html#brms",
    "title": "Final Exam",
    "section": "BRMS",
    "text": "BRMS\n\nReference materials for BRMS can be found here and here.\nInstructions for installing BRMS can be found here and here. The basic steps (in order) are\n\nConfigure the C++ toolchain (use RTools in Windows)\nInstall Stan and verify the Stan installation\nInstall BRMS"
  },
  {
    "objectID": "slides/BSMM_8740_lec_12.html#thank-you-enjoy-the-holidays",
    "href": "slides/BSMM_8740_lec_12.html#thank-you-enjoy-the-holidays",
    "title": "Final Exam",
    "section": "Thank You & Enjoy the Holidays",
    "text": "Thank You & Enjoy the Holidays\n\nIf you have emailed me and your questions have not yet been resolved, I will be working to complete everything before I start grading.\nFeel free to send a reminder if you think I may have missed your earlier email(s)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recap-of-last-lecture",
    "href": "slides/BSMM_8740_lec_02.html#recap-of-last-lecture",
    "title": "The recipes package",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\nLast week we introduced the tidyverse verbs used to manipulate (tidy) data\nWe also used the tidyverse verbs for exploratory data analysis and to engineer features into our datasets."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#todays-outline",
    "href": "slides/BSMM_8740_lec_02.html#todays-outline",
    "title": "The recipes package",
    "section": "Today’s Outline",
    "text": "Today’s Outline\n\nWe’ll review the R model formula approach to model specification,\nWe’ll introduce data pre-processing and design/model matrix generation with the recipes package, and\nWe’ll show how using recipes will facilitate developing feature engineering workflows that can be applied to multiple datasets (e.g. train and test as well as cross-validation1 datasets)\n\nCross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#r-model-formulas",
    "href": "slides/BSMM_8740_lec_02.html#r-model-formulas",
    "title": "The recipes package",
    "section": "R Model Formulas",
    "text": "R Model Formulas\nHere’s a simple formula used in a linear model to predict house prices (using the dataset Sacremento from the package modeldata):\n\n\nCode\n&gt; Sacramento &lt;- modeldata::Sacramento\n&gt; mod1 &lt;- stats::lm(\n+   log(price) ~ type + sqft\n+   , data = Sacramento\n+   , subset = beds &gt; 2\n+   )\n\n\n\nThe purpose of this code chunk:\n\nsubset some of the observations (using the subset argument)\ncreate a design matrix for 2 predictor variables (but 3 model terms)\nlog transform the outcome variables\nfit a linear regression model\n\nThe first two steps create the design- or model- matrix."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example",
    "href": "slides/BSMM_8740_lec_02.html#example",
    "title": "The recipes package",
    "section": "Example",
    "text": "Example\nThe dataset Sacramento has three categorical variables:\n\n\nCode\n&gt; skimr::skim(Sacramento) %&gt;% \n+   dplyr::select(c(skim_variable,contains('factor')) ) %&gt;% \n+   tidyr::drop_na() %&gt;% \n+   gt::gt() %&gt;% \n+   gtExtras:::gt_theme_espn() %&gt;% \n+   gt::tab_options( table.font.size = gt::px(20) ) %&gt;% \n+   gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nskim_variable\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\n\n\n\n\ncity\nFALSE\n37\nSAC: 438, ELK: 114, ROS: 48, CIT: 35\n\n\nzip\nFALSE\n68\nz95: 61, z95: 45, z95: 44, z95: 37\n\n\ntype\nFALSE\n3\nRes: 866, Con: 53, Mul: 13"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-design-matrix",
    "href": "slides/BSMM_8740_lec_02.html#example-design-matrix",
    "title": "The recipes package",
    "section": "Example design matrix",
    "text": "Example design matrix\n\n&gt;  mm &lt;- model.matrix(\n+    log(price) ~ type + sqft\n+    , data = Sacramento\n+ )\n\n\n\n   (Intercept) typeMulti_Family typeResidential sqft\n1            1                0               1  836\n2            1                0               1 1167\n3            1                0               1  796\n4            1                0               1  852\n5            1                0               1  797\n6            1                0               0 1122\n7            1                0               1 1104\n8            1                0               1 1177\n9            1                0               0  941\n10           1                0               1 1146\n11           1                0               1  909\n12           1                0               1 1289\n13           1                0               1  871\n14           1                0               1 1020"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#summary-model-formula-method",
    "href": "slides/BSMM_8740_lec_02.html#summary-model-formula-method",
    "title": "The recipes package",
    "section": "Summary: Model Formula Method",
    "text": "Summary: Model Formula Method\n\nModel formulas are very expressive in that they can represent model terms easily\nThe formula/terms framework does some elegant functional programming\nFunctions can be embedded inline to do fairly complex things (on single variables) and these can be applied to new data sets."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#model-formula-examples",
    "href": "slides/BSMM_8740_lec_02.html#model-formula-examples",
    "title": "The recipes package",
    "section": "Model formula examples",
    "text": "Model formula examples\n\nleave outinteractioncrossingnestingas-is\n\n\n\n&gt; model.matrix(log(price) ~ -1 + type + sqft, data = Sacramento) %&gt;% head()\n\n  typeCondo typeMulti_Family typeResidential sqft\n1         0                0               1  836\n2         0                0               1 1167\n3         0                0               1  796\n4         0                0               1  852\n5         0                0               1  797\n6         1                0               0 1122\n\n\n\n\n\n&gt; model.matrix(log(price) ~ type : sqft, data = Sacramento) %&gt;% head()\n\n  (Intercept) typeCondo:sqft typeMulti_Family:sqft typeResidential:sqft\n1           1              0                     0                  836\n2           1              0                     0                 1167\n3           1              0                     0                  796\n4           1              0                     0                  852\n5           1              0                     0                  797\n6           1           1122                     0                    0\n\n\n\n\n\n&gt; model.matrix(log(price) ~ type * sqft, data = Sacramento) %&gt;% head()\n\n  (Intercept) typeMulti_Family typeResidential sqft typeMulti_Family:sqft\n1           1                0               1  836                     0\n2           1                0               1 1167                     0\n3           1                0               1  796                     0\n4           1                0               1  852                     0\n5           1                0               1  797                     0\n6           1                0               0 1122                     0\n  typeResidential:sqft\n1                  836\n2                 1167\n3                  796\n4                  852\n5                  797\n6                    0\n\n\n\n\n\n&gt; model.matrix(log(price) ~ type %in% sqft, data = Sacramento) %&gt;% head()\n\n  (Intercept) typeCondo:sqft typeMulti_Family:sqft typeResidential:sqft\n1           1              0                     0                  836\n2           1              0                     0                 1167\n3           1              0                     0                  796\n4           1              0                     0                  852\n5           1              0                     0                  797\n6           1           1122                     0                    0\n\n\n\n\n\n&gt; model.matrix(log(price) ~ type + sqft + I(sqft^2), data = Sacramento) %&gt;% head()\n\n  (Intercept) typeMulti_Family typeResidential sqft I(sqft^2)\n1           1                0               1  836    698896\n2           1                0               1 1167   1361889\n3           1                0               1  796    633616\n4           1                0               1  852    725904\n5           1                0               1  797    635209\n6           1                0               0 1122   1258884\n\n\ncontrast with log(price) ~ type + sqft + sqft^2"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#summary-model-formula-method-1",
    "href": "slides/BSMM_8740_lec_02.html#summary-model-formula-method-1",
    "title": "The recipes package",
    "section": "Summary: Model Formula Method",
    "text": "Summary: Model Formula Method\nThere are significant limitations to what this framework can do and, in some cases, it can be very inefficient.\nThis is mostly due to being written well before large scale modeling and machine learning were commonplace."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#limitations-of-the-current-system",
    "href": "slides/BSMM_8740_lec_02.html#limitations-of-the-current-system",
    "title": "The recipes package",
    "section": "Limitations of the Current System",
    "text": "Limitations of the Current System\n\nFormulas are not very extensible especially with nested or sequential operations (e.g. y ~ scale(center(knn_impute(x)))).\nWhen used in modeling functions, you cannot recycle the previous computations.\nFor wide data sets with lots of columns, the formula method can be very inefficient and consume a significant proportion of the total execution time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#limitations-of-the-current-system-1",
    "href": "slides/BSMM_8740_lec_02.html#limitations-of-the-current-system-1",
    "title": "The recipes package",
    "section": "Limitations of the Current System",
    "text": "Limitations of the Current System\n\nMultivariate outcomes are kludgy by requiring cbind .\nFormulas have a limited set of roles for measurements (just predictor and outcome). We’ll look further at roles in the next two slides.\n\nA more in-depth discussion of these issues can be found in this blog post (recommended to read)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#variable-roles",
    "href": "slides/BSMM_8740_lec_02.html#variable-roles",
    "title": "The recipes package",
    "section": "Variable Roles",
    "text": "Variable Roles\nFormulas have been re-implemented in different packages for a variety of different reasons:\n\n&gt; # ?lme4::lmer\n&gt; # Subjects need to be in the data but are not part of the model\n&gt; lme4::lmer(Reaction ~ Days + (Days | Subject), data = lme4::sleepstudy)\n&gt; \n&gt; # BradleyTerry2\n&gt; # We want to make the outcomes to be a function of a \n&gt; # competitor-specific function of reach \n&gt; BradleyTerry2::BTm(outcome = 1, player1 = winner, player2 = loser,\n+     formula = ~ reach[..] + (1|..), \n+     data = boxers)\n&gt; \n&gt; # modeltools::ModelEnvFormula (using the modeltools package for formulas)\n&gt; # mob\n&gt; data(PimaIndiansDiabetes, package = 'mlbench')\n&gt; modeltools::ModelEnvFormula(diabetes ~ glucose | pregnant + mass +  age,\n+     data = PimaIndiansDiabetes)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#variable-roles-1",
    "href": "slides/BSMM_8740_lec_02.html#variable-roles-1",
    "title": "The recipes package",
    "section": "Variable Roles",
    "text": "Variable Roles\nA general list of possible variable roles could be:\n\n\noutcomes\npredictors\nstratification\nmodel performance data (e.g. loan amount to compute expected loss)\nconditioning or faceting variables (e.g. lattice or ggplot2)\nrandom effects or hierarchical model ID variables\ncase weights (*)\noffsets (*)\nerror terms (limited to Error in the aov function)(*)\n\n(*) Can be handled in formulas but are hard-coded into functions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recipes",
    "href": "slides/BSMM_8740_lec_02.html#recipes",
    "title": "The recipes package",
    "section": "Recipes",
    "text": "Recipes\nWe can approach the design matrix and preprocessing steps by first specifying a sequence of steps.\n\nprice is an outcome\ntype and sqft are predictors\nlog transform price\nconvert type to dummy variables"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recipes-1",
    "href": "slides/BSMM_8740_lec_02.html#recipes-1",
    "title": "The recipes package",
    "section": "Recipes",
    "text": "Recipes\nA recipe is a specification of intent.\nOne issue with the formula method is that it couples the specification for your predictors along with the model implementation.\nRecipes separate the planning from the doing.\n\n\n\n\n\n\nNote\n\n\nThe Recipes website is found at: https://topepo.github.io/recipes"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recipes-2",
    "href": "slides/BSMM_8740_lec_02.html#recipes-2",
    "title": "The recipes package",
    "section": "Recipes",
    "text": "Recipes\n\n\nrecipes workflow\n&gt; ## Create an initial recipe with only predictors and outcome\n&gt; rec &lt;- recipes::recipe(price ~ type + sqft, data = Sacramento)\n&gt; \n&gt; rec &lt;- rec %&gt;% \n+   recipes::step_log(price) %&gt;%\n+   recipes::step_dummy(type)\n&gt; \n&gt; # estimate any parameters\n&gt; rec_trained &lt;- recipes::prep(rec, training = Sacramento, retain = TRUE)\n&gt; # apply the computations to new_data\n&gt; design_mat  &lt;- recipes::bake(rec_trained, new_data = Sacramento)\n\n\nOnce created, a recipe can be prepped on training data then baked with any other data.\n\n\nthe prep step calculates and stores variables related to the steps (e.g. (min,max) for scaling), using the training data\nthe bake step applies the steps to new data"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#selecting-variables",
    "href": "slides/BSMM_8740_lec_02.html#selecting-variables",
    "title": "The recipes package",
    "section": "Selecting Variables",
    "text": "Selecting Variables\nIn the last slide, we used dplyr-like syntax for selecting variables such as step_dummy(type).\nIn some cases, the names of the predictors may not be known at the time when you construct a recipe (or model formula). For example:\n\ndummy variable columns\nPCA feature extraction when you keep components that capture \\(\\mathrm{X}\\%\\) of the variability.\ndiscretized predictors with dynamic bins"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-1",
    "href": "slides/BSMM_8740_lec_02.html#example-1",
    "title": "The recipes package",
    "section": "Example",
    "text": "Example\nUsing the airquality dataset in the datasets package\n\n&gt; dat &lt;- datasets::airquality\n&gt; \n&gt; dat %&gt;% skimr::skim() %&gt;% \n+   dplyr::select(skim_variable:numeric.sd) %&gt;% \n+   gt::gt() %&gt;% \n+   gtExtras:::gt_theme_espn() %&gt;% \n+   gt::tab_options( table.font.size = gt::px(20) ) %&gt;% \n+   gt::as_raw_html()\n\n\n  \n  \n\n\n\nskim_variable\nn_missing\ncomplete_rate\nnumeric.mean\nnumeric.sd\n\n\n\n\nOzone\n37\n0.7581699\n42.129310\n32.987885\n\n\nSolar.R\n7\n0.9542484\n185.931507\n90.058422\n\n\nWind\n0\n1.0000000\n9.957516\n3.523001\n\n\nTemp\n0\n1.0000000\n77.882353\n9.465270\n\n\nMonth\n0\n1.0000000\n6.993464\n1.416522\n\n\nDay\n0\n1.0000000\n15.803922\n8.864520"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-create-basic-recipe",
    "href": "slides/BSMM_8740_lec_02.html#example-create-basic-recipe",
    "title": "The recipes package",
    "section": "Example: create basic recipe",
    "text": "Example: create basic recipe\n\n&gt; # create recipe\n&gt; aq_recipe &lt;- recipes::recipe(Ozone ~ ., data = aq_df_train)\n&gt; \n&gt; summary(aq_recipe)\n\n# A tibble: 6 × 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 Solar.R  &lt;chr [2]&gt; predictor original\n2 Wind     &lt;chr [2]&gt; predictor original\n3 Temp     &lt;chr [2]&gt; predictor original\n4 Month    &lt;chr [2]&gt; predictor original\n5 Day      &lt;chr [2]&gt; predictor original\n6 Ozone    &lt;chr [2]&gt; outcome   original"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-summary-of-basic-recipe",
    "href": "slides/BSMM_8740_lec_02.html#example-summary-of-basic-recipe",
    "title": "The recipes package",
    "section": "Example: summary of basic recipe",
    "text": "Example: summary of basic recipe\n\n&gt; # update roles for variables with missing data\n&gt; aq_recipe &lt;- aq_recipe %&gt;% \n+   recipes::update_role(Ozone, Solar.R, new_role = 'NA_Variable')\n&gt; \n&gt; summary(aq_recipe)\n\n# A tibble: 6 × 4\n  variable type      role        source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;       &lt;chr&gt;   \n1 Solar.R  &lt;chr [2]&gt; NA_Variable original\n2 Wind     &lt;chr [2]&gt; predictor   original\n3 Temp     &lt;chr [2]&gt; predictor   original\n4 Month    &lt;chr [2]&gt; predictor   original\n5 Day      &lt;chr [2]&gt; predictor   original\n6 Ozone    &lt;chr [2]&gt; NA_Variable original"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-add-recipe-steps",
    "href": "slides/BSMM_8740_lec_02.html#example-add-recipe-steps",
    "title": "The recipes package",
    "section": "Example: add recipe steps",
    "text": "Example: add recipe steps\n\n&gt; aq_recipe &lt;- aq_recipe %&gt;% \n+   # impute Ozone missing values using the mean\n+   step_impute_mean(has_role('NA_Variable'), -Solar.R) %&gt;%\n+   # impute Solar.R missing values using knn\n+   step_impute_knn(contains('.R'), neighbors = 3) %&gt;%\n+   # center all variable except the NA_Variable\n+   step_center(all_numeric(), -has_role('NA_Variable')) %&gt;%\n+   # scale all variable except the NA_Variable\n+   step_scale(all_numeric(), -has_role('NA_Variable'))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-recipe-prep-and-bake",
    "href": "slides/BSMM_8740_lec_02.html#example-recipe-prep-and-bake",
    "title": "The recipes package",
    "section": "Example: recipe prep and bake",
    "text": "Example: recipe prep and bake\n\n&gt; # prep with training data\n&gt; aq_prep_train &lt;- aq_recipe %&gt;% prep(aq_df_train)\n&gt; \n&gt; # bake with testing data\n&gt; aq_bake &lt;- aq_prep_train %&gt;% bake(aq_df_test)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-recipe-prep-values",
    "href": "slides/BSMM_8740_lec_02.html#example-recipe-prep-values",
    "title": "The recipes package",
    "section": "Example: recipe prep values",
    "text": "Example: recipe prep values\nThe prepped recipe is a data structure that contains any computed values.\n\n&gt; aq_prep_train |&gt; recipes::tidy()\n\n# A tibble: 4 × 6\n  number operation type        trained skip  id               \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;            \n1      1 step      impute_mean TRUE    FALSE impute_mean_wwW3J\n2      2 step      impute_knn  TRUE    FALSE impute_knn_W5Vfi \n3      3 step      center      TRUE    FALSE center_o6vLp     \n4      4 step      scale       TRUE    FALSE scale_qIXIn"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-recipe-prep-values-1",
    "href": "slides/BSMM_8740_lec_02.html#example-recipe-prep-values-1",
    "title": "The recipes package",
    "section": "Example: recipe prep values",
    "text": "Example: recipe prep values\nWe can examine any computed values by using the step number as an argument to recipes::tidy.\n\n&gt; aq_prep_train |&gt; recipes::tidy(3)\n\n# A tibble: 4 × 3\n  terms value id          \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 Wind  10.2  center_o6vLp\n2 Temp  78.0  center_o6vLp\n3 Month  7.07 center_o6vLp\n4 Day   15.3  center_o6vLp"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-change-roles-again",
    "href": "slides/BSMM_8740_lec_02.html#example-change-roles-again",
    "title": "The recipes package",
    "section": "Example: change roles again",
    "text": "Example: change roles again\nHere we update the original recipe to set the required roles.\n\n&gt; aq_recipe &lt;- aq_recipe %&gt;% \n+   recipes::update_role(Ozone, new_role = 'outcome') %&gt;% \n+   recipes::update_role(Solar.R, new_role = 'predictor')"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recommended-baseline-steps",
    "href": "slides/BSMM_8740_lec_02.html#recommended-baseline-steps",
    "title": "The recipes package",
    "section": "Recommended Baseline Steps",
    "text": "Recommended Baseline Steps\nBaseline preprocessing methods can be categorized as:\n\n\ndummy: Do qualitative predictors require a numeric encoding (e.g., via dummy variables or other methods)?\nzv: Should columns with a single unique value be removed?\nimpute: If some predictors are missing, should they be estimated via imputation?\ndecorrelate: If there are correlated predictors, should this correlation be mitigated? This might mean filtering out predictors, using principal component analysis, or a model-based technique (e.g., regularization).\nnormalize: Should predictors be centered and scaled?\ntransform: Is it helpful to transform predictors to be more symmetric?\n\nSee recommended preprocessing for recipe steps."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#available-steps",
    "href": "slides/BSMM_8740_lec_02.html#available-steps",
    "title": "The recipes package",
    "section": "Available Steps",
    "text": "Available Steps\n\nBasic: logs, roots, polynomials, logits, hyperbolics\nEncodings: dummy variables, “other” factor level collapsing, discretization\nDate Features: Encodings for day/doy/month etc, holiday indicators\nFilters: correlation, near-zero variables, linear dependencies\nImputation: K-nearest neighbors, bagged trees, mean/mode imputation"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#available-steps-1",
    "href": "slides/BSMM_8740_lec_02.html#available-steps-1",
    "title": "The recipes package",
    "section": "Available Steps",
    "text": "Available Steps\n\nNormalization/Transformations: center, scale, range, Box-Cox, Yeo-Johnson\nDimension Reduction: PCA, kernel PCA, ICA, Isomap, data depth features, class distances\nOthers: spline basis functions, interactions, spatial sign\n\nMore on the way (i.e. autoencoders, more imputation methods, etc.)\nOne of the package vignettes shows how to write your own step functions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#extending",
    "href": "slides/BSMM_8740_lec_02.html#extending",
    "title": "The recipes package",
    "section": "Extending",
    "text": "Extending\nNeed to add more pre-processing or other operations?\n\n&gt; standardized &lt;- rec_trained %&gt;%\n+   recipes::step_center(recipes::all_numeric()) %&gt;%\n+   recipes::step_scale(recipes::all_numeric()) %&gt;%\n+   recipes::step_pca(recipes::all_numeric())\n&gt;           \n&gt; ## Only estimate the new parts:\n&gt; standardized &lt;- recipes::prep(standardized)\n\nIf an initial step is computationally expensive, you don’t have to redo those operations to add more."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#extending-1",
    "href": "slides/BSMM_8740_lec_02.html#extending-1",
    "title": "The recipes package",
    "section": "Extending",
    "text": "Extending\nRecipes can also be created with different roles manually (note: no formula)\n\n&gt; rec &lt;- \n+   recipes::recipe(data = Sacramento) %&gt;%\n+   recipes::update_role(price, new_role = \"outcome\") %&gt;%\n+   recipes::update_role(type, sqft, new_role = \"predictor\") %&gt;%\n+   recipes::update_role(zip, new_role = \"strata\")\n\nAlso, the sequential nature of steps means that steps don’t have to be R operations and could call other compute engines (e.g. Weka, scikit-learn, Tensorflow, etc. )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#extending-2",
    "href": "slides/BSMM_8740_lec_02.html#extending-2",
    "title": "The recipes package",
    "section": "Extending",
    "text": "Extending\nWe can create wrappers to work with recipes too:\n\n&gt; lin_reg.recipe &lt;- function(rec, data, ...) {\n+   trained &lt;- recipes::prep(rec, training = data)\n+   lm.fit(\n+     x = recipes::bake(trained, newdata = data, all_predictors())\n+     , y = recipes::bake(trained, newdata = data, all_outcomes())\n+     , ...\n+   )\n+ }"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#an-example",
    "href": "slides/BSMM_8740_lec_02.html#an-example",
    "title": "The recipes package",
    "section": "An Example",
    "text": "An Example\nKuhn and Johnson (2013) analyze a data set where thousands of cells are determined to be well-segmented (WS) or poorly segmented (PS) based on 58 image features. We would like to make predictions of the segmentation quality based on these features.\n\n\n\n\n\n\n\nNote\n\n\nThe dataset segmentationData is in the package caret and represents the results of automated microscopy to collect images of cultured cells. The images are subjected to segmentation algorithms to identify cellular structures and quantitate their morphology, for hundreds to millions of individual cells."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#an-example-1",
    "href": "slides/BSMM_8740_lec_02.html#an-example-1",
    "title": "The recipes package",
    "section": "An Example",
    "text": "An Example\nThe segmentationData dataset has 61 columns\n\n&gt; data(segmentationData, package = \"caret\")\n&gt; \n&gt; seg_train &lt;- segmentationData %&gt;% \n+   dplyr::filter(Case == \"Train\") %&gt;% \n+   dplyr::select(-Case)\n&gt; \n&gt; seg_test  &lt;- segmentationData %&gt;% \n+   dplyr::filter(Case == \"Test\")  %&gt;% \n+   dplyr::select(-Case)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#a-simple-recipe",
    "href": "slides/BSMM_8740_lec_02.html#a-simple-recipe",
    "title": "The recipes package",
    "section": "A Simple Recipe",
    "text": "A Simple Recipe\n\n\nCode\n&gt; rec &lt;- recipes::recipe(Class  ~ ., data = seg_train)\n&gt; \n&gt; basic &lt;- rec %&gt;%\n+   # the column Cell contains identifiers\n+   recipes::update_role(Cell, new_role = 'ID') %&gt;%\n+   # Correct some predictors for skewness\n+   recipes::step_YeoJohnson(recipes::all_predictors()) %&gt;%\n+   # Standardize the values\n+   recipes::step_center(recipes::all_predictors()) %&gt;%\n+   recipes::step_scale(recipes::all_predictors())\n&gt; \n&gt; # Estimate the transformation and standardization parameters \n&gt; basic &lt;- \n+   recipes::prep(\n+     basic\n+     , training = seg_train\n+     , verbose = FALSE\n+     , retain = TRUE\n+   )  \n\n\n\n\n\n\n\n\n\nNote\n\n\nThe Yeo-Johnson is similar to the Box-Cox method, however it allows for the transformation of nonpositive data as well. A Box Cox transformation is a transformation of non-normal dependent variables into a normal shape. Both transformations have a single parameter \\(\\lambda\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#a-simple-recipe-1",
    "href": "slides/BSMM_8740_lec_02.html#a-simple-recipe-1",
    "title": "The recipes package",
    "section": "A Simple Recipe",
    "text": "A Simple Recipe\nWe can examine the center, scale, and Yeo Johnson parameters computed for each continuous measurement.\n\nYeo Johnsonmeansstd deviations\n\n\n\n&gt; basic |&gt; recipes::tidy(1) |&gt; dplyr::slice_head(n=8)\n\n# A tibble: 8 × 3\n  terms                value id              \n  &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;           \n1 AngleCh1             0.806 YeoJohnson_OPOIu\n2 AreaCh1             -0.861 YeoJohnson_OPOIu\n3 AvgIntenCh1         -0.340 YeoJohnson_OPOIu\n4 AvgIntenCh2          0.434 YeoJohnson_OPOIu\n5 AvgIntenCh3          0.219 YeoJohnson_OPOIu\n6 AvgIntenCh4          0.213 YeoJohnson_OPOIu\n7 DiffIntenDensityCh1 -0.929 YeoJohnson_OPOIu\n8 DiffIntenDensityCh3  0.116 YeoJohnson_OPOIu\n\n\n\n\n\n&gt; basic |&gt; recipes::tidy(2) |&gt; dplyr::slice_head(n=8)\n\n# A tibble: 8 × 3\n  terms                    value id          \n  &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;       \n1 AngleCh1                45.0   center_C8obn\n2 AreaCh1                  1.15  center_C8obn\n3 AvgIntenCh1              2.24  center_C8obn\n4 AvgIntenCh2             17.4   center_C8obn\n5 AvgIntenCh3              6.99  center_C8obn\n6 AvgIntenCh4              7.56  center_C8obn\n7 ConvexHullAreaRatioCh1   1.21  center_C8obn\n8 ConvexHullPerimRatioCh1  0.893 center_C8obn\n\n\n\n\n\n&gt; basic |&gt; recipes::tidy(3) |&gt; dplyr::slice_head(n=8)\n\n# A tibble: 8 × 3\n  terms                      value id         \n  &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;      \n1 AngleCh1                21.1     scale_hic8Y\n2 AreaCh1                  0.00334 scale_hic8Y\n3 AvgIntenCh1              0.204   scale_hic8Y\n4 AvgIntenCh2              9.42    scale_hic8Y\n5 AvgIntenCh3              2.50    scale_hic8Y\n6 AvgIntenCh4              3.04    scale_hic8Y\n7 ConvexHullAreaRatioCh1   0.210   scale_hic8Y\n8 ConvexHullPerimRatioCh1  0.0777  scale_hic8Y"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca",
    "href": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca",
    "title": "The recipes package",
    "section": "More sophisticated steps: PCA",
    "text": "More sophisticated steps: PCA\nPrincipal Component Analysis (PCA) is a technique used in data analysis to simplify a large dataset (many measurements/columns) by reducing its number of dimensions (columns) while still retaining as much important information as possible.\nA pretty good description of PCA can be found here."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-1",
    "href": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-1",
    "title": "The recipes package",
    "section": "More sophisticated steps: PCA",
    "text": "More sophisticated steps: PCA\nPCA step-by-step:\n\n\nStandardize the Data:\n\nBefore applying PCA, we often standardize the data, which means adjusting all variables to have the same scale. This is important because PCA is sensitive to the scale of the data.\n\nFind the Principal Components:\n\nPCA identifies the directions (principal components) in which the data varies the most. Think of these as new axes in a new coordinate system that best capture the variation in the data.\nThe first principal component captures the most variation. The second principal component is orthogonal (at a right angle) to the first and captures the next most variation, and so on.\n\nTransform the Data:\n\nWe then transform the original data into this new set of principal components. Each data point can now be represented in terms of these principal components rather than the original variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-2",
    "href": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-2",
    "title": "The recipes package",
    "section": "More sophisticated steps: PCA",
    "text": "More sophisticated steps: PCA\n\nYou have test scores in Math, Science, and English. You want to find a way to understand overall performance without looking at all three subjects separately.\n\nOriginal Data: You have three scores for each student: Math, Science, and English.\nStandardize the Data: You adjust the scores so that Math, Science, and English scores are on the same scale.\nFind Principal Components:\n\nPCA finds that the first principal component might be a combination of Math and Science scores, capturing the overall academic ability in quantitative subjects.\nThe second principal component might represent the difference between scores in English and the average of Math and Science scores, capturing a different aspect of performance.\n\nTransform the Data:\n\nEach student’s performance can now be described using these new principal components instead of the original scores. For example, a student might have a high score on the first principal component (strong in Math and Science) but a lower score on the second (relatively weaker in English)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-3",
    "href": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-3",
    "title": "The recipes package",
    "section": "More sophisticated steps: PCA",
    "text": "More sophisticated steps: PCA\n\nBenefits of PCA\n\nSimplification: Reduces the number of variables, making it easier to analyze and visualize the data.\nNoise Reduction: Helps to remove noise from the data by focusing on the main components that capture the most variation.\nFeature Extraction: Identifies the most important variables (principal components) that explain the majority of the variance in the data.\n\nPCA is like finding the most important “directions” in your data, where most of the interesting stuff happens. It helps you see the big picture by reducing complexity while keeping the essential information. This makes it a powerful tool for data analysis, especially when dealing with high-dimensional datasets."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#principal-component-analysis",
    "href": "slides/BSMM_8740_lec_02.html#principal-component-analysis",
    "title": "The recipes package",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n\nCode\n&gt; pca &lt;- basic %&gt;% \n+   recipes::step_pca(\n+     recipes::all_predictors()\n+     , num_comp = 5\n+   )\n\n\n\n\n# A tibble: 60 × 4\n   variable                type      role      source  \n   &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Cell                    &lt;chr [2]&gt; ID        original\n 2 AngleCh1                &lt;chr [2]&gt; predictor original\n 3 AreaCh1                 &lt;chr [2]&gt; predictor original\n 4 AvgIntenCh1             &lt;chr [2]&gt; predictor original\n 5 AvgIntenCh2             &lt;chr [2]&gt; predictor original\n 6 AvgIntenCh3             &lt;chr [2]&gt; predictor original\n 7 AvgIntenCh4             &lt;chr [2]&gt; predictor original\n 8 ConvexHullAreaRatioCh1  &lt;chr [2]&gt; predictor original\n 9 ConvexHullPerimRatioCh1 &lt;chr [2]&gt; predictor original\n10 DiffIntenDensityCh1     &lt;chr [2]&gt; predictor original\n# ℹ 50 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#principal-component-analysis-1",
    "href": "slides/BSMM_8740_lec_02.html#principal-component-analysis-1",
    "title": "The recipes package",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca %&lt;&gt;% recipes::prep() \n\n\nsummarycomponents\n\n\n\n&gt; pca %&gt;% summary()\n\n# A tibble: 7 × 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 Cell     &lt;chr [2]&gt; ID        original\n2 Class    &lt;chr [3]&gt; outcome   original\n3 PC1      &lt;chr [2]&gt; predictor derived \n4 PC2      &lt;chr [2]&gt; predictor derived \n5 PC3      &lt;chr [2]&gt; predictor derived \n6 PC4      &lt;chr [2]&gt; predictor derived \n7 PC5      &lt;chr [2]&gt; predictor derived \n\n\n\n\n\n&gt; pca %&gt;% recipes::tidy(4)\n\n# A tibble: 3,364 × 4\n   terms                      value component id       \n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 AngleCh1                 0.00521 PC1       pca_VWCKz\n 2 AreaCh1                  0.0823  PC1       pca_VWCKz\n 3 AvgIntenCh1             -0.204   PC1       pca_VWCKz\n 4 AvgIntenCh2             -0.209   PC1       pca_VWCKz\n 5 AvgIntenCh3             -0.0873  PC1       pca_VWCKz\n 6 AvgIntenCh4             -0.203   PC1       pca_VWCKz\n 7 ConvexHullAreaRatioCh1   0.191   PC1       pca_VWCKz\n 8 ConvexHullPerimRatioCh1 -0.181   PC1       pca_VWCKz\n 9 DiffIntenDensityCh1     -0.185   PC1       pca_VWCKz\n10 DiffIntenDensityCh3     -0.0760  PC1       pca_VWCKz\n# ℹ 3,354 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#principal-component-analysis-2",
    "href": "slides/BSMM_8740_lec_02.html#principal-component-analysis-2",
    "title": "The recipes package",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca %&lt;&gt;% \n+   recipes::bake(\n+     new_data = seg_test\n+     , everything()\n+   )\n&gt; pca[1:8, 1:7]\n\n# A tibble: 8 × 7\n       Cell Class   PC1    PC2    PC3    PC4   PC5\n      &lt;int&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 207827637 PS     4.86 -5.85  -0.891 -4.13  1.84 \n2 207932455 PS     3.28 -1.51   0.353 -2.24  0.441\n3 207827656 WS    -7.03 -1.77  -2.42  -0.652 3.22 \n4 207827659 WS    -6.96 -2.08  -2.89  -1.79  3.20 \n5 207827661 PS     6.52 -3.77  -0.924 -2.61  2.49 \n6 207932479 WS     2.87  1.66   1.75  -5.41  0.324\n7 207932480 WS     2.72  0.433 -1.05  -5.45  1.18 \n8 207827711 WS    -3.01  1.94   2.68  -0.409 3.55"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#principal-component-analysis-3",
    "href": "slides/BSMM_8740_lec_02.html#principal-component-analysis-3",
    "title": "The recipes package",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n\nPCs are predictive\n&gt; pca %&gt;% ggplot(aes(x = PC1, y = PC2, color = Class)) + \n+   geom_point(alpha = .4) +\n+   theme_bw(base_size = 25)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recap",
    "href": "slides/BSMM_8740_lec_02.html#recap",
    "title": "The recipes package",
    "section": "Recap",
    "text": "Recap\n\nWe’ve used the recipes package to create a workflow for data pre-processing and feature engineering\nThe recipe verbs define the pre-processing and feature engineering steps\nusing the recipe object, the verb prep prepares the data on a training set, storing the meta-parameters.\nthe verb bake applies the prepped recipe to new data, using the meta-parameters."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recap-1",
    "href": "slides/BSMM_8740_lec_02.html#recap-1",
    "title": "The recipes package",
    "section": "Recap",
    "text": "Recap\n\nWhen first created, the recipe object contains the the steps defined for pre-processing\nOnce the recipe has been prepped, usually on training data, it contains the calculations required to perform pre-processing of data in the context of the training set, e.g. mean and stdev of normalized columns, PCA components, etc. ( the meta-parameters).\nthe prepped recipe is used to preprocess datasets in context of the training data. E.g. to normalize a column in the test data we subtract the corresponding training set mean and divide by the corresponding training set stdev."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#announcements",
    "href": "slides/BSMM_8740_lec_01.html#announcements",
    "title": "Tidyverse, EDA & git",
    "section": "Announcements",
    "text": "Announcements\n\n\nPlease go to the course website to review the weekly slides, access the labs, read the syllabus, etc.\nMy intent is to have assigned lab exercises each week, which will be started during class.\n\nLab assignments will be due at 5:00pm sharp on the Sunday following the lecture.\n\nLab 1 is due Sunday September 15 at 5pm.\nMy regular office hour will be on Wednesday from 2:30pm - 3:30pm or via MS Teams as requested. Please email ahead of time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#expected-course-topics",
    "href": "slides/BSMM_8740_lec_01.html#expected-course-topics",
    "title": "Tidyverse, EDA & git",
    "section": "Expected Course Topics",
    "text": "Expected Course Topics\n\n\n\n\n  \n  \n\n\n\nweek\ntopic\n\n\n\n\n1\nThe Tidyverse, EDA & Git\n\n\n2\nThe Recipes Package\n\n\n3\nRegression Methods\n\n\n4\nThe Tidymodels Packages\n\n\n5\nClassification & Clustering Methods\n\n\n6\nTime Series Methods\n\n\n7\nCausality: DAGs\n\n\n8\nCausality: Methods\n\n\n9\nMonte-Carlo Methods\n\n\n10\nBayesian Methods"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#todays-outline",
    "href": "slides/BSMM_8740_lec_01.html#todays-outline",
    "title": "Tidyverse, EDA & git",
    "section": "Today’s Outline",
    "text": "Today’s Outline\n\nIntroduction to Tidy data & Tidyverse syntax in R.\nIntroduction to EDA and feature egineering.\nIntroduction to Git workflows and version control."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidy-data",
    "href": "slides/BSMM_8740_lec_01.html#tidy-data",
    "title": "Tidyverse, EDA & git",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nA dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative).\nValues are organised in two ways. Every value belongs to a variable and an observation.\nA variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidy-data-1",
    "href": "slides/BSMM_8740_lec_01.html#tidy-data-1",
    "title": "Tidyverse, EDA & git",
    "section": "Tidy Data",
    "text": "Tidy Data\nTidy data in practice:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidy-data-examples",
    "href": "slides/BSMM_8740_lec_01.html#tidy-data-examples",
    "title": "Tidyverse, EDA & git",
    "section": "Tidy Data Examples",
    "text": "Tidy Data Examples\n\ntable 1table 2table 3\n\n\n\ntidyr::table3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\n\ntidyr::table2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\ntidyr::table1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidyverse-principles",
    "href": "slides/BSMM_8740_lec_01.html#tidyverse-principles",
    "title": "Tidyverse, EDA & git",
    "section": "Tidyverse principles",
    "text": "Tidyverse principles\n\nDesign for humans\nReuse existing data structures\nDesign for the pipe and functional programming"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidyverse-packages",
    "href": "slides/BSMM_8740_lec_01.html#tidyverse-packages",
    "title": "Tidyverse, EDA & git",
    "section": "Tidyverse packages",
    "text": "Tidyverse packages\nSome packages in the tidyverse."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#a-grammar-for-data-wrangling",
    "href": "slides/BSMM_8740_lec_01.html#a-grammar-for-data-wrangling",
    "title": "Tidyverse, EDA & git",
    "section": "A grammar for data wrangling",
    "text": "A grammar for data wrangling\nThe dplyr package gives a grammar for data wrangling, including these 5 verbs for working with data frames.\n\n\nselect(): take a subset of the columns (i.e., features, variables)\nfilter(): take a subset of the rows (i.e., observations)\nmutate(): add or modify existing columns\narrange(): sort the rows\nsummarize(): aggregate the data across rows (e.g., group it according to some criteria)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#a-grammar-for-data-wrangling-1",
    "href": "slides/BSMM_8740_lec_01.html#a-grammar-for-data-wrangling-1",
    "title": "Tidyverse, EDA & git",
    "section": "A grammar for data wrangling",
    "text": "A grammar for data wrangling\nEach of these functions takes a data frame as its first argument, and returns a data frame.\nBeing able to combine these verbs with nouns (i.e., data frames) and adverbs (i.e., arguments) creates a flexible and powerful way to wrangle data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#filter",
    "href": "slides/BSMM_8740_lec_01.html#filter",
    "title": "Tidyverse, EDA & git",
    "section": "filter()",
    "text": "filter()\nThe two simplest of the five verbs are filter() and select(), which return a subset of the rows or columns of a data frame, respectively.\n\nThe filter() function. At left, a data frame that contains matching entries in a certain column for only a subset of the rows. At right, the resulting data frame after filtering."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#select",
    "href": "slides/BSMM_8740_lec_01.html#select",
    "title": "Tidyverse, EDA & git",
    "section": "select()",
    "text": "select()\n\nThe select() function. At left, a data frame, from which we retrieve only a few of the columns. At right, the resulting data frame after selecting those columns."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example",
    "href": "slides/BSMM_8740_lec_01.html#example",
    "title": "Tidyverse, EDA & git",
    "section": "Example",
    "text": "Example\n\nggplot2::presidential\n\n# A tibble: 12 × 4\n   name       start      end        party     \n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;     \n 1 Eisenhower 1953-01-20 1961-01-20 Republican\n 2 Kennedy    1961-01-20 1963-11-22 Democratic\n 3 Johnson    1963-11-22 1969-01-20 Democratic\n 4 Nixon      1969-01-20 1974-08-09 Republican\n 5 Ford       1974-08-09 1977-01-20 Republican\n 6 Carter     1977-01-20 1981-01-20 Democratic\n 7 Reagan     1981-01-20 1989-01-20 Republican\n 8 Bush       1989-01-20 1993-01-20 Republican\n 9 Clinton    1993-01-20 2001-01-20 Democratic\n10 Bush       2001-01-20 2009-01-20 Republican\n11 Obama      2009-01-20 2017-01-20 Democratic\n12 Trump      2017-01-20 2021-01-20 Republican"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-select",
    "href": "slides/BSMM_8740_lec_01.html#example-select",
    "title": "Tidyverse, EDA & git",
    "section": "Example: select",
    "text": "Example: select\nTo get just the names and parties of these presidents, use select(). The first argument is the data frame, followed by the column names.\n\ndplyr::select(presidential, name, party)\n\n# A tibble: 12 × 2\n   name       party     \n   &lt;chr&gt;      &lt;chr&gt;     \n 1 Eisenhower Republican\n 2 Kennedy    Democratic\n 3 Johnson    Democratic\n 4 Nixon      Republican\n 5 Ford       Republican\n 6 Carter     Democratic\n 7 Reagan     Republican\n 8 Bush       Republican\n 9 Clinton    Democratic\n10 Bush       Republican\n11 Obama      Democratic\n12 Trump      Republican"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-filter",
    "href": "slides/BSMM_8740_lec_01.html#example-filter",
    "title": "Tidyverse, EDA & git",
    "section": "Example: filter",
    "text": "Example: filter\nSimilarly, the first argument to filter() is a data frame, and subsequent arguments are logical conditions that are evaluated on any involved columns. \n\ndplyr::filter(presidential, party == \"Republican\")\n\n# A tibble: 7 × 4\n  name       start      end        party     \n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;     \n1 Eisenhower 1953-01-20 1961-01-20 Republican\n2 Nixon      1969-01-20 1974-08-09 Republican\n3 Ford       1974-08-09 1977-01-20 Republican\n4 Reagan     1981-01-20 1989-01-20 Republican\n5 Bush       1989-01-20 1993-01-20 Republican\n6 Bush       2001-01-20 2009-01-20 Republican\n7 Trump      2017-01-20 2021-01-20 Republican"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-combined-operations",
    "href": "slides/BSMM_8740_lec_01.html#example-combined-operations",
    "title": "Tidyverse, EDA & git",
    "section": "Example: combined operations",
    "text": "Example: combined operations\nCombining the filter() and select() commands enables one to drill down to very specific pieces of information.\n\ndplyr::select(\n  dplyr::filter(\n    presidential\n    , lubridate::year(start) &gt; 1973 & party == \"Democratic\"\n  )\n  , name\n)\n\n# A tibble: 3 × 1\n  name   \n  &lt;chr&gt;  \n1 Carter \n2 Clinton\n3 Obama"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-pipe",
    "href": "slides/BSMM_8740_lec_01.html#example-pipe",
    "title": "Tidyverse, EDA & git",
    "section": "Example: pipe",
    "text": "Example: pipe\nAs written the filter() operation is nested inside the select() operation.\nWith the pipe (%&gt;%), we can write the same expression as above in a more readable syntax.\n\npresidential %&gt;% \n  dplyr::filter(lubridate::year(start) &gt; 1973 & party == \"Democratic\") %&gt;%\n  dplyr::select(name)\n\n# A tibble: 3 × 1\n  name   \n  &lt;chr&gt;  \n1 Carter \n2 Clinton\n3 Obama"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#mutate",
    "href": "slides/BSMM_8740_lec_01.html#mutate",
    "title": "Tidyverse, EDA & git",
    "section": "mutate()",
    "text": "mutate()\nWe might want to create, re-define, or rename some of our variables. A graphical illustration of the mutate() operation is shown below\n\nThe mutate() function creating a column. At right, the resulting data frame after adding a new column."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mutate-new-column",
    "href": "slides/BSMM_8740_lec_01.html#example-mutate-new-column",
    "title": "Tidyverse, EDA & git",
    "section": "Example: mutate, new column",
    "text": "Example: mutate, new column\n\nmy_presidents &lt;- presidential %&gt;% \n  dplyr::mutate( \n    term.length = lubridate::interval(start, end) / lubridate::dyears(1) \n  )\nmy_presidents\n\n# A tibble: 12 × 5\n   name       start      end        party      term.length\n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;\n 1 Eisenhower 1953-01-20 1961-01-20 Republican        8   \n 2 Kennedy    1961-01-20 1963-11-22 Democratic        2.84\n 3 Johnson    1963-11-22 1969-01-20 Democratic        5.16\n 4 Nixon      1969-01-20 1974-08-09 Republican        5.55\n 5 Ford       1974-08-09 1977-01-20 Republican        2.45\n 6 Carter     1977-01-20 1981-01-20 Democratic        4   \n 7 Reagan     1981-01-20 1989-01-20 Republican        8   \n 8 Bush       1989-01-20 1993-01-20 Republican        4   \n 9 Clinton    1993-01-20 2001-01-20 Democratic        8   \n10 Bush       2001-01-20 2009-01-20 Republican        8   \n11 Obama      2009-01-20 2017-01-20 Democratic        8   \n12 Trump      2017-01-20 2021-01-20 Republican        4"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mutate-existing-column",
    "href": "slides/BSMM_8740_lec_01.html#example-mutate-existing-column",
    "title": "Tidyverse, EDA & git",
    "section": "Example: mutate, existing column",
    "text": "Example: mutate, existing column\nThe mutate() function can be used to modify existing columns. Below we add a variable containing the year in which each president was elected assuming that every president was elected in the year before he took office.\n\n\nCode\nmy_presidents %&lt;&gt;% \n  dplyr::mutate(elected = year(start) - 1)\n\n\n\n\n# A tibble: 4 × 6\n  name       start      end        party      term.length elected\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n2 Kennedy    1961-01-20 1963-11-22 Democratic        2.84    1960\n3 Johnson    1963-11-22 1969-01-20 Democratic        5.16    1962\n4 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-new-column",
    "href": "slides/BSMM_8740_lec_01.html#example-new-column",
    "title": "Tidyverse, EDA & git",
    "section": "Example: new column",
    "text": "Example: new column\nSome entries in this data set are wrong, because presidential elections are only held every four years, and some presidents are not elected (e.g. Johnson and Ford).\n\n\nCode\nmy_presidents  %&lt;&gt;% \n  dplyr::mutate(elected = ifelse(elected %in% c(1962, 1973), NA, elected))\n\n\n\n\n# A tibble: 6 × 6\n  name       start      end        party      term.length elected\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n2 Kennedy    1961-01-20 1963-11-22 Democratic        2.84    1960\n3 Johnson    1963-11-22 1969-01-20 Democratic        5.16      NA\n4 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968\n5 Ford       1974-08-09 1977-01-20 Republican        2.45      NA\n6 Carter     1977-01-20 1981-01-20 Democratic        4       1976"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#rename",
    "href": "slides/BSMM_8740_lec_01.html#rename",
    "title": "Tidyverse, EDA & git",
    "section": "rename()",
    "text": "rename()\nIt is considered bad practice to use a period in names (functions, variables, columns) - we should change the name of the term.length column that we created earlier. \n\nmy_presidents %&lt;&gt;% \n  dplyr::rename(term_length = term.length)\n\n\n\n# A tibble: 9 × 6\n  name       start      end        party      term_length elected\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n2 Kennedy    1961-01-20 1963-11-22 Democratic        2.84    1960\n3 Johnson    1963-11-22 1969-01-20 Democratic        5.16      NA\n4 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968\n5 Ford       1974-08-09 1977-01-20 Republican        2.45      NA\n6 Carter     1977-01-20 1981-01-20 Democratic        4       1976\n7 Reagan     1981-01-20 1989-01-20 Republican        8       1980\n8 Bush       1989-01-20 1993-01-20 Republican        4       1988\n9 Clinton    1993-01-20 2001-01-20 Democratic        8       1992"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#arrange",
    "href": "slides/BSMM_8740_lec_01.html#arrange",
    "title": "Tidyverse, EDA & git",
    "section": "arrange()",
    "text": "arrange()\nThe function sort() will sort a vector but not a data frame. The arrange()function sorts a data frame: \n\nThe arrange() function. At left, a data frame with an ordinal variable. At right, the resulting data frame after sorting the rows in descending order of that variable."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-arrange---sort-column",
    "href": "slides/BSMM_8740_lec_01.html#example-arrange---sort-column",
    "title": "Tidyverse, EDA & git",
    "section": "Example: arrange - sort column",
    "text": "Example: arrange - sort column\nTo use arrange you have to specify the data frame, and the column by which you want it to be sorted. You also have to specify the direction in which you want it to be sorted.\n\nmy_presidents %&gt;% \n  dplyr::arrange(desc(term_length))\n\n\nmy_presidents %&gt;% \n  dplyr::arrange(desc(term_length)) %&gt;% \n  dplyr::slice_head(n=9)\n\n# A tibble: 9 × 6\n  name       start      end        party      term_length elected\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n2 Reagan     1981-01-20 1989-01-20 Republican        8       1980\n3 Clinton    1993-01-20 2001-01-20 Democratic        8       1992\n4 Bush       2001-01-20 2009-01-20 Republican        8       2000\n5 Obama      2009-01-20 2017-01-20 Democratic        8       2008\n6 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968\n7 Johnson    1963-11-22 1969-01-20 Democratic        5.16      NA\n8 Carter     1977-01-20 1981-01-20 Democratic        4       1976\n9 Bush       1989-01-20 1993-01-20 Republican        4       1988"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-arrange---multiple-columns",
    "href": "slides/BSMM_8740_lec_01.html#example-arrange---multiple-columns",
    "title": "Tidyverse, EDA & git",
    "section": "Example, arrange - multiple columns",
    "text": "Example, arrange - multiple columns\nTo break ties, we can further sort by other variables\n\nmy_presidents %&gt;% \n  dplyr::arrange(desc(term_length), party, elected)\n\n# A tibble: 12 × 6\n   name       start      end        party      term_length elected\n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1 Clinton    1993-01-20 2001-01-20 Democratic        8       1992\n 2 Obama      2009-01-20 2017-01-20 Democratic        8       2008\n 3 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n 4 Reagan     1981-01-20 1989-01-20 Republican        8       1980\n 5 Bush       2001-01-20 2009-01-20 Republican        8       2000\n 6 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968\n 7 Johnson    1963-11-22 1969-01-20 Democratic        5.16      NA\n 8 Carter     1977-01-20 1981-01-20 Democratic        4       1976\n 9 Bush       1989-01-20 1993-01-20 Republican        4       1988\n10 Trump      2017-01-20 2021-01-20 Republican        4       2016\n11 Kennedy    1961-01-20 1963-11-22 Democratic        2.84    1960\n12 Ford       1974-08-09 1977-01-20 Republican        2.45      NA"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#summarize-with-group_by",
    "href": "slides/BSMM_8740_lec_01.html#summarize-with-group_by",
    "title": "Tidyverse, EDA & git",
    "section": "summarize() with group_by()",
    "text": "summarize() with group_by()\nThe summarize verb is often used with group_by\n\nThe summarize() function. At left, a data frame. At right, the resulting data frame after aggregating four of the columns."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-summarize---no-groups",
    "href": "slides/BSMM_8740_lec_01.html#example-summarize---no-groups",
    "title": "Tidyverse, EDA & git",
    "section": "Example: summarize - no groups",
    "text": "Example: summarize - no groups\nWhen used without grouping, summarize() collapses a data frame into a single row.\n\nmy_presidents %&gt;% \n  dplyr::summarize(\n    N = n(), \n    first_year = min(year(start)), \n    last_year = max(year(end)), \n    num_dems = sum(party == \"Democratic\"), \n    years = sum(term_length), \n    avg_term_length = mean(term_length)\n  )\n\n# A tibble: 1 × 6\n      N first_year last_year num_dems years avg_term_length\n  &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1    12       1953      2021        5    68            5.67"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-pipe---groups",
    "href": "slides/BSMM_8740_lec_01.html#example-pipe---groups",
    "title": "Tidyverse, EDA & git",
    "section": "Example: pipe - groups",
    "text": "Example: pipe - groups\nTo make comparisons, we can first group then summarize, giving us one summary row for each group.\n\nmy_presidents %&gt;% \n  dplyr::group_by(party) %&gt;% \n  dplyr::summarize(\n    N = n(), \n    first_year = min(year(start)), \n    last_year = max(year(end)), \n    num_dems = sum(party == \"Democratic\"), \n    years = sum(term_length), \n    avg_term_length = mean(term_length)\n  )\n\n# A tibble: 2 × 7\n  party          N first_year last_year num_dems years avg_term_length\n  &lt;chr&gt;      &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1 Democratic     5       1961      2017        5    28            5.6 \n2 Republican     7       1953      2021        0    40            5.71"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-1",
    "href": "slides/BSMM_8740_lec_01.html#example-1",
    "title": "Tidyverse, EDA & git",
    "section": "Example",
    "text": "Example\n\n# attach package magrittr\nrequire(magrittr)\n\nurl &lt;- \n  \"https://data.cityofchicago.org/api/views/5neh-572f/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\n\nall_stations &lt;- \n  # Step 1: Read in the data.\n  readr::read_csv(url) %&gt;% \n  # Step 2: select columns and rename stationname\n  dplyr::select(station = stationname, date, rides) %&gt;% \n  # Step 3: Convert the character date field to a date encoding.\n  # Also, put the data in units of 1K rides\n  dplyr::mutate(date = lubridate::mdy(date), rides = rides / 1000) %&gt;% \n  # Step 4: Summarize the multiple records using the maximum.\n  dplyr::group_by(date, station) %&gt;% \n  dplyr::summarize(rides = max(rides), .groups = \"drop\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#magrittr-vs-native-pipe",
    "href": "slides/BSMM_8740_lec_01.html#magrittr-vs-native-pipe",
    "title": "Tidyverse, EDA & git",
    "section": "Magrittr vs native pipe",
    "text": "Magrittr vs native pipe\n\n\n\n\n\n\n\n\n\nTopic\nMagrittr 2.0.3\nBase 4.3.0\n\n\n\n\nOperator\n%&gt;% %&lt;&gt;% %T&gt;%\n|&gt; (since 4.1.0)\n\n\nFunction call\n1:3 %&gt;% sum()\n1:3 |&gt; sum()\n\n\n\n1:3 %&gt;% sum\nNeeds brackets / parentheses\n\n\n\n1:3 %&gt;% `+`(4)\nSome functions are not supported\n\n\nPlaceholder\n.\n_ (since 4.2.0)\n\n\n\n\n\nbased on a stackoverflow post comparing magrittr pipe to base R pipe."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#use-cases-for-the-magrittr-pipe",
    "href": "slides/BSMM_8740_lec_01.html#use-cases-for-the-magrittr-pipe",
    "title": "Tidyverse, EDA & git",
    "section": "Use cases for the Magrittr pipe",
    "text": "Use cases for the Magrittr pipe\n\n# functional programming\nairlines &lt;- fivethirtyeight::airline_safety %&gt;% \n  # filter rows\n  dplyr::filter( stringr::str_detect(airline, 'Air') )\n\n# assignment\nairlines %&lt;&gt;% \n  # filter columns and assign result to airlines\n  dplyr::select(avail_seat_km_per_week, incidents_85_99, fatalities_85_99)\n\n# side effects\nairlines %T&gt;% \n  # report the dimensions\n  ( \\(x) print(dim(x)) ) %&gt;% \n  # summarize\n  dplyr::summarize(avail_seat_km_per_week = sum(avail_seat_km_per_week))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#functions-in-r",
    "href": "slides/BSMM_8740_lec_01.html#functions-in-r",
    "title": "Tidyverse, EDA & git",
    "section": "Functions in R",
    "text": "Functions in R\n\n# named function\nis_awesome &lt;- function(x = 'Bob') {\n  paste(x, 'is awesome!')\n}\nis_awesome('Keith')\n\n# anonymous function\n(function (x) {paste(x, 'is awesome!')})('Keith')\n\n# also anonymous function\n(\\(x) paste(x, 'is awesome!'))('Keith')\n\n# a function from a formula in the tidyverse\nc('Bob','Ted') %&gt;% purrr::map_chr(~paste(.x, 'is awesome!'))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#data-wrangling",
    "href": "slides/BSMM_8740_lec_01.html#data-wrangling",
    "title": "Tidyverse, EDA & git",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nThe Tidyverse offers a consistent and efficient framework for manipulating, transforming, and cleaning datasets.\nFunctions like filter(), select(), mutate(), and group_by() allow users to easily subset, reorganize, add, and aggregate data, and the pipe (%&gt;% or |&gt;) enables a sequential and readable flow of operations.\nThe following examples show a few more of the many useful data wrangling functions in the tidyverse."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-1-mutate",
    "href": "slides/BSMM_8740_lec_01.html#example-1-mutate",
    "title": "Tidyverse, EDA & git",
    "section": "Example 1: mutate",
    "text": "Example 1: mutate\n\nmutatemutate across\n\n\n\nopenintro::email %&gt;%\n  dplyr::select(-from, -sent_email) %&gt;%\n  dplyr::mutate(\n    day_of_week = lubridate::wday(time)       # new variable: day of week\n    , month = lubridate::month(time)          # new variable: month\n  ) %&gt;%\n  dplyr::select(-time) %&gt;%\n  dplyr::mutate(\n    cc       = cut(cc, breaks = c(0, 1))      # discretize cc\n    , attach = cut(attach, breaks = c(0, 1))  # discretize attach\n    , dollar = cut(dollar, breaks = c(0, 1))  # discretize dollar\n  ) %&gt;%\n  dplyr::mutate(\n    inherit = \n      cut(inherit, breaks = c(0, 1, 5, 10, 20))  # discretize inherit, by intervals\n    , password = dplyr::ntile(password, 5)       # discretize password, by quintile\n  )\n\n\n\n\niris %&gt;%\n  dplyr::mutate(across(c(Sepal.Length, Sepal.Width), round))\n\niris %&gt;%\n  dplyr::mutate(across(c(1, 2), round))\n\niris %&gt;%\n  dplyr::group_by(Species) %&gt;%\n  dplyr::summarise(\n    across( starts_with(\"Sepal\"), list(mean = mean, sd = sd) )\n  )\n\niris %&gt;%\n  dplyr::group_by(Species) %&gt;%\n  dplyr::summarise(\n    across( starts_with(\"Sepal\"), ~ mean(.x, na.rm = TRUE) )\n  )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-2-rowwise-operations",
    "href": "slides/BSMM_8740_lec_01.html#example-2-rowwise-operations",
    "title": "Tidyverse, EDA & git",
    "section": "Example 2: rowwise operations",
    "text": "Example 2: rowwise operations\n\nrowwise operationsusing c_across\n\n\nThe verb rowwise creates a special type of grouping where each group consists of a single row.\n\niris %&gt;%\n  dplyr::rowwise() %&gt;%\n  dplyr::mutate( \n    mean_length = \n      mean( c(Sepal.Length, Petal.Length) )\n    , .before = 1\n  ) %&gt;% \n  dplyr::ungroup()\n\n\n\n\niris %&gt;%\n  dplyr::rowwise() %&gt;%\n  dplyr::mutate( \n    mean_length = \n      mean(\n        dplyr::c_across(c(Sepal.Length:Petal.Width))\n      )\n    , .before = 1 \n  ) %&gt;% \n  dplyr::ungroup()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-3-nesting-operations",
    "href": "slides/BSMM_8740_lec_01.html#example-3-nesting-operations",
    "title": "Tidyverse, EDA & git",
    "section": "Example 3: nesting operations",
    "text": "Example 3: nesting operations\nA nested data frame is a data frame where one (or more) columns is a list of data frames.\n\nlist columnsgroup-nestingmapping\n\n\n\n\ncreate a list-column of data frames\n(df1 &lt;- tibble::tibble(\n  g = c(1, 2, 3),\n  data = list(\n    tibble::tibble(x = 1, y = 2),\n    tibble::tibble(x = 4:5, y = 6:7),\n    tibble::tibble(x = 10)\n  )\n) )\n\n\n# A tibble: 3 × 2\n      g data            \n  &lt;dbl&gt; &lt;list&gt;          \n1     1 &lt;tibble [1 × 2]&gt;\n2     2 &lt;tibble [2 × 2]&gt;\n3     3 &lt;tibble [1 × 1]&gt;\n\n\n\n\n\n\nnest groups by continent, country\n(gapminder_nest &lt;- gapminder::gapminder %&gt;% \n  dplyr::mutate(year1950 = year - 1950) %&gt;% \n  dplyr::group_nest(continent, country)\n)\n\n\n# A tibble: 142 × 3\n   continent country                                data\n   &lt;fct&gt;     &lt;fct&gt;                    &lt;list&lt;tibble[,5]&gt;&gt;\n 1 Africa    Algeria                            [12 × 5]\n 2 Africa    Angola                             [12 × 5]\n 3 Africa    Benin                              [12 × 5]\n 4 Africa    Botswana                           [12 × 5]\n 5 Africa    Burkina Faso                       [12 × 5]\n 6 Africa    Burundi                            [12 × 5]\n 7 Africa    Cameroon                           [12 × 5]\n 8 Africa    Central African Republic           [12 × 5]\n 9 Africa    Chad                               [12 × 5]\n10 Africa    Comoros                            [12 × 5]\n# ℹ 132 more rows\n\n\n\n\n\n\nFit a linear model for each country:\n(gapminder_model &lt;- gapminder_nest %&gt;% \n  dplyr::mutate(\n    model = \n      purrr::map(\n        data\n        , ~lm(lifeExp ~ year1950, data = .))\n  ))\n\n\n# A tibble: 142 × 4\n   continent country                                data model \n   &lt;fct&gt;     &lt;fct&gt;                    &lt;list&lt;tibble[,5]&gt;&gt; &lt;list&gt;\n 1 Africa    Algeria                            [12 × 5] &lt;lm&gt;  \n 2 Africa    Angola                             [12 × 5] &lt;lm&gt;  \n 3 Africa    Benin                              [12 × 5] &lt;lm&gt;  \n 4 Africa    Botswana                           [12 × 5] &lt;lm&gt;  \n 5 Africa    Burkina Faso                       [12 × 5] &lt;lm&gt;  \n 6 Africa    Burundi                            [12 × 5] &lt;lm&gt;  \n 7 Africa    Cameroon                           [12 × 5] &lt;lm&gt;  \n 8 Africa    Central African Republic           [12 × 5] &lt;lm&gt;  \n 9 Africa    Chad                               [12 × 5] &lt;lm&gt;  \n10 Africa    Comoros                            [12 × 5] &lt;lm&gt;  \n# ℹ 132 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-4-stringr-string-functions",
    "href": "slides/BSMM_8740_lec_01.html#example-4-stringr-string-functions",
    "title": "Tidyverse, EDA & git",
    "section": "Example 4: stringr string functions",
    "text": "Example 4: stringr string functions\nMain verbs, each taking a pattern as input\n\nstringr::str_{X}stringr::str_glue\n\n\n\nx &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\n\nstringr::str_detect(x, \"[aeiou]\")       # identifies any matches\nstringr::str_count(x, \"[aeiou]\")        # counts number of patterns\nstringr::str_subset(x, \"[aeiou]\")       # extracts matching components\nstringr::str_extract(x, \"[aeiou]\")      # extracts text of the match\nstringr::str_replace(x, \"[aeiou]\", \"?\") # replaces matches with new text:\nstringr::str_split(x, \",\")              # splits up a string\n\n\n\n\nmtcars %&gt;% \n  tibble::rownames_to_column(var = \"car\") %&gt;% \n  tibble::as_tibble() %T&gt;% \n  (\\(x) print(names(x)) ) %&gt;% \n  dplyr::mutate(\n    note = stringr::str_glue(\"The {car} has {cyl} cylinders\")) %&gt;% \n  dplyr::slice_head(n=3)\n\n\n\n\ncheat sheet"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-5-database-functions",
    "href": "slides/BSMM_8740_lec_01.html#example-5-database-functions",
    "title": "Tidyverse, EDA & git",
    "section": "Example 5: Database functions",
    "text": "Example 5: Database functions\n\nCreate DBExtract SQLExecute Query\n\n\n\n# directly like a tibble\ndb &lt;- \n  dbplyr::memdb_frame(\n    x = runif(100)\n    , y = runif(100)\n    , .name = 'test_tbl'\n  )\n\n# using an existing table\nmtcars_db &lt;- dbplyr::tbl_memdb(mtcars)\n\n\n\n\n\nGenerate SQL without executing\nmtcars_db %&gt;% \n  dplyr::group_by(cyl) %&gt;% \n  dplyr::summarise(n = n()) %&gt;% \n  dplyr::show_query()\n\n\n&lt;SQL&gt;\nSELECT `cyl`, COUNT(*) AS `n`\nFROM `mtcars`\nGROUP BY `cyl`\n\n\n\n\n\n\nExecute Query on DB\nmtcars_db %&gt;% \n  dplyr::group_by(cyl) %&gt;% \n  dplyr::summarise(n = n()) %&gt;% \n  dplyr::collapse()\n\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [:memory:]\n    cyl     n\n  &lt;dbl&gt; &lt;int&gt;\n1     4    11\n2     6     7\n3     8    14"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#extract-sql-example",
    "href": "slides/BSMM_8740_lec_01.html#extract-sql-example",
    "title": "Tidyverse, EDA & git",
    "section": "Extract SQL Example",
    "text": "Extract SQL Example\n\n\nExecute Query on DB\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = \":memory:\")\ndplyr::copy_to(con, tibble::tibble(x = 1:100), \"temp_table\")\n\ndplyr::tbl(con, \"temp_table\") %&gt;% \n  dplyr::count(\n    x_bin = cut(\n      x\n      , breaks = c(0, 33, 66, 100)\n      , labels = c(\"low\", \"mid\", \"high\")\n    )\n  ) %&gt;% \n  dplyr::show_query()\n\n\n&lt;SQL&gt;\nSELECT `x_bin`, COUNT(*) AS `n`\nFROM (\n  SELECT\n    `temp_table`.*,\n    CASE\nWHEN (`x` &lt;= 0.0) THEN NULL\nWHEN (`x` &lt;= 33.0) THEN 'low'\nWHEN (`x` &lt;= 66.0) THEN 'mid'\nWHEN (`x` &lt;= 100.0) THEN 'high'\nWHEN (`x` &gt; 100.0) THEN NULL\nEND AS `x_bin`\n  FROM `temp_table`\n) AS `q01`\nGROUP BY `x_bin`"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#pivoting",
    "href": "slides/BSMM_8740_lec_01.html#pivoting",
    "title": "Tidyverse, EDA & git",
    "section": "Pivoting",
    "text": "Pivoting\n\nlongerwider\n\n\nWhen some of the column names are not names of variables, but values of a variable.\n\n\n\ntidyr::table4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\n\n\nPivot longer\ntidyr::table4a %&gt;% \n  pivot_longer(\n    c(`1999`, `2000`)\n    , names_to = \"year\", values_to = \"cases\")\n\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\n\n\nWhen an observation is scattered across multiple rows.\n\n\n\ntidyr::table2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\nPivot wider\ntidyr::table2 %&gt;%\n    pivot_wider(\n      names_from = type\n      , values_from = count)\n\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#relational-data1",
    "href": "slides/BSMM_8740_lec_01.html#relational-data1",
    "title": "Tidyverse, EDA & git",
    "section": "Relational data1",
    "text": "Relational data1\nWe can join related tables in a variety of ways:\n\nbased on material here"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#relational-data",
    "href": "slides/BSMM_8740_lec_01.html#relational-data",
    "title": "Tidyverse, EDA & git",
    "section": "Relational data",
    "text": "Relational data\n\n\nexample tables\nx &lt;- tibble::tibble(key=1:3, val_x= paste0('x',1:3))\ny &lt;- tibble::tibble(key=c(1,2,4), val_y= paste0('y',1:3))\n\n\n\ninner joinfull joinleft joinright join\n\n\n\nx %&gt;% dplyr::inner_join(y, by = \"key\")\n\n# A tibble: 2 × 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n\n\n\n\n\nx %&gt;% dplyr::full_join(y, by = \"key\")\n\n# A tibble: 4 × 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n3     3 x3    &lt;NA&gt; \n4     4 &lt;NA&gt;  y3   \n\n\n\n\n\nx %&gt;% dplyr::left_join(y, by = \"key\")\n\n# A tibble: 3 × 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n3     3 x3    &lt;NA&gt; \n\n\n\n\n\nx %&gt;% dplyr::right_join(y, by = \"key\")\n\n# A tibble: 3 × 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n3     4 &lt;NA&gt;  y3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#relational-data-1",
    "href": "slides/BSMM_8740_lec_01.html#relational-data-1",
    "title": "Tidyverse, EDA & git",
    "section": "Relational data",
    "text": "Relational data\nKeys used in the join:\n\ndefault (e.g. by=NULL): all variables that appear in both tables\na character vector (e.g. by = “x”) uses only the common variables named\na named character vector (e.g. by = c(“a” = “b”)) matches variable ‘a’ in x with variable ‘b’ in y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#relational-data-2",
    "href": "slides/BSMM_8740_lec_01.html#relational-data-2",
    "title": "Tidyverse, EDA & git",
    "section": "Relational data",
    "text": "Relational data\nFiltering Joins:\n\nsemi_join(x, y) keeps all observations in x that have a match in y ( i.e. no NAs).\nanti_join(x, y) drops all observations in x that have a match in y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#set-operations",
    "href": "slides/BSMM_8740_lec_01.html#set-operations",
    "title": "Tidyverse, EDA & git",
    "section": "Set operations",
    "text": "Set operations\nWhen tidy dataset x and y have the same variables, set operations work as expected:\n\nintersect(x, y): return only observations in both x and y.\nunion(x, y): return unique observations in x and y.\nsetdiff(x, y): return observations in x, but not in y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidying",
    "href": "slides/BSMM_8740_lec_01.html#tidying",
    "title": "Tidyverse, EDA & git",
    "section": "Tidying",
    "text": "Tidying\n\ntidyr::table3 %&gt;% \n  tidyr::separate_wider_delim(\n    cols = rate\n    , delim = \"/\"\n    , names = c(\"cases\", \"population\") \n)\n\n# A tibble: 6 × 4\n  country      year cases  population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#exploratory-data-analysis-eda-1",
    "href": "slides/BSMM_8740_lec_01.html#exploratory-data-analysis-eda-1",
    "title": "Tidyverse, EDA & git",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nExploratory data analysis is the process of understanding a new dataset by looking at the data, constructing graphs, tables, and models. We want to understand three aspects:\n\neach individual variable by itself;\neach individual variable in the context of other, relevant, variables; and\nthe data that are not there."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#exploratory-data-analysis-eda-2",
    "href": "slides/BSMM_8740_lec_01.html#exploratory-data-analysis-eda-2",
    "title": "Tidyverse, EDA & git",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nWe will perform two broad categories of EDA:\n\nDescriptive Statistics, which includes mean, median, mode, inter-quartile range, and so on.\nGraphical Methods, which includes histogram, density estimation, box plots, and so on."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-view-all-data",
    "href": "slides/BSMM_8740_lec_01.html#eda-view-all-data",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: view all data",
    "text": "EDA: view all data\nOur first dataset is a sample of categorical variables from the General Social Survey, a long-running US survey conducted by the independent research organization NORC at the University of Chicago.\n\n\n\n\nCode\ndat &lt;- forcats::gss_cat\ndat %&gt;% utils::head()\n\n\n# A tibble: 6 × 9\n   year marital         age race  rincome        partyid     relig denom tvhours\n  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;       &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1  2000 Never married    26 White $8000 to 9999  Ind,near r… Prot… Sout…      12\n2  2000 Divorced         48 White $8000 to 9999  Not str re… Prot… Bapt…      NA\n3  2000 Widowed          67 White Not applicable Independent Prot… No d…       2\n4  2000 Never married    39 White Not applicable Ind,near r… Orth… Not …       4\n5  2000 Divorced         25 White Not applicable Not str de… None  Not …       1\n6  2000 Married          25 White $20000 - 24999 Strong dem… Prot… Sout…      NA\n\n\n\n\nexecute ?forcats::gss_cat to see the data dictionary"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-view-all-columns",
    "href": "slides/BSMM_8740_lec_01.html#eda-view-all-columns",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: view all columns",
    "text": "EDA: view all columns\nUse dplyr::glimpse() to see every column in a data.frame\n\n\ndat %&gt;% dplyr::slice_head(n=10) %&gt;% dplyr::glimpse()\n\nRows: 10\nColumns: 9\n$ year    &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000\n$ marital &lt;fct&gt; Never married, Divorced, Widowed, Never married, Divorced, Mar…\n$ age     &lt;int&gt; 26, 48, 67, 39, 25, 25, 36, 44, 44, 47\n$ race    &lt;fct&gt; White, White, White, White, White, White, White, White, White,…\n$ rincome &lt;fct&gt; $8000 to 9999, $8000 to 9999, Not applicable, Not applicable, …\n$ partyid &lt;fct&gt; \"Ind,near rep\", \"Not str republican\", \"Independent\", \"Ind,near…\n$ relig   &lt;fct&gt; Protestant, Protestant, Protestant, Orthodox-christian, None, …\n$ denom   &lt;fct&gt; \"Southern baptist\", \"Baptist-dk which\", \"No denomination\", \"No…\n$ tvhours &lt;int&gt; 12, NA, 2, 4, 1, NA, 3, NA, 0, 3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-view-some-rows",
    "href": "slides/BSMM_8740_lec_01.html#eda-view-some-rows",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: view some rows",
    "text": "EDA: view some rows\nUse dplyr::slice_sample() to see a random selection of rows in a data.frame\n\n\ndat %&gt;% dplyr::slice_sample(n=10)\n\n# A tibble: 10 × 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Married          61 White Not applicable Not str d… Prot… Sout…       4\n 2  2000 Married          32 Other $25000 or more Not str r… Cath… Not …       2\n 3  2014 Married          34 Black $3000 to 3999  No answer  Prot… Bapt…      NA\n 4  2012 Divorced         48 White $25000 or more Independe… None  Not …      NA\n 5  2006 Married          75 White Not applicable Ind,near … Prot… Other      NA\n 6  2000 Married          42 White Refused        Not str d… Cath… Not …      NA\n 7  2012 Divorced         51 Black Not applicable Strong de… Prot… Bapt…       8\n 8  2004 Never married    58 White $20000 - 24999 Strong re… Cath… Not …       4\n 9  2002 Divorced         52 White $25000 or more Strong re… Cath… Not …      NA\n10  2010 Never married    25 White $15000 - 19999 Ind,near … None  Not …       1\n\n\nThere are many dplyr::slice_{X} variants, along with dplyr::filter"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#bad-data-rightarrow-bad-results",
    "href": "slides/BSMM_8740_lec_01.html#bad-data-rightarrow-bad-results",
    "title": "Tidyverse, EDA & git",
    "section": "bad data \\(\\rightarrow\\) bad results",
    "text": "bad data \\(\\rightarrow\\) bad results"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-descriptive-statistics",
    "href": "slides/BSMM_8740_lec_01.html#eda-descriptive-statistics",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: descriptive statistics",
    "text": "EDA: descriptive statistics\nThe base R function summary() can be used for key summary statistics of the data.\n\n\ndat %&gt;% summary()\n\n      year               marital           age                    race      \n Min.   :2000   No answer    :   17   Min.   :18.00   Other         : 1959  \n 1st Qu.:2002   Never married: 5416   1st Qu.:33.00   Black         : 3129  \n Median :2006   Separated    :  743   Median :46.00   White         :16395  \n Mean   :2007   Divorced     : 3383   Mean   :47.18   Not applicable:    0  \n 3rd Qu.:2010   Widowed      : 1807   3rd Qu.:59.00                         \n Max.   :2014   Married      :10117   Max.   :89.00                         \n                                      NA's   :76                            \n           rincome                   partyid            relig      \n $25000 or more:7363   Independent       :4119   Protestant:10846  \n Not applicable:7043   Not str democrat  :3690   Catholic  : 5124  \n $20000 - 24999:1283   Strong democrat   :3490   None      : 3523  \n $10000 - 14999:1168   Not str republican:3032   Christian :  689  \n $15000 - 19999:1048   Ind,near dem      :2499   Jewish    :  388  \n Refused       : 975   Strong republican :2314   Other     :  224  \n (Other)       :2603   (Other)           :2339   (Other)   :  689  \n              denom          tvhours      \n Not applicable  :10072   Min.   : 0.000  \n Other           : 2534   1st Qu.: 1.000  \n No denomination : 1683   Median : 2.000  \n Southern baptist: 1536   Mean   : 2.981  \n Baptist-dk which: 1457   3rd Qu.: 4.000  \n United methodist: 1067   Max.   :24.000  \n (Other)         : 3134   NA's   :10146"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-packages-for-eda",
    "href": "slides/BSMM_8740_lec_01.html#eda-packages-for-eda",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: packages for EDA",
    "text": "EDA: packages for EDA\n\nThe function skimr::skim() gives an enhanced version of base R’s summary() .\nOther packages, such as DataExplorer:: rely more on graphing."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#skimrskim",
    "href": "slides/BSMM_8740_lec_01.html#skimrskim",
    "title": "Tidyverse, EDA & git",
    "section": "skimr::skim()",
    "text": "skimr::skim()\n\nnumeric variablesfactor variables\n\n\n\n\nskim_dat[[\"numeric\"]]\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n2006.50\n4.45\n2000\n2002\n2006\n2010\n2014\n▇▃▇▂▆\n\n\nage\n76\n1.00\n47.18\n17.29\n18\n33\n46\n59\n89\n▇▇▇▅▂\n\n\ntvhours\n10146\n0.53\n2.98\n2.59\n0\n1\n2\n4\n24\n▇▂▁▁▁\n\n\n\n\n\n\n\n\n\n\nskim_dat[[\"factor\"]]\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nmarital\n0\n1\nFALSE\n6\nMar: 10117, Nev: 5416, Div: 3383, Wid: 1807\n\n\nrace\n0\n1\nFALSE\n3\nWhi: 16395, Bla: 3129, Oth: 1959, Not: 0\n\n\nrincome\n0\n1\nFALSE\n16\n$25: 7363, Not: 7043, $20: 1283, $10: 1168\n\n\npartyid\n0\n1\nFALSE\n10\nInd: 4119, Not: 3690, Str: 3490, Not: 3032\n\n\nrelig\n0\n1\nFALSE\n15\nPro: 10846, Cat: 5124, Non: 3523, Chr: 689\n\n\ndenom\n0\n1\nFALSE\n30\nNot: 10072, Oth: 2534, No : 1683, Sou: 1536"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-factor-variable-counts",
    "href": "slides/BSMM_8740_lec_01.html#eda-factor-variable-counts",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: factor variable counts",
    "text": "EDA: factor variable counts\nMost of the columns here are factors (categories). Use these to count the number of observations per category.\n\nforecats::fct_countdplyr::counttable()\n\n\n\nforcats::fct_count(dat$relig) %&gt;% dplyr::arrange(desc(n))\n\n# A tibble: 16 × 2\n   f                           n\n   &lt;fct&gt;                   &lt;int&gt;\n 1 Protestant              10846\n 2 Catholic                 5124\n 3 None                     3523\n 4 Christian                 689\n 5 Jewish                    388\n 6 Other                     224\n 7 Buddhism                  147\n 8 Inter-nondenominational   109\n 9 Moslem/islam              104\n10 Orthodox-christian         95\n11 No answer                  93\n12 Hinduism                   71\n13 Other eastern              32\n14 Native american            23\n15 Don't know                 15\n16 Not applicable              0\n\n\n\n\n\ndat |&gt; dplyr::count(relig) |&gt; dplyr::arrange(desc(n))\n\n# A tibble: 15 × 2\n   relig                       n\n   &lt;fct&gt;                   &lt;int&gt;\n 1 Protestant              10846\n 2 Catholic                 5124\n 3 None                     3523\n 4 Christian                 689\n 5 Jewish                    388\n 6 Other                     224\n 7 Buddhism                  147\n 8 Inter-nondenominational   109\n 9 Moslem/islam              104\n10 Orthodox-christian         95\n11 No answer                  93\n12 Hinduism                   71\n13 Other eastern              32\n14 Native american            23\n15 Don't know                 15\n\n\n\n\n\ndat$relig |&gt; table() |&gt; as.data.frame() %&gt;% dplyr::arrange(desc(Freq))\n\n                      Var1  Freq\n1               Protestant 10846\n2                 Catholic  5124\n3                     None  3523\n4                Christian   689\n5                   Jewish   388\n6                    Other   224\n7                 Buddhism   147\n8  Inter-nondenominational   109\n9             Moslem/islam   104\n10      Orthodox-christian    95\n11               No answer    93\n12                Hinduism    71\n13           Other eastern    32\n14         Native american    23\n15              Don't know    15\n16          Not applicable     0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-binary-factors",
    "href": "slides/BSMM_8740_lec_01.html#eda-binary-factors",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: binary factors",
    "text": "EDA: binary factors\n\ntable()xtabs()\n\n\n\ndat_bin &lt;- dat |&gt; \n  dplyr::mutate(\n    is_protestant = dplyr::case_when(relig == 'Protestant' ~ 1, TRUE ~ 0)\n  )\n\ndat_bin$is_protestant |&gt; table() / length(dat_bin$is_protestant)\n\n\n        0         1 \n0.4951357 0.5048643 \n\n\n\n\n\ndat_bin |&gt; xtabs(~ partyid + is_protestant, data = _)\n\n                    is_protestant\npartyid                 0    1\n  No answer           102   52\n  Don't know            1    0\n  Other party         233  160\n  Strong republican   720 1594\n  Not str republican 1198 1834\n  Ind,near rep        878  913\n  Independent        2436 1683\n  Ind,near dem       1473 1026\n  Not str democrat   1972 1718\n  Strong democrat    1624 1866"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-classifying-missing-data",
    "href": "slides/BSMM_8740_lec_01.html#eda-classifying-missing-data",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: classifying missing data",
    "text": "EDA: classifying missing data\nThere are three main categories of missing data\n\nMissing Completely At Random (MCAR);\n\nmissing and independent of other measurements\n\nMissing at Random (MAR);\n\nmissing in a way related to other measurements\n\nMissing Not At Random (MNAR).\n\nmissing as a property of the variable or some other unmeasured variable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-handling-missing-data",
    "href": "slides/BSMM_8740_lec_01.html#eda-handling-missing-data",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: handling missing data",
    "text": "EDA: handling missing data\nWe can think of a few options for dealing with missing data\n\nDrop observations with missing data.\nImpute the mean of observations without missing data.\nUse multiple imputation.\n\n\n\n\n\n\n\n\nNote\n\n\nMultiple imputation involves generating several estimates for the missing values and then averaging the outcomes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mcar-or-mar",
    "href": "slides/BSMM_8740_lec_01.html#example-mcar-or-mar",
    "title": "Tidyverse, EDA & git",
    "section": "Example: MCAR or MAR?",
    "text": "Example: MCAR or MAR?\n\n\nCode\ndat %&gt;% dplyr::select(partyid) %&gt;% table() %&gt;% tibble::as_tibble() %&gt;% \n  dplyr::left_join(\n    dat %&gt;% dplyr::filter(is.na(age)) %&gt;% \n      dplyr::select(na_partyid = partyid) %&gt;% table() %&gt;% tibble::as_tibble()\n    , by = c(\"partyid\" = \"na_partyid\")\n    , suffix = c(\"_partyid\", \"_na_partyid\")\n  ) %&gt;% \n  dplyr::mutate(pct_na = n_na_partyid / n_partyid)\n\n\n# A tibble: 10 × 4\n   partyid            n_partyid n_na_partyid   pct_na\n   &lt;chr&gt;                  &lt;int&gt;        &lt;int&gt;    &lt;dbl&gt;\n 1 No answer                154            9 0.0584  \n 2 Don't know                 1            0 0       \n 3 Other party              393            3 0.00763 \n 4 Strong republican       2314            8 0.00346 \n 5 Not str republican      3032            8 0.00264 \n 6 Ind,near rep            1791            2 0.00112 \n 7 Independent             4119           18 0.00437 \n 8 Ind,near dem            2499            2 0.000800\n 9 Not str democrat        3690           11 0.00298 \n10 Strong democrat         3490           15 0.00430"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mcar-mar-or-mnar",
    "href": "slides/BSMM_8740_lec_01.html#example-mcar-mar-or-mnar",
    "title": "Tidyverse, EDA & git",
    "section": "Example: MCAR, MAR or MNAR?",
    "text": "Example: MCAR, MAR or MNAR?\n\n\n\n\n\n\n\n\n\n\n\nCustomer ID\nAge\nIncome\nPurchase Frequency\nSatisfaction Rating\n\n\n\n\n1\n35\n$60,000\nHigh\n8\n\n\n2\n28\n$45,000\nLow\n-\n\n\n3\n42\n$70,000\nMedium\n7\n\n\n4\n30\n$50,000\nLow\n-\n\n\n5\n55\n$80,000\nHigh\n9\n\n\n6\n26\n$40,000\nLow\n-\n\n\n7\n50\n$75,000\nMedium\n8\n\n\n8\n29\n$48,000\nLow\n-"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mcar-mar-or-mnar-1",
    "href": "slides/BSMM_8740_lec_01.html#example-mcar-mar-or-mnar-1",
    "title": "Tidyverse, EDA & git",
    "section": "Example: MCAR, MAR or MNAR?",
    "text": "Example: MCAR, MAR or MNAR?\n\n\n\n\n\n\n\n\n\n\n\nEmployee ID\nAge\nJob Tenure\nPerformance Score\nPromotion Status\n\n\n\n\n1\n30\n5 years\n85\nYes\n\n\n2\n45\n10 years\n-\nNo\n\n\n3\n28\n3 years\n90\nYes\n\n\n4\n50\n12 years\n70\nNo\n\n\n5\n35\n6 years\n-\nNo\n\n\n6\n32\n4 years\n95\nYes\n\n\n7\n40\n8 years\n-\nNo\n\n\n8\n29\n2 years\n88\nYes"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-missing-data",
    "href": "slides/BSMM_8740_lec_01.html#eda-missing-data",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: missing data",
    "text": "EDA: missing data\nFinally, be aware that how missing data is encoded depends on the dataset\n\nR defaults to NA when reading data, in joins, etc.\nThe creator(s) of the dataset may use a different encoding.\nMissing data can have multiple representations according to semantics of the measurement.\nRemember that entire measurements can be missing (i.e. from all observations, not just some)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-summary",
    "href": "slides/BSMM_8740_lec_01.html#eda-summary",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: summary",
    "text": "EDA: summary\n\nUnderstand what the measurements represent and confirm constraints (if any) and suitability of encoding.\nMake a decision on how to deal with missing data.\nUnderstand shape of measurements (may identify an issue or suggest a data transformation)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-1",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-1",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nfor continuous variables (usually the independent variables or covariates):\n\nnormalization (scale values to \\([0,1]\\))\n\n\\(X_\\text{norm} = \\frac{X-X_\\text{min}}{X_\\text{max}-X_\\text{min}}\\)\n\nstandardization (subtract mean and scale by stdev)\n\n\\(X_\\text{std} = \\frac{X-\\mu_X}{\\sigma_X}\\)\n\nscaling (multiply / divide by a constant)\n\n\\(X_\\text{scaled} = K\\times X\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-2",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-2",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nOther common transformations:\n\nBox-cox: with \\(\\tilde{x}\\) the geometric mean of the (positive) predictor data (\\(\\tilde{x}=\\left(\\prod_{i=1}^{n}x_{i}\\right)^{1/n}\\))\n\n\\[\nx_i(\\lambda) = \\left\\{\n\\begin{array}{cc}\n\\frac{x_i^{\\lambda}-1}{\\lambda\\tilde{x}^{\\lambda-1}} & \\lambda\\ne 0\\\\\n\\tilde{x}\\log x_i & \\lambda=0\n\\end{array}\n\\right .  \n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nBox-cox is an example of a power transform; it is a technique used to stabilize variance, make the data more normal distribution-like."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-3",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-3",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nOne last common transformation:\n\nlogit transformation for bounded target variables (scaled to lie in \\([0,1]\\))\n\n\\[\n\\text{logit}\\left(p\\right)=\\log\\frac{p}{1-p},\\;p\\in [0,1]\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nThe Logit transform is primarily used to transform binary response data, such as survival/non-survival or present/absent, to provide a continuous value in the range \\(\\left(-\\infty,\\infty\\right)\\), where p is the proportion of each sample that is 1 (or 0)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-4",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-4",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nWhy normalize or standardize?\n\nvariation in the range of feature values can lead to biased model performance or difficulties during the learning process, particularly in distance-based algorithms.\n\ne.g. income and age\n\nreduce the impact of outliers\nmake results more explainable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-5",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-5",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nfor continuous variables (usually the target variables):\n\ntransformation (arithmetic, basis functions, polynomials, splines, differencing)\n\n\\(y = \\log(y),\\sqrt{y},\\frac{1}{y}\\), etc.\n\\(y = \\sum_i \\beta_i\\text{f}_i(x)\\;\\text{s.t.}\\;0=\\int\\text{f}_i(x)\\text{f}_j(x)\\; \\forall i\\ne j\\)\n\\(y = \\beta_0+\\beta_1 x+\\beta_2 x^2+\\beta_3 x^3+\\ldots\\)\n\\(y = \\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_3 x_1 x_2+\\ldots\\)\n\\(y'_i = y_i-y_{i-1}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-6",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-6",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nfor categorical variables (either target or explanatory variables):\n\nbinning / bucketing\n\nrepresent a numerical value as a categorical value\n\ncategorical\\(\\rightarrow\\)ordinal and ordinal\\(\\rightarrow\\)categorical\n\nfor date variables:\n\ntimestamp\\(\\rightarrow\\)date or date part"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-7",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-7",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nWhy transform?\n\nit can make your model perform better\n\ne.g. \\(\\log\\) transform makes exponential data linear, and log-Normal data Gaussian\n\\(\\log\\) transforms also make multiplicative models additive\ne.g. polynomials, basis functions and splines help model non-linearities in data"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-8",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-8",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ncreation\n\n\noutliers (due to data entry, measurement/experiment, intentional errors)\n\noutliers can be identified by quantile methods (Gaussian data)\noutliers can be removed, treated as missing, or capped\n\nlag variables (either target or explanatory variables)\n\nuseful in time series models, e.g. \\(y_t,y_{t-1},\\ldots y_{t-n}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-9",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-9",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ncreation\n\n\nbinning / bucketing\n\nrepresent numerical as categorical and vice versa\n\ninterval and ratio levels"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-summary",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-summary",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering: summary",
    "text": "Feature engineering: summary\n\nrequires an advanced technical skill set\nrequires domain expertise\nis time-consuming and resource intensive\ndifferent analytics algorithms require different feature engineering"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#recap",
    "href": "slides/BSMM_8740_lec_01.html#recap",
    "title": "Tidyverse, EDA & git",
    "section": "Recap",
    "text": "Recap\n\nToday we have introduced tidy data, tidyverse verbs and the pipe operator.\nWe briefly discussed EDA\nIn the lab we will introduce Git and the data backup workflows."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#announcements",
    "href": "slides/BSMM_8740_lec_02_alt.html#announcements",
    "title": "EDA and feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\nFor this week (September 25 - 29), office hours will be on Friday, from 2:00pm - 4:00pm.\nRegular Thursday office hours resume the week of October 02."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#todays-outline",
    "href": "slides/BSMM_8740_lec_02_alt.html#todays-outline",
    "title": "EDA and feature engineering",
    "section": "Today’s Outline",
    "text": "Today’s Outline\n\nComplete last week’s lab\nReview this week’s material\n\nIntroduction to exploratory data analysis (EDA), and\nFeature engineering in the tidyverse\n\nStart (and finish?) this week’s lab\n\n\n\n\n\n\n\nNote\n\n\nThe first two labs are designed to give you practice with the Tidyverse tools for manipulating data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-1",
    "title": "EDA and feature engineering",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nExploratory data analysis is the process of understanding a new dataset by looking at the data, constructing graphs, tables, and models. We want to understand three aspects:\n\neach individual variable by itself;\neach individual variable in the context of other, relevant, variables; and\nthe data that are not there."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-2",
    "href": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-2",
    "title": "EDA and feature engineering",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nDuring EDA we want to come to understand the issues and features of the dataset and how this may affect analysis decisions.\nWe are especially concerned about missing values and outliers."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-3",
    "href": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-3",
    "title": "EDA and feature engineering",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nWe are going to perform two broad categories of EDA:\n\nDescriptive Statistics, which includes mean, median, mode, inter-quartile range, and so on.\nGraphical Methods, which includes histogram, density estimation, box plots, and so on."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-4",
    "href": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-4",
    "title": "EDA and feature engineering",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nDescriptive statistics and graphical methods support the following process:\n\nUnderstand the distribution and properties of individual variables.\nUnderstand relationships between variables.\nUnderstand what is not there."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nOur first dataset is a sample of categorical variables from the General Social Survey, a long-running US survey conducted by the independent research organization NORC at the University of Chicago.\n\n\n\n\nCode\n&gt; dat &lt;- forcats::gss_cat\n&gt; dat %&gt;% utils::head()\n\n\n# A tibble: 6 × 9\n   year marital         age race  rincome        partyid            relig    denom tvhours\n  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;              &lt;fct&gt;    &lt;fct&gt;   &lt;int&gt;\n1  2000 Never married    26 White $8000 to 9999  Ind,near rep       Protest… Sout…      12\n2  2000 Divorced         48 White $8000 to 9999  Not str republican Protest… Bapt…      NA\n3  2000 Widowed          67 White Not applicable Independent        Protest… No d…       2\n4  2000 Never married    39 White Not applicable Ind,near rep       Orthodo… Not …       4\n5  2000 Divorced         25 White Not applicable Not str democrat   None     Not …       1\n6  2000 Married          25 White $20000 - 24999 Strong democrat    Protest… Sout…      NA\n\n\n\n\nexecute ?forcats::gss_cat to see the data dictionary"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example-1",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nUse dplyr::glimpse() to see every column in a data.frame\n\n\n&gt; dat %&gt;% dplyr::slice_head(n=10) %&gt;% dplyr::glimpse()\n\nRows: 10\nColumns: 9\n$ year    &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000\n$ marital &lt;fct&gt; Never married, Divorced, Widowed, Never married, Divorced, Married, Neve…\n$ age     &lt;int&gt; 26, 48, 67, 39, 25, 25, 36, 44, 44, 47\n$ race    &lt;fct&gt; White, White, White, White, White, White, White, White, White, White\n$ rincome &lt;fct&gt; $8000 to 9999, $8000 to 9999, Not applicable, Not applicable, Not applic…\n$ partyid &lt;fct&gt; \"Ind,near rep\", \"Not str republican\", \"Independent\", \"Ind,near rep\", \"No…\n$ relig   &lt;fct&gt; Protestant, Protestant, Protestant, Orthodox-christian, None, Protestant…\n$ denom   &lt;fct&gt; \"Southern baptist\", \"Baptist-dk which\", \"No denomination\", \"Not applicab…\n$ tvhours &lt;int&gt; 12, NA, 2, 4, 1, NA, 3, NA, 0, 3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example-2",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example-2",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nUse dplyr::slice_sample() to see a random selection of rows in a data.frame\n\n\n&gt; dat %&gt;% dplyr::slice_sample(n=10)\n\n# A tibble: 10 × 9\n    year marital         age race  rincome        partyid           relig    denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;             &lt;fct&gt;    &lt;fct&gt;   &lt;int&gt;\n 1  2004 Divorced         57 White $25000 or more Not str democrat  Catholic Not …      NA\n 2  2004 Never married    33 White $25000 or more Strong democrat   Catholic Not …       2\n 3  2012 Married          64 Black $25000 or more Strong democrat   None     Not …       3\n 4  2004 Married          43 White Not applicable Independent       Catholic Not …      NA\n 5  2000 Never married    24 Other $20000 - 24999 Ind,near dem      Catholic Not …      NA\n 6  2012 Married          65 White $25000 or more Not str democrat  Protest… Pres…       2\n 7  2004 Divorced         56 White Not applicable Independent       Protest… Epis…      NA\n 8  2002 Married          30 White $8000 to 9999  Ind,near dem      Protest… Pres…      NA\n 9  2008 Married          54 Other Refused        Not str democrat  Catholic Not …       2\n10  2006 Married          45 White $25000 or more Strong republican None     Not …      NA"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example-3",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example-3",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nMost of the columns here are factors (categories). Use forcats::fct_count() to count the factor entries.\n\n\n&gt; forcats::fct_count(dat$relig) %&gt;% dplyr::arrange(desc(n))\n\n# A tibble: 16 × 2\n   f                           n\n   &lt;fct&gt;                   &lt;int&gt;\n 1 Protestant              10846\n 2 Catholic                 5124\n 3 None                     3523\n 4 Christian                 689\n 5 Jewish                    388\n 6 Other                     224\n 7 Buddhism                  147\n 8 Inter-nondenominational   109\n 9 Moslem/islam              104\n10 Orthodox-christian         95\n11 No answer                  93\n12 Hinduism                   71\n13 Other eastern              32\n14 Native american            23\n15 Don't know                 15\n16 Not applicable              0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example-4",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example-4",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nThe base R function summary() can be used for key summary statistics of the data.\n\n\n&gt; dat %&gt;% summary()\n\n      year               marital           age                    race      \n Min.   :2000   No answer    :   17   Min.   :18.00   Other         : 1959  \n 1st Qu.:2002   Never married: 5416   1st Qu.:33.00   Black         : 3129  \n Median :2006   Separated    :  743   Median :46.00   White         :16395  \n Mean   :2007   Divorced     : 3383   Mean   :47.18   Not applicable:    0  \n 3rd Qu.:2010   Widowed      : 1807   3rd Qu.:59.00                         \n Max.   :2014   Married      :10117   Max.   :89.00                         \n                                      NA's   :76                            \n           rincome                   partyid            relig      \n $25000 or more:7363   Independent       :4119   Protestant:10846  \n Not applicable:7043   Not str democrat  :3690   Catholic  : 5124  \n $20000 - 24999:1283   Strong democrat   :3490   None      : 3523  \n $10000 - 14999:1168   Not str republican:3032   Christian :  689  \n $15000 - 19999:1048   Ind,near dem      :2499   Jewish    :  388  \n Refused       : 975   Strong republican :2314   Other     :  224  \n (Other)       :2603   (Other)           :2339   (Other)   :  689  \n              denom          tvhours      \n Not applicable  :10072   Min.   : 0.000  \n Other           : 2534   1st Qu.: 1.000  \n No denomination : 1683   Median : 2.000  \n Southern baptist: 1536   Mean   : 2.981  \n Baptist-dk which: 1457   3rd Qu.: 4.000  \n United methodist: 1067   Max.   :24.000  \n (Other)         : 3134   NA's   :10146"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-packages-for-eda",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-packages-for-eda",
    "title": "EDA and feature engineering",
    "section": "EDA: packages for EDA",
    "text": "EDA: packages for EDA\n\nThe function skimr::skim() gives an enhanced version of base R’s summary() .\nOther packages, such as DataExplorer:: rely more on graphing.\n\nWe’ll look at a few of the DataExplorer:: functions next."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerintroduce",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerintroduce",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::introduce",
    "text": "DataExplorer::introduce\nThe function DataExplorer::introduce produces a basic description of the data in a data.frame.\n\n\nCode\n&gt; dat %&gt;% DataExplorer::introduce() %&gt;% dplyr::glimpse()\n\n\nRows: 1\nColumns: 9\n$ rows                 &lt;int&gt; 21483\n$ columns              &lt;int&gt; 9\n$ discrete_columns     &lt;int&gt; 6\n$ continuous_columns   &lt;int&gt; 3\n$ all_missing_columns  &lt;int&gt; 0\n$ total_missing_values &lt;int&gt; 10222\n$ complete_rows        &lt;int&gt; 11299\n$ total_observations   &lt;int&gt; 193347\n$ memory_usage         &lt;dbl&gt; 784776"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_intro",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_intro",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::plot_intro",
    "text": "DataExplorer::plot_intro\nThe function DataExplorer::plot_intro is a visual version of DataExplorer::introduce.\n\n\nCode\n&gt; dat %&gt;% DataExplorer::plot_intro()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_missing",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_missing",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::plot_missing",
    "text": "DataExplorer::plot_missing\nThe function DataExplorer::plot_missing shows information on missing data visually.\n\n\nCode\n&gt; dat %&gt;% DataExplorer::plot_missing()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#d_explorerprofile_missing",
    "href": "slides/BSMM_8740_lec_02_alt.html#d_explorerprofile_missing",
    "title": "EDA and feature engineering",
    "section": "D_Explorer::profile_missing",
    "text": "D_Explorer::profile_missing\n\n\nCode\n&gt; dat %&gt;% DataExplorer::profile_missing() %&gt;% \n+   gt::gt('feature') %&gt;% \n+   gtExtras::gt_theme_espn() %&gt;% \n+   gt::tab_options( table.font.size = gt::px(28) ) %&gt;% \n+   gt::as_raw_html()\n\n\n\n  \n  \n\n\n\n\nnum_missing\npct_missing\n\n\n\n\nyear\n0\n0.000000000\n\n\nmarital\n0\n0.000000000\n\n\nage\n76\n0.003537681\n\n\nrace\n0\n0.000000000\n\n\nrincome\n0\n0.000000000\n\n\npartyid\n0\n0.000000000\n\n\nrelig\n0\n0.000000000\n\n\ndenom\n0\n0.000000000\n\n\ntvhours\n10146\n0.472280408"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_density",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_density",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::plot_density",
    "text": "DataExplorer::plot_density\n\n\nCode\n&gt; dat %&gt;% \n+   DataExplorer::plot_density(\n+     ggtheme = theme_bw(base_size = 18) + theme(legend.position = \"top\")\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#d_explorerplot_histogram",
    "href": "slides/BSMM_8740_lec_02_alt.html#d_explorerplot_histogram",
    "title": "EDA and feature engineering",
    "section": "D_Explorer::plot_histogram",
    "text": "D_Explorer::plot_histogram\n\n\nCode\n&gt; dat %&gt;% \n+   DataExplorer::plot_histogram(\n+     ggtheme = theme_bw(base_size = 18) + theme(legend.position = \"top\")\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_bar",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_bar",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::plot_bar",
    "text": "DataExplorer::plot_bar\n\n\nCode\n&gt; dat %&gt;% DataExplorer::plot_bar()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions",
    "title": "EDA and feature engineering",
    "section": "EDA: additional DataExplorer functions",
    "text": "EDA: additional DataExplorer functions\nWhen the data has more numerical values consider looking at the relationships with the following functions:\n\nDataExplorer::plot_correlation() creates a correlation heatmap.\n\n\n&gt; iris %&gt;% DataExplorer::plot_correlation(type = \"c\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions-1",
    "title": "EDA and feature engineering",
    "section": "EDA: additional DataExplorer functions",
    "text": "EDA: additional DataExplorer functions\nWhen the data has more numerical values consider looking at the relationships with the following functions:\n\nDataExplorer::plot_scatterplot() creates a scatterplot for all measurements.\n\n\n&gt; iris %&gt;% DataExplorer::plot_scatterplot(by = \"Species\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions-2",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions-2",
    "title": "EDA and feature engineering",
    "section": "EDA: additional DataExplorer functions",
    "text": "EDA: additional DataExplorer functions\nWhen the data has more numerical values consider looking at the relationships with the following functions:\n\nDataExplorer::plot_qq() creates a quantile-quantile plot for each continuous feature.\n\n\n&gt; iris %&gt;% DataExplorer::plot_qq(by = \"Species\", ncol = 2L)\n\n\n\n\n\n\n\n\nNote\n\n\nA Q–Q plot (quantile-quantile plot) is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-classifying-missing-data",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-classifying-missing-data",
    "title": "EDA and feature engineering",
    "section": "EDA: classifying missing data",
    "text": "EDA: classifying missing data\nThere are three main categories of missing data\n\nMissing Completely At Random (MCAR);\n\nmissing but independent of other measurements\n\nMissing at Random (MAR);\n\nmissing in a way related to other measurements\n\nMissing Not At Random (MNAR).\n\nmissing as a property of the variable or some other unmeasured variable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-handling-missing-data",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-handling-missing-data",
    "title": "EDA and feature engineering",
    "section": "EDA: handling missing data",
    "text": "EDA: handling missing data\nWe can think of a few options for dealing with missing data\n\nDrop observations with missing data.\nImpute the mean of observations without missing data.\nUse multiple imputation.\n\n\n\n\n\n\n\n\nNote\n\n\nMultiple imputation involves generating several estimates for the missing values and then averaging the outcomes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#example-mcar-or-mar",
    "href": "slides/BSMM_8740_lec_02_alt.html#example-mcar-or-mar",
    "title": "EDA and feature engineering",
    "section": "Example: MCAR or MAR?",
    "text": "Example: MCAR or MAR?\n\n\nCode\n&gt; dat %&gt;% dplyr::select(partyid) %&gt;% table() %&gt;% tibble::as_tibble() %&gt;% \n+   dplyr::left_join(\n+     dat %&gt;% dplyr::filter(is.na(age)) %&gt;% \n+       dplyr::select(na_partyid = partyid) %&gt;% table() %&gt;% tibble::as_tibble()\n+     , by = c(\"partyid\" = \"na_partyid\")\n+   ) %&gt;% \n+   dplyr::mutate(pct_na = n.y / n.x)\n\n\n# A tibble: 10 × 4\n   partyid              n.x   n.y   pct_na\n   &lt;chr&gt;              &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1 No answer            154     9 0.0584  \n 2 Don't know             1     0 0       \n 3 Other party          393     3 0.00763 \n 4 Strong republican   2314     8 0.00346 \n 5 Not str republican  3032     8 0.00264 \n 6 Ind,near rep        1791     2 0.00112 \n 7 Independent         4119    18 0.00437 \n 8 Ind,near dem        2499     2 0.000800\n 9 Not str democrat    3690    11 0.00298 \n10 Strong democrat     3490    15 0.00430"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-missing-data",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-missing-data",
    "title": "EDA and feature engineering",
    "section": "EDA: missing data",
    "text": "EDA: missing data\nFinally, be aware that how missing data is encoded depends on the dataset\n\nR defaults to NA when reading data, in joins, etc.\nThe creator(s) of the dataset may use a different encoding.\nMissing data can have multiple representations according to semantics of the measurement.\nRemember that entire measurements can be missing (i.e. from all observations, not just some)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-summary",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-summary",
    "title": "EDA and feature engineering",
    "section": "EDA: summary",
    "text": "EDA: summary\n\nUnderstand what the measurements represent and confirm constraints (if any) and suitability of encoding.\nMake a decision on how to deal with missing data.\nUnderstand shape of measurements (may identify an issue or suggest a data transformation)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-bad-data-rightarrow-bad-results",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-bad-data-rightarrow-bad-results",
    "title": "EDA and feature engineering",
    "section": "EDA: bad data \\(\\rightarrow\\) bad results",
    "text": "EDA: bad data \\(\\rightarrow\\) bad results"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nfor continuous variables (usually the independent variables or covariates):\n\nnormalization (scale values to \\([0,1]\\))\n\n\\(X_\\text{norm} = \\frac{X-X_\\text{min}}{X_\\text{max}-X_\\text{min}}\\)\n\nstandardization (subtract mean and scale by stdev)\n\n\\(X_\\text{std} = \\frac{X-\\mu_X}{\\sigma_X}\\)\n\nscaling (multiply / divide by a constant)\n\n\\(X_\\text{scaled} = K\\times X\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-1",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nOther common transformations:\n\nBox-cox: with \\(\\tilde{x}\\) the geometric mean of the (positive) predictor data (\\(\\tilde{x}=\\left(\\prod_{i=1}^{n}x_{i}\\right)^{1/n}\\))\n\n\\[\nx_i(\\lambda) = \\left\\{\n\\begin{array}{cc}\n\\frac{x_i^{\\lambda}-1}{\\lambda\\tilde{x}^{\\lambda-1}} & \\lambda\\ne 0\\\\\n\\tilde{x}\\log x_i & \\lambda=0\n\\end{array}\n\\right .  \n\\]\n\n\n\n\n\n\n\nNote\n\n\nBox-cox is an example of a power transform; it is a technique used to stabilize variance, make the data more normal distribution-like."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-2",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-2",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nOne last common transformation:\n\nlogit transformation for bounded target variables (scaled to lie in \\([0,1]\\))\n\n\\[\n\\text{logit}\\left(p\\right)=\\log\\frac{p}{1-p},\\;p\\in [0,1]\n\\]\n\n\n\n\n\n\n\nNote\n\n\nThe Logit transform is primarily used to transform binary response data, such as survival/non-survival or present/absent, to provide a continuous value in the range \\(\\left(-\\infty,\\infty\\right)\\), where p is the proportion of each sample that is 1 (or 0)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-3",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-3",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nWhy normalize or standardize?\n\nvariation in the range of feature values can lead to biased model performance or difficulties during the learning process, particularly in distance-based algorithms.\n\ne.g. income and age\n\nreduce the impact of outliers\nmake results more explainable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-4",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-4",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nfor continuous variables (usually the target variables):\n\ntransformation (arithmetic, basis functions, polynomials, splines, differencing)\n\n\\(y = \\log(y),\\sqrt{y},\\frac{1}{y}\\), etc.\n\\(y = \\sum_i \\beta_i\\text{f}_i(x)\\;\\text{s.t.}\\;0=\\int\\text{f}_i(x)\\text{f}_j(x)\\; \\forall i\\ne j\\)\n\\(y = \\beta_0+\\beta_1 x+\\beta_2 x^2+\\beta_3 x^3+\\ldots\\)\n\\(y = \\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_3 x_1 x_2+\\ldots\\)\n\\(y'_i = y_i-y_{i-1}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-5",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-5",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nfor categorical variables (either target or explanatory variables):\n\nbinning / bucketing\n\nrepresent a numerical value as a categorical value\n\ncategorical\\(\\rightarrow\\)ordinal and ordinal\\(\\rightarrow\\)categorical\n\nfor date variables:\n\ntimestamp\\(\\rightarrow\\)date or date part"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-6",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-6",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nWhy transform?\n\nit can make your model perform better\n\ne.g. \\(\\log\\) transform makes exponential data linear, and log-Normal data Gaussian\n\\(\\log\\) transforms also make multiplicative models additive\ne.g. polynomials, basis functions and splines help model non-linearities in data"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-creation",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-creation",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: creation",
    "text": "Feature engineering: creation\n\noutliers (due to data entry, measurement/experiment, intentional errors)\n\noutliers can be identified by quantile methods (Gaussian data)\noutliers can be removed, treated as missing, or capped\n\nlag variables (either target or explanatory variables)\n\nuseful in time series models, e.g. \\(y_t,y_{t-1},\\ldots y_{t-n}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-creation-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-creation-1",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: creation",
    "text": "Feature engineering: creation\n\nbinning / bucketing\n\nrepresent numerical as categorical and vice versa\n\ninterval and ratio levels"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-fails",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-fails",
    "title": "EDA and feature engineering",
    "section": "Feature engineering fails",
    "text": "Feature engineering fails"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-summary",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-summary",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: summary",
    "text": "Feature engineering: summary\n\nrequires an advanced technical skill set\nrequires domain expertise\nis time-consuming and resource intensive\ndifferent analytics algorithms require different feature engineering"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#recap",
    "href": "slides/BSMM_8740_lec_02_alt.html#recap",
    "title": "EDA and feature engineering",
    "section": "Recap",
    "text": "Recap\n\nToday we reviewed key elements of exploratory data analysis, the process that helps us understand the data we have and evaluate how it can help us solve the business problems we are interested in.\nWe also reviewed feature engineering - methods to we can use to facilitate our analysis and make our results more interpretable."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_09b.html#recap-of-last-week",
    "title": "Monte Carlo Methods",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced the fundamental problems of inference and the biases of some intuitive estimators.\nWe also built a basic understanding of the tools used to state and then satisfy causality assumptions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#this-week",
    "href": "slides/BSMM_8740_lec_09b.html#this-week",
    "title": "Monte Carlo Methods",
    "section": "This week",
    "text": "This week\n\nWe will get explore Monte Carlo methods as a way to integrate difficult functions, and sample from difficult probability distributions.\nAlong the way we will look at Markov Chains which both underlie sampling methods and provide a way to model the generation of data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#monte-carlo-mc-methods",
    "href": "slides/BSMM_8740_lec_09b.html#monte-carlo-mc-methods",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo (MC) Methods",
    "text": "Monte Carlo (MC) Methods\nMonte Carlo methods are a class of simulation-based methods that seek to avoid complicated and/or intractable mathematical computations.\nEspecially those that arise from probability distributions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nWe can use MC methods to estimate probabilities: for a random variable \\(Z\\) with outcomes in a set \\(\\Omega\\), with some subset \\(S\\subset\\Omega\\) and event \\(E\\equiv Z\\in S\\), we can compute the probability of \\(E\\) (i.e. \\(\\mathbb{P}(E)\\)) with of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) as\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-1",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-1",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample\nIf \\(Z\\sim\\mathscr{N}(1,3)\\) and \\(S=Z:0\\le Z\\le 3\\) then\n\\[\n\\mathbb{P}(E) = \\mathbb{P}(0\\le Z\\le 3) = \\int_0^3\\frac{1}{\\sqrt{2\\pi3}}e^{-\\frac{(t-1)^2}{2*3}}dt\n\\] In which case it is easier to just use R and calculate:\n\n\nCode\npnorm(3, mean=1, sd=sqrt(3)) - pnorm(0, mean=1, sd=sqrt(3))\n\n\n[1] 0.594\n\n\nthan it is to compute the integral."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-2",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-2",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample continued\nSurprisingly, in case we don’t know that pnorm exists, we could do this:\n\n\nMC computation\n# define the event\nevent_E_happened &lt;- function( x ) {\n  if( 0 &lt;= x & x &lt;= 3 ) {\n    return( TRUE ) # The event happened\n  } else {\n    return( FALSE ) # The event DIDN'T happen\n  }\n}\n\n# Now MC says that we should generate lots of copies of Z...\nNMC &lt;- 10000; # 10000 seems like \"a lot\".\nrnorm( NMC, mean=1, sd=sqrt(3) ) |&gt; \n  purrr::map_lgl(event_E_happened) |&gt; \n  sum()/NMC\n\n\n[1] 0.6099"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-3",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-3",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nNow\n\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]\n\nis the MC estimate of \\(\\mathbb{E}[E]\\), which is unbiased because for each \\(i\\), \\(\\mathbb{E}[1_{z_i\\in S}]=\\mathbb{E}[E]\\), and the variance is\n\n\\[\n\\mathrm{Var}\\left({\\mathbb{P}(E)}\\right)=\\frac{1}{M^2}\\mathrm{Var}\\left(\\sum_{i=1}^M 1_{z_i\\in S}\\right)=\\frac{1}{M^2}\\sum_{i=1}^M \\mathrm{Var}\\left(1_{z_i\\in S}\\right)=\\frac{1}{M} \\mathrm{Var}\\left(1_{z_i\\in S}\\right)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-4",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-4",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nThis is true for any function of the random variable \\(Z\\), and\n\\[\n\\mathbb{E}\\left[h(Z)\\right]\\approx \\hat{h}=\\frac{1}{M}\\sum_{i=1}^M h(z_i)\n\\]\nand the variance of \\(h(Z)\\) decreases as \\(1/M\\) by the same reasoning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nMonte Carlo simulation starts with random number generation, usually split into 2 stages:\n\ngeneration of independent uniform \\((0, 1)\\) random variables, and\nconversion into random variables with a particular distribution (e.g. Normal)\n\nVery important: never write your own generator, always use a well validated generator from a reputable source (e.g. R)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation-1",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation-1",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nPseudo-random generators found in computers use a deterministic (i.e. repeatable) algorithm to generate a sequence of (apparently) random numbers on the \\((0, 1)\\) interval.\nWhat defines a good random number generator (RNG) has a long period – how long it takes before the sequence repeats itself \\(2^{32}\\) is not enough (need at least \\(2^{40}\\)). various statistical tests to measure “randomness” – well validated software will have gone through these checks."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation-2",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation-2",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nRecall that \\(\\mathscr{N}(0, 1)\\) Normal random variables (mean 0, variance 1) have the probability density function:\n\\[\np(x)=\\frac{1}{2\\pi}e^{-\\frac{1}{2}x^2}\\equiv\\phi(x)\n\\] and if \\(X\\sim\\mathscr{N}(0, 1)\\) then its CDF is:\n\\[\n\\mathbb{P}[X\\le x] = \\int_{-\\infty}^x\\phi(x)dx\n\\] ## Random Number Generation\nThe Box-Muller transformation method takes two independent uniform \\((0, 1)\\) random numbers \\(y_1\\), \\(y_2\\), and defines:\n\\[\n\\begin{align*}\nx_{1} & =\\sqrt{-2\\log y_{1}}\\cos(2\\pi y_{2})\\\\\nx_2& =\\sqrt{-2\\log y_{1}}\\sin(2\\pi y_{2})\n\\end{align*}\n\\] It can be proved that \\(x_1\\) and \\(x_2\\) are independent \\(\\mathscr{N}(0, 1)\\) random variables"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation-3",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation-3",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nCode\nsamples &lt;- matrix(runif(10000), ncol=2) |&gt; data.frame() |&gt; \n  dplyr::mutate(\n    normals = \n      purrr::map2(\n        X1, X2\n        ,(\\(x1,x2){\n          data.frame(\n            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)\n            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) \n          )\n        })\n      )\n  ) |&gt; \n  tidyr::unnest(normals)  \n  \n\nsamples |&gt; \n  tidyr::pivot_longer(-c(X1,X2)) |&gt; \n  ggplot(aes(x=value, color=name, fill=name)) + \n  geom_histogram(aes(y=..density..), bins = 60, position=\"identity\", alpha=0.3) + \n  labs(x=\"Value\", y=\"Density\") + theme_minimal()\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in\nggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nCode\nsamples |&gt; \nggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation-4",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation-4",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nYour computer is only capable of producing pseudorandom numbers. These are made by running a pseudorandom number generator algorithm which is deterministic, e.g.\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\nOnce the RNG seed is set, the “random” numbers that R generates aren’t random at all. But someone looking at these random numbers would have a very hard time distinguishing these numbers from truly random numbers. That is what “statistical randomness” means!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#central-limit-theorem",
    "href": "slides/BSMM_8740_lec_09b.html#central-limit-theorem",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT says that for \\(f=\\mathbb{P}(E)\\) if \\(\\sigma^2\\equiv\\mathrm{Var}\\left(f\\right)\\) is finite then the error of the MC estimate\n\\[\ne_N(f)=\\bar{f}-\\mathbb{E}[f]\n\\]\nis approximately Normal in distribution for large \\(M\\), i.e.\n\\[\ne_N(f)\\sim\\sigma M^{1/2}Z\n\\] where \\(Z\\sim\\mathscr{N}(0,1)\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-5",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-5",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nSuppose we need to compute an expectation \\(\\mathbb{E}[g(Z)]\\) for some random variable \\(Z\\) and some function \\(g:\\mathbb{R}\\to\\mathbb{R}\\). Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) and estimate \\(\\mathbb{E}[g(Z)]\\) as \\(\\frac{1}{M}\\sum_{i=1}^Mg(z_i)\\). The law of large numbers states that this sample mean should be close to \\(\\mathbb{E}[g(Z)]\\).\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-6",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-6",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nWe can use Monte Carlo to estimate probabilities of the form \\(\\mathbb{P}\\left[E\\right]\\) by approximating expectations of the form \\(\\mathbb{E}[1_{X\\in E}]\\).\nIf \\(X\\sim\\mathscr{N}(\\mu,\\sigma)\\) and we want to compute \\(\\mathbb{E}[\\log|X|]\\), we could set up and solve the integral\n\\[\n\\mathbb{E} \\log |X|\n= \\int_{-\\infty}^\\infty \\left( \\log |t| \\right) f( t; \\mu, \\sigma) dt\n= \\int_{-\\infty}^\\infty \\frac{ \\log |t| }{ \\sqrt{2\\pi \\sigma^2} }\n                  \\exp\\left\\{ \\frac{ -(t-\\mu)^2 }{ 2\\sigma^2 } \\right\\}dt\n\\]\nAlternatively, we could just draw lots of Monte Carlo replicates \\(X_1,X_2,\\cdots,X_M\\) from a normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and look at the sample mean \\(M^{-1}\\sum_{i=1}^M\\log|x_i|\\), once again appealing to the law of large numbers to ensure that this sample mean is close to its expectation.\nMonte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-7",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-7",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis idea can be pushed still further. Suppose that we want to compute an integral\n\\[\n\\int_Dg(x)dx\n\\] where \\(D\\) is some domain of integration and \\(g(.)\\) is a function.\nLet \\(f(x)\\) be the density of some random variable with \\(f(x)&gt;0, \\forall x\\in D\\). In other words, \\(f\\) is the density of a random variable supported on \\(D\\). Then we can rewrite the integral as\n\\[\n\\int_Dg(x)dx = \\int_D\\frac{g(x)}{f(x)}f(x)dx = \\mathbb{E}[h(x)]\n\\] where \\(h(x)=g(x)/f(x)\\) and \\(X\\sim f\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-8",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-8",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nNow suppose we are given \\(h(x)\\) and we want to compute \\(\\mathbb{E}[h(X)]\\) where \\(X\\sim f,\\,x\\in D\\). So we need to sample from \\(f\\), but what if we could not do that directly?\nIf there were some other distribution \\(g(x)\\) we could sample from, such that \\(g(x)&gt;0,\\,x\\in D\\), then\n\\[\n\\begin{align*}\n\\mathbb{E}_{f}\\left[h(x)\\right] & =\\int_{D}h(x)f(x)dx\\\\\n& =\\int_{S}h(x)\\frac{f(x)}{g(x)}g(x)dx=\\mathbb{E}_{g}\\left[h(x)\\frac{f(x)}{g(x)}\\right]\\\\\n& =\\frac{1}{n}\\sum_{i=1}^{n}h(x_{i})\\frac{f(x_{i})}{g(x_{i})}\\quad x_{i}\\sim g\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-9",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-9",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis is called importance sampling (IS).\n\ndraw iid \\(x_1,x_2,\\ldots,x_n\\) from g and calculate the importance weight \\[\nw(x_i)=\\frac{f(x_{i})}{g(x_{i})}\n\\]\nestimate \\(\\mathbb{E}_f(h)\\) by \\[\n\\hat{\\mu}_h=\\frac{1}{n}\\sum_{i=1}^nw(x_i)h(x_i)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-10",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-10",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nexample\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = \\sqrt{2/\\pi}e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution)\n\n\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X) / sqrt(2 * pi)\n\nmu_h  &lt;- mean(W*X)\nvar_h &lt;- var(W*X)/n\nse_h  &lt;- sqrt(var_h)\n\ntibble::tibble(mean = mu_h,  variance = var_h, 'standard error' = se_h) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4)\n\n\n\n\n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.8030\n0.0003\n0.0178"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-11",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-11",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nunknown normalizing constant\n\nSuppose that \\(q(x)&gt;0;\\;x\\in D\\) and \\(\\int_Dq(x)dx=Z_q&lt;\\infty\\) The \\(q()\\) is an un-normalized density on \\(D\\) whereas the corresponding normalized density is \\(\\frac{1}{Z_q}q(x)\\).\nUsing IS, let \\(g(x) = \\frac{1}{Z_r}r(x);\\;Z_r=\\int r(x)dx\\), so \\(r\\) is an un-normalized density with \\(Z_r\\) possibly unknown.\n\nDraw \\(x_1,x_2,\\ldots,x_n\\) from \\(g(x)\\) and calculate importance weights \\(w(x_i)=g(x_i)/r(x_i)\\)\nEstimate \\(\\mathbb{E}_f\\left[h(X)\\right]\\) by \\[\n\\hat{\\mu_h}=\\frac{\\sum_{i=1}^nw(x_i)h(x_i)}{\\sum_{i=1}^nw(x_i)}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-12",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-12",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nunknown normalizing constant\n\nsince\n\\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}\\right]=\\int\\frac{q(X)}{r(X)}g(x)dx=\\frac{Z_{q}}{Z_{r}}\\\\\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}h(X)\\right]=\\int\\frac{q(X)}{r(X)}g(x)h(x)dx=\\frac{1}{Z_{r}}\\int g(x)h(x)dx\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-13",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-13",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrepeating the prior example, but un-normalized\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution, un-normalized)\n\n\n# un-normalized weights\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X)\n\nmu_h2  &lt;- sum(W*X)/sum(W)\nvar_h2 &lt;- var(W/mean(W))\nse_h2  &lt;- sqrt(var_h2)\n\ntibble::tibble(mean = mu_h2,  variance = var_h2, 'standard error' = se_h2) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4)\n\n\n\n\n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.7895\n0.4058\n0.6370"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-14",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-14",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling procedure\n\nAssume we have an un-normalized \\(g(x)\\), i.e. \\(\\pi(x)=cg(x)\\) but \\(c\\) is unknown.\nWe want to generate iid samples \\(x_1,x_2,\\ldots,x_M\\sim \\pi\\) to estimate \\(\\mathbb{E}_\\pi[h]\\)\nNow assume we have an easily sampled density \\(f\\), and known \\(K&gt;0\\), such that \\(Kf(x)\\ge g(x),\\;\\forall x\\), i.e. \\(Kf(x)\\ge \\pi(x)/c\\) ( or \\(cKf(x)\\ge \\pi(x)\\)).\nThen use the following procedure:\n\nsample \\(X\\sim f\\) and \\(U\\sim \\mathrm{uniform}[0,1]\\)\nif \\(U\\le\\frac{g(X)}{Kf(x)}\\), the accept X as a draw from \\(\\pi\\)\notherwise reject the sample and repeat"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-15",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-15",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nSince \\(0\\le\\frac{g(x)}{Kf(x)}\\le 1\\) we know that \\(\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\\)\nand so\n\n\\[\n\\mathbb{E}_f\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]=\\mathbb{E}_f\\left[\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^\\infty\\frac{g(X)}{Kf(X)}f(x)dx=\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-16",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-16",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nSince \\(0\\le\\frac{g(x)}{Kf(x)}\\le 1\\) we know that \\(\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\\)\nand so\n\n\\[\n\\mathbb{E}_f\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]=\\mathbb{E}_f\\left[\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^\\infty\\frac{g(X)}{Kf(X)}f(x)dx=\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-17",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-17",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nSimilarly, for any \\(y\\in\\mathbb{R}\\), we can calculate the joint probability\n\\[\n\\begin{align*}\n\\mathbb{P}\\left(X\\le y,U\\le\\frac{g(X)}{Kf(X)}\\right) & =\\mathbb{E}\\left[1_{X\\le y}1_{U\\le\\frac{g(X)}{Kf(X)}}\\right]\\\\\n& =\\mathbb{E}\\left[1_{X\\le y}\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X\\right)\\right]\\\\\n& =\\mathbb{E}\\left[1_{X\\le y}\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^{y}\\frac{g(x)}{Kf(x)}f(x)dx\\\\\n& =\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx\n\\end{align*}\n\\]\n\nand so we have the joint probability (above - \\(\\mathbb{P}(A,B)\\)), and the probability of acceptance (previous slide - \\(\\mathbb{P}(B)\\)), so the probability, conditional on acceptance (\\(\\mathbb{P}(A|B)\\)) is \\(\\frac{\\mathbb{P}(A,B)}{\\mathbb{P}(B)}\\) by Bayes rule."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-18",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-18",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\\[\n\\mathbb{P}\\left(X\\le y|U\\le\\frac{g(X)}{Kf(X)}\\right)=\\frac{\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx}{\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx}=\\int_{-\\infty}^{y}\\pi(x)dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-19",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-19",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nconsider random variable \\(X\\) with pdf/pmf \\(q(x)&gt;0;\\;x\\in D\\) which is difficult to sample from\nwe will sample from \\(q\\) using a proposal pdf/pmf \\(f\\) which we can sample from\nif we can find a constant \\(K\\) such that \\(q(x)\\le Kf(x); \\forall x\\in D\\). Alternatively \\(\\frac{q(x)}{f(x)}\\le K\\)\nthen there is a rejection method that returns \\(X\\sim q\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-20",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-20",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling method\n\ngiven a proposal pdf/pmf \\(f\\) we can sample from, and constant \\(K\\) such that \\(\\frac{q(x)}{f(x)}\\le K; \\forall x\\in D\\)\nsample \\(Y_i\\sim f\\) and \\(U_i\\sim\\mathrm{U}[0,1]\\)\nfor \\(U_i\\le\\frac{q(Y_i)}{Kf(Y_i)}\\) return \\(X_i=Y_i\\); otherwise return nothing and continue."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-21",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-21",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nWe have \\(\\mathbb{P}(X=x) = \\sum_{i=1}^n\\mathbb{P}(\\mathrm{reject }\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw }\\,Y=x\\,\\mathrm{and\\, accept})\\)\nWe also have\n\\[\n\\begin{align*}\n& \\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n= & \\mathbb{P}(\\mathrm{draw}\\,Y=x)\\mathbb{P}(\\left.\\mathrm{accept}\\,Y\\right|Y=x)\\\\\n= & f(x)\\mathbb{P}(\\left.U\\le\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n= & \\frac{q(x)}{K}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-22",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-22",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nThe probability of rejection of a draw is\n\\[\n\\begin{align*}\n\\mathbb{P}(\\mathrm{{reject}}\\,Y) & =\\sum_{x\\in D}\\mathbb{P}(\\mathrm{{draw}}\\,Y=x\\,\\mathrm{and\\,reject\\,it})\\\\\n& =\\sum_{x\\in D}f(x)\\mathbb{P}(\\left.U\\ge\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n& =\\sum_{x\\in D}f(x)(1-\\frac{q(x)}{Kf(x)})=1-\\frac{1}{K}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-23",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-23",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nand so1\n\\[\n\\begin{align*}\n\\mathbb{P}(X=x) & =\\sum_{n=1}^{\\infty}\\mathbb{P}(\\mathrm{reject}\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n& =\\sum_{n=1}^{\\infty}\\left(1-\\frac{1}{K}\\right)^{n-1}\\frac{q(x)}{K}=q(x)\n\\end{align*}\n\\]\n\nThe geometric distribution is a discrete distribution that can be interpreted as the number of failures before the first success (\\(\\mathbb{P}(X=k)=(1-p)^{k-1}p\\), with mean \\(p\\))."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-24",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-24",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for continuous scalar rv\n\nRecal that we accept the proposal \\(Y\\) whenever \\((U,Y)\\sim q_{U,Y}\\) where \\(q_{U,Y}\\left(u,y\\right)=q(y)U_{0,1}(u)\\) such that \\(U\\le q(Y)/(Kf(Y))\\)\nWe have\n\\[\n\\begin{align*}\n\\mathbb{P}\\left(X\\le x\\right) & =\\mathbb{P}\\left(\\left.Y\\le x\\right|U\\le q(Y)/Kf(Y))\\right)\\\\\n& =\\frac{\\mathbb{P}\\left(Y\\le x,U\\le q(Y)/Kf(Y))\\right)}{\\mathbb{P}\\left(U\\le q(Y)/Kf(Y))\\right)}\\\\\n& =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}\\\\\n& =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}=\\int_{-\\infty}^{x}q(y)dy\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-25",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-25",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: unknown normalizing constants\n\nIn most practical scenarios, we know \\(f(x)\\) and \\(q(x)\\) only up to some normalizing constants\n\\[\nf(x)=\\bar{f}(x)/K_f\\;\\mathrm{and}\\;q(x)=\\bar{q}(x)/K_q\n\\] We can still use rejection sampling since\n\\[\n\\frac{q(x)}{f(x)}\\le K\\;\\mathrm{iff}\\;\\frac{\\bar{q}(x)}{\\bar{f}(x)}\\le\\hat{K}\\equiv  K\\frac{K_q}{K_f}\n\\] In practice this means we can ignore the normalizing constants if we can find \\(\\hat{K}\\) to bound \\(\\frac{\\bar{q}(x)}{\\bar{f}(x)}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-26",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-26",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nSuppose we need to compute an expectation \\(\\mathbb{E}[g(Z)]\\) for some random variable \\(Z\\) and some function \\(g:\\mathbb{R}\\to\\mathbb{R}\\). Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) and estimate \\(\\mathbb{E}[g(Z)]\\) as \\(\\frac{1}{M}\\sum_{i=1}^Mg(z_i)\\). The law of large numbers states that this sample mean should be close to \\(\\mathbb{E}[g(Z)]\\).\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#stochastic-processes",
    "href": "slides/BSMM_8740_lec_09b.html#stochastic-processes",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nA stochastic process, which we will usually write as \\((X_n)\\), is an indexed sequence of random variables that are (usually) dependent on each other.\nEach random variable \\(X_n\\) takes a value in a state space \\(\\mathcal S\\) which is the set of possible values for the process. As with usual random variables, the state space \\(\\mathcal S\\) can be discrete or continuous. A discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, \\(\\mathcal S = \\{\\text{Heads},\\text{Tails}\\}\\) is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers \\(\\mathcal S = \\mathbb Z_+ = \\{0,1,2,\\dots\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#stochastic-processes-1",
    "href": "slides/BSMM_8740_lec_09b.html#stochastic-processes-1",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nFurther, the process has an index set that puts the random variables that make up the process in order. The index set is usually interpreted as a time variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at distinct points, often denoted by \\(n = 0,1,2,\\dots\\), while continuous time denotes a process monitored constantly over time, often denoted by \\(t \\in \\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-property",
    "href": "slides/BSMM_8740_lec_09b.html#markov-property",
    "title": "Monte Carlo Methods",
    "section": "Markov property",
    "text": "Markov property\nThink of a simple board game where we roll a dice and move that many squares forward on the board. Suppose we are currently on the square \\(X_n\\). Then what can we say about which square \\(X_{n+1}\\) we move to on our next turn?\n\n\\(X_{n+1}\\) is random, since it depends on the roll of the dice.\n\\(X_{n+1}\\) depends on where we are now \\(X_n\\), since the score of dice will be added onto the number our current square,\nGiven the square \\(X_n\\) we are now, \\(X_{n+1}\\) doesn’t depend any further on which sequence of squares \\(X_0, X_1, \\dots, X_{n-1}\\) we used to get here.\n\nThe third point is called the Markov property or memoryless property. We say “memoryless”, because we only need to remember what square we’ve reached, not which squares we used to get here. The stochastic process before this moment has no bearing on the future, given where we are now. A mathematical way to say this is that “the past and the future are conditionally independent given the present.”"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider the following simple random walk on the integers \\(\\mathbb Z\\): We start at \\(0\\), then at each time step, we go up by one with probability \\(p\\) and down by one with probability \\(q = 1-p\\). When \\(p = q = \\frac12\\), we’re equally as likely to go up as down, and we call this the simple symmetric random walk.\nThe simple random walk is a simple but very useful model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In many modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the “drunkard’s walk”, suggesting it could model a drunk person trying to stagger home."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-1",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-1",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nrandom walks\nrequire(ggplot2, quietly = TRUE)\nset.seed(315)\n\nrrw &lt;- function(n, p = 1/2) {\n  q &lt;- 1 - p\n  Z &lt;- sample(c(1, -1), n, replace = TRUE, prob = c(p, q))\n  X &lt;- c(0, cumsum(Z))\n  c(0, cumsum(Z))\n}\n\nn &lt;- 2000\nrw_dat &lt;- tibble::tibble(x=0:n) |&gt; \n  dplyr::mutate(\n    \"p = 2/3\" = rrw(n, 2/3)\n    , \"p = 1/3\" = rrw(n, 1/3)\n    , \"p = 1/2\" = rrw(n, 1/2)\n  )\n\np0 &lt;- rw_dat |&gt; dplyr::slice_head(n=20) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np1 &lt;- rw_dat |&gt; dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np3 &lt;- rw_dat |&gt; #dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np0+p1+p3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-2",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-2",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nWe can write this as a stochastic process \\((X_n)\\) with discrete time \\(n = \\{0,1,2,\\dots\\} = \\mathbb Z_+\\) and discrete state space \\(\\mathcal S = \\mathbb Z\\), where \\(X_0 = 0\\) and, for \\(n \\geq 0\\), we have \\[ X_{n+1} = \\begin{cases} X_n + 1 & \\text{with probability $p$,} \\\\\n                             X_n - 1 & \\text{with probability $q$.} \\end{cases} \\]\nIt’s clear from this definition that \\(X_{n+1}\\) (the future) depends on \\(X_n\\) (the present), but, given \\(X_n\\), does not depend on \\(X_{n-1}, \\dots, X_1, X_0\\) (the past). Thus the Markov property holds, and the simple random walk is a discrete time Markov process or Markov chain."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-3",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-3",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nSo far we’ve seen a a few examples of stochastic processes in discrete time and discrete space with the Markov memoryless property. Now we will develop the theory more generally.\nTo define a so-called “Markov chain”, we first need to say where we start from, and second what the probabilities of transitions from one state to another are.\nIn our examples of the simple random walk and gambler’s ruin, we specified the start point \\(X_0 = i\\) exactly, but we could pick the start point at random according to some distribution \\(\\lambda_i = \\mathbb P(X_0 = i)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-4",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-4",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nAfter that, we want to know the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for \\(i,j \\in \\mathcal S\\). Here, because of the Markov property, the transition probability only needs to condition on the state we’re in now \\(X_n = i\\), and not on the whole history of the process.\nIn the case of the simple random walk, for example, we had initial distribution \\[ \\lambda_i = \\mathbb P(X_0 = i) = \\begin{cases} 1 & \\text{if $i = 0$} \\\\ 0 & \\text{otherwise} \\end{cases} \\] and transition probabilities \\[ \\mathbb P(X_{n+1} = j \\mid X_n = i) = \\begin{cases} p & \\text{if $j = i+1$} \\\\ q & \\text{if $j = i-1$} \\\\ 0 & \\text{otherwise.} \\end{cases} \\]\nFor the random walk (and also the gambler’s ruin), the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) don’t depend on \\(n\\); in other words, the transition probabilities stay the same over time. A Markov process with this property is called time homogeneous. We will always consider time homogeneous processes from now on (unless we say otherwise)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-5",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-5",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nLet’s write \\(p_{ij} = \\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for the transition probabilities, which are independent of \\(n\\). We must have \\(p_{ij} \\geq 0\\), since it is a probability, and we must also have \\(\\sum_j p_{ij} = 1\\) for all states \\(i\\), as this is the sum of the probabilities of all the places you can move to from state i.\nWhen the state space is finite (and even sometimes when it’s not), it’s convenient to write the transition probabilities \\((p_{ij})\\) as a matrix \\(\\mathsf P\\), called the transition matrix, whose \\((i,j)\\)th entry is \\(p_{ij}\\). Then the condition that \\(\\sum_j p_{ij} = 1\\) is the condition that each of the rows of \\(\\mathsf P\\) add up to \\(1\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-6",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-6",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nConsider a simple two-state Markov chain with state space \\(\\mathcal S = \\{0,1\\}\\) and transition matrix \\[ \\mathsf P = \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\beta & 1-\\beta \\end{pmatrix}  \\] for some \\(0 &lt; \\alpha, \\beta &lt; 1\\). Note that the rows of \\(\\mathsf P\\) add up to \\(1\\), as they must.\nWe can illustrate \\(\\mathsf P\\) by a transition diagram, where the blobs are the states and the arrows give the transition probabilities. (We don’t draw the arrow if \\(p_{ij} = 0\\).) In this case, our transition diagram looks like this:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-7",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-7",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTransition diagram for the two-state Markov chain"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-8",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-8",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nWe can use this as a simple model of customer churn, for example. If the customer has closed their account (state 0) on one period, then with probability \\(\\alpha\\) we will be able to entice them to open their account again (state 1) by the next period; while if the customer has an account (state 1), then with probability \\(\\beta\\) they will have closed their account (state 0) by the next period."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-9",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-9",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIf the customer has an account on period n, what’s the probability they also have an account on period n+2?\n\\[\np_{11}(2) = \\mathbb P (X_{n+2} = 1 \\mid X_n = 1)\n\\]\nThe key to calculating this is to condition on the first step again – that is, on whether the printer is working on Tuesday. We have\n\\[\n\\begin{align*}\n  p_{11}(2) &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0, X_n = 1) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1, X_n = 1) \\\\\n  &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1) \\\\\n  &= p_{10}p_{01} + p_{11}p_{11} \\\\\n  &= \\beta\\alpha + (1-\\beta)^2 .\n\\end{align*}\n\\]\nIn the second equality, we used the Markov property to mean conditional probabilities like \\(\\mathbb P(X_{n+2} = 1 \\mid X_{n+1} = k)\\) did not have to depend on \\(X_n\\).\nAnother way to think of this as we summing the probabilities of all length-2 paths from 1 to 1, which are \\(1\\to 0\\to 1\\) with probability \\(\\beta\\alpha\\) and \\(1 \\to 1 \\to 1\\) with probability \\((1-\\beta)^2\\)\n\nIn the above example, we calculated a two-step transition probability \\(p_{ij}(2) = \\mathbb P (X_{n+2} = j \\mid X_n = i)\\) by conditioning on the first step. That is, by considering all the possible intermediate steps \\(k\\), we have\n\\[\np_{ij}(2) = \\sum_{k\\in\\mathcal S} \\mathbb P (X_{n+1} = k \\mid X_n = i)\\mathbb P (X_{n+2} = j \\mid X_{n+1} = k) = \\sum_{k\\in\\mathcal S} p_{ik}p_{kj}\n\\]\nBut this is exactly the formula for multiplying the matrix \\(\\mathsf P\\) with itself! In other words, \\(p_{ij}(2) = \\sum_{k} p_{ik}p_{kj}\\) is the \\((i,j)\\)th entry of the matrix square \\(\\mathsf P^2 = \\mathsf{PP}\\). If we write \\(\\mathsf P(2)  = (p_{ij}(2))\\) for the matrix of two-step transition probabilities, we have \\(\\mathsf P(2) = \\mathsf P^2\\).\nMore generally, we see that this rule holds over multiple steps, provided we sum over all the possible paths \\(i\\to k_1 \\to k_2 \\to \\cdots \\to k_{n-1} \\to j\\) of length \\(n\\) from \\(i\\) to \\(j\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-10",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-10",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTheorem 1 Let \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). For \\(i,j \\in \\mathcal S\\), write \\[ p_{ij}(n) = \\mathbb P(X_n = j \\mid X_0 = i) \\] for the \\(n\\)-step transition probability. Then\n\\[\np_{ij}(n) = \\sum_{k_1, k_2, \\dots, k_{n-1} \\in \\mathcal S} p_{ik_1} p_{k_1k_2} \\cdots p_{k_{n-2}k_{n-1}} p_{k_{n-1}j}\n\\]\nIn particular, \\(p_{ij}(n)\\) is the \\((i,j)\\)th element of the matrix power \\(\\mathsf P^n\\), and the matrix of \\(n\\)-step transition probabilities is given by \\(\\mathsf P(n) = \\mathsf P^n\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-11",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-11",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nThe so-called Chapman–Kolmogorov equations follow immediately from this.\n\nLet \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). Then, for non-negative integers \\(n,m\\), we have \\[ p_{ij}(n+m) = \\sum_{k \\in \\mathcal S} p_{ik}(n)p_{kj}(m) , \\] or, in matrix notation, \\(\\mathsf P(n+m) = \\mathsf P(n)\\mathsf P(m)\\).\n\nIn other words, a trip of length \\(n + m\\) from \\(i\\) to \\(j\\) is a trip of length \\(n\\) from \\(i\\) to some other state \\(k\\), then a trip of length \\(m\\) from \\(k\\) back to \\(j\\), and this intermediate stop \\(k\\) can be any state, so we have to sum the probabilities.\nOf course, once we know that \\(\\mathsf P(n) = \\mathsf P^n\\) is given by the matrix power, it’s clear to see that \\(\\mathsf P(n+m) = \\mathsf P^{n+m} = \\mathsf P^n \\mathsf P^m = \\mathsf P(n)\\mathsf P(m)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-12",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-12",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nIf we start from a state given by a distribution \\(\\boldsymbol \\pi = (\\pi_i)\\), then after step 1 the probability we’re in state \\(j\\) is \\(\\sum_i \\pi_i p_{ij}\\). So if \\(\\pi_j = \\sum_i \\pi_i p_{ij}\\), we stay in this distribution forever. We call such a distribution a stationary distribution. We again recognise this formula as a matrix-vector multiplication, so this is \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\), where \\(\\boldsymbol \\pi\\) is a row vector.\n\nLet \\((X_n)\\) be a Markov chain on a state space \\(\\mathcal S\\) with transition matrix \\(\\mathsf P\\). Let \\(\\boldsymbol \\pi = (\\pi_i)\\) be a distribution on \\(\\mathcal S\\), in that \\(\\pi_i \\geq 0\\) for all \\(i \\in \\mathcal S\\) and \\(\\sum_{i \\in \\mathcal S} \\pi_i = 1\\). We call \\(\\boldsymbol \\pi\\) a stationary distribution if\n\\[\n\\pi_j =* \\sum_{i\\in \\mathcal S} \\pi_i p_{ij} \\quad \\text{for all $j \\in \\mathcal S$}\n\\]\nor, equivalently, if \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\).\n\nNote that we’re saying the distribution \\(\\mathbb P(X_n = i)\\) stays the same; the Markov chain \\((X_n)\\) itself will keep moving. One way to think is that if we started off a thousand Markov chains, choosing each starting position to be \\(i\\) with probability \\(\\pi_i\\), then (roughly) \\(1000 \\pi_j\\) of them would be in state \\(j\\) at any time in the future – but not necessarily the same ones each time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-13",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-13",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nproperties\nA Markov Chain is irreducible if you have positive probability of eventually getting from anywhere to anywhere else.\nA Markov Chain is aperiodic if there are no forced cycles, i.e. there do not exist disjoint non-empty subsets X1,X2,…,Xd ford≥2,suchthatP(x,Xi+1)=1 forallx∈Xi (1≤i≤d−1),andP(x,X1)=1forallx∈Xd. [Diagram.]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-14",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-14",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nMC with cycle"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chain-monte-carlo",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chain-monte-carlo",
    "title": "Monte Carlo Methods",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\nSuppose have complicated, high-dimensional density \\(\\pi = cg\\) and we want samples \\(X_1, X_2,\\dots \\sim \\pi\\). (Then can do Monte Carlo.)\nDefine a Markovchain (dependent random process) \\(X_0, X_1,X_2\\dots\\) in such a way that for large enough \\(n\\), \\(X_n\\sim \\pi\\).\nThen we can estimate \\(\\mathbb{E}_{\\pi}(h) ≡ \\int h(x) \\pi(x) dx\\) by:\n\\[\n\\mathbb{E}_{\\pi}[h] \\approx \\frac{1}{M-B}\\sum_{i=B+1}^{M}\n\\]\nwhere \\(B\\) (“burn-in”) is chosen large enough so \\(X_B\\sim\\pi\\), and \\(M\\) is chosen large enough to get good Monte Carlo estimates."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mcmc-metropolis-algo",
    "href": "slides/BSMM_8740_lec_09b.html#mcmc-metropolis-algo",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\nchoose some initial value \\(X_0\\), then\ngiven \\(X_{n-1}\\), choose a proposal state \\(Y_n\\sim \\textrm{MVN}(X_{n-1},\\sigma^2\\textrm{I})\\) for some fixed \\(\\sigma^2&gt;0\\)\nlet \\(A_n=\\pi(Y_n)/\\pi(X_{n-1}=g(Y_n)/g(X_{n-1})\\) and \\(U_n\\sim\\textrm{U}[0,1]\\), then\nid \\(U_n&lt;A_n\\) set \\(X_n=Y_n\\) (“accept”), otherwise set $X_n = X_{n-1} ) “reject”\nrepeat for \\(n=1,2,3,\\ldots,M\\)\n\nThis version is called random-walk Metropolis"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mcmc-metropolis-algo-1",
    "href": "slides/BSMM_8740_lec_09b.html#mcmc-metropolis-algo-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\n\na simple Metropolis algorithm in one dimension\ng = function(y) {\n    if ( (y&lt;0) || (y&gt;1) )\n    return(0)\n    else\n    return( y^3 * sin(y^4) * cos(y^5) )\n}\n\nh = function(y) { return(y^2) }\n\nM = 11000  # run length\nB = 1000  # amount of burn-in\nX = runif(1)  # overdispersed starting distribution\nsigma = 1  # proposal scaling\nxlist = rep(0,M)  # for keeping track of chain values\nhlist = rep(0,M)  # for keeping track of h function values\nnumaccept = 0;\n\nfor (i in 1:M) {\n  Y = X + sigma * rnorm(1)  # proposal value\n  U = runif(1)              # for accept/reject\n  alpha = g(Y) / g(X)       # for accept/reject\n  if (U &lt; alpha) {\n    X = Y                   # accept proposal\n    numaccept = numaccept + 1;\n  }\n    xlist[i] = X;\n    hlist[i] = h(X);\n}\n\ncat(\"ran Metropolis algorithm for\", M, \"iterations, with burn-in\", B, \"\\n\");\n\n\nran Metropolis algorithm for 11000 iterations, with burn-in 1000 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"acceptance rate =\", numaccept/M, \"\\n\");\n\n\nacceptance rate = 0.1046 \n\n\na simple Metropolis algorithm in one dimension\nu = mean(hlist[(B+1):M])\ncat(\"mean of h is about\", u, \"\\n\")\n\n\nmean of h is about 0.773 \n\n\na simple Metropolis algorithm in one dimension\nse1 =  sd(hlist[(B+1):M]) / sqrt(M-B)\ncat(\"iid standard error would be about\", se1, \"\\n\")\n\n\niid standard error would be about 0.001658 \n\n\na simple Metropolis algorithm in one dimension\nvarfact &lt;- function(xxx) { 2 * sum(acf(xxx, plot=FALSE)$acf) - 1 }\nthevarfact = varfact(hlist[(B+1):M])\nse = se1 * sqrt( thevarfact )\ncat(\"varfact = \", thevarfact, \"\\n\")\n\n\nvarfact =  21.02 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"true standard error is about\", se, \"\\n\")\n\n\ntrue standard error is about 0.007601 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"approximate 95% confidence interval is (\", u - 1.96 * se, \",\",\n                        u + 1.96 * se, \")\\n\\n\")\n\n\napproximate 95% confidence interval is ( 0.7581 , 0.7879 )\n\n\na simple Metropolis algorithm in one dimension\nplot(xlist, type='l')\n\n\n\na simple Metropolis algorithm in one dimension\n# acf(xlist)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#more",
    "href": "slides/BSMM_8740_lec_09b.html#more",
    "title": "Monte Carlo Methods",
    "section": "More",
    "text": "More\n\nRead Bayes Rules!\nRead Think Bayes\nRead Statistical Rethinking"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#recap",
    "href": "slides/BSMM_8740_lec_09b.html#recap",
    "title": "Monte Carlo Methods",
    "section": "Recap",
    "text": "Recap\n\nWe’ve had the smallest possible taste of statistical programming using Bayes theorem and sampling methods, in the context of adressing the limitations of off-the-shelf implementations of statistical methods and algorithms."
  },
  {
    "objectID": "slides/misc_causality.html",
    "href": "slides/misc_causality.html",
    "title": "Misc Causality",
    "section": "",
    "text": "[Applied Causal Analysis in R](https://bookdown.org/paul/applied-causal-analysis/att.html)\n\n[Intro to Causality](https://digitalcausalitylab.github.io/lecture/lecture_1_intro/Lecture_1_intro.html#/introduction-to-causality-4)\n\n[Causal inference in R](https://www.r-causal.org/)\n\n[CausInfinR-github](https://github.com/r-causal/causal-inference-in-R/blob/main/chapters/chapter-05.qmd)\n\n[Statistical tools for causal inference](https://chabefer.github.io/STCI/index.html)\n\n[Causal graphs](https://github.com/NickCH-K/causalgraphs)\n\n[Causal data sets](https://cran.r-project.org/web/packages/causaldata/causaldata.pdf)\n\n[Causality slides](https://github.com/NickCH-K/CausalitySlides/tree/main)\n\n[more slides](https://users.ssc.wisc.edu/~felwert/causality/?page_id=66)\n\n[workshop](https://github.com/r-causal/causal_inference_r_workshop)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_08.html#recap-of-last-week",
    "title": "Causality Part 2",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced DAGs as a method to represent our causal assumptions and infer how we might estimate the causal effects of interest.\nWe worked through a method to estimate causal effects using IPW."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#this-week",
    "href": "slides/BSMM_8740_lec_08.html#this-week",
    "title": "Causality Part 2",
    "section": "This week",
    "text": "This week\nWe will look at other methods used to estimate causal effects, along with methods used for special situations, including:\n\nregression adjustment\ndoubly robust estimation\nmatching\ndifference in differences\nfixed effects and methods for panel data"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\nLast week we used IPW to create a pseudopopulation where, for every confounder level, the numbers of treated and untreated were balanced.\nIPW requires us to build a model to predict the treatment, depending on the confounders (assuming we have data for all the confounders)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-1",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-1",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\nRepeating our prior process, first we calculate the ipw weights and estimate the ATE using the weights.\n\n\nATE estimate by IPW\ndat_ &lt;- causalworkshop::net_data |&gt; dplyr::mutate(net = as.numeric(net))\n\npropensity_model &lt;- glm(\n  net ~ income + health + temperature\n  , data = dat_, family = binomial()\n)\n\nnet_data_wts &lt;- propensity_model |&gt;\n  broom::augment(newdata = dat_, type.predict = \"response\") |&gt;\n  # .fitted is the value predicted by the model\n  # for a given observation\n  dplyr::mutate(wts = propensity::wt_ate(.fitted, .exposure = net))\n\nnet_data_wts |&gt;\n  lm(malaria_risk ~ net, data = _, weights = wts) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt; gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n42.74\n0.4420\n96.69\n0.000e+00\n41.87\n43.61\n\n\nnet\n-12.54\n0.6243\n-20.10\n5.498e-81\n-13.77\n-11.32"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-2",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-2",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nThe function 1propensity::wt_ate calculates unstabilized weights by default. In the last lecture we looked at the distribution of weights and decided they were stable enough, because none of the weights were too big or too small.\nLet’s explore the question of IPW stabilization a bit more.\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\ntables created with functions from the package gtsummary."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-3",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-3",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\nStabilized weights\n\nThe IP weights (given covariate set \\(L\\)) are \\(W^A=1/\\mathbb{E}[D|L]\\). Because the denominator can be very close to \\(0\\) or \\(1\\) the estimates using these weights can be unstable.\nStabilized weights are often used to address this, e.g. the function propensity::wt_ate with stabilize = TRUE multiplies the weights by the mean of the treatment so in this case \\(SW^A=\\mathbb{E}[D]/\\mathbb{E}[D|L]\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-4",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-4",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\nV-Stabilized weights\nBaseline covariates (\\(V\\subset L\\)) are also used to stabilize IP weights: \\(SW^A(V)=\\mathbb{E}[D|V]/\\mathbb{E}[D|L]\\).\n\nNote the the variables \\(V\\) need to be included in the numerator and the denominator (as part of \\(L\\))\nV-stabilization results in IP weights that are more stabilized than the ones without V."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-5",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-5",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nWe know that IPW weighting should create a pseudo-population that make the covariates more balanced by treatment. In the last lecture we checked this using histograms. Here we check this via the statistics of the data:\n\n\nunweightedweighted\n\n\n\n\nCode\ngtsummary::tbl_summary(\n  net_data_wts \n  , by = net\n  , include = c(net,income,health,temperature,malaria_risk)\n) |&gt;\n  # add an overall column to the table\n  gtsummary::add_overall(last = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n0 N = 1,2981\n1 N = 4541\nOverall N = 1,7521\n\n\n\n\nincome\n880 (751, 1,010)\n954 (806, 1,081)\n893 (765, 1,031)\n\n\nhealth\n49 (35, 61)\n54 (40, 68)\n50 (37, 64)\n\n\ntemperature\n23.9 (21.0, 27.1)\n23.5 (20.6, 26.9)\n23.8 (20.9, 27.1)\n\n\nmalaria_risk\n41 (32, 54)\n26 (20, 32)\n36 (28, 50)\n\n\n\n1 Median (Q1, Q3)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nnet_svy &lt;- survey::svydesign(\n  ids = ~1,\n  data = net_data_wts,\n  weights = ~wts\n)\n\ngtsummary::tbl_svysummary(\n  net_svy,\n  by = net,\n  include = c(net,income,health,temperature,malaria_risk)\n) |&gt;\n  gtsummary::add_overall(last = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n0 N = 1,7511\n1 N = 1,7601\nOverall N = 3,5111\n\n\n\n\nincome\n892 (767, 1,025)\n892 (755, 1,039)\n892 (761, 1,031)\n\n\nhealth\n50 (36, 63)\n49 (37, 64)\n50 (37, 64)\n\n\ntemperature\n23.9 (20.9, 27.1)\n23.8 (20.8, 27.3)\n23.8 (20.8, 27.2)\n\n\nmalaria_risk\n40 (31, 53)\n28 (23, 34)\n33 (26, 45)\n\n\n\n1 Median (Q1, Q3)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-6",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-6",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nNext we use bootstrapping to generate proper confidence intervals for the effect, first by creating a function to generate (unstabilized) ipw effect estimates for each bootstrap split:\n\nfit_ipw &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  if(\"rsplit\" %in% class(split)){\n    .df &lt;- rsample::analysis(split)\n  }else if(\"data.frame\" %in% class(split)){\n    .df &lt;- split\n  }\n\n  # fit propensity score model\n  propensity_model &lt;- glm(\n    net ~ income + health + temperature,\n    data = .df,\n    family = binomial()\n  )\n\n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\n  # fit correctly bootstrapped ipw model\n  lm(malaria_risk ~ net, data = .df, weights = wts) |&gt;\n    broom::tidy()\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-7",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-7",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nNext we use our function to bootstrap proper confidence intervals:\n\n\n# create bootstrap samples\nbootstrapped_net_data &lt;- rsample::bootstraps(\n  dat_,\n  times = 1000,\n  # required to calculate CIs later\n  apparent = TRUE\n)\n\n# create ipw and fit each bootstrap sample\nresults &lt;- bootstrapped_net_data |&gt;\n  dplyr::mutate(\n    ipw_fits = purrr::map(splits, fit_ipw))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#regression-adjustment",
    "href": "slides/BSMM_8740_lec_08.html#regression-adjustment",
    "title": "Causality Part 2",
    "section": "Regression Adjustment",
    "text": "Regression Adjustment\n\nIPW estimates rely on a model to predict the treatment using covariate/confounder values. We know that we can also predict the effect of treatment by building a model to predict the outcome using a regression model, regressing the effect on the treatment and covariate/confounder values.\n\n\n\nregression adjustment\noutcome_model &lt;- glm(\n  malaria_risk ~ net + income + health + temperature + insecticide_resistance +\n    I(health^2) + I(temperature^2) + I(income^2),\n  data = dat_\n)\n\noutcome_model |&gt; broom::tidy(conf.int = TRUE)\n\n\n# A tibble: 9 × 7\n  term  estimate std.error statistic   p.value conf.low\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Int…  1.08e+2   3.55e+0    30.5   1.89e-164  1.01e+2\n2 net   -1.24e+1   2.30e-1   -53.8   0         -1.28e+1\n3 inco… -1.65e-1   4.58e-3   -36.1   3.48e-213 -1.74e-1\n4 heal…  2.01e-1   2.82e-2     7.13  1.44e- 12  1.46e-1\n5 temp…  7.69e-1   2.62e-1     2.93  3.42e-  3  2.55e-1\n6 inse…  2.19e-1   6.89e-3    31.8   4.97e-175  2.05e-1\n7 I(he… -6.60e-4   2.65e-4    -2.49  1.28e-  2 -1.18e-3\n8 I(te…  5.01e-3   5.46e-3     0.919 3.58e-  1 -5.68e-3\n9 I(in…  5.02e-5   2.50e-6    20.1   1.04e- 80  4.53e-5\n# ℹ 1 more variable: conf.high &lt;dbl&gt;"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#regression-adjustment-1",
    "href": "slides/BSMM_8740_lec_08.html#regression-adjustment-1",
    "title": "Causality Part 2",
    "section": "Regression Adjustment",
    "text": "Regression Adjustment\n\nAnd we can also bootstrap the regression adjustment estimates to get confidence intervals, first by creating the estimation function, then generating bootstrapped estimates, like we did with IPWs:\n\n\nfit_reg &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  if(\"rsplit\" %in% class(split)){\n    .df &lt;- rsample::analysis(split)\n  }else if(\"data.frame\" %in% class(split)){\n    .df &lt;- split\n  }\n\n  # fit outcome model\n  glm(malaria_risk ~ net + income + health + temperature + insecticide_resistance +\n      I(health^2) + I(temperature^2) + I(income^2), data = .df\n    )|&gt;\n    broom::tidy()\n}\n\nboth_results &lt;- results |&gt;\n  dplyr::mutate(\n    reg_fits = purrr::map(splits, fit_reg))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment",
    "href": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment",
    "title": "Causality Part 2",
    "section": "IPW vs Regression Adjustment",
    "text": "IPW vs Regression Adjustment\n\nWe can compare the results:\n\n\nboth_results_dat &lt;- both_results |&gt;\n  dplyr::mutate(\n    reg_estimate = purrr::map_dbl(\n      reg_fits,\n      # pull the `estimate` for net for each fit\n      \\(.fit) .fit |&gt;\n        dplyr::filter(term == \"net\") |&gt;\n        dplyr::pull(estimate)\n    )\n    , ipw_estimate = purrr::map_dbl(\n      ipw_fits,\n      # pull the `estimate` for net for each fit\n      \\(.fit) .fit |&gt;\n        dplyr::filter(term == \"net\") |&gt;\n        dplyr::pull(estimate)\n    )\n  )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-1",
    "href": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-1",
    "title": "Causality Part 2",
    "section": "IPW vs Regression Adjustment",
    "text": "IPW vs Regression Adjustment\n\nWe can plot the results:\n\n\n\nCode\ndat &lt;- both_results_dat |&gt; \n  dplyr::select(reg_estimate, ipw_estimate) |&gt; \n  tidyr::pivot_longer(cols=everything(), names_to = \"method\", values_to = \"effect estimate\")\n\ndat |&gt;\n  dplyr::mutate(method=factor(method)) |&gt;\n  ggplot( bins = 50, alpha = .5\n    , aes(\n      `effect estimate`, after_stat(density) , colour = method, fill = method\n      )\n  ) +\n  geom_histogram(alpha = 0.2, position = 'identity') +\n  geom_density(alpha = 0.2) +\n  theme(legend.text=element_text(size=10), legend.title = element_blank())"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-2",
    "href": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-2",
    "title": "Causality Part 2",
    "section": "IPW vs Regression Adjustment",
    "text": "IPW vs Regression Adjustment\n\nWe can compare the means and the confidence intervals of the regression adjustment and IPW effect estimates:\n\n\ncompare IPW and regression effect estimates\ndat |&gt; \n  dplyr::group_by(method) |&gt; \n  dplyr::summarise(\n  mean = mean(`effect estimate`)\n  , lower_ci = quantile(`effect estimate`, 0.025)\n  , upper_ci = quantile(`effect estimate`, 0.975)\n) |&gt; gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nmethod\nmean\nlower_ci\nupper_ci\n\n\n\n\nipw_estimate\n-12.54\n-13.39\n-11.63\n\n\nreg_estimate\n-12.37\n-12.93\n-11.83\n\n\n\n\n\n\n\nNote that each mean ATE estimate is within the CIs of the other ATE mean estimate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-3",
    "href": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-3",
    "title": "Causality Part 2",
    "section": "IPW vs Regression Adjustment",
    "text": "IPW vs Regression Adjustment\n\nThere is a package (boot) that performs cross-validation and CI estimation at the same time:\n\n\nuse the boot package for CI estimation of regression adjustment\n# bootstrap (1000 times) using the fit_reg function\nboot_out_reg &lt;- boot::boot(\n  data = causalworkshop::net_data |&gt; dplyr::mutate(net = as.numeric(net))\n  , R = 1000\n  , sim = \"ordinary\"\n  , statistic =\n    (\\(x,y){ # x is the data, y is a vector of row numbers for the bootstrap sample\n      fit_reg(x[y,]) |&gt;\n        dplyr::filter(term == \"net\") |&gt;\n        dplyr::pull(estimate)\n    })\n)\n# calculate the CIs\nCIs &lt;- boot_out_reg |&gt;\n  boot::boot.ci(L = boot::empinf(boot_out_reg, index=1L, type=\"jack\"))\n\ntibble::tibble(CI = c(\"lower\", \"upper\"), normal = CIs$normal[-1], basic = CIs$basic[-(1:3)]\n               , percent = CIs$percent[-(1:3)]) |&gt; gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nCI\nnormal\nbasic\npercent\n\n\n\n\nlower\n-12.92\n-12.92\n-12.90\n\n\nupper\n-11.85\n-11.85\n-11.83"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\nDon’t Put All your Eggs in One Basket\n\nWe’ve learned how to use linear regression and propensity score weighting to estimate \\(E[Y|D=1] - E[Y|D=0] | X\\). But which one should we use and when?\nWhen in doubt, just use both! Doubly Robust Estimation is a way of combining propensity score and linear regression in a way you don’t have to rely on either of them."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-2",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-2",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nThe estimator is as follows.\n\\[\n\\hat{ATE} = \\frac{1}{N}\\sum \\bigg( \\dfrac{D_i(Y_i - \\hat{\\mu_1}(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg) - \\frac{1}{N}\\sum \\bigg( \\dfrac{(1-D_i)(Y_i - \\hat{\\mu_0}(X_i))}{1-\\hat{P}(X_i)} + \\hat{\\mu_0}(X_i) \\bigg)\n\\]\nwhere\n\n\\(\\hat{P}(x)\\) is an estimation of the propensity score (using logistic regression, for example),\n\\(\\hat{\\mu_1}(x)\\) is an estimation of \\(E[Y|X, D=1]\\) (using linear regression, for example).\n\nThe first part of the doubly robust estimator estimates \\(E[Y^1]\\) and the second part estimates \\(E[Y^0]\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-3",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-3",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nFirst let’s examine the code. Note we use a recipe with the regression to add the polynomial confounders for each\n\ndataDR functionestimates\n\n\n\nD &lt;- \"net\"\nY &lt;- \"malaria_risk\"\nX &lt;- paste0(c('income', 'health', 'temperature'),c(rep('_poly_1',3),rep('_poly_2',3)))\n\ndoubly_robust_rec &lt;- causalworkshop::net_data |&gt; \n  dplyr::mutate(net = as.numeric(net)) |&gt; \n  recipes::recipe(malaria_risk ~ net + income + health + temperature) |&gt; \n  # NOTE: these are orthogonal polynomial terms, were not used in regression adjustment earlier\n  recipes::step_poly(income, health, temperature) |&gt; \n  recipes::prep() \n\ndoubly_robust_dat &lt;- doubly_robust_rec |&gt; recipes::bake(new_data=NULL)\n\n\n\n\ndoubly_robust &lt;- function(df, X, D, Y){\n  ps &lt;- # propensity score\n    as.formula(paste(D, \" ~ \", paste(X, collapse= \"+\"))) |&gt;\n    stats::glm( data = df, family = binomial() ) |&gt;\n    broom::augment(type.predict = \"response\", data = df) |&gt;\n    dplyr::pull(.fitted)\n  \n  lin_frml &lt;- formula(paste(Y, \" ~ \", paste(X, collapse= \"+\")))\n  \n  idx &lt;- df[,D] |&gt; dplyr::pull(1) == 0\n  mu0 &lt;- # mean response D == 0\n    lm(lin_frml, data = df[idx,]) |&gt; \n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt;\n    dplyr::pull(.fitted)\n  \n  idx &lt;- df[,D] |&gt; dplyr::pull(1) == 1\n  mu1 &lt;- # mean response D == 1\n    lm(lin_frml, data = df[idx,]) |&gt;  \n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt; \n    dplyr::pull(.fitted)\n  \n  # convert treatment factor to integer | recast as vectors\n  d &lt;- df[,D] |&gt; dplyr::pull(1) |&gt; as.character() |&gt; as.numeric()\n  y &lt;- df[,Y] |&gt; dplyr::pull(1)\n  \n  mean( d*(y - mu1)/ps + mu1 ) -\n    mean(( 1-d)*(y - mu0)/(1-ps) + mu0 )\n}\n\n\n\n\ndoubly_robust_dat |&gt; \n  doubly_robust(X, D, Y)\n\n[1] -12.9"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-4",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-4",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nOnce again, we can use bootstrap to construct confidence intervals.\n\n\n\nuse bootstrap fold to estimate using DRE\nall_results_dat &lt;- both_results_dat |&gt;\n  dplyr::mutate(\n    dre_estimate = \n      purrr::map_dbl(\n        splits\n        , (\\(x){\n            doubly_robust_rec |&gt; \n              recipes::bake(new_data = rsample::analysis(x)) |&gt; \n              doubly_robust(X, D, Y)\n        })\n      )\n  )\n\nall_results_dat\n\n\n# Bootstrap sampling with apparent sample \n# A tibble: 1,001 × 7\n   splits             id            ipw_fits reg_fits\n   &lt;list&gt;             &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;  \n 1 &lt;split [1752/651]&gt; Bootstrap0001 &lt;tibble&gt; &lt;tibble&gt;\n 2 &lt;split [1752/657]&gt; Bootstrap0002 &lt;tibble&gt; &lt;tibble&gt;\n 3 &lt;split [1752/634]&gt; Bootstrap0003 &lt;tibble&gt; &lt;tibble&gt;\n 4 &lt;split [1752/644]&gt; Bootstrap0004 &lt;tibble&gt; &lt;tibble&gt;\n 5 &lt;split [1752/631]&gt; Bootstrap0005 &lt;tibble&gt; &lt;tibble&gt;\n 6 &lt;split [1752/652]&gt; Bootstrap0006 &lt;tibble&gt; &lt;tibble&gt;\n 7 &lt;split [1752/630]&gt; Bootstrap0007 &lt;tibble&gt; &lt;tibble&gt;\n 8 &lt;split [1752/666]&gt; Bootstrap0008 &lt;tibble&gt; &lt;tibble&gt;\n 9 &lt;split [1752/659]&gt; Bootstrap0009 &lt;tibble&gt; &lt;tibble&gt;\n10 &lt;split [1752/626]&gt; Bootstrap0010 &lt;tibble&gt; &lt;tibble&gt;\n# ℹ 991 more rows\n# ℹ 3 more variables: reg_estimate &lt;dbl&gt;,\n#   ipw_estimate &lt;dbl&gt;, dre_estimate &lt;dbl&gt;"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-5",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-5",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nLooking at the histogram/distributions of all the estimates:\n\n\n\nCode\ndat &lt;- all_results_dat |&gt; \n  dplyr::select(reg_estimate, ipw_estimate, dre_estimate) |&gt; \n  tidyr::pivot_longer(cols=everything(), names_to = \"method\", values_to = \"effect estimate\")\n\ndat |&gt;\n  dplyr::mutate(method=factor(method)) |&gt;\n  ggplot( bins = 50, alpha = .5\n    , aes(\n      `effect estimate`, after_stat(density) , colour = method, fill = method\n      )\n  ) +\n  geom_histogram(alpha = 0.2, position = 'identity') +\n  geom_density(alpha = 0.2) +\n  theme(legend.text=element_text(size=10), legend.title = element_blank())"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-6",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-6",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nConfidence intervals, including for the dre estimate are:\n\n\n\ncompute CIs for IPW, rgression and DRE\ndat |&gt; \n  dplyr::group_by(method) |&gt; \n  dplyr::summarise(\n  mean = mean(`effect estimate`)\n  , lower_ci = quantile(`effect estimate`, 0.025)\n  , upper_ci = quantile(`effect estimate`, 0.975)\n) |&gt; gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nmethod\nmean\nlower_ci\nupper_ci\n\n\n\n\ndre_estimate\n-12.89\n-13.53\n-12.28\n\n\nipw_estimate\n-12.54\n-13.39\n-11.63\n\n\nreg_estimate\n-12.37\n-12.93\n-11.83\n\n\n\n\n\n\n\n\nAll methods are consistent."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-7",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-7",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nThe doubly robust estimator is called doubly robust because it only requires one of the models, \\(\\hat{P}(x)\\) or \\(\\hat{\\mu}(x)\\), to be correctly specified.\nLook at the first part that estimates \\(E[Y^1]\\)\n\\[\n\\hat{E}[Y^1] = \\frac{1}{N}\\sum \\bigg( \\dfrac{D_i(Y_i - \\hat{\\mu_1}(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg)\n\\]\nAssume that \\(\\hat{\\mu_1}(x)\\) is correct. If the propensity score model is wrong, we wouldn’t need to worry. Because if \\(\\hat{\\mu_1}(x)\\) is correct, then \\(E[D_i(Y_i - \\hat{\\mu_1}(X_i))]=0\\). That is because the multiplication by \\(D_i\\) selects only the treated and the residual of \\(\\hat{\\mu_1}\\) on the treated have, by definition, mean zero.\nThis causes the whole thing to reduce to \\(\\hat{\\mu_1}(X_i)\\), which is correctly estimated \\(E[Y^1]\\) by assumption. Similar reasoning applies to the estimator of \\(E[Y^0]\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-8",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-8",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nHere we deliberately bias the IPW estimate by replacing the propensity score by a random uniform variable that goes from 0.1 to 0.9 (we don’t want very small weights to blow up the propensity score variance). Since this is random, there is no way it is a good propensity score model, but the doubly robust estimator still manages to produce an estimation that is very close to when the propensity score was estimated with logistic regression.\n\n\ndefine a DRE with a bad IPW model\ndoubly_robust_bad_ipw &lt;- function(df, X, D, Y){\n  ps &lt;- runif(dim(df)[1], 0.1, 0.9) # wrong propensity score \n  \n  lin_frml &lt;- formula(paste(Y, \" ~ \", paste(X, collapse= \"+\")))\n  \n  idx &lt;- df[,D] |&gt; dplyr::pull(1) == 0\n  mu0 &lt;- # mean response D == 0\n    lm(lin_frml, data = df[idx,]) |&gt;\n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt;\n    dplyr::pull(.fitted)\n  \n  idx &lt;- df[,D] |&gt; dplyr::pull(1) == 1\n  mu1 &lt;- # mean response D == 1\n    lm(lin_frml, data = df[idx,]) |&gt;\n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt;\n    dplyr::pull(.fitted)\n  \n  # convert treatment factor to integer | recast as vectors\n  d &lt;- df[,D] |&gt; dplyr::pull(1) |&gt; as.character() |&gt; as.numeric()\n  y &lt;- df[,Y] |&gt; dplyr::pull(1)\n  \n  mean( d*(y - mu1)/ps + mu1 ) -\n    mean(( 1-d)*(y - mu0)/(1-ps) + mu0 )\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-9",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-9",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\n\n\nCode\nall_results_dat &lt;- all_results_dat |&gt;\n  dplyr::mutate(\n    dre_bad_ipw_estimate = \n      purrr::map_dbl(\n        splits\n        , (\\(x){\n          # rsample::analysis(x) |&gt; \n          #   # dplyr::mutate(net = as.numeric(net)) |&gt; # not needed\n          #   recipes::bake(new_data = rsample::analysis(x)) |&gt; \n          doubly_robust_rec |&gt; \n            recipes::bake(new_data = rsample::analysis(x)) |&gt; \n            doubly_robust_bad_ipw(X, D, Y)\n        })\n      )\n  )\n\ndat &lt;- all_results_dat |&gt; \n  dplyr::select(reg_estimate, ipw_estimate, dre_estimate, dre_bad_ipw_estimate) |&gt; \n  tidyr::pivot_longer(cols=everything(), names_to = \"method\", values_to = \"effect estimate\")\n\ndat |&gt;\n  dplyr::mutate(method=factor(method)) |&gt;\n  ggplot( bins = 50, alpha = .5\n    , aes(\n      `effect estimate`, after_stat(density) , colour = method, fill = method\n      )\n  ) +\n  geom_histogram(alpha = 0.2, position = 'identity') +\n  geom_density(alpha = 0.2) +\n  theme(legend.text=element_text(size=10), legend.title = element_blank())"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-10",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-10",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\n\nsummarize the models so far.\ndat |&gt; \n  dplyr::group_by(method) |&gt; \n  dplyr::summarise(\n  mean = mean(`effect estimate`)\n  , lower_ci = quantile(`effect estimate`, 0.025)\n  , upper_ci = quantile(`effect estimate`, 0.975)\n) |&gt; gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nmethod\nmean\nlower_ci\nupper_ci\n\n\n\n\ndre_bad_ipw_estimate\n-12.89\n-13.67\n-12.18\n\n\ndre_estimate\n-12.89\n-13.53\n-12.28\n\n\nipw_estimate\n-12.54\n-13.39\n-11.63\n\n\nreg_estimate\n-12.37\n-12.93\n-11.83"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-11",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-11",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nMessing up the propensity score yields slightly different ATEs, but not by much. Now let’s again take a good look at the first part of the estimator, rearranging some terms:\n\\[\n\\begin{align*}\n\\hat{E}[Y^{1}] & =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}(Y_{i}-\\hat{\\mu_{1}}(X_{i}))}{\\hat{P}(X_{i})}+\\hat{\\mu_{1}}(X_{i})\\bigg)\\\\\n& =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}Y_{i}}{\\hat{P}(X_{i})}-\\dfrac{D_{i}\\hat{\\mu_{1}}(X_{i})}{\\hat{P}(X_{i})}+\\hat{\\mu_{1}}(X_{i})\\bigg)\\\\\n& =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}Y_{i}}{\\hat{P}(X_{i})}-\\bigg(\\dfrac{D_{i}}{\\hat{P}(X_{i})}-1\\bigg)\\hat{\\mu_{1}}(X_{i})\\bigg)\\\\\n& =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}Y_{i}}{\\hat{P}(X_{i})}-\\bigg(\\dfrac{D_{i}-\\hat{P}(X_{i})}{\\hat{P}(X_{i})}\\bigg)\\hat{\\mu_{1}}(X_{i})\\bigg)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-12",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-12",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nAssume that the propensity score \\(\\hat{P}(X_i)\\) is correctly specified. In this case, \\(E[D_i - \\hat{P}(X_i)]=0\\), which wipes out the part dependent on \\(\\hat{\\mu_1}(X_i)\\). This reduces the doubly robust estimator to the propensity score weighting estimator \\(\\frac{D_iY_i}{\\hat{P}(X_i)}\\), which is correct by assumption.\nSo, even if the \\(\\hat{\\mu_1}(X_i)\\) is wrong, the estimator will still be correct, provided that the propensity score is correctly specified."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-13",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-13",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\n\ndefine a DRE with a bad regression model\ndoubly_robust_bad_reg &lt;- function(df, X, D, Y){\n  ps &lt;- # propensity score\n    as.formula(paste(D, \" ~ \", paste(X, collapse= \"+\"))) |&gt;\n    stats::glm( data = df, family = binomial() ) |&gt;\n    broom::augment(type.predict = \"response\", data = df) |&gt;\n    dplyr::pull(.fitted)\n  \n  mu0 &lt;- rnorm(dim(df)[1], 0, 1) # wrong mean response D == 0\n  mu1 &lt;- rnorm(dim(df)[1], 0, 1) # wrong mean response D == 1\n  \n  # convert treatment factor to integer | recast as vectors\n  d &lt;- df[,D] |&gt; dplyr::pull(1) |&gt; as.character() |&gt; as.numeric()\n  y &lt;- df[,Y] |&gt; dplyr::pull(1)\n  \n  mean( d*(y - mu1)/ps + mu1 ) -\n    mean(( 1-d)*(y - mu0)/(1-ps) + mu0 )\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-14",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-14",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\n\nCode\nall_results_dat &lt;- all_results_dat |&gt;\n  dplyr::mutate(\n    dre_bad_reg_estimate = \n      purrr::map_dbl(\n        splits\n        , (\\(x){\n          # rsample::analysis(x) |&gt; \n          #   # dplyr::mutate(net = as.numeric(net)) |&gt; # not needed\n          #   recipes::bake(new_data = rsample::analysis(x)) |&gt; \n          doubly_robust_rec |&gt; \n            recipes::bake(new_data = rsample::analysis(x)) |&gt; \n            doubly_robust_bad_reg(X, D, Y)\n        })\n      )\n  )\n\ndat &lt;- all_results_dat |&gt; \n  dplyr::select(\n    reg_estimate, ipw_estimate, dre_estimate, dre_bad_ipw_estimate, dre_bad_reg_estimate\n  ) |&gt; \n  tidyr::pivot_longer(cols=everything(), names_to = \"method\", values_to = \"effect estimate\")\n\ndat |&gt;\n  dplyr::mutate(method=factor(method)) |&gt;\n  ggplot( bins = 50, alpha = .5\n    , aes(\n      `effect estimate`, after_stat(density) , colour = method, fill = method\n      )\n  ) +\n  geom_histogram(alpha = 0.2, position = 'identity') +\n  geom_density(alpha = 0.2) +\n  theme(legend.text=element_text(size=10), legend.title = element_blank())"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-15",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-15",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\nOnce again, we can use bootstrap and see that the variance is just slightly higher.\n\n\ncompare the results so far\ndat |&gt; \n  dplyr::group_by(method) |&gt; \n  dplyr::summarise(\n  mean = mean(`effect estimate`)\n  , lower_ci = quantile(`effect estimate`, 0.025)\n  , upper_ci = quantile(`effect estimate`, 0.975)\n) |&gt; gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nmethod\nmean\nlower_ci\nupper_ci\n\n\n\n\ndre_bad_ipw_estimate\n-12.89\n-13.67\n-12.18\n\n\ndre_bad_reg_estimate\n-12.73\n-13.56\n-11.99\n\n\ndre_estimate\n-12.89\n-13.53\n-12.28\n\n\nipw_estimate\n-12.54\n-13.39\n-11.63\n\n\nreg_estimate\n-12.37\n-12.93\n-11.83"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-16",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-16",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nOnce more, messing up the conditional mean model alone yields only slightly different ATE. The magic of doubly robust estimation happens because in causal inference, there are two ways to remove bias from our causal estimates: you either model the treatment mechanism or the outcome mechanism. If either of these models are correct, you are good to go.\nOne caveat is that, in practice, it’s very hard to model precisely either of those. More often, what ends up happening is that neither the propensity score nor the outcome model are 100% correct. They are both wrong, but in different ways. When this happens, it is not exactly settled [1] [2] [3] if it’s better to use a single model or doubly robust estimation. At least it gives you two possibilities of being correct."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-1",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-1",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\n\nWe know that not accounting for confounders or blocking open backdoor paths can bias our causal estimates, but it turns out that even after accounting for all confounders, we may still get a biased estimate with finite samples. Many of the properties we tout in statistics rely on large samples—how “large” is defined can be opaque. Let’s look at a quick simulation. Here, we have an exposure/treatment, \\(X\\), an outcome, \\(Y\\), and one confounder, \\(Z\\). We will simulate \\(Y\\), which is only dependent on \\(Z\\) (so the true treatment effect is 0), and \\(X\\), which also depends on \\(Z\\).\n\n\\[\n\\begin{align*}\nZ &\\sim \\mathscr{N}(0,1)\\\\\nX & = \\mathrm{ifelse}(0.5+Z&gt;0,1,0)\\\\\nY & = Z +  \\mathscr{N}(0,1)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-2",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-2",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\nSampling code\n\nset.seed(928)\nn &lt;- 100\nfinite_sample &lt;- tibble::tibble(\n  # z is normally distributed with a mean: 0 and sd: 1\n  z = rnorm(n),\n  # x is defined from a probit selection model with normally distributed errors\n  x = dplyr::case_when(\n    0.5 + z + rnorm(n) &gt; 0 ~ 1,\n    TRUE ~ 0\n  ),\n  # y is continuous, dependent only on z with normally distrbuted errors\n  y = z + rnorm(n)\n)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-3",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-3",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\n\nIf we fit a propensity score model using the one confounder \\(Z\\) and calculate the weighted estimator, we should get an unbiased result (which in this case would be \\(0\\)).\n\n\n\nfit the propensity score model with finite samples\n## fit the propensity score model\nfinite_sample_wts &lt;- glm(\n  x ~ z,\n  data = finite_sample,\n  family = binomial(\"probit\")\n) |&gt;\n  broom::augment(newdata = finite_sample, type.predict = \"response\") |&gt;\n  dplyr::mutate(wts = propensity::wt_ate(.fitted, x))\n\nfinite_sample_wts |&gt;\n  dplyr::summarize(\n    effect = sum(y * x * wts) / sum(x * wts) -\n      sum(y * (1 - x) * wts) / sum((1 - x) * wts)\n  )\n\n\n# A tibble: 1 × 1\n  effect\n   &lt;dbl&gt;\n1  0.197"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-4",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-4",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\n\nOur effect of is pretty far from 0, although it’s hard to know if this is really bias, or something we are just seeing by chance in this particular simulated sample.\nTo explore the potential for finite sample bias, we can rerun this simulation many times at different sample sizes:\n\n\nfit the propensity score model with different number of finite samples\nsim &lt;- function(n) {\n  ## create a simulated dataset\n  finite_sample &lt;- tibble::tibble(\n    z = rnorm(n),\n    x = dplyr::case_when(\n      0.5 + z + rnorm(n) &gt; 0 ~ 1,\n      TRUE ~ 0\n    ),\n    y = z + rnorm(n)\n  )\n  finite_sample_wts &lt;- glm(\n    x ~ z,\n    data = finite_sample,\n    family = binomial(\"probit\")\n  ) |&gt;\n    broom::augment(newdata = finite_sample, type.predict = \"response\") |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(.fitted, x))\n  bias &lt;- finite_sample_wts |&gt;\n    dplyr::summarize(\n      effect = sum(y * x * wts) / sum(x * wts) -\n        sum(y * (1 - x) * wts) / sum((1 - x) * wts)\n    ) |&gt;\n    dplyr::pull()\n  tibble::tibble(\n    n = n,\n    bias = bias\n  )\n}\n\n## Examine 5 different sample sizes, simulate each 1000 times\nset.seed(1)\nfinite_sample_sims &lt;- purrr::map_df(\n  rep(\n    c(50, 100, 500, 1000, 5000, 10000),\n    each = 1000\n  ),\n  sim\n)\n\nbias &lt;- finite_sample_sims |&gt;\n  dplyr::group_by(n) |&gt;\n  dplyr::summarise(bias = mean(bias))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-5",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-5",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\nPlotting the results:\n\nFinite sample bias present with ATE weights created using correctly specified propensity score model, varying the sample size from n = 50 to n = 10,000"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-6",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-6",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\n\nThis is demonstrates finite sample bias. Notice that even when the sample size is quite large (5,000) we still see some bias away from the “true” effect of 0. It isn’t until a sample size larger than 10,000 that we see this bias disappear.\nEstimands that utilize weights that are unbounded (i.e. that theoretically can be infinitely large) are more likely to suffer from finite sample bias. The likelihood of falling into finite sample bias depends on:\n\nthe estimand you have chosen (i.e. are the weights bounded?)\nthe distribution of the covariates in the exposed and unexposed groups (i.e. is there good overlap? Potential positivity violations, when there is poor overlap, are the regions where weights can become too large)\nthe sample size."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-1",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nRegression is good at controlling for additional variables when we do a test vs control comparison. If we have independence, \\((Y^0, Y^1)\\perp D | X\\), then regression can identify the ATE by controlling for \\(X\\).\nTo get some intuition about controlling for \\(X\\), let’s remember the case when all variables \\(X\\) are dummy variables.\nIf that is the case, regression partitions the data into the dummy cells and computes the mean difference between test and control. Effectively we are calculating doing\n\\[\nE[Y|D=1, X=x] - E[Y|D=0, X=x]\n\\]\nwhere \\(x\\) is a dummy cell (all dummies set to 1, for example).\nRegression then combines the estimate in each of the cells to produce a final ATE. The way it does this is by applying weights to the cell proportional to the variance of the treatment on that group."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-2",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nTo give an example, suppose we try to estimate the effect of a drug and I have drug data for 6 men and 4 women. The response variable is days hospitalised and I hope my drug can lower that. On men, the true causal effect is -3, so the drug lowers the stay period by 3 days. On women, it is -2.\nTo make matters more interesting, men are much more affected by this illness and stay longer at the hospital. They also get much more of the drug. Only 1 out of the 6 men does not get the drug. On the other hand, women are more resistant to this illness, so they stay less at the hospital. 50% of the women get the drug.\n\n\n\n\n\n\n\n\n\nsex\ndrug\ndays\n\n\n\n\nM\n1\n5\n\n\nM\n1\n5\n\n\nM\n1\n5\n\n\nM\n1\n5\n\n\nM\n1\n5\n\n\nM\n0\n8\n\n\nW\n1\n2\n\n\nW\n0\n4\n\n\nW\n1\n2\n\n\nW\n0\n4"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-3",
    "href": "slides/BSMM_8740_lec_08.html#matching-3",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nNote that simple comparison of treatment and control yields a negatively biased effect, that is, the drug seems less effective than it truly is. This is expected, since we’ve omitted the sex confounder.\nIn this case, the estimated ATE is smaller than the true one because men get more of the drug and are more affected by the illness.\n\n\n\n\n\n\n\n\n\ndrug\nmean_effect\nATE\n\n\n\n\n0\n5.333\nNA\n\n\n1\n4.143\n-1.19"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-4",
    "href": "slides/BSMM_8740_lec_08.html#matching-4",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nSince the true effect for men is -3 and the true effect for women is -2, the ATE should be\n\n\\[\nATE=\\dfrac{(-3*6) + (-2*4)}{10}=-2.6\n\\]\n\nThis estimate is done by\n\npartitioning the data into confounder cells, in this case, men and women,\nestimating the effect on each cell and\ncombining the estimate with a weighted average, where the weight is the sample size of the cell or covariate group."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-5",
    "href": "slides/BSMM_8740_lec_08.html#matching-5",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nIf we had exactly the same number of men and women in the data, the ATE estimate would be right in the middle of the ATE of the 2 groups, -2.5. Since there are more men than women in our dataset, the ATE estimate is a little bit closer to the men’s ATE.\nThis is called a non-parametric estimate, since it places no assumption on how the data was generated.\nIf we control for sex using regression, we will add the assumption of linearity. Regression will also partition the data into men and women and estimate the effect on both of these groups.\nSo far, so good. However, when it comes to combining the effect on each group, it does not weigh them by the sample size."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-6",
    "href": "slides/BSMM_8740_lec_08.html#matching-6",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nInstead, regression uses weights that are proportional to the variance of the treatment in that group. In our case, the variance of the treatment in men is smaller than in women, since only one man is in the control group.\nTo be exact, the variance of D for men is \\(0.139=1/6*(1 - 1/6)\\) and for women is \\(0.25=2/4*(1 - 2/4)\\).\nSo regression will give a higher weight to women in our example and the ATE will be a bit closer to the women’s ATE of -2.\n\n\n\n\n\n\n\n\n\nRegression summary (ATE)\n\n\ndays ~ drug + sex\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n7.5455\n0.188\n40.093\n0.000\n\n\ndrug\n−2.4545\n0.188\n−13.042\n0.000\n\n\nsexW\n−3.3182\n0.176\n−18.849\n0.000"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-7",
    "href": "slides/BSMM_8740_lec_08.html#matching-7",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nThe result is more intuitive with dummy variables, but regression also keeps continuous variables constant while estimating the effect.\nAlso, with continuous variables, the ATE will point in the direction where covariates have more variance.\nSo we’ve seen that regression has its idiosyncrasies. It is linear, parametric, likes high variance features… This can be good or bad, depending on the context.\nBecause of this, it’s important to be aware of other techniques we can use to control for confounders. Not only are they an extra tool in your causal tool belt, but understanding different ways to deal with confounding expands our understanding of the problem.\nFor this reason, we’ll now examne the Subclassification Estimator!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-subclassification",
    "href": "slides/BSMM_8740_lec_08.html#matching-subclassification",
    "title": "Causality Part 2",
    "section": "Matching: subclassification",
    "text": "Matching: subclassification\n\nIn general, if there is some causal effect we want to estimate, but it is hard to do so because of confounding of some variables X, what we need to do is make the treatment vs control comparison within small groups where X is the same.\nIf we have conditional independence \\((Y^0, Y^1)\\perp D | X\\) , then we can write the ATE as follows.\n\n\\[\nATE = \\int(E[Y|X=x, D=1] - E[Y|X=x, D=0])dP(x)\n\\]\n\nWhat the integral does is it goes through all the space of the distribution of features X, computes the difference in means for all those tiny spaces and combines everything into the ATE."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-subclassification-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-subclassification-1",
    "title": "Causality Part 2",
    "section": "Matching: subclassification",
    "text": "Matching: subclassification\n\nAnother way to see this is to think about a discrete set of features.\nIn this case, we can say that values the feature set X takes on falls into K different sets \\(\\{X_1, X_2, ..., X_k\\}\\) and what we are doing is computing the treatment effect in each set and combining them into the ATE.\nIn this discrete case, converting the integral to a sum, we can derive the subclassifications estimator:\n\n\\[\n\\hat{ATE} = \\sum^K_{k=1}(\\bar{Y}^1_k - \\bar{Y}^0_k) * \\dfrac{N_k}{N}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-subclassification-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-subclassification-2",
    "title": "Causality Part 2",
    "section": "Matching: subclassification",
    "text": "Matching: subclassification\n\\[\n\\hat{ATE} = \\sum^K_{k=1}(\\bar{Y}^1_k - \\bar{Y}^0_k) * \\dfrac{N_k}{N}\n\\]\n\nAs you can see, we are computing a local ATE for each cell and combining them using a weighted average, where the weights are the sample size of the cell. In our medicine example above, this would be the first estimate, which gave us −2.6.\nThe subclassification estimator isn’t used much in practice, because of the curse of dimensionality."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\nThe subclassification estimator gives us a nice intuition of what a causal inference estimator should do, how it should control for confounders.\nThis allows us to explore other kinds of estimators, such as the Matching Estimator."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-1",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nWhen some confounder X makes it so that treated and untreated are not initially comparable, we can make them so by matching each treated unit with a similar untreated unit - finding an untreated twin for every treated unit. In making such comparisons, treated and untreated become again comparable.\nAs an example, let’s suppose we are trying to estimate the effect of a trainee program on earnings. Here is what the trainees looks like:\n\ntraineesnon trainees\n\n\n\n\nCode\ndata_trainees &lt;- readr::read_csv(\"data/trainees.csv\", show_col_types = FALSE)\n\ndata_trainees |&gt; \n  dplyr::filter(trainees==1) |&gt; \n  dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nunit\ntrainees\nage\nearnings\n\n\n\n\n1\n1\n28\n17700\n\n\n2\n1\n34\n10200\n\n\n3\n1\n29\n14400\n\n\n4\n1\n25\n20800\n\n\n5\n1\n29\n6100\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata_trainees |&gt; \n  dplyr::filter(trainees==0) |&gt; \n  dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nunit\ntrainees\nage\nearnings\n\n\n\n\n20\n0\n43\n20900\n\n\n21\n0\n50\n31000\n\n\n22\n0\n30\n21000\n\n\n23\n0\n27\n9300\n\n\n24\n0\n54\n41100"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-2",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nA simple comparison in means, identifies that the trainees earn less money than those that didn’t go through the program.\n\n\n\nCode\ndata_trainees |&gt; \n  dplyr::group_by(trainees) |&gt; \n  dplyr::summarize(mean_effect = mean(earnings)) |&gt; \n  dplyr::mutate(ATE = mean_effect - dplyr::lag( mean_effect) ) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\ntrainees\nmean_effect\nATE\n\n\n\n\n0\n20724\nNA\n\n\n1\n16426\n-4297\n\n\n\n\n\n\n\n\nHowever, if we look at the data tables, we notice that trainees are much younger than non trainees, which indicates that age is probably a confounder."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-3",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-3",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nWe can use matching on age to try to correct that.\nWe will take unit 1 from the treated and pair it with unit 27, since both are 28 years old. Unit 2 we will pair it with unit 34, unit 3 with unit 37, unit 4 we will pair it with unit 35… When it comes to unit 5, we need to find someone with age 29 from the non treated, but that is unit 37, which is already paired.\nThis is not a problem, since we can use the same unit multiple times. If more than 1 unit is a match, we can choose randomly between them.\n\n\nCode\nunique_on_age &lt;- data_trainees |&gt; \n  dplyr::filter(trainees == 0) |&gt; \n  dplyr::distinct(age, .keep_all = TRUE)\n\nmatches &lt;- data_trainees |&gt; \n  dplyr::filter(trainees == 1) |&gt; \n  dplyr::left_join(unique_on_age, by = 'age', suffix = c(\"_t_1\", \"_t_0\")) |&gt; \n  dplyr::mutate(t1_minus_t0 = earnings_t_1 - earnings_t_0) \n\nmatches |&gt; dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nunit_t_1\ntrainees_t_1\nage\nearnings_t_1\nunit_t_0\ntrainees_t_0\nearnings_t_0\nt1_minus_t0\n\n\n\n\n1\n1\n28\n17700\n27\n0\n8800\n8900\n\n\n2\n1\n34\n10200\n34\n0\n24200\n-14000\n\n\n3\n1\n29\n14400\n37\n0\n6200\n8200\n\n\n4\n1\n25\n20800\n35\n0\n23300\n-2500\n\n\n5\n1\n29\n6100\n37\n0\n6200\n-100"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-4",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-4",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nIf we take the mean of this last column we get the ATET estimate while controlling for age. Notice how the estimate is now very positive, compared to the previous one where we used a simple difference in means.\n\n\n\nCode\nmatches |&gt; dplyr::summarize(ATE = mean(t1_minus_t0)) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nATE\n\n\n\n\n2458\n\n\n\n\n\n\n\n\nThis is a contrived example, just to introduce matching."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-5",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-5",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nIn practice, we usually have more than one feature and units don’t match perfectly. In this case, we have to define some measurement of proximity to compare how units are close to each other.\nOne common metric for this is the euclidean norm \\(||X_i - X_j||\\). This difference, however, is not invariant to the scale of the features.\nThis means that features like age, that take values on the tenths, will be much less important when computing this norm compared to features like income, which take the order of hundreds.\nFor this reason, before applying the norm, we need to scale the features so that they are on roughly the same scale."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-6",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-6",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nHaving defined a distance measure, we can now define the match as the nearest neighbour to that sample we wish to match.\nWe can write the matching estimator the following way:\n\n\\[\n\\hat{ATE} = \\frac{1}{N} \\sum^N_{i=1} (2D_i - 1)\\big(Y_i - Y_{jm}(i)\\big)\n\\]\n\nWhere \\(Y_{jm}(i)\\) is the sample from the other treatment group which is most similar to \\(Y_i\\).\nWe scale by \\(2D_i - 1\\) to match both ways: treated with controls and controls with the treatment."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-7",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-7",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nTo test this estimator, let’s consider a medicine example.\nOnce again, we want to find the effect of a medication on days until recovery.\nUnfortunately, this effect is confounded by severity, sex and age. We have reasons to believe that patients with more severe conditions have a higher chance of receiving the medicine.\n\n\n\nCode\ndata_med &lt;- readr::read_csv(\"data/medicine_impact_recovery.csv\", show_col_types = FALSE)\n\ndata_med |&gt; \n  dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nsex\nage\nseverity\nmedication\nrecovery\n\n\n\n\n0\n35.05\n0.8877\n1\n31\n\n\n1\n41.58\n0.8998\n1\n49\n\n\n1\n28.13\n0.4863\n0\n38\n\n\n1\n36.38\n0.3231\n0\n35\n\n\n0\n25.09\n0.2090\n0\n15"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-8",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-8",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nIf we look at a simple difference in means, \\(E[Y|D=1]-E[Y|D=0]\\), we get that the treated take, on average, 16.9 more days to recover than the untreated.\nThis is probably due to confounding, since we don’t expect the medicine to cause harm to the patient.\n\n\n\nCode\ndata_med |&gt; \n  dplyr::group_by(medication) |&gt; \n  dplyr::summarize(mean_effect = mean(recovery)) |&gt; \n  dplyr::mutate(ATE = mean_effect - dplyr::lag( mean_effect) ) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nmedication\nmean_effect\nATE\n\n\n\n\n0\n21.68\nNA\n\n\n1\n38.57\n16.9"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-9",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-9",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nTo correct for this bias, we will control for X using matching.\nFirst, we need to remember to scale our features, otherwise, features like age will have higher importance than features like severity when we compute the distance between points.\nTo do so, we can standardise the features.\n\n\n\nCode\ndata_med &lt;- data_med |&gt; recipes::recipe(recovery ~ .) |&gt; \n  recipes::update_role(medication, new_role = 'treatment') |&gt; \n  recipes::step_normalize(recipes::all_predictors()) |&gt; \n  recipes::prep() |&gt; \n  recipes::bake(new_data=NULL)\n\ndata_med |&gt;   \n  dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nsex\nage\nseverity\nmedication\nrecovery\n\n\n\n\n-0.997\n0.2808\n1.4598\n1\n31\n\n\n1.003\n0.8654\n1.5022\n1\n49\n\n\n1.003\n-0.3387\n0.0578\n0\n38\n\n\n1.003\n0.3995\n-0.5126\n0\n35\n\n\n-0.997\n-0.6105\n-0.9111\n0\n15"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-10",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-10",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nInstead of coding a matching function, we will use the K nearest neighbour algorithm from caret::knnreg.\nThis algorithm makes predictions by finding the nearest data point in an estimation or training set.\nFor matching, we will need 2 of those. One, \\(mt_0\\), will store the untreated points and will find matches in the untreated when asked to do so.\nThe other, \\(mt_1\\), will store the treated point and will find matches in the treated when asked to do so. After this fitting step, we can use these KNN models to make predictions, which will be our matches."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-11",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-11",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\n\nCode\ntreated   &lt;- data_med |&gt; dplyr::filter(medication==1)\nuntreated &lt;- data_med |&gt; dplyr::filter(medication==0)\n\nmt0 &lt;- # untreated knn model predicting recovery\n  caret::knnreg(x = untreated |&gt; dplyr::select(sex,age,severity), y = untreated$recovery, k=1)\nmt1 &lt;- # treated knn model predicting recovery\n  caret::knnreg(x = treated |&gt; dplyr::select(sex,age,severity), y = treated$recovery, k=1)\n\npredicted &lt;-\n  # combine the treated and untreated matches\n  c(\n    # find matches for the treated looking at the untreated knn model\n    treated |&gt;\n      tibble::rowid_to_column(\"ID\") |&gt; \n      {\\(y)split(y,y$ID)}() |&gt; # hack for native pipe \n      # split(.$ID) |&gt;         # this vesion works with magrittr\n      purrr::map(\n        (\\(x){\n          x |&gt; \n            dplyr::mutate(\n              match = predict( mt0, x[1,c('sex','age','severity')] )\n            )\n        })\n      )\n    # find matches for the untreated looking at the treated knn model\n    , untreated |&gt;\n        tibble::rowid_to_column(\"ID\") |&gt; \n        {\\(y)split(y,y$ID)}() |&gt; \n        # split(.$ID) |&gt; \n        purrr::map(\n          (\\(x){\n            x |&gt; \n              dplyr::mutate(\n                match = predict( mt1, x[1,c('sex','age','severity')] )\n              )\n          })\n        )\n  ) |&gt;\n  # bind the treated and untreated data \n  dplyr::bind_rows()\n\npredicted |&gt;   \n  dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(columns = c('sex','age','severity'), decimals = 6) |&gt;\n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nID\nsex\nage\nseverity\nmedication\nrecovery\nmatch\n\n\n\n\n1\n−0.996980\n0.280787\n1.459800\n1\n31\n39\n\n\n2\n1.002979\n0.865375\n1.502164\n1\n49\n52\n\n\n3\n−0.996980\n1.495134\n1.268540\n1\n38\n46\n\n\n4\n1.002979\n−0.106534\n0.545911\n1\n34\n45\n\n\n5\n−0.996980\n0.043034\n1.428732\n1\n30\n39"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-12",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-12",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nWith the matches, we can now apply the matching estimator formula\n\n\\[\n\\hat{ATE} = \\frac{1}{N} \\sum^N_{i=1} (2D_i - 1)\\big(Y_i - Y_{jm}(i)\\big)\n\\]\n\n\nCode\npredicted |&gt; \n  dplyr::summarize(\n    \"ATE (est)\" = mean( (2*medication - 1) * (recovery - match) )\n  ) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nATE (est)\n\n\n\n\n-0.9954\n\n\n\n\n\n\n\n\nUsing this sort of matching, we can see that the effect of the medicine is not positive anymore.\nThis means that, controlling for \\(X\\), the medicine reduces the recovery time by about 1 day, on average.\nThis is already a huge improvement on top of the biased estimate that predicted a 16.9 increase in recovery time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nIt turns out the matching estimator as we’ve designed above is biased.\nTo see this, let’s consider the ATET estimator, instead of the ATE, just because it is simpler to write.\nThe intuition will apply to the ATE as well.\n\n\\[\n\\hat{ATET} = \\frac{1}{N_1}\\sum (Y_i - Y_j(i))\n\\]\n\nwhere \\(N_1\\) is the number of treated individuals and \\(Y_j(i)\\) is the untreated match of treated unit i."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-1",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nTo check for bias, what we do is hope we can apply the Central Limit Theorem so that this estimate converges to a normal distribution with mean zero.\n\n\\[\n\\sqrt{N_1}(\\hat{ATET} - ATET)\n\\]\n\nHowever, this doesn’t alway happen. If we define the mean outcome for the untreated given X, \\(\\mu_0(x)=E[Y|X=x, D=0]\\), we will have that\n\n\\[\n\\begin{align*}\n\\mathbb{E}\\left [\\sqrt{N_1}(\\hat{ATET} - ATET)\\right] & = \\\\\n\\mathbb{E}\\left[\\sqrt{N_1}(\\mu_0(X_i) - \\mu_0(X_j(i))\\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-2",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nNow, \\(\\mu_0(X_i) - \\mu_0(X_j(i))\\) is not so simple to understand, so let’s look at it more carefully.\n\n\\(\\mu_0(X_i)\\) is the outcome Y value of a treated unit \\(i\\) had it not been treated. It is the counterfactual outcome \\(Y^0\\) for unit \\(i\\).\n\\(\\mu_0(X_j(i))\\) is the outcome of the untreated unit \\(j\\) that is the match of unit \\(i\\).\nIt is also the \\(Y^0\\), but for unit \\(j\\) now.\nOnly this time, it is a factual outcome, because \\(j\\) is in the non treated group.\nNow, because \\(j\\) and \\(i\\) are only similar, but not the same, this will likely not be zero. In other words, \\(X_i \\approx X_j\\), so, \\(Y^0_i \\approx Y^0_j\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-3",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-3",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nAs we increase the sample size, there will be more units to match, so the difference between unit \\(i\\) and its match \\(j\\) will also get smaller.\nBut this difference converges to zero slowly.\nAs a result \\(E[\\sqrt{N_1}(\\mu_0(X_i) - \\mu_0(X_j(i)))]\\) may not converge to zero, because the \\(\\sqrt{N_1}\\) grows faster than \\((\\mu_0(X_i) - \\mu_0(X_j(i)))\\) diminishes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-4",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-4",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nBias arises when the matching discrepancies are huge. Fortunately, we know how to correct it. Each observation contributes \\((\\mu_0(X_i) - \\mu_0(X_j(i))\\) to the bias so all we need to do is subtract this quantity from each matching comparison in our estimator.\nTo do so, we can replace \\(\\mu_0(X_j(i))\\) with some sort of estimate of this quantity \\(\\hat{\\mu}_0(X_j(i))\\), which can be obtained with models like linear regression. This updates the ATET estimator to the following equation:\n\n\\[\n\\hat{ATET} = \\frac{1}{N_1}\\sum \\big((Y_i - Y_{j(i)}) - (\\hat{\\mu_0}(X_i) - \\hat{\\mu_0}(X_{j(i)})\\big)\n\\]\n\nwhere \\(\\hat{\\mu_0}(x)\\) is some estimative of \\(E[Y|X, D=0]\\), like a linear regression fitted only on the untreated sample."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-5",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-5",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\n\nCode\nols0 &lt;- lm(recovery ~ sex + age + severity, data = untreated) \nols1 &lt;- lm(recovery ~ sex + age + severity, data = treated) \n\n# find the units that match to the treated\ntreated_match_index &lt;- # RANN::nn2 does Nearest Neighbour Search\n  (RANN::nn2(mt0$learn$X, treated |&gt; dplyr::select(sex,age,severity), k=1))$nn.idx |&gt; \n  as.vector()\n\n# find the units that match to the untreated\nuntreated_match_index &lt;- # RANN::nn2 does Nearest Neighbour Search\n  (RANN::nn2(mt1$learn$X, untreated |&gt; dplyr::select(sex,age,severity), k=1))$nn.idx |&gt; \n  as.vector()\n\npredicted &lt;- \nc(\n  purrr::map2(\n    .x = \n      treated |&gt; tibble::rowid_to_column(\"ID\") |&gt; {\\(y)split(y,y$ID)}() # split(.$ID) \n    , .y = treated_match_index\n    , .f = (\\(x,y){\n          x |&gt; \n            dplyr::mutate(\n              match = predict( mt0, x[1,c('sex','age','severity')] )\n              , bias_correct =\n                  predict( ols0, x[1,c('sex','age','severity')] ) -\n                  predict( ols0, untreated[y,c('sex','age','severity')] )\n            )\n        })\n  )\n  , purrr::map2(\n    .x = \n      untreated |&gt; tibble::rowid_to_column(\"ID\") |&gt; {\\(y)split(y,y$ID)}() # split(.$ID) \n    , .y = untreated_match_index\n    , .f = (\\(x,y){\n          x |&gt; \n            dplyr::mutate(\n              match = predict( mt1, x[1,c('sex','age','severity')] )\n              , bias_correct =\n                  predict( ols1, x[1,c('sex','age','severity')] ) -\n                  predict( ols1, treated[y,c('sex','age','severity')] )\n            )\n        })\n  )\n) |&gt; \n  # bind the treated and untreated data \n  dplyr::bind_rows()\n\npredicted |&gt;   \n  dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(columns = c('sex','age','severity'), decimals = 6) |&gt;\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nID\nsex\nage\nseverity\nmedication\nrecovery\nmatch\nbias_correct\n\n\n\n\n1\n−0.996980\n0.280787\n1.459800\n1\n31\n39\n4.404\n\n\n2\n1.002979\n0.865375\n1.502164\n1\n49\n52\n12.915\n\n\n3\n−0.996980\n1.495134\n1.268540\n1\n38\n46\n1.871\n\n\n4\n1.002979\n−0.106534\n0.545911\n1\n34\n45\n-0.497\n\n\n5\n−0.996980\n0.043034\n1.428732\n1\n30\n39\n2.610"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-6",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-6",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nDoesn’t this defeat the point of matching? If I have to run a linear regression anyway, why don’t I use only that, instead of this complicated model.\nFirst of all, this linear regression that we are fitting doesn’t extrapolate on the treatment dimension to get the treatment effect. Instead, its purpose is just to correct bias.\nLinear regression here is local, in the sense that it doesn’t try to see how the treated would be if it looked like the untreated. It does none of that extrapolation. This is left to the matching part.\nThe meat of the estimator is still the matching component. The point is that OLS is secondary to this estimator."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-7",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-7",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nThe second point is that matching is a non-parametric estimator. It doesn’t assume linearity or any kind of parametric model.\nAs such, it is more flexible than linear regression and can work in situations where linear regression will not, namely, those where non linearity is very strong."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-8",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-8",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\nWith the bias correction formula, I get the following ATE estimation.\n\npredicted |&gt; \n  dplyr::summarize(\n    \"ATE (est)\" = \n      mean( (2*medication - 1) * (recovery - match - bias_correct) )) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nATE (est)\n\n\n\n\n-7.363"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-ci",
    "href": "slides/BSMM_8740_lec_08.html#matching-ci",
    "title": "Causality Part 2",
    "section": "Matching CI",
    "text": "Matching CI\n\nOf course, we also need to place a confidence interval around this measurement.\nIn practice, we can simply use someone else’s code and just import a matching estimator. Here is one from the library Matching.\n\n\n\nCode\nrequire(Matching)\n#  See https://www.jsekhon.com for additional documentation.\nc(\"ATE\", \"ATC\", \"ATT\") |&gt; \n  purrr::map(\n    (\\(x){\n      pairmatching &lt;- Matching::Match(\n        Y = data_med$recovery\n        , Tr = data_med$medication\n        , X = data_med |&gt; dplyr::select('sex','age','severity')\n        , estimand = x\n        , BiasAdjust = TRUE\n      )\n      tibble::tibble(measure = x, est = pairmatching$est[1,1], se = pairmatching$se)\n    })\n  ) |&gt; \n  dplyr::bind_rows() |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(columns = c('est','se'), decimals = 3) |&gt;\n  gt::tab_header(\n    title = \"Treatment Effect Estimates: Matching\"\n    , subtitle = \"using R package Matching\"\n  ) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nTreatment Effect Estimates: Matching\n\n\nusing R package Matching\n\n\nmeasure\nest\nse\n\n\n\n\nATE\n−7.708\n0.962\n\n\nATC\n−6.664\n1.644\n\n\nATT\n−9.680\n0.123\n\n\n\n\n\n\n\n\nFinally, we can say with confidence that our medicine does indeed lower the time someone spends at the hospital. The ATE estimate is just a little bit lower than our algorithm, due to the difference in tie breaking of matches of knn implementation and the Matching R package."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-again",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-again",
    "title": "Causality Part 2",
    "section": "Matching bias again",
    "text": "Matching bias again\n\nWe saw that matching is biased when the unit and its match are not so similar. But what causes them to be so different?\nAs it turns out, the answer is quite simple and intuitive. It is easy to find people that match on a few characteristics, like sex. But if we add more characteristics, like age, income, city of birth and so on, it becomes harder and harder to find matches. In more general terms, the more features we have, the higher will be the distance between units and their matches.\nThis is not something that hurts only the matching estimator. It ties back to the subclassification estimator we saw earlier. In that contrived medicine example where with man and woman, it was quite easy to build the subclassification estimator. That was because we only had 2 cells: man and woman. But what would happen if we had more?\nLet’s say we have 2 continuous features like age and income and we manage to discretise them into 5 buckets each. This will give us 25 cells, or \\(5^2\\). And what if we had 10 covariates with 3 buckets each? Doesn’t seem like a lot right? Well, this would give us 59049 cells, or \\(3^{10}\\). It’s easy to see how this can blow out of proportion pretty quickly. This is a phenomena pervasive in all data science, which is called the The Curse of Dimensionality!!!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-again-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-again-1",
    "title": "Causality Part 2",
    "section": "Matching bias again",
    "text": "Matching bias again\n\nIn the context of the subclassification estimator, the curse of dimensionality means that it will suffer if we have lots of features.\nLots of features imply multiple cells in X. If there are multiple cells, some of them will have very few data. Some of them might even have only treated or only control, so it won’t be possible to estimate the ATE there, which would break our estimator.\nIn the matching context, this means that the feature space will be very sparse and units will be very far from each other. This will increase the distance between matches and cause bias problems."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-again-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-again-2",
    "title": "Causality Part 2",
    "section": "Matching bias again",
    "text": "Matching bias again\n\nAs for linear regression, it actually handles this problem quite well.\nWhat it does is project all the features X into a single one, the Y dimension. It then makes treatment and control comparison on that projection.\nSo, in some way, linear regression performs some sort of dimensionality reduction to estimate the ATE. It’s quite elegant."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-1",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-1",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nOne problem we might have is that we can’t really control for things if we can’t measure them\nAnd there are lots of things we can’t measure or don’t have data for!\nSo what can we do?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-2",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-2",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nThe Solution\n\nIf we observe each entity/person/firm/country multiple times, then we can forget about controlling for the actual back-door variable we’re interested in\nAnd just control for entity/person/firm/country identity instead!\nThis will control for EVERYTHING unique to that individual, whether we can measure it or not!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-3",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-3",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nConsider data that tracks life expectancy and GDP per capita in many countries over time.\n\ndata(gapminder, package = 'gapminder')\n\ngapminder &lt;- gapminder |&gt; dplyr::group_by(country) |&gt; \n  dplyr::mutate(\n    lifeExp.r = lifeExp - mean(lifeExp),\n    logGDP.r = log(gdpPercap) - mean(log(gdpPercap))\n  ) |&gt; \n  dplyr::ungroup()\n\nand then group by country, and estimate the effect of GDP per capita on life expectancy, controlling for country."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-4",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-4",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nNote that there are LOTS of things that might be back doors between GDP per capita and life expectancy\nWar, disease, political institutions, trade relationships, health of the population, economic institutions…"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-5",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-5",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\n\nCode\ndag &lt;- ggdag::dagify(\n  LifeEx~GDPpc+A+B+C+D+E+F+G+H,\n  GDPpc~A+B+C+D+E+F+G+H,\n  coords=list(\n    x=c(LifeEx=4,GDPpc=2,A=1,B=2,C=3,D=4,E=1,F=2,G=3,H=4),\n    y=c(LifeEx=2,GDPpc=2,A=3,B=3,C=3,D=3,E=1,F=1,G=1,H=1)\n  )\n)  |&gt;  ggdag::tidy_dagitty()\nggdag::ggdag_classic(dag,node_size=20) + \n  ggdag::theme_dag_blank()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-6",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-6",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nThere’s no way we can identify this\nThe list of back doors is very long\nAnd likely includes some things we can’t measure!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-7",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-7",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nHOWEVER! If we think that these things are likely to be constant within country…\nThen we don’t really have a big long list of back doors, we just have one: “country”"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-8",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-8",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nWe can now identify our effect even if some of our back doors include variables that we can’t actually measure\nWhen we do this, we’re basically comparing countries to themselves at different time periods!\nPretty good way to do an apples-to-apples comparison!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-9",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-9",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-10",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-10",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\n\nThe post-fixed-effects dots are basically a bunch of “Raw Country X” pasted together.\nImagine taking “Raw Pakistan” and moving it to the center, then taking “Raw Britain” and moving it to the center, etc.\nIgnoring the baseline differences between Pakistan, Britain, China, etc., in their GDP per capita and life expectancy, and just looking within each country.\nWe are ignoring all differences between countries (since that way back doors lie!) and looking only at differences within countries.\nFixed Effects is sometimes also referred to as the “within” estimator"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-11",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-11",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-12",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-12",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nThis does assume, of course, that all those back door variables CAN be described by country\nIn other words, that these back doors operate by things that are fixed within country\nIf something is a back door and changes over time in that country, fixed effects won’t help!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-13",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-13",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nFor example, earlier we mentioned war… that’s not fixed within country! A given country is at war sometimes and not other times."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-14",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-14",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\n\nOf course, in this case, we could control for War as well and be good!\nTime-varying things doesn’t mean that fixed effects doesn’t work, it just means you need to control for that stuff too\nIt always comes down to thinking carefully about your diagram\nFixed effects mainly works as a convenient way of combining together lots of different constant-within-country back doors into something that lets us identify the model even if we can’t measure them all"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-regression",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-regression",
    "title": "Causality Part 2",
    "section": "Fixed Effects Regression",
    "text": "Fixed Effects Regression\n\nWe can just do fixed effects as we did-subtract out the group means and analyze (perhaps with regression) what’s left, or\nWe can also include dummy variables for each group/individual, which accomplishes the same thing\n\n\\[ Y = \\beta_0 + \\beta_1Group1 + \\beta_2Group2 + ... + \\] \\[ \\beta_NGroupN + \\beta_{N+1}X + \\varepsilon \\] \\[ Y = \\beta_i + \\beta_1X + \\varepsilon \\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-regression-1",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-regression-1",
    "title": "Causality Part 2",
    "section": "Fixed Effects Regression",
    "text": "Fixed Effects Regression\n\nWhy does that work?\nWe want to “control for group/individual” right? So… just… put in a control for group/individual\nOf course, like all categorical variables as predictors, we leave out a reference group\nBut here, unlike with, say, a binary predictor, we’re rarely interested in the FE coefficients themselves. Most software works with the mean-subtraction approach (or a variant) and don’t even report them!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-regression-variation",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-regression-variation",
    "title": "Causality Part 2",
    "section": "Fixed Effects Regression: Variation",
    "text": "Fixed Effects Regression: Variation\n\nRemember we are isolating within variation\nIf an individual has no within variation, say their treatment never changes, they basically get washed out entirely!\nA fixed-effects regression wouldn’t represent them. And can’t use FE to study things that are fixed over time\nAnd in general if there’s not a lot of within variation, FE is going to be very noisy. Make sure there’s variation to study!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-in-regression-notes",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-in-regression-notes",
    "title": "Causality Part 2",
    "section": "Fixed Effects in Regression: Notes",
    "text": "Fixed Effects in Regression: Notes\n\nIt’s common to cluster standard errors at the level of the fixed effects, since it seems likely that errors would be correlated over time (autocorrelated errors)\nIt’s possible to have more than one set of fixed effects. \\(Y = \\beta_i + \\beta_j + \\beta_1X + \\varepsilon\\)\nBut interpretation gets tricky - think through what variation in \\(X\\) you’re looking at at that point!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#coding-up-fixed-effects",
    "href": "slides/BSMM_8740_lec_08.html#coding-up-fixed-effects",
    "title": "Causality Part 2",
    "section": "Coding up Fixed Effects",
    "text": "Coding up Fixed Effects\n\nWe can use the fixest package\nIt’s very fast, and can be easily adjusted to do FE with other regression methods like logit, or combined with instrumental variables\nClusters at the first listed fixed effect by default\n\n\nm1 &lt;- fixest::feols(outcome ~ predictors | FEs, data = data)\nmsummary(m1)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fe-example-sentencing",
    "href": "slides/BSMM_8740_lec_08.html#fe-example-sentencing",
    "title": "Causality Part 2",
    "section": "FE Example: Sentencing",
    "text": "FE Example: Sentencing\n\nWhat effect do sentencing reforms have on crime?\nOne purpose of punishment for crime is to deter crime\nIf sentences are more clear and less risky, that may reduce a deterrent to crime and so increase crime\nMarvell & Moody study this using data on reforms in US states from 1969-1989"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fe-example-sentencing-1",
    "href": "slides/BSMM_8740_lec_08.html#fe-example-sentencing-1",
    "title": "Causality Part 2",
    "section": "FE Example: Sentencing",
    "text": "FE Example: Sentencing\n\nIn our data we have multiple observations per state\n\n\n\n# A tibble: 6 × 6\n  state   year assault robbery pop1000 sentreform\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 \"ALA \"    70    7413    1731    3450          0\n2 \"ALA \"    71    7645    2005    3497          0\n3 \"ALA \"    72    7431    2407    3540          0\n4 \"ALA \"    73    8362    2809    3581          0\n5 \"ALA \"    74    8429    3562    3628          0\n6 \"ALA \"    75    8440    4446    3681          0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-15",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-15",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nWe can see how robbery rates evolve in each state over time as states implement reform"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-16",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-16",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nYou can tell that states are more or less likely to implement reform in a way that’s correlated with the level of robbery they already had\nSo SOMETHING about the state is driving both the level of robberies AND the decision to implement reform\nWho knows what!\nOur diagram has reform -&gt; robberies and reform &lt;- state -&gt; robberies, which is something we can address with fixed effects."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#fixed-effects-17",
    "href": "slides/BSMM_8740_lec_08.html#fixed-effects-17",
    "title": "Causality Part 2",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                OLS\n                FE\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  1.254***\n                           \n                \n                \n                             \n                  (0.036) \n                           \n                \n                \n                  sentreform \n                  0.352***\n                  0.245**  \n                \n                \n                             \n                  (0.082) \n                  (0.076)  \n                \n                \n                  Num.Obs.   \n                  1000    \n                  1000     \n                \n                \n                  R2         \n                  0.018   \n                  0.919    \n                \n                \n                  R2 Within  \n                          \n                  0.062    \n                \n                \n                  RMSE       \n                  1.02    \n                  0.29     \n                \n                \n                  Std.Errors \n                          \n                  by: state"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#example",
    "href": "slides/BSMM_8740_lec_08.html#example",
    "title": "Causality Part 2",
    "section": "Example",
    "text": "Example\n\nThe 1.254, 0.352 included the fact that different kinds of states tend to institute reform\nThe 0.245 doesn’t!\nLooks like the deterrent effect was real! Although important to consider if there might be time-varying back doors too, we don’t account for those in our analysis\nWhat things might change within state over time that would be related to robberies and to sentencing reform?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#event-studies-1",
    "href": "slides/BSMM_8740_lec_08.html#event-studies-1",
    "title": "Causality Part 2",
    "section": "Event studies",
    "text": "Event studies\n\nEvent studies examine how outcomes evolve around the timing of specific events or policy interventions. Event studies can serve as a standalone causal inference method by exploiting sharp temporal variation in treatment timing.\nIf treatment timing is as-good-as-random (conditional on observables), then deviations from pre-treatment trends after the event can be causally attributed to treatment."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#event-studies-2",
    "href": "slides/BSMM_8740_lec_08.html#event-studies-2",
    "title": "Causality Part 2",
    "section": "Event studies",
    "text": "Event studies\nRegression specification\n\\[Y_{it} = α_i + λ_t + Σ_{k≠-1} β_k × D_{it}^k + ε_{it}\\] Where \\(D_{it}^k = 1\\) if unit \\(i\\) is \\(k\\) periods from treatment at time \\(t\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#event-studies-3",
    "href": "slides/BSMM_8740_lec_08.html#event-studies-3",
    "title": "Causality Part 2",
    "section": "Event studies",
    "text": "Event studies\nWhen Event Studies Work as Causal Inference\n\nSharp timing: Clear before/after distinction (regulatory changes, court decisions, natural disasters)\nExogenous timing: Treatment timing uncorrelated with potential outcomes\nStable trends: Outcomes would follow predictable path absent treatment\nNo anticipation: Agents don’t adjust behavior before official treatment"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#event-studies-4",
    "href": "slides/BSMM_8740_lec_08.html#event-studies-4",
    "title": "Causality Part 2",
    "section": "Event studies",
    "text": "Event studies\n\nValidity Tests and Diagnostics\n\nPre-trend Analysis: Examine \\(β_k\\) for \\(k &lt; 0\\)\nShould be statistically zero if assumptions hold\nGradual buildup suggests anticipation or confounding trends\n\nPlacebo Tests:\n\nEstimate “fake” treatment at random times in pre-period\nApply treatment to similar but untreated units\n\nRobustness Checks:\n\nVary event window length\nTest sensitivity to functional form assumptions\nExamine heterogeneity across subgroups"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#example-1",
    "href": "slides/BSMM_8740_lec_08.html#example-1",
    "title": "Causality Part 2",
    "section": "Example",
    "text": "Example\n\n\nCode\nset.seed(10)\n# Create data with 20 groups and 10 time periods\ndf &lt;- tidyr::crossing(id = 1:20, t = 1:10) |&gt;\n # Add an event in period 6 with a one-period positive effect\n dplyr::mutate(Y = rnorm(dplyr::n()) + 1*(t == 6))\n\n# Use i() in feols to include time dummies,\n# specifying that we want to drop t = 5 as the reference\nm &lt;- fixest::feols(Y ~ i(t, ref = 5), data = df, cluster = 'id')\n\n# Plot the results, except for the intercept,# and add a line joining \n# them and a space and line for the reference group\nfixest::coefplot(m, drop = '(Intercept)',\n pt.join = TRUE, ref = c('t:5' = 6), ref.line = TRUE)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-2x2-did",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-2x2-did",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences (2x2 DiD)",
    "text": "Difference-in-Differences (2x2 DiD)\n\nPanel data on \\(Y_{it}\\) for \\(t=1,2\\) and \\(i = 1,...,N\\)\nTreatment timing: Some units (\\(D_i=1\\)) are treated in period \\(2\\); every other unit is untreated \\((D_i=0)\\)\nPotential outcomes (POs): Observe \\(Y_{it}(1) \\equiv Y_{it}(0,1)\\) for treated units; and \\(Y_{it}(0) \\equiv Y_{it}(0,0)\\) for comparison."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#did-estimation-and-inference",
    "href": "slides/BSMM_8740_lec_08.html#did-estimation-and-inference",
    "title": "Causality Part 2",
    "section": "DiD Estimation and Inference",
    "text": "DiD Estimation and Inference\n\nThe most conceptually simple estimator replaces population means with sample analogs: \\[\\hat{\\tau}_{DiD} = (\\bar{Y}_{12} - \\bar{Y}_{11}) - (\\bar{Y}_{02} - \\bar{Y}_{01}) \\] where \\(\\bar{Y}_{dt}\\) is sample mean for group \\(d\\) in period \\(t\\)\nThis is also know as “4 averages and three subtractions”\nCausally we evaluate before treatment vs after treatment"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#did-estimation-and-inference-1",
    "href": "slides/BSMM_8740_lec_08.html#did-estimation-and-inference-1",
    "title": "Causality Part 2",
    "section": "DiD Estimation and Inference",
    "text": "DiD Estimation and Inference\n\nConveniently, \\(\\hat\\tau_{DiD}\\) is algebraically equal to OLS coefficient \\(\\hat\\beta\\) from \\[\\begin{align*}\nY_{it} = \\alpha_i + \\phi_t + D_{it} \\beta  + \\epsilon_{it},\n\\end{align*}\\] where \\(D_{it} = D_i * 1[t=2]\\). Also equivalent to \\(\\beta\\) from \\(\\Delta Y_{i} = \\alpha +  \\Delta D_i \\beta + u_{it}\\).\nInference: And clustered standard errors are valid as number of clusters grows large."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#did-estimation-and-inference-2",
    "href": "slides/BSMM_8740_lec_08.html#did-estimation-and-inference-2",
    "title": "Causality Part 2",
    "section": "DiD Estimation and Inference",
    "text": "DiD Estimation and Inference\nAggregating individuals into treated and untreated groups:\n\\[\\begin{align*}\nY_{it} = \\alpha_i + \\beta_1 D_i + \\beta_2\\text{post}_t + \\delta (D_i\\times\\text{post}_t) + \\epsilon_{it}\n\\end{align*}\\]\n\\(Y_{i0}^0=\\alpha_i\\), \\(Y_{i1}^0=\\alpha_i+\\beta_2\\) and \\(Y_{i0}^1=\\alpha_i + \\beta_1\\), \\(Y_{i1}^1=\\alpha_i+\\beta_1+\\beta_2+\\delta\\)\nwith \\(Y_{i1}^0-Y_{i0}^0=\\beta_2\\) and \\(Y_{i1}^1-Y_{i0}^1=\\beta_2+\\delta\\)\nso \\((Y_{i1}^1-Y_{i0}^1) - (Y_{i1}^0-Y_{i0}^0)=\\delta\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#did-causal-inference",
    "href": "slides/BSMM_8740_lec_08.html#did-causal-inference",
    "title": "Causality Part 2",
    "section": "DiD Causal Inference",
    "text": "DiD Causal Inference\nUnder what conditions does DiD have a causal interpretation?\nOur estimator is \\[E[Y_{i2}(1) - Y_{i1}(1)| D_i =1] - E[Y_{i2}(0) - Y_{i1}(0)| D_i =0]\\]\nIf we add zero\n\\[\n\\begin{align*}\nDID & =E[Y_{i2}(1)-Y_{i1}(1)]-E[Y_{i2}(0)-Y_{i1}(0)]\\\\\n& =E[Y_{i2}(1)-Y_{i1}(1)|D_{i}=1]-E[Y_{i2}(0)-Y_{i1}(0)|D_{i}=0]+\\\\\n& E\\left[Y_{i2}(1)|D_{i}=0\\right]-E\\left[Y_{i2}(1)|D_{i}=0\\right]\\\\\n& =E[Y_{i2}(1)|D_{i}=1]-E\\left[Y_{i2}(1)|D_{i}=0\\right]+\\\\\n& E[Y_{i2}(1)-Y_{i1}(1)|D_{i}=0]-E[Y_{i2}(0)-Y_{i1}(0)|D_{i}=0]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#causal-inference",
    "href": "slides/BSMM_8740_lec_08.html#causal-inference",
    "title": "Causality Part 2",
    "section": "Causal Inference",
    "text": "Causal Inference\nSo for our DiD estimator to be a causal estimate (ATT) we need\n\nNo anticipation: \\(E[Y_{i1}(1)|D_{i}=0] = E[Y_{i1}(0)|D_{i}=0]\\)\nParallel trends: \\(0=E[Y_{i2}(1)-Y_{i1}(1)|D_{i}=0]-E[Y_{i2}(0)-Y_{i1}(0)|D_{i}=0]\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#causal-inference-1",
    "href": "slides/BSMM_8740_lec_08.html#causal-inference-1",
    "title": "Causality Part 2",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nNo anticipation: no treatment before treatment, so make sure your baseline is untreated (be aware of ”announcement dates” vs ”implementation dates”)\nParallel trends: this needs to be supported by data, e.g. pre-trends or trends in similar groups"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#causal-inference-2",
    "href": "slides/BSMM_8740_lec_08.html#causal-inference-2",
    "title": "Causality Part 2",
    "section": "Causal Inference",
    "text": "Causal Inference\n\\[\n\\begin{align*}\nDID & =\\text{ATT}_{\\text{post}} \\\\\n& + \\text{non PT bias}\\\\\n& + \\text{ATT}_{\\text{pre}}\\,\\text{bias}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#did",
    "href": "slides/BSMM_8740_lec_08.html#did",
    "title": "Causality Part 2",
    "section": "DiD",
    "text": "DiD\nThe 2x2 method is interpreted as before and after, but these periods can be multiple time periods"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#test",
    "href": "slides/BSMM_8740_lec_08.html#test",
    "title": "Causality Part 2",
    "section": "test",
    "text": "test\n\n\n\n# A tibble: 4 × 3\n# Groups:   post [2]\n   post treat mean_l_homicide\n  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1     0     0            1.25\n2     0     1            1.79\n3     1     0            1.20\n4     1     1            1.80\n\n\n[1] 0.06824\n\n\nOLS estimation, Dep. Var.: l_homicide\nObservations: 462\nStandard-errors: Clustered (state) \n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)  1.25185    0.10268 12.1918 3.2179e-15 ***\npost::1     -0.05540    0.03302 -1.6775 1.0105e-01    \ntreat::1     0.53422    0.17048  3.1337 3.1842e-03 ** \npost:treat   0.06824    0.08417  0.8107 4.2221e-01    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.532721   Adj. R2: 0.189901\n\n\nOLS estimation, Dep. Var.: l_homicide\nObservations: 462\nWeights: popwt\nStandard-errors: Clustered (treat) \n            Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept)  1.56018  6.970e-16  2.237e+15 2.8452e-16\npost::1     -0.09338  5.570e-16 -1.676e+14 3.7992e-15\ntreat::1     0.35026  7.090e-16  4.939e+14 1.2889e-15\npost:treat   0.04565  1.095e-15  4.170e+13 1.5267e-14\n               \n(Intercept) ***\npost::1     ***\ntreat::1    ***\npost:treat  ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 962.9   Adj. R2: 0.134205"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#longitudinal-data",
    "href": "slides/BSMM_8740_lec_08.html#longitudinal-data",
    "title": "Causality Part 2",
    "section": "Longitudinal Data",
    "text": "Longitudinal Data\n\nTwo types of longitudinal data:\n\nPanel Data: same units tracked over time (e.g., National Longitudinal Survey of Youth 1997)\n\nRepeated Cross-Sections: different units sampled at each time (e.g., Census, Current Population Survey)\n\nViolations of parallel trends can arise differently across data types."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#longitudinal-data-1",
    "href": "slides/BSMM_8740_lec_08.html#longitudinal-data-1",
    "title": "Causality Part 2",
    "section": "Longitudinal Data",
    "text": "Longitudinal Data\nWe have a balanced panel is when all units are observed in every period.\n\nImbalanced panel is when units missing in some periods.\n\nAnthony is in periods 1-3,\nBob is in periods 1-3,\nInez is in periods 1 and 3 only,\nDignan is in periods 1 and 2 only\n\nMissingness alone does not violate parallel trends, though it does change the parameter."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#longitudinal-data-2",
    "href": "slides/BSMM_8740_lec_08.html#longitudinal-data-2",
    "title": "Causality Part 2",
    "section": "Longitudinal Data",
    "text": "Longitudinal Data\nIn the potential outcomes framework, a treatment effect is defined at the individual level, \\(\\delta_{it}\\)\n\nSo if you are missing a person, i, in a period, t, then it does not contribute\nThe more heterogeneity in the treatment effects, the more the broken panel will shift away from what you think you’re after"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#repeated-cross-sections",
    "href": "slides/BSMM_8740_lec_08.html#repeated-cross-sections",
    "title": "Causality Part 2",
    "section": "Repeated cross-sections",
    "text": "Repeated cross-sections\n\nOne of the risks of a repeated cross-section is that the composition of the sample may have changed between the pre and post period in ways that are correlated with treatment\nHong (2013) uses repeated cross-sectional data from the Consumer Expenditure Survey (CEX) containing music expenditure and internet use for a random sample of households\nStudy exploits the emergence of Napster (first file sharing software widely used by Internet users) in June 1999 as a natural experiment\nStudy compares internet users and internet non-users before and after emergence of Napster"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#repeated-cross-section-risks",
    "href": "slides/BSMM_8740_lec_08.html#repeated-cross-section-risks",
    "title": "Causality Part 2",
    "section": "Repeated Cross Section Risks",
    "text": "Repeated Cross Section Risks\n\nRepeated cross sections have their own challenges that panels don’t in that the group could be shifting compositionally\nDetect using a balance table with covariates highly predictive of the missing E[Y0|D= 1] for this exercise\n\nPercent of cat owners is probably irrelevant to trends in potential outcomes\nBut age and income is probably relevant for spending habits\nWe’ll discuss covariates more later, but for now just consider what characteristics are relevant to your outcome\n\nDocumenting covariates that cannot be affected by the treatment like this table is a way to check for compositional changes in the sample"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#repeated-cross-section-risks-1",
    "href": "slides/BSMM_8740_lec_08.html#repeated-cross-section-risks-1",
    "title": "Causality Part 2",
    "section": "Repeated Cross Section Risks",
    "text": "Repeated Cross Section Risks\n\nChanges Between Internet and Non-Internet Users Over Time\n\n\nCharacteristic\n1997\n1998\n1999\n\n\n\nUser\nNon-user\nUser\nNon-user\nUser\nNon-user\n\n\nDemographics\n\n\n\n\n\n\n\n\nAge\n40.2\n49.0\n42.3\n49.0\n44.1\n49.4\n\n\nIncome\n$52,887\n$30,459\n$51,995\n$26,189\n$49,970\n$26,649\n\n\nHigh school graduate\n0.18\n0.31\n0.17\n0.32\n0.21\n0.32\n\n\nSome college\n0.37\n0.28\n0.35\n0.27\n0.34\n0.27\n\n\nCollege grad\n0.43\n0.21\n0.45\n0.21\n0.42\n0.20\n\n\nManager\n0.16\n0.08\n0.16\n0.08\n0.14\n0.08"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-covariates",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-covariates",
    "title": "Causality Part 2",
    "section": "2X2 DiD & covariates",
    "text": "2X2 DiD & covariates\n\n2x2 relies on parallel trends (PT) and no anticipation (NA) to be causal\nWhat if PT is violated and the violation is due to covariates:\n\nimbalance between treated an untreated where covariates are related to outcome dynamics\nheterogeneous treatment effects related to covariates"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-covariates-1",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-covariates-1",
    "title": "Causality Part 2",
    "section": "2X2 DiD & covariates",
    "text": "2X2 DiD & covariates\nAs before, for our DiD estimator to be a causal estimate (ATT) we need (note: conditional on covariates X)\n\nNo anticipation: \\(E[Y_{t=1}^1|X,D=1] = E[Y_{t=1}^0|X,D=0]\\)\nParallel trends: \\(0=E[Y_{t=2}^0-Y_{t=1}^0|X,D=1]-E[Y_{t=2}^0-Y_{t=1}^0|X,D=0]\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-covariates-reg",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-covariates-reg",
    "title": "Causality Part 2",
    "section": "2X2 DiD & covariates (reg)",
    "text": "2X2 DiD & covariates (reg)\nUnder our assumptions, we can model the outcome evolution and estimate the counterfactual untreated outcome in period 2 for the treated group using regression, and our estimator\n\\[\\hat{\\tau}_{DiD} = (\\bar{Y}_{12} - \\bar{Y}_{11}) - (\\bar{Y}_{02} - \\bar{Y}_{01}) \\]\nbecomes\n\\[\\hat{\\tau}_{DiD} = (\\bar{Y}_{12} - \\bar{Y}_{11}) - (\\bar{\\hat{\\mu}}(X)_{02} - \\bar{\\hat{\\mu}}(X)_{01}) \\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-covariates-reg-1",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-covariates-reg-1",
    "title": "Causality Part 2",
    "section": "2X2 DiD & covariates (reg)",
    "text": "2X2 DiD & covariates (reg)\nUsing regression the \\(\\hat{\\tau}_{DiD}\\) estimate uses\n\\[\n\\bar{Y}_{d,t}=\\sum_{i\\vert D_i=d,T_i=t}Y_{it}/n_{d,t}\n\\]\nand \\(\\hat{\\mu}(x)_{dt}\\) estimates the true, unknown\n\\[\nm_{d,t}\\equiv\\mathbb{E}\\left[Y_y\\vert D=d,X=x\\right]\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-covariates-reg-2",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-covariates-reg-2",
    "title": "Causality Part 2",
    "section": "2X2 DiD & covariates (reg)",
    "text": "2X2 DiD & covariates (reg)\nWhy not just use fixed effects regression?:\n\\[\nY_{it}=\\beta_1+\\beta_2T_i+\\beta_3D_i+\\tau(T_i\\times D_i)+\\theta'X_i+\\epsilon_{it}\n\\]\n\nassumes homogeneous treatment effects\ntrends will not be \\(X\\)-specific"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-covariates-ipw",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-covariates-ipw",
    "title": "Causality Part 2",
    "section": "2X2 DiD & covariates (IPW)",
    "text": "2X2 DiD & covariates (IPW)\nConsider the following expression, with propensity \\(\\pi=\\mathbb{P}(D=1\\vert X)\\) and \\(\\rho(D)=\\frac{D-\\pi}{\\pi\\left(1-\\pi\\right)}\\):\n\\[\n\\begin{align*}\nE[\\rho(D)\\left(Y_{t=2}-Y_{t=1}\\right)|X] & =\\\\\nE[\\rho(D)\\left(Y_{t=2}-Y_{t=1}\\right)|X,D=1]\\pi+E[\\rho(D)\\left(Y_{t=2}-Y_{t=1}\\right)|X,D=0]\\left(1-\\pi\\right) & =\\\\\nE[\\frac{1}{\\pi}\\left(Y_{t=2}-Y_{t=1}\\right)|X,D=1]\\pi+E[\\frac{-1}{\\left(1-\\pi\\right)}\\left(Y_{t=2}-Y_{t=1}\\right)|X,D=0]\\left(1-\\pi\\right) & =\\\\\nE[Y_{t=2}-Y_{t=1}|X,D=1]-E[Y_{t=2}-Y_{t=1}|X,D=0] & =\\\\\nE[Y_{t=2}^{1}-Y_{t=1}^{0}|X,D=1]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-covariates-ipw-1",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-covariates-ipw-1",
    "title": "Causality Part 2",
    "section": "2X2 DiD & covariates (IPW)",
    "text": "2X2 DiD & covariates (IPW)\nso \\(E[\\rho(D)\\left(Y_{t=2}-Y_{t=1}\\right)|X]=E[Y_{t=2}^{1}-Y_{t=1}^{0}|X,D=1]\\) and so averaging over the covariates:\n\\[\n\\begin{align*}\nE[Y_{t=2}^{1}-Y_{t=1}^{0}|D=1] & =\\int E[Y_{t=2}^{1}-Y_{t=1}^{0}|X,D=1]d\\mathbb{P}(x\\vert D=1)\\\\\n& =\\int E[\\rho(D)\\left(Y_{t=2}-Y_{t=1}\\right)|X]d\\mathbb{P}(x\\vert D=1)\\\\\n& =E[\\rho(D)\\left(Y_{t=2}-Y_{t=1}\\right)\\frac{\\mathbb{P}(D=1\\vert X)}{\\mathbb{P}(D=1)}]\\\\\n& =E[\\frac{Y_{t=2}-Y_{t=1}}{\\mathbb{P}(D=1)}\\frac{D-\\mathbb{P}(D=1\\vert X)}{1-\\mathbb{P}(D=1\\vert X)}]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-covariates-dr",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-covariates-dr",
    "title": "Causality Part 2",
    "section": "2X2 DiD & covariates (DR)",
    "text": "2X2 DiD & covariates (DR)\nWe can also use a doubly robust estimator:\n\\[\n\\mathbb{E}\\left[\\left(\\frac{D}{\\mathbb{E}[D]}-\\frac{\\frac{\\pi(1-D)}{1-\\pi}}{\\mathbb{E\\left[\\frac{\\pi(1-D)}{1-\\pi}\\right]}}\\right)(\\Delta Y-\\hat{\\mu}_{0\\Delta}(X))\\right]\n\\]\nfor …"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-fixed-effects",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-fixed-effects",
    "title": "Causality Part 2",
    "section": "2X2 DiD & Fixed Effects",
    "text": "2X2 DiD & Fixed Effects\nThe regression estimator for the 2x2 DiD (2 groups, 2 time periods) is\n\\[\nY_{it} = \\alpha + \\beta \\cdot \\text{Post}_t + \\gamma \\cdot \\text{Treat}_i + \\delta \\cdot (\\text{Post}_t \\times \\text{Treat}i) + \\epsilon{it}\n\\]\nwhere \\(\\delta\\) is the DiD estimator: it captures the treatment effect. This model is identical to the DiD formula:\n\\[\n\\delta = (Y_{treat, post} - Y_{treat, pre}) - (Y_{control, post} - Y_{control, pre})\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#x2-did-fixed-effects-1",
    "href": "slides/BSMM_8740_lec_08.html#x2-did-fixed-effects-1",
    "title": "Causality Part 2",
    "section": "2X2 DiD & Fixed Effects",
    "text": "2X2 DiD & Fixed Effects\nThe fixed effect estimator is used when you have more than 2 time periods and/or many units, typically in panel data.\n\\[\nY_{it} = \\alpha_i + \\lambda_t + \\delta \\cdot D_{it} + \\epsilon_{it}\n\\]\nIt generalizes the 2x2 model by removing group and time differences via demeaning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#section",
    "href": "slides/BSMM_8740_lec_08.html#section",
    "title": "Causality Part 2",
    "section": "",
    "text": "2X2 DiD & Fixed Effects\n\n\n\n\n\n\n\n\n\n2×2 Regression DiD\nFixed-Effects DiD\n\n\n\n\nGroups\n2 (treated/control)\nMany units/groups\n\n\nControls\nOnly group/time/treatment\nTime and unit fixed effects\n\n\nEstimator\nCoefficient on interaction term\nCoefficient on treatment dummy in FE model\n\n\nParallel trends\nApplies across 2 groups\nApplies across time for all units\n\n\nFlexibility\nLimited\nHandles richer settings (e.g. unbalanced panels, staggered timing)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#event-studies-fixed-effects",
    "href": "slides/BSMM_8740_lec_08.html#event-studies-fixed-effects",
    "title": "Causality Part 2",
    "section": "Event Studies & Fixed Effects",
    "text": "Event Studies & Fixed Effects\nIt is very common to evaluate the common trend assumption using event studies via a dynamic (TWFE) regression specification including indicators for time relative to treatment. This permits “visual inference” to determine whether the treated group’s trends appear to have deviated from the comparison group’s trends right around the time of treatment."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#event-studies-fixed-effects-1",
    "href": "slides/BSMM_8740_lec_08.html#event-studies-fixed-effects-1",
    "title": "Causality Part 2",
    "section": "Event Studies & Fixed Effects",
    "text": "Event Studies & Fixed Effects\nThe dynamic (TWFE) regression specification is, for period \\(t\\in{-T,\\ldots,T}\\) and treatment at T=0:\n\\[\nY_{it} = \\alpha_i + \\lambda_t + \\sum_{r\\ne 1}\\delta_r \\cdot D_{it}\\cdot 1_{t=r+1} + \\epsilon_{it}\n\\]\nSo \\[\n\\delta_r=\\left(\\mathbb{E}[Y_{i,r+1}\\vert D=1]-\\mathbb{E}[Y_{i,r+1}\\vert D=0]\\right)-\\left(\\mathbb{E}[Y_{i,0}\\vert D=1]-\\mathbb{E}[Y_{i,0}\\vert D=0]\\right)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#event-studies-fixed-effects-2",
    "href": "slides/BSMM_8740_lec_08.html#event-studies-fixed-effects-2",
    "title": "Causality Part 2",
    "section": "Event Studies & Fixed Effects",
    "text": "Event Studies & Fixed Effects\nIn the case where there are no effects and e.g. \\(Y^1_{it}=Y^0_{it}=\\gamma\\cdot t + \\epsilon_{it}\\) and \\(\\mathbb{E}[\\epsilon_{it}]=0\\)\n\\[\n\\delta_r=\\gamma\\cdot(r+1)\n\\]\nand the event study looks like:\n\n\nEvent plot\n#Set simulation params ----\nN &lt;- 100; firstT &lt;- -15; lastT &lt;- 10; slope &lt;- 0.5; seed &lt;- 123\n\n#Generate data ----\nset.seed(seed)\n\n# df &lt;- purrr::cross_df(.l = list(t = seq(from = firstT, to = lastT),i = seq(from = 1, to = N)))\ndf &lt;- tidyr::expand_grid(t = seq(from = firstT, to = lastT),i = seq(from = 1, to = N))\n\nstart_dates &lt;- data.frame(i = seq(from =1 , to = N),g = c(rep(1,N/2), rep(Inf, N/2)))\n\ndf &lt;- dplyr::left_join(df, start_dates, by = \"i\"); df$d &lt;- df$g &lt;= df$t\n\ndf$y &lt;- slope * df$t * (df$g == 1) + rnorm(NROW(df), mean =0)\n\n# Run TWFE event-study ----\ntwfe &lt;- fixest::feols(y ~ i(t, g==1, ref=0) | i + t, data = df, cluster = \"i\") \n\neventplotdf &lt;- as.data.frame(twfe$coeftable[1:(lastT-firstT),])\neventplotdf$relativeTime &lt;- c(seq(firstT,-1),seq(1,lastT))-1\neventplotdf$ub &lt;- eventplotdf$Estimate + 1.96 * eventplotdf$`Std. Error`\neventplotdf$lb &lt;- eventplotdf$Estimate - 1.96 * eventplotdf$`Std. Error`\n\nggplot(eventplotdf,\n       aes(x = relativeTime,\n           y = Estimate,\n           ymax = ub,\n           ymin = lb)) +\n  geom_point() +\n  geom_point(x=-1,y=0) +\n  geom_errorbar(width = 0.1) +\n  xlab(\"Relative Time\") +\n  ggtitle(\"TWFE Event-study Coefficients\") +\n  theme(plot.title= element_text(hjust=0.5)) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-1",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-1",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nThree Billboards in the South of Brazil\n\nTo figure out how good billboards were as a marketing channel, A bank placed 3 billboards in the city of Porto Alegre, the capital of the state of Rio Grande do Sul.\nThey wanted to see if that boosted deposits into our savings account.\nRio Grande do Sul is part of the south of Brazil, one of the most developed regions.\nData was also obtained from another capital from the south, Florianopolis, the capital city of the state of Santa Catarina in Brasil. The idea is that Florianopolis could be used as a control sample to estimate the counterfactual when compared to Porto Alegre. The billboard was placed in Porto Alegre for the entire month of June. The resulting data like this:\n\n\n\nCode\ndat &lt;- tibble::tibble(\n  y = c(10,8,7.5,3)\n  , x = c(2,4,2,4)# c(\"pre\",\"post\",\"pre\",\"post\")\n  , cls = as.factor(c(\"untreated\",\"untreated\",\"treated\",\"treated\"))\n) |&gt; dplyr::group_by(cls) \n\ndat |&gt; \n  ggplot(aes(x=x, y=y, color = cls)) +\n  geom_line() +\n  geom_line(\n    inherit.aes = FALSE,\n    data = tibble::tibble(x = c(2,4), y = c(7.5,5.5)), aes(x=x, y=y, fill = 'black'), linetype = 'dotted') +\n  xlim(1,5) + ylim(0,12) +\n  theme_minimal() + \n  theme(legend.title = element_blank(), axis.title.x = element_blank()) +\n  scale_x_continuous(breaks=c(2,4), labels=c(\"Pre\", \"Post\"), limits = c(1.5,4.5)) +\n  ggbrace::stat_brace(\n    data = data.frame(x = c(3.8, 4.0), y = c(5.5, 3)),\n    aes(x, y),\n    rotate = 90, size = .5, col = \"blue\"\n  ) +\n  annotate(\"text\",\n           x = 4.2, y = 4.35,\n           label = expression(delta),\n           parse = TRUE, size = 3.5, col = \"blue\"\n  )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-2",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-2",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nThree Billboards in the South of Brazil\n\nIn this example deposits are our outcome variable, the one the bank wishes to increase with the billboards. POA is a dummy variable for the city of Porto Alegre. When it is zero, it means the samples are from Florianopolis. Jul is a dummy for the month of July, or for the post intervention period. When it is zero it refers to samples from May, the pre-intervention period."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-3",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-3",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nLet \\(Y^D(T)\\) be the potential outcome for treatment D on period T. In an ideal world where we have the ability to observe the counterfactual, we would estimate the treatment effect of an intervention the following way:\n\n\\[\n\\hat{ATET} = E[Y^1(1) - Y^0(1)|D=1]\n\\]\n\nIn words, the causal effect is the outcome in the period post intervention in case of a treatment minus the outcome in also in the period after the intervention, but in the case of no treatment. Of course, we can’t measure this because \\(Y^0(1)\\) is counterfactual."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-4",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-4",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nOne way around this is a before and after comparison.\n\\[\n\\hat{ATET} = E[Y(1)|D=1] - E[Y(0)|D=1]\n\\]\nIn our example, we would compare the average deposits from POA before and after the billboard was placed.\n\n\nCode\ndata_bboard &lt;- readr::read_csv(\"./data/billboard_impact.csv\", show_col_types = FALSE)\npoa_before &lt;- data_bboard |&gt; dplyr::filter(poa==1 & jul==0) |&gt; dplyr::pull(deposits) |&gt; mean()\npoa_after  &lt;- data_bboard |&gt; dplyr::filter(poa==1 & jul==1) |&gt; dplyr::pull(deposits) |&gt; mean()\npoa_after - poa_before\n\n\n[1] 41.05\n\n\nThis estimator is telling us that we should expect deposits to increase R\\(\\$ 41.05\\) after the intervention. But can we trust this?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-5",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-5",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nNotice that \\(E[Y(0)|D=1]=E[Y^0(0)|D=1]\\), the observed outcome for the treated unit before the intervention is equal to the counterfactual outcome for the treated unit also before the intervention. Since we are using this to estimate \\(E[Y^0(1)|D=1]\\), the counterfactual after the intervention, this estimation above assumes that \\(E[Y^0(1)|D=1] = E[Y^0(0)|D=1]\\).\nIt is saying that in the case of no intervention, the outcome in the latter period would be the same as the outcome from the starting period. This would obviously be false if your outcome variable follows any kind of trend."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-6",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-6",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nFor example, if deposits are going up in POA, \\(E[Y^0(1)|D=1] &gt; E[Y^0(0)|D=1]\\), i.e. the outcome of the latter period would be greater than that of the starting period even in the absence of the intervention.\nWith a similar argument, if the trend in Y is going down, \\(E[Y^0(1)|D=1] &lt; E[Y^0(0)|D=1]\\). This is to show that this before and after thing is not a great estimator."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-7",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-7",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nAnother idea is to compare the treated group with an untreated group that didn’t get the intervention:\n\\[\n\\hat{ATET} = E[Y(1)|D=1] - E[Y(1)|D=0]\n\\]\nIn our example, it would be to compare the deposits from POA to that of Florianopolis in the post intervention period.\n\n\nCode\nfl_after  &lt;- data_bboard |&gt; dplyr::filter(poa==0 & jul==1) |&gt; dplyr::pull(deposits) |&gt; mean()\npoa_after - fl_after\n\n\n[1] -119.1\n\n\nThis estimator is telling us that the campaign is detrimental and that customers will decrease deposits by R$ 119.10."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-8",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-8",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nThis estimator is telling us that the campaign is detrimental and that customers will decrease deposits by R\\(\\$ 119.10\\).\nNotice that \\(E[Y(1)|D=0]=E[Y^0(1)|D=0]\\). And since we are using \\(E[Y(1)|D=0]\\) to estimate the counterfactual for the treated after the intervention, we are assuming we can replace the missing counterfactual like this: \\(E[Y^0(1)|D=0] = E[Y^0(1)|D=1]\\).\nBut notice that this would only be true if both groups have a very similar baseline level. For instance, if Florianopolis has way more deposits than Porto Alegre, this would not be true because \\(E[Y^0(1)|D=0] &gt; E[Y^0(1)|D=1]\\).\nOn the other hand, if the level of deposits are lower in Florianopolis, we would have \\(E[Y^0(1)|D=0] &lt; E[Y^0(1)|D=1]\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-9",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-9",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nNotice that \\(E[Y(1)|D=0]=E[Y^0(1)|D=0]\\). And since we are using \\(E[Y(1)|D=0]\\) to estimate the counterfactual for the treated after the intervention, we are assuming we can replace the missing counterfactual like this: \\(E[Y^0(1)|D=0] = E[Y^0(1)|D=1]\\). But notice that this would only be true if both groups have a very similar baseline level.\nFor instance, if Florianopolis has way more deposits than Porto Alegre, this would not be true because \\(E[Y^0(1)|D=0] &gt; E[Y^0(1)|D=1]\\).\nOn the other hand, if the level of deposits are lower in Florianopolis, we would have \\(E[Y^0(1)|D=0] &lt; E[Y^0(1)|D=1]\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-10",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-10",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nAgain, this is not a great idea. To solve this, we can use both space and time comparison. This is the idea of the difference in difference approach. It works by replacing the missing counterfactual the following way:\n\n\\[\n\\begin{align*}\nE[Y^0(1)|D=1] & = E[Y^0(0)|D=1] + \\\\\n& (E[Y^0(1)|D=0] - E[Y^0(0)|D=0])\n\\end{align*}\n\\]\n\nWhat this does is take the treated unit before the intervention and adds a trend component to it, which is estimated using the control \\(E[Y^0(1)|D=0] - E[Y^0(0)|D=0]\\). In words, it is saying that the treated after the intervention, had it not been treated, would look like the treated before the treatment plus a growth factor that is the same as the growth of the control."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-11",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-11",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nIt is important to notice that this assumes that the trends in the treatment and control are the same:\n\\[\nE[Y^0(1) − Y^0(0)|D=1] = E[Y^0(1) − Y^0(0)|D=0]\n\\]\nwhere the left hand side is the counterfactual trend. Now, we can replace the estimated counterfactual in the treatment effect definition\n\\(E[Y^1(1)|D=1] - E[Y^0(1)|D=1]\\)\n\\[\n\\hat{ATET} = E[Y(1)|D=1] - (E[Y(0)|D=1] + (E[Y(1)|D=0] - E[Y(0)|D=0])\n\\]\nIt gets that name because it gets the difference between the difference between treatment and control after and before the treatment."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-12",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-12",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nHere is what that looks in code.\n\n\n\nCode\nfl_before &lt;- data_bboard |&gt; dplyr::filter(poa==0 & jul==0) |&gt; dplyr::pull(deposits) |&gt; mean()\ndiff_in_diff = (poa_after-poa_before)-(fl_after-fl_before)\ndiff_in_diff\n\n\n[1] 6.525\n\n\n\nDiff-in-Diff is telling us that we should expect deposits to increase by R\\(\\$ 6.52\\) per customer.\nNotice that the assumption that diff-in-diff makes is much more plausible than the other 2 estimators. It just assumes that the growth pattern between the 2 cities are the same. But it doesn’t require them to have the same base level nor does it require the trend to be zero."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-13",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-13",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nTo visualize what diff-in-diff is doing, we can project the growth trend from the untreated into the treated to see the counterfactual, that is, the number of deposits we should expect if there were no intervention.\n\n\n\nThe small difference between the solid and the dashed lines shows the small treatment effect on Porto Alegre."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-14",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-14",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nBut how much can we trust this estimator? To get standard errors we use a neat trick that uses regression. Specifically, we will estimate the following linear model\n\n\\[\nY_i = \\beta_0 + \\beta_1 POA_i + \\beta_2 Jul_i + \\beta_3 POA_i*Jul_i + e_i\n\\]\n\nNotice that \\(\\beta_0\\) is the baseline of the control. In our case, is the level of deposits in Florianopolis in the month of May.\nIf we turn on the treated city dummy, we get \\(\\beta_1\\). So \\(\\beta_0+\\beta_1\\) is the baseline of Porto Alegre in May, before the intervention, and \\(\\beta_1\\) is the increase of Porto Alegre baseline on top of Florianopolis.\nIf we turn the POA dummy off and turn the July dummy on, we get \\(\\beta_0+\\beta_2\\), which is the level of Florianópolis in July, after the intervention period. \\(\\beta_2\\) is then the trend of the control, since we add it on top of the baseline to get the level of the control at the period post intervention."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-15",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-15",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nAs a recap, \\(\\beta_1\\) is the increment we get by going from the control to the treated, \\(\\beta_2\\) is the increment we get by going from the period before to the period after the intervention. Finally, if we turn both dummies on, we get \\(\\beta_3\\). \\(\\beta_0+\\beta_1+\\beta_2+\\beta_3\\) is the level in Porto Alegre after the intervention. So \\(\\beta_3\\) is the incremental impact when you go from May to July and from Florianopolis to POA. In other words, it is the Difference in Difference estimator.\n\n\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   172.        2.36     72.6  0        \n2 poa          -126.        4.48    -28.0  1.39e-159\n3 jul            34.5       3.04     11.4  1.43e- 29\n4 poa:jul         6.52      5.73      1.14 2.55e-  1"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-16",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-16",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nNon Parallel Trends\n\nOne obvious potential problem with Diff-in-Diff is failure to satisfy the parallel trend assumption. If the growth trend from the treated is different from the trend of the control, diff-in-diff will be biased.\nThis is a common problem with non-random data, where the decision to treat a region is based on its potential to respond well to the treatment, or when the treatment is targeted at regions that are not performing very well. In our marketing example we decided to test billboards in Porto Alegre not in order to check the effect of billboards in general. The reason is simply because sales perform poorly there.\nPerhaps online marketing is not working there. In this case, it could be that the growth we would see in Porto Alegre without a billboard would be lower than the growth we observe in other cities. This would cause us to underestimate the effect of the billboard there."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-17",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-17",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nNon Parallel Trends\n\nOne way to check if this is happening is to plot the trend using past periods. For example, let’s suppose POA had a small decreasing trend but Florianopolis was on a steep ascent. In this case, showing periods from before would reveal those trends and we would know Diff-in-Diff is not a reliable estimator."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-18",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-18",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nNon Parallel Trends\n\nOne final issue that is worth mentioning is that you won’t be able to place confidence intervals around your Diff-in-Diff estimator if you only have aggregated data.\nSay for instance you don’t have data on what each of our customers from Florianópolis or Porto Alegre did. Instead, you only have the average deposits before and after the intervention for both cities.\nIn this case, you will still be able to estimate the causal effect by Diff-in-Diff, but you won’t know the variance of it. That’s because all the variability in your data got squashed out in aggregation."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-1",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-1",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\n\nIn the simple Diff-in-Diff setup, we have a treated and a control group (the city of POA and FLN, respectively) and only two periods, a pre-intervention and a post-intervention period. But what would happen if we had more periods? Or more groups?\nThis setup is so common and powerful for causal inference and gets its own name: panel data.\nA panel is when we have repeated observations of the same unit over multiple periods of time. This is incredibly common in the industry, where companies track user data over multiple weeks and months.\nTo understand how we can leverage such data structure, let’s first continue with our diff-in-diff example, where we wanted to estimate the impact of placing a billboard (treatment) in the city of Porto Alegre (POA). Specifically, we want to know how much deposits in our investments account would increase if we placed more billboards."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-2",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-2",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\n\nIn the previous section, we motivated the DiD estimator as an imputation strategy of what would have happened to Porto Alegre had we not placed the billboards in it.\nWe said that the counterfactual outcome \\(Y^0\\) for Porto Alegre after the intervention (placing a billboard) could be imputed as the number of deposits in Porto Alegre before the intervention plus a growth factor.\nThis growth factor was estimated in a control city, Florianopolis (FLN)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-3",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-3",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\n\nTo recap some notation, here is how we can estimate this counterfactual outcome\n\n\n\\[\n\\begin{align*}\n\\begin{split}\n\\underbrace{\\widehat{Y^0(1)|D=1}}_{\\substack{\\text{POA outcome after intervention} \\\\ \\text{if no intervention had taken place}}}\n= \\underbrace{Y^0(0)|D=1}_{\\substack{\\text{POA outcome} \\\\ \\text{before intervention}}}\n+ \\big( \\underbrace{Y^0(1)|D=0}_{\\substack{\\text{FLN outcome after} \\\\ \\text{intervention in POA}}}\n- \\underbrace{Y^0(0)|D=0}_{\\substack{\\text{FLN outcome before} \\\\ \\text{intervention in POA}}} \\big)\n\\end{split}\n\\end{align*}\n\\]\n\n\nwhere \\(t\\) denotes time, \\(D\\) denotes the treatment (since \\(t\\) is taken), \\(Y^D(t)\\) denote the potential outcome for treatment \\(D\\) in period \\(t\\) (for example, \\(Y^0(1)\\) is the outcome under the control in period 1)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-4",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-4",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\n\nNow, if we take that imputed potential outcome, we can recover the treatment effect for POA (ATT) as follows\n\n\\[\n\\begin{split}\n\\widehat{ATT} = \\underbrace{Y^1(1)|D=1}_{\\substack{\\text{POA outcome} \\\\ \\text{after intervention}}} - \\widehat{Y^0(1)|D=1}\n\\end{split}\n\\]\n\nThe effect of placing a billboard in POA is the outcome we saw on POA after placing the billboard minus our estimate of what would have happened if we hadn’t placed the billboard. Recall that the power of DiD comes from the fact that estimating the mentioned counterfactual only requires that the growth deposits in POA matches the growth in deposits in FLW. This is the key parallel trends assumption."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-5",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-5",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nParallel Trends\n\nOne way to see the parallel (or common) trends assumptions is as an independence assumption. Recall that the independence assumption requires that the treatment assignment is independent from the potential outcomes:\n\n\\[\nY^d \\perp  D\n\\]\n\nSo we don’t give more treatment to units with higher outcome (causing upward bias in the effect estimation) or lower outcome (causing downward bias).\nIf the marketing manager decides to add billboards only to cities that already have very high deposits, the manager can later boast that cities with billboards generate more deposits, and that the marketing campaign was a success.\nThis violates the independence assumption: we are giving the treatment to cities with high \\(Y^0\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-6",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-6",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nParallel Trends\n\nRecall that a natural extension of this assumption is the conditional independence assumption, which allows the potential outcomes to be dependent on the treatment at first, but independent once we condition on the confounders \\(X\\)\n\n\\[\nY^d \\perp D | X\n\\]\n\nIf the traditional independence assumption states that the treatment assignment can’t be related to the levels of potential outcomes, the parallel trends states that the treatment assignment can’t be related to the growth in potential outcomes over time, and one way to write the parallel trends assumption is as follows\n\n\\[\n\\big(Y^d(t) - Y^d(t-1) \\big)  \\perp D\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-7",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-7",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nParallel Trends\n\nThis assumption says it is fine that we assign the treatment to units that have a higher or lower level of the outcome. What we can’t do is assign the treatment to units based on how the outcome is growing.\nIn our billboard example, this means it is OK to place billboards only in cities with originally high deposit levels. What we can’t do is place billboards only in cities where the deposits are growing the most.\nRemember that DiD is imputing the counterfactual growth in the treated unit with the growth in the control unit. If growth in the treated unit under the control is different than the growth in the control unit, then we are in trouble."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-8",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-8",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nControlling What you Cannot See\n\nMethods like propensity score, linear regression and matching are very good at controlling for confounding in non-random/observational data, but they rely on a key assumption: conditional unconfoundedness\n\n\\[\n(Y^0, Y^1) \\perp D | X\n\\]\n\nTo put it in words, they require that all the confounders are known and measured, so that we can condition on them and make the treatment as good as random."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-9",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-9",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nControlling What you Cannot See\n\nOne issue is that sometimes we can’t measure a confounder. For instance, take a classical labor economics problem of figuring out the impact of marriage on men’s earnings. It’s considered a fact in economics that married men earn more than single men. However, it is not clear if this relationship is causal or not.\nIt could be that more educated men are both more likely to marry and more likely to have a high earnings job, which would mean that education is a confounder of the effect of marriage on earnings. For this confounder, we could measure the education of the person in the study and run a regression controlling for that.\nBut another confounder could be physical attractiveness. It could be that more handsome men are both more likely to get married and more likely to have a high paying job. Unfortunately, attractiveness is one of those characteristics like intelligence. It’s something we can’t measure very well."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-10",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-10",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\n\n\n\n\nload panel data\ndata_panel &lt;- readr::read_csv(\"data/wage_panel.csv\", show_col_types = FALSE)\n\ndata_panel |&gt; \n  dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gt::tab_options( table.font.size = gt::px(28) ) |&gt;\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nnr\nyear\nblack\nexper\nhisp\nhours\nmarried\neduc\nunion\nlwage\nexpersq\noccupation\n\n\n\n\n13\n1980\n0\n1\n0\n2672\n0\n14\n0\n1.198\n1\n9\n\n\n13\n1981\n0\n2\n0\n2320\n0\n14\n1\n1.853\n4\n9\n\n\n13\n1982\n0\n3\n0\n2940\n0\n14\n0\n1.344\n9\n9\n\n\n13\n1983\n0\n4\n0\n2960\n0\n14\n0\n1.433\n16\n9\n\n\n13\n1984\n0\n5\n0\n3071\n0\n14\n0\n1.568\n25\n5\n\n\n\n\n\n\n\n\n\n\nGenerally, the fixed effect model is defined as\n\\[\ny_{it} = \\beta X_{it} + \\gamma U_i + e_{it}\n\\]\nwhere \\(y_{it}\\) is the outcome of individual \\(i\\) at time \\(t\\), \\(X_{it}\\) is the vector of variables for individual \\(i\\) at time \\(t\\). \\(U_i\\) is a set of unobservables for individual \\(i\\) .\nNotice that those unobservables are unchanging through time, hence the lack of the time subscript.\nFinally, \\(e_{it}\\) is the error term. For the education example, \\(y_{it}\\) is log wages, \\(X_{it}\\) are the observable variables that change in time, like marriage and experience and \\(U_i\\) are the variables that are not observed but constant for each individual, like beauty and intelligence."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-11",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-11",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nUsing panel data with a fixed effect model is as simple as adding a dummy for the entities.\nThis is true, but in practice, we don’t actually do it.\nImagine a dataset where we have 1 million customers. If we add one dummy for each of them, we would end up with 1 million columns, which is probably not a good idea. Instead, we use the trick of partitioning the linear regression into 2 separate models.\nWe’ve seen this before, but now is a good time to recap it."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-12",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-12",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nSuppose you have a linear regression model with a set of features \\(X_1\\) and another set of features \\(X_2\\).\n\\[\n\\hat{Y} = \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2\n\\]\nwhere \\(X_1\\) and \\(X_2\\) are feature matrices (one row per feature and one column per observation) and \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_2}\\) are row vectors. You can get the exact same \\(\\hat{\\beta_1}\\) parameter by doing1\n\nregress the outcome \\(y\\) on the second set of features \\(\\hat{y^*} = \\hat{\\gamma_1} X_2\\)\nregress the first set of features on the second \\(\\hat{X_1} = \\hat{\\gamma_2} X_2\\)\nobtain the residuals \\(\\tilde{X}_1 = X_1 - \\hat{X_1}\\) and \\(\\tilde{y} = y - \\hat{y^*}\\)\nregress the residuals of the outcome on the residuals of the features \\(\\tilde{y} = \\hat{\\beta_1} \\tilde{X}_1\\)\n\n\nSee the FWL Theorem"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-13",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-13",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\nThe parameter from this last regression will be exactly the same as running the regression with all the features.\nBut how exactly does this help us? Well, we can break the estimation of the model with the entity dummies into 2. First, we use the dummies to predict the outcome and the features. These are steps 1 and 2 above."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-14",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-14",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nRecall how running a regression on a dummy variable is as simple as estimating the mean for that dummy? Let’s run a model where we predict wages as a function of the year dummy.\n\nlm(lwage ~ year, data = data_panel |&gt; dplyr::mutate(year = factor(year))) |&gt; \n  broom::tidy()\n\n# A tibble: 8 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.39     0.0220     63.5  0       \n2 year1981       0.119    0.0311      3.84 1.22e- 4\n3 year1982       0.178    0.0311      5.74 1.02e- 8\n4 year1983       0.226    0.0311      7.27 4.21e-13\n5 year1984       0.297    0.0311      9.56 1.94e-21\n6 year1985       0.346    0.0311     11.1  1.93e-28\n7 year1986       0.406    0.0311     13.1  2.19e-38\n8 year1987       0.473    0.0311     15.2  4.40e-51"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-15",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-15",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nNotice how this model is predicting the average income in 1980 to be 1.3935, in 1981 to be 1.5129 (1.3935+0.1194) and so on. Now, if we compute the average by year, we get the exact same result. (Remember that the base year, 1980, is the intercept. So you have to add the intercept to the parameters of the other years to get the mean lwage for the year).\n\n\n\nCode\ndata_panel |&gt; dplyr::group_by(year) |&gt; \n  dplyr::summarise(avg_lwage = mean(lwage)) |&gt; \n  gt::gt('year') |&gt; \n  gt::tab_options( table.font.size = gt::px(28) ) |&gt;\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\n\navg_lwage\n\n\n\n\n1980\n1.393\n\n\n1981\n1.513\n\n\n1982\n1.572\n\n\n1983\n1.619\n\n\n1984\n1.690\n\n\n1985\n1.739\n\n\n1986\n1.800\n\n\n1987\n1.866"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-16",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-16",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nThis means that if we get the average for every person in our panel, we are essentially regressing the individual dummy on the other variables. This motivates the following estimation procedure:\n\nCreate time-demeaned variables by subtracting the mean for the individual:\n\n\\[\n    \\begin{align*}\n    \\ddot{Y}_{it} & =Y_{it}-\\bar{Y}_{i}\\\\\n    \\ddot{X}_{it} & =X_{it}-\\bar{X}_{i}\n    \\end{align*}\n\\] 2. Regress \\(\\ddot{Y}_{it}\\) on \\(\\ddot{X}_{it}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-17",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-17",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nNotice that when we do so, the unobserved \\(U_i\\) vanishes. Since \\(U_i\\) is constant across time, we have that \\(\\bar{U_i}=U_i\\). If we have the following system of two equations\n\n\\[\n\\begin{split}\n\\begin{align}\nY_{it} & = \\beta X_{it} + \\gamma U_i + e_{it} \\\\\n\\bar{Y}_{i} & = \\beta \\bar{X}_{it} + \\gamma \\bar{U}_i + \\bar{e}_{it} \\\\\n\\end{align}\n\\end{split}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-18",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-18",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nAnd we subtract one from the other, we get\n\n\\[\n\\begin{split}\n\\begin{align}\n(Y_{it} - \\bar{Y}_{i}) & = (\\beta X_{it} - \\beta \\bar{X}_{it}) + (\\gamma U_i - \\gamma U_i) + (e_{it}-\\bar{e}_{it}) \\\\\n(Y_{it} - \\bar{Y}_{i}) & = \\beta(X_{it} - \\bar{X}_{it}) + (e_{it}-\\bar{e}_{it}) \\\\\n\\ddot{Y}_{it} & = \\beta \\ddot{X}_{it} + \\ddot{e}_{it} \\\\\n\\end{align}\n\\end{split}\n\\]\n\nwhich wipes out all unobserved that are constant across time. To be honest, not only do the unobserved variables vanish. This happens to all the variables that are constant in time. For this reason, you can’t include any variables that are constant across time, as they would be a linear combination of the dummy variables and the model wouldn’t run."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-19",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-19",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nTo check which variables are those, we can group our data by individual and get the sum of the standard deviations. If it is zero, it means the variable isn’t changing across time for any of the individuals.\n\n\n\nCode\ndata_panel |&gt; dplyr::group_by(nr) |&gt; \n  dplyr::mutate(across(!starts_with(\"nr\"),sd)) |&gt; \n  dplyr::distinct() |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::summarise(across(!starts_with(\"nr\"),sum)) |&gt; \n  tidyr::pivot_longer(everything()) |&gt; \n  gt::gt('nr') |&gt; \n  gt::tab_options( table.font.size = gt::px(28) ) |&gt;\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\nyear\n1335.0\n\n\nblack\n0.0\n\n\nexper\n1335.0\n\n\nhisp\n0.0\n\n\nhours\n203098.2\n\n\nmarried\n140.4\n\n\neduc\n0.0\n\n\nunion\n106.5\n\n\nlwage\n173.9\n\n\nexpersq\n17608.2\n\n\noccupation\n739.2"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-20",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-20",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nSo, for our data, we need to remove ethnicity dummies, black and hisp, since they are constant for the individual. Also, we need to remove education. We will also not use occupation, since this is probably mediating the effect of marriage on wage (it could be that single men are able to take more time demanding positions). Having selected the features we will use, it’s time to estimate this model.\nTo run our fixed effect model, first, let’s get our mean data. We can achieve this by grouping everything by individuals and taking the mean.\n\n\n\nCode\ndata_panel |&gt; dplyr::group_by(nr) |&gt;\n  dplyr::select(married, expersq, union, hours, lwage) |&gt; \n  dplyr::summarize(across(!starts_with(\"nr\"),mean), .groups='drop') |&gt; \n  dplyr::slice_head(n=5) |&gt; \n  gt::gt('nr') |&gt; \n  gt::tab_options( table.font.size = gt::px(28) ) |&gt;\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\n\nmarried\nexpersq\nunion\nhours\nlwage\n\n\n\n\n13\n0.000\n25.5\n0.125\n2808\n1.256\n\n\n17\n0.000\n61.5\n0.000\n2504\n1.638\n\n\n18\n1.000\n61.5\n0.000\n2350\n2.034\n\n\n45\n0.125\n35.5\n0.250\n2226\n1.774\n\n\n110\n0.500\n77.5\n0.125\n2108\n2.055"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-21",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-21",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nTo demean the data, we need to set the index of the original data to be the individual identifier, nr. Then, we can simply subtract one data frame from the mean data frame.\n\n\n\n\n\n\n\n\n\n\nmarried\nexpersq\nunion\nhours\nlwage\n\n\n\n\n13\n0\n-24.5\n-0.125\n-135.6\n-0.05811\n\n\n13\n0\n-21.5\n0.875\n-487.6\n0.59741\n\n\n13\n0\n-16.5\n-0.125\n132.4\n0.08881\n\n\n13\n0\n-9.5\n-0.125\n152.4\n0.17756\n\n\n13\n0\n-0.5\n-0.125\n263.4\n0.31247"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-22",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-22",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nFinally, we can run our fixed effect model on the time-demeaned data.\n\n\n\nCode\nlm(lwage ~ married + expersq + union + hours, data = data_panel_baked) |&gt; \n  broom::tidy()\n\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -4.71e-17 0.00507   -9.28e-15 1.00e+  0\n2 married      1.15e- 1 0.0170     6.76e+ 0 1.61e- 11\n3 expersq      3.95e- 3 0.000180   2.20e+ 1 1.92e-101\n4 union        7.84e- 2 0.0184     4.26e+ 0 2.08e-  5\n5 hours       -8.46e- 5 0.0000125 -6.74e+ 0 1.74e- 11"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-23",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-23",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nIf we believe that fixed effect eliminates the all omitted variable bias, this model is telling us that marriage increases a man’s wage by 11%. This result is very significant. One detail here is that for fixed effect models, the standard errors need to be clustered. So, instead of doing all our estimation by hand (which is only nice for pedagogical reasons), we can use the library linearmodels and set the argument cluster_entity to True.\n\n\n\nCode\n# see https://www.r-bloggers.com/2021/05/clustered-standard-errors-with-r/\nestimatr::lm_robust(\n  lwage ~ expersq+union+married+hours \n  , data = data_panel_baked\n  , fixed_effects = ~nr ) |&gt; broom::tidy()\n\n\n     term   estimate std.error statistic   p.value\n1 expersq  0.0039509 1.861e-04    21.232 1.208e-94\n2   union  0.0784442 1.991e-02     3.940 8.287e-05\n3 married  0.1146543 1.825e-02     6.281 3.739e-10\n4   hours -0.0000846 1.842e-05    -4.593 4.512e-06\n    conf.low  conf.high   df outcome\n1  0.0035861  4.316e-03 3811   lwage\n2  0.0394117  1.175e-01 3811   lwage\n3  0.0788661  1.504e-01 3811   lwage\n4 -0.0001207 -4.849e-05 3811   lwage"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-24",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-24",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nNotice how the parameter estimates are identical to the ones we’ve got with time-demeaned data. The only difference is that the standard errors are a bit larger. Compare this to the simple OLS model that doesn’t take the time structure of the data into account. For this model, we add back the variables that are constant in time.\n\n\n# A tibble: 8 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.265     0.0647        4.10  4.16e-  5\n2 expersq      0.00324   0.000206     15.7   2.14e- 54\n3 union        0.183     0.0173       10.6   6.27e- 26\n4 married      0.141     0.0158        8.93  6.11e- 19\n5 hours       -0.0000532 0.0000134    -3.98  7.05e-  5\n6 black       -0.135     0.0237       -5.68  1.44e-  8\n7 hisp         0.0132    0.0210        0.632 5.28e-  1\n8 educ         0.106     0.00469      22.6   1.38e-106\n\n\nThis model is saying that marriage increases the man’s wage by 14%. A somewhat larger effect than the one we found with the fixed effect model. This suggests some omitted variable bias due to fixed individual factors, like intelligence and beauty, not being added to the model."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#more",
    "href": "slides/BSMM_8740_lec_08.html#more",
    "title": "Causality Part 2",
    "section": "More",
    "text": "More\n\nRead Statistical tools for causal inference\nRead Causal inference in R\nRead Scott Cunningham: Causal Inference The Mixtape\nTake a course from Scott Cunningham: Mixtape Sessions\nRead A Crash Course in Good and Bad Control\nRead A Causal Approach for Business Optimization"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#recap",
    "href": "slides/BSMM_8740_lec_08.html#recap",
    "title": "Causality Part 2",
    "section": "Recap",
    "text": "Recap\nWe looked in some detail at other methods used to estimate causal effects, along with methods used for special situations, including:\n\nregression adjustment\ndoubly robust estimation\nmatching\ndifference in differences\nfixed effects and methods for panel data"
  },
  {
    "objectID": "supplemental/omitted_variables.html",
    "href": "supplemental/omitted_variables.html",
    "title": "Omitted variable bias, FWL",
    "section": "",
    "text": "This fact will about covariance estimation be useful for the following discussion:\n∑i(xi−x‾)(yi−y‾)=∑i(xi−x‾)yi−∑i(xi−x‾)y‾=∑i(xi−x‾)yi−y‾∑i(xi−x‾)=∑i(xi−x‾)yi\n\\begin{align*}\n\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right) & =\\sum_{i}\\left(x_{i}-\\bar{x}\\right)y_{i}-\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\bar{y}\\\\\n & =\\sum_{i}\\left(x_{i}-\\bar{x}\\right)y_{i}-\\bar{y}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\\\\n & =\\sum_{i}\\left(x_{i}-\\bar{x}\\right)y_{i}\n\\end{align*}\n and by the same argument ∑i(xi−x‾)(yi−y‾)=∑i(yi−y‾)xi\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)=\\sum_{i}\\left(y_{i}-\\bar{y}\\right)x_{i} and ∑i(xi−x‾)(xi−x‾)=∑i(xi−x‾)xi\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)=\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#olr",
    "href": "supplemental/omitted_variables.html#olr",
    "title": "Omitted variable bias, FWL",
    "section": "OLR",
    "text": "OLR\nPer our population model\ny=β0+β1x+u\ny = \\beta_0 + \\beta_1x + u\n and so our sample regression is\nyi=β0+β1xi+ui(1)\ny_i = \\beta_0 + \\beta_1x_i + u_i\n \\qquad(1) where the index ii identifies each sample, and our prediction is yî=β0̂+β1̂xi\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i. with residuals ui=yi−yîu_i=y_i-\\hat{y_i}.\nWe know that the OLS formula for the regression coefficient β1\\beta_1 is\nβ1̂=Cov(xi,yi)Var(xi)=∑i(xi−x‾)(yi−y‾)∑i(xi−x‾)(xi−x‾)=∑i(xi−x‾)yi∑i(xi−x‾)xi\n\\begin{align*}\n\\hat{\\beta_{1}} & =\\frac{\\text{Cov}\\left(x_{i},y_{i}\\right)}{\\text{Var}\\left(x_{i}\\right)}\\\\\n & =\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)}\\\\\n & =\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)y_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\n\\end{align*}\n\nbut per our model Equation 1 we can write\nβ1̂=∑i(xi−x‾)(β0+β1xi+ui)∑i(xi−x‾)xi=β0∑i(xi−x‾)+β1∑i(xi−x‾)xi+∑i(xi−x‾)ui∑i(xi−x‾)xi==β1+∑i(xi−x‾)ui∑i(xi−x‾)xi\n\\begin{align*}\n\\hat{\\beta_{1}} & =\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(\\beta_{0}+\\beta_{1}x_{i}+u_{i}\\right)}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\\\\\n & =\\frac{\\beta_{0}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)+\\beta_{1}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}+\\sum_{i}\\left(x_{i}-\\bar{x}\\right)u_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}=\\\\\n & =\\beta_{1}+\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)u_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\n\\end{align*}\n\nand so, our estimate β1̂\\hat{\\beta_1} is equal to the population parameter β\\beta IF the noise (i.e. residuals) is uncorrelated with our predictor.\nThe assumption that the residual term is uncorrelated with our predictor is one of the assumptions we used in setting up ordinary linear regression.",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#omitted-variable-bias-ovb",
    "href": "supplemental/omitted_variables.html#omitted-variable-bias-ovb",
    "title": "Omitted variable bias, FWL",
    "section": "Omitted variable bias (OVB)",
    "text": "Omitted variable bias (OVB)\nBut what if the residuals are not uncorrelated with the predictor? For example what if our population model was really\ny=β0+β1x+β2z+u\ny = \\beta_0 + \\beta_1x + \\beta_2z + u\n with unobserved variable zz, but our estimates are yî=β0̂+β1̂xi\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i, then\nβ1̂=∑i(xi−x‾)(β0+β1xi+β2zi+ui)∑i(xi−x‾)xi=β0∑i(xi−x‾)+β1∑i(xi−x‾)xi+β2∑i(xi−x‾)zi+∑i(xi−x‾)ui∑i(xi−x‾)xi=𝔼[β1̂|x]=β1+β2∑i(xi−x‾)zi∑i(xi−x‾)xi\n\\begin{align*}\n\\hat{\\beta_{1}} & =\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(\\beta_{0}+\\beta_{1}x_{i}+\\beta_2z_{i}+u_{i}\\right)}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\\\\\n & =\\frac{\\beta_{0}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)+\\beta_{1}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}+\\beta_{2}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)z_{i}+\\sum_{i}\\left(x_{i}-\\bar{x}\\right)u_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}=\\\\\n \\mathbb{E}[\\hat{\\beta_{1}}|x] &=\\beta_{1}+\\beta_{2}\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)z_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\n\\end{align*}\n and the bias in our estimate of β1\\beta_1 is ∑i(xi−x‾)zi∑i(xi−x‾)xi\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)z_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#ovb-in-action",
    "href": "supplemental/omitted_variables.html#ovb-in-action",
    "title": "Omitted variable bias, FWL",
    "section": "OVB in action:",
    "text": "OVB in action:\nIn this example we will simulate what happens with linearly dependent predictors.\nWe have seen the data below in lab-4, where\n\ny=demand1y = \\text{demand1}\nx=price1,β1=−0.5x = \\text{price1},\\;\\beta_1=-0.5\nz=unobserved1,β2=−1z = \\text{unobserved1},\\;\\beta_2=-1\n\n\n&gt; set.seed(1966)\n&gt; \n&gt; dat1 &lt;- tibble::tibble(\n+   unobserved1 = rnorm(500)\n+   , price1 = 10 + unobserved1 + rnorm(500)\n+   , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))\n+ )\n\nWithout including the unobserved variable, the fit is\n\n&gt; fit1 &lt;- lm(demand1 ~ price1, data = dat1)\n&gt; broom::tidy(fit1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   27.8      0.387       71.8 5.65e-265\n2 price1        -0.989    0.0384     -25.8 9.44e- 94\n\n\nand this fit estimates β1̂=−0.99&lt;−0.5\\hat{\\beta_1}=-0.99&lt;-0.5 so it is incorrect & biased. Checking using the covariance formula:\n\n&gt; cov(dat1$demand1, dat1$price1)/var(dat1$price1)\n\n[1] -0.9893123\n\n\nBut we know we have an unobserved variable (and we know β2=−1\\beta_2=-1) so we can correct the bias.\n\n&gt; # biased estimate\n&gt; biased_est &lt;- broom::tidy(fit1) %&gt;% \n+   dplyr::filter(term == 'price1') %&gt;% \n+   dplyr::pull(estimate)\n&gt; \n&gt; # bias\n&gt; bias &lt;- -cov(dat1$unobserved1, dat1$price1)/var(dat1$price1)\n&gt; \n&gt; #corrected estimate\n&gt; biased_est - bias\n\n[1] -0.4841394\n\n\n\n&gt; lm(demand1 ~ price1 + unobserved1, data = dat1) %&gt;% \n+   broom::tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   22.4      0.443      50.5  1.10e-197\n2 price1        -0.443    0.0444     -9.98 1.74e- 21\n3 unobserved1   -1.08     0.0638    -17.0  3.21e- 51",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#frischwaughlovell-theorem",
    "href": "supplemental/omitted_variables.html#frischwaughlovell-theorem",
    "title": "Omitted variable bias, FWL",
    "section": "Frisch–Waugh–Lovell theorem",
    "text": "Frisch–Waugh–Lovell theorem\n\nFWL or decomposition theorem:\nWhen estimating a model of the form\ny=β0+β1x1+β2x2+u\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + u\n\nthen the following estimators of β1\\beta_1 are equivalent\n\nthe OLS estimator obtained by regressing yy on x1x_1 and x2x_2\nthe OLS estimator obtained by regressing yy on x̌1\\check{x}_1\n\nwhere x̌1\\check{x}_1 is the residual from the regression of x1x_1 on x2x_2\n\nthe OLS estimator obtained by regressing y̌\\check{y} on x̌1\\check{x}_1\n\nwhere y̌\\check{y} is the residual from the regression of yy on x2x_2\n\n\n\n\nInterpretation:\nThe Frisch-Waugh-Lowell theorem is telling us that there are multiple ways to estimate a single regression coefficient. One possibility is to run the full regression of yy on xx, as usual.\nHowever, we can also regress x1x_1 on x2x_2, take the residuals, and regress yy only those residuals. The first part of this process is sometimes referred to as partialling-out (or orthogonalization, or residualization) of x1x_1 with respect to x2x_2. The idea is that we are isolating the variation in x1x_1 that is independent of (orthogonal to) x2x_2. Note that x2x_2 can be also be multi-dimensional (i.e. include multiple variables and not just one).\nWhy would one ever do that?\nThis seems like a way more complicated procedure. Instead of simply doing the regression in 1 step, now we need to do 2 or even 3 steps. It’s not intuitive at all. The main advantage comes from the fact that we have reduced a multivariate regression to a univariate one, making more tractable and more intuitive.\n\n\nExample1\nUsing the data from OVB example:\n\n&gt; # partial out unobserved1 (predictor) from price1 (predictor)\n&gt; fit_price &lt;- lm(price1 ~ unobserved1, data = dat1)\n&gt; \n&gt; # regress demand1 (outcome) on price residuals\n&gt; lm(\n+   demand1 ~ price_resid\n+   , data = tibble::tibble(demand1 = dat1$demand1, price_resid = fit_price$residuals)\n+ ) %&gt;% \n+   broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)   17.9      0.0830    216.   0          \n2 price_resid   -0.443    0.0828     -5.35 0.000000135\n\n\n\n&gt; # partial out unobserved1 (predictor) from demand1 (outcome)\n&gt; fit_demand &lt;- lm(demand1 ~ unobserved1, data = dat1)\n&gt; \n&gt; # regress demand residuals on price residuals\n&gt; lm(\n+   demand_resid ~ price_resid\n+   , data = tibble::tibble(demand_resid = fit_demand$residuals, price_resid = fit_price$residuals)\n+ ) %&gt;% \n+   broom::tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  1.37e-16    0.0445  3.08e-15 1.00e+ 0\n2 price_resid -4.43e- 1    0.0444 -9.99e+ 0 1.59e-21",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#partial-identification",
    "href": "supplemental/omitted_variables.html#partial-identification",
    "title": "Omitted variable bias, FWL",
    "section": "Partial Identification",
    "text": "Partial Identification\nIn our linear regression models we include errors (aka noise) in the model of the outcome\nyi=β0+β1xi+ui\ny_i = \\beta_0 + \\beta_1x_i + u_i\n\nwhere each sample uiu_i is drawn from a Gaussian distributions with zero mean and variance σ2\\sigma^2.\nWhat if we are also uncertain about xix_i? In particular what if instead of xix_i we can only use x̃i=xi+wi\\tilde{x}_i=x_i + w_i where\nCov(w,x)=Cov(w,u)=𝔼[w]=0\n\\text{Cov}\\left(w,x\\right)=\\text{Cov}\\left(w,u\\right)=\\mathbb{E}\\left[w\\right]=0\n\ni.e. x̃i\\tilde{x}_i reflects measurement error in the predictor xix_i, where the measurement error has zero mean and is independent of xx and uu (e.g. the measurement error is an independent Gaussian).\nIn this case 𝔼[x̃]=𝔼[x+w]=𝔼[x]\\mathbb{E}\\left[\\tilde{x}\\right]=\\mathbb{E}\\left[x+w\\right]=\\mathbb{E}\\left[x\\right] and\nCov(x̃,y)=Cov(x+w,y)=Cov(x,y)+Cov(w,y)=Cov(x,y)+Cov(w,β0+β1x+u)=Cov(x,y)+Cov(w,u)+β1Cov(w,x)=Cov(x,y)\n\\begin{align*}\n\\text{Cov}\\left(\\tilde{x},y\\right) & =\\text{Cov}\\left(x+w,y\\right)=\\text{Cov}\\left(x,y\\right)+\\text{Cov}\\left(w,y\\right)\\\\\n & =\\text{Cov}\\left(x,y\\right)+\\text{Cov}\\left(w,\\beta_{0}+\\beta_{1}x+u\\right)\\\\\n & =\\text{Cov}\\left(x,y\\right)+\\text{Cov}\\left(w,u\\right)+\\beta_{1}\\text{Cov}\\left(w,x\\right)\\\\\n & =\\text{Cov}\\left(x,y\\right)\n\\end{align*}\n\nhowever Var(x̃)=Var(x+w)≥Var(x)\\text{Var}\\left(\\tilde{x}\\right)=\\text{Var}\\left(x+w\\right)\\ge\\text{Var}\\left(x\\right) so\nCov(x̃,y)Var(x̃)=Cov(x,y)Var(x)+Var(w)=Cov(x,y)/Var(x)1+Var(w)/Var(x)=β11+Var(w)/Var(x)\n\\begin{align*}\n\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)} & =\\frac{\\text{Cov}\\left(x,y\\right)}{\\text{Var}\\left(x\\right)+\\text{Var}\\left(w\\right)}\\\\\n & =\\frac{\\text{Cov}\\left(x,y\\right)/\\text{Var}\\left(x\\right)}{1+\\text{Var}\\left(w\\right)/\\text{Var}\\left(x\\right)}\\\\\n & =\\frac{\\beta_{1}}{1+\\text{Var}\\left(w\\right)/\\text{Var}\\left(x\\right)}\n\\end{align*}\n and since Var(w)/Var(x)\\text{Var}\\left(w\\right)/\\text{Var}\\left(x\\right) is non-negative Cov(x̃,y)Var(x̃)\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)} has the same sign as β1\\beta_1 and our data gives us a lower bound for β1\\beta_1:\n|Cov(x̃,y)Var(x̃)|≤|β1|\n\\left|\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\\right|\\le\\left|\\beta_{1}\\right|\n\nIf we reverse the regression (regress x̃\\tilde{x} on yy)\nCov(x̃,y)Var(y)=Cov(x,y)β12Var(x)+Var(u)=β1Var(x)β12Var(x)+Var(u)\n\\begin{align*}\n\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(y\\right)} & =\\frac{\\text{Cov}\\left(x,y\\right)}{\\beta_{1}^{2}\\text{Var}\\left(x\\right)+\\text{Var}\\left(u\\right)}\\\\\n & =\\frac{\\beta_{1}\\text{Var}\\left(x\\right)}{\\beta_{1}^{2}\\text{Var}\\left(x\\right)+\\text{Var}\\left(u\\right)}\n\\end{align*}\n and taking the reciprocal:\nVar(y)Cov(x̃,y)=β1+Var(u)𝛃𝟏𝐕𝐚𝐫(𝐱)=β1[1+Var(u)𝛃𝟏𝟐𝐕𝐚𝐫(𝐱)]\n\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}=\\beta_{1}+\\frac{\\text{Var}\\left(u\\right)}{\\mathbf{\\beta_{1}\\text{Var}\\left(x\\right)}}=\\beta_{1}\\left[1+\\frac{\\text{Var}\\left(u\\right)}{\\mathbf{\\beta_{1}^{2}\\text{Var}\\left(x\\right)}}\\right]\n and the factor in the brackets is greater than 1 and as before Cov(x̃,y)Var(x̃)\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)} has the same sign as β1\\beta_1 so\n|Var(y)Cov(x̃,y)|≥|β1|\n\\left|\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}\\right|\\ge\\left|\\beta_{1}\\right|\n\nSome terminology:\n\na bound is sharp if it cannot be improved (under our assumptions)\na bound is tight if it is short enough to be useful in practice\n\nWe have sharp bounds for β1\\beta_1 under measurement error:\n|Var(y)Cov(x̃,y)|≥|β1|≥|Cov(x̃,y)Var(x̃)|(2)\n\\left|\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}\\right|\\ge\\left|\\beta_{1}\\right|\\ge\\left|\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\\right|\n \\qquad(2)\nWith respect to how tight the bounds are, let rr denote the correlation between x̃\\tilde{x} and yy, then\nr2=Cov(x̃,y)2Var(x̃)Var(y)=Cov(x̃,y)Var(x̃)×Cov(x̃,y)Var(y)r2×Var(y)Cov(x̃,y)=Cov(x̃,y)Var(x̃)\n\\begin{align*}\nr^{2} & =\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)^{2}}{\\text{Var}\\left(\\tilde{x}\\right)\\text{Var}\\left(y\\right)}=\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\\times\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(y\\right)}\\\\\nr^{2}\\times\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)} & =\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\n\\end{align*}\n\nand the the width of the bounds in Equation 2 is\nwidth=|Var(y)Cov(x̃,y)−Cov(x̃,y)Var(x̃)|=(1−r2)|Var(y)Cov(x̃,y)|\n\\text{width}=\\left|\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}-\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\\right|=\\left(1-r^{2}\\right)\\left|\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}\\right|\n\nSo the bounds for β1\\beta_1 are tighter when x̃\\tilde{x} and yy are strongly correlated.\nsee ovb",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra.",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have nn observations. Let the ithi^{th} be (xi1,…,xip,yi)(x_{i1}, \\ldots, x_{ip}, y_i), such that xi1,…,xipx_{i1}, \\ldots, x_{ip} are the explanatory variables (predictors) and yiy_i is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\ny=β0+β1x1+…+βpxp(1)\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \n \\qquad(1)\nWe can write the response for the ithi^{th} observation as shown in Equation 2\nyi=β0+β1xi1+…+βpxip+ϵi(2)\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i \n \\qquad(2)\nsuch that ϵi\\epsilon_i is the amount yiy_i deviates from μ{y|xi1,…,xip}\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}, the mean response for a given combination of explanatory variables. We assume each ϵi∼N(0,σ2)\\epsilon_i \\sim N(0,\\sigma^2), where σ2\\sigma^2 is a constant variance for the distribution of the response yy for any combination of explanatory variables x1,…,xpx_1, \\ldots, x_p.",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n𝐘=[y1y2⋮yn]𝐗=[x11x12…x1px21x22…x2p⋮⋮⋱⋮xn1xn2…xnp]𝛃=[β0β1⋮βp]𝛜=[ϵ1ϵ2⋮ϵn](3)\n\\begin{align*}\\mathbf{Y}=\\left[\\begin{matrix}y_{1}\\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{matrix}\\right]\\hspace{1cm}\\mathbf{X}=\\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p}\\\\\nx_{21} & x_{22} & \\dots & x_{2p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np}\n\\end{bmatrix}\\hspace{1cm}\\boldsymbol{\\beta}=\\begin{bmatrix}\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{p}\n\\end{bmatrix}\\hspace{1cm}\\boldsymbol{\\epsilon}=\\begin{bmatrix}\\epsilon_{1}\\\\\n\\epsilon_{2}\\\\\n\\vdots\\\\\n\\epsilon_{n}\n\\end{bmatrix}\\end{align*}\n \\qquad(3)\nThus,\n𝐘=𝐗𝛃+𝛜\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n𝐘̂=𝐗𝛃̂𝐞=𝐘−𝐗𝛃̂(4)\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{1cm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n \\qquad(4)",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, 𝛃̂\\hat{\\boldsymbol{\\beta}} that minimizes\n∑i=1nei2=𝐞T𝐞=(𝐘−𝐗𝛃̂)T(𝐘−𝐗𝛃̂)(5)\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n \\qquad(5)\nwhere 𝐞T\\mathbf{e}^T, the transpose of the matrix 𝐞\\mathbf{e}.\n(𝐘−𝐗𝛃̂)T(𝐘−𝐗𝛃̂)=(𝐘T𝐘−𝐘T𝐗𝛃̂−(𝛃̂T𝐗T𝐘+𝛃̂T𝐗T𝐗𝛃̂)(6)\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} - \n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n \\qquad(6)\nNote that (𝐘𝐓𝐗𝛃̂)T=𝛃̂T𝐗T𝐘(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}. Since these are both constants (i.e. 1×11\\times 1 vectors), 𝐘𝐓𝐗𝛃̂=𝛃̂T𝐗T𝐘\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}. Thus, Equation 6 becomes\n𝐘T𝐘−2𝐗T𝛃̂T𝐘+𝛃̂T𝐗T𝐗𝛃̂\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\nSince we want to find the 𝛃̂\\hat{\\boldsymbol{\\beta}} that minimizes Equation 5, will find the value of 𝛃̂\\hat{\\boldsymbol{\\beta}} such that the derivative with respect to 𝛃̂\\hat{\\boldsymbol{\\beta}} is equal to 0.\n∂𝐞T𝐞∂𝛃̂=∂∂𝛃̂(𝐘T𝐘−2𝐗T𝛃̂T𝐘+𝛃̂T𝐗T𝐗𝛃̂)=0⇒−2𝐗T𝐘+2𝐗T𝐗𝛃̂=0⇒2𝐗T𝐘=2𝐗T𝐗𝛃̂⇒𝐗T𝐘=𝐗T𝐗𝛃̂⇒(𝐗T𝐗)−1𝐗T𝐘=(𝐗T𝐗)−1𝐗T𝐗𝛃̂⇒(𝐗T𝐗)−1𝐗T𝐘=𝐈𝛃̂(7)\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n \\qquad(7)\nThus, the estimate of the model coefficients is 𝛃̂=(𝐗T𝐗)−1𝐗T𝐘\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}.",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\nE[𝛜𝛜T]=σ2IE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\n𝛃̂=𝛃+(𝐗T𝐗)−1ϵ\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\n\nFirst, we will show that E[𝛜𝛜T]=σ2IE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\nE[𝛜𝛜T]=E[ϵ1ϵ2…ϵn][ϵ1ϵ2⋮ϵn]=E[ϵ12ϵ1ϵ2…ϵ1ϵnϵ2ϵ1ϵ22…ϵ2ϵn⋮⋮⋱⋮ϵnϵ1ϵnϵ2…ϵn2]=[E[ϵ12]E[ϵ1ϵ2]…E[ϵ1ϵn]E[ϵ2ϵ1]E[ϵ22]…E[ϵ2ϵn]⋮⋮⋱⋮E[ϵnϵ1]E[ϵnϵ2]…E[ϵn2]](8)\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\ \n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2 \n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n \\qquad(8)\nRecall, the regression assumption that the errors ϵi′s\\epsilon_i's are Normally distributed with mean 0 and variance σ2\\sigma^2. Thus, E(ϵi2)=Var(ϵi)=σ2E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2 for all ii. Additionally, recall the regression assumption that the errors are uncorrelated, i.e. E(ϵiϵj)=Cov(ϵi,ϵj)=0E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0 for all i,ji,j. Using these assumptions, we can write Equation 8 as\nE[𝛜𝛜T]=[σ20…00σ2…0⋮⋮⋱⋮00…σ2]=σ2𝐈(9)\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n \\qquad(9)\nwhere 𝐈\\mathbf{I} is the n×nn \\times n identity matrix.\nNext, we show that 𝛃̂=𝛃+(𝐗T𝐗)−1ϵ\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon.\nRecall that the 𝛃̂=(𝐗T𝐗)−1𝐗T𝐘\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} and 𝐘=𝐗𝛃+𝛜\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}. Then,\n𝛃̂=(𝐗T𝐗)−1𝐗T𝐘=(𝐗T𝐗)−1𝐗T(𝐗𝛃+𝛜)=𝛃+(𝐗T𝐗)−1𝐗T𝛜(10)\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n \\qquad(10)\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is E[(𝛃̂−𝛃)(𝛃̂−𝛃)T]E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\nE[(𝛃̂−𝛃)(𝛃̂−𝛃)T]=E[(𝛃+(𝐗T𝐗)−1𝐗T𝛜−𝛃)(𝛃+(𝐗T𝐗)−1𝐗T𝛜−𝛃)T]=E[(𝐗T𝐗)−1𝐗T𝛜𝛜T𝐗(𝐗T𝐗)−1]=(𝐗T𝐗)−1𝐗TE[𝛜𝛜T]𝐗(𝐗T𝐗)−1=(𝐗T𝐗)−1𝐗T(σ2𝐈)𝐗(𝐗T𝐗)−1=σ2𝐈(𝐗T𝐗)−1𝐗T𝐗(𝐗T𝐗)−1=σ2𝐈(𝐗T𝐗)−1=σ2(𝐗T𝐗)−1(11)\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n \\qquad(11)",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/covariance_decomposition.html",
    "href": "supplemental/covariance_decomposition.html",
    "title": "Covariance Decomposition",
    "section": "",
    "text": "Here we show that the empirical covariance can be expressed as a difference equation.\nFor Δkat≡at+k−at\\Delta_k a_t \\equiv a_{t+k}-a_t (the difference) and a‾≡1T∑t=1Tat\\bar{a}\\equiv\\frac{1}{T}\\sum_{t=1}^T a_t (the mean)\nGiven sequences {at}t=1T\\left\\{a_t\\right\\}_{t=1}^T and {bt}t=1T\\left\\{b_t\\right\\}_{t=1}^T, we have\n1T∑t=1T(at−a‾)(bt−b‾)=1T2∑k=1T−1∑t=1T−kΔkatΔkbt\\frac{1}{T}\\sum_{t=1}^T(a_t-\\bar{a})(b_t-\\bar{b})= \\frac{1}{T^2}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k} \\Delta_k a_t \\Delta_k b_t"
  },
  {
    "objectID": "supplemental/covariance_decomposition.html#strategy",
    "href": "supplemental/covariance_decomposition.html#strategy",
    "title": "Covariance Decomposition",
    "section": "Strategy",
    "text": "Strategy\nThe empirical covariance can be written as follows:\n1T∑t=1T(at−a‾)(bt−b‾)=1T∑t=1T(atbt−a‾bt−atb‾+a‾b‾)=(1T∑t=1Tatbt)−a‾b‾\n\\begin{align*}\n\\frac{1}{T}\\sum_{t=1}^{T}(a_{t}-\\bar{a})(b_{t}-\\bar{b}) & =\\frac{1}{T}\\sum_{t=1}^{T}\\left(a_{t}b_{t}-\\bar{a}b_{t}-a_{t}\\bar{b}+\\bar{a}\\bar{b}\\right)\\\\\n & =\\left(\\frac{1}{T}\\sum_{t=1}^{T}a_{t}b_{t}\\right)-\\bar{a}\\bar{b}\n\\end{align*}\n\nso the proof strategy is to show that the difference equation can be reduced to the same form."
  },
  {
    "objectID": "supplemental/covariance_decomposition.html#proof",
    "href": "supplemental/covariance_decomposition.html#proof",
    "title": "Covariance Decomposition",
    "section": "Proof",
    "text": "Proof\nSince Δkat≡at+k−at\\Delta_k a_t \\equiv a_{t+k}-a_t we can expand the difference equation as:\n1T2∑k=1T−1∑t=1T−kΔkatΔkbt=1T2∑k=1T−1∑t=1T−k(at+k−at)(bt+k−bt)=1T2∑k=1T−1∑t=1T−k(at+kbt+k+atbt)−1T2∑k=1T−1∑t=1T−katbt+k−1T2∑k=1T−1∑t=1T−kat+kbt\n\\begin{align*}\n\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}\\Delta_{k}a_{t}\\Delta_{k}b_{t} & =\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}\\left(a_{t+k}-a_{t}\\right)\\left(b_{t+k}-b_{t}\\right)\\\\\n & =\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}\\left(a_{t+k}b_{t+k}+a_{t}b_{t}\\right)\\\\\n & -\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}a_{t}b_{t+k}-\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}a_{t+k}b_{t}\n\\end{align*}\n and we’ll evaluate each of the 4 terms in turn.\n\nterm 1:\nMaking a change of variable: s=t+ks=t+k and noting that when tt ranges from 11 to T−kT−k, ss ranges from 1+k1+k to TT, we have:\n1T2∑k=1T−1∑t=1T−kat+kbt+k=1T2∑k=1T−1∑s=k+1Tasbs=1T2∑s=2T∑k=1s−1asbs=1T2∑s=2T(s−1)asbs\n\\begin{align*}\n\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}a_{t+k}b_{t+k} & =\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{s=k+1}^{T}a_{s}b_{s}\\\\\n & =\\frac{1}{T^{2}}\\sum_{s=2}^{T}\\sum_{k=1}^{s-1}a_{s}b_{s}\\\\\n & =\\frac{1}{T^{2}}\\sum_{s=2}^{T}\\left(s-1\\right)a_{s}b_{s}\n\\end{align*}\n\nwhere in the second-last line we note that asbsa_s b_s doesn’t depend on kk so we count each asbsa_s b_s exactly s−1s-1 times for s∈[2,T]s\\in[2,T]. Note that this is\n1T2(a2b2+2a3b3+3a4b4+⋯++(T−2)aT−1bT−1+(T−1)aTbT)\n\\frac{1}{T^{2}}\\left(a_{2}b_{2}+2a_{3}b_{3}+3a_{4}b_{4}+\\cdots++\\left(T-2\\right)a_{T-1}b_{T-1}+\\left(T-1\\right)a_{T}b_{T}\\right)\n\n\n\nterm 2:\nThe second term is\n1T2∑k=1T−1∑t=1T−katbt\n\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}a_t b_t\n and it can be decomposed directly to:\n1T2((T−1)a1b1+(T−2)a2b2+(T−3)a3b3+⋯+aT−1bT−1)\n\\frac{1}{T^{2}}\\left((T-1)a_{1}b_{1}+(T-2)a_{2}b_{2}+(T-3)a_{3}b_{3}+\\cdots+a_{T-1}b_{T-1}\\right)\n\nSo combining the first and second terms we get T−1T2∑t=1Tatbt\\frac{T-1}{T^2}\\sum_{t=1}^{T}a_t b_t, and we’re on our way towards showing that\n1T2∑k=1T−1∑t=1T−kΔkatΔkbt=(1T∑t=1Tatbt)−a‾b‾\n\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}\\Delta_{k}a_{t}\\Delta_{k}b_{t} = \\left(\\frac{1}{T}\\sum_{t=1}^{T}a_{t}b_{t}\\right)-\\bar{a}\\bar{b}\n\n\n\nterm 3:\nThe third term is −1T2∑k=1T−1∑t=1T−kat+kbt-\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}a_{t+k}b_{t} and again we will use the change of variables s=t+ks=t+k.\nBefore changing variables, the outer sum goes from k=1k=1 to k=T−1k=T-1 and the inner sum goes from t=1t=1 to t=T−kt=T-k. The (k,t)(k,t) indices used in −1T2∑k=1T−1∑t=1T−kat+kbt-\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}a_{t+k}b_{t} are shown in figure (A) below (for T=10T=10), along with the s=t+ks=t+k values.\nWith the change of variable, the outer sum goes from s=t+k=2s=t+k=2 to s=t+k=Ts=t+k=T and the inner sum goes from t=1t=1 to t=s−1t=s-1. The (s,t)(s,t) indices used to sum −1T2∑s=2T∑t=1s−1asbt-\\frac{1}{T^{2}}\\sum_{s=2}^{T}\\sum_{t=1}^{s-1}a_{s}b_{t} are shown in figure (B) below (for T=10T=10).\nThe first index (ss) is along the horizontal axis, while the second index (tt) is along the vertical axis.\n\n\nCode\nT &lt;- 10\nres &lt;- 1:(T-1) |&gt; \n  purrr::map( \n    (\\(k){ 1:(T-k) |&gt; \n        purrr::map(\\(t)data.frame(\"k\"=k,\"t\"=t, \"s\"=t+k)) }\n    ) \n  ) |&gt; dplyr::bind_rows()\n\np1 &lt;- res |&gt; \n  ggplot(aes(x=k,y=t, label = as.character(s))) + geom_label(color = \"#3B528B\") +\n  #geom_point(color = \"#3B528B\") +\n  scale_x_continuous(breaks = 1:T) +\n  scale_y_continuous(\n    \"t\"\n    , breaks = 1:T\n    , labels = 1:T |&gt; as.character()\n  ) +\n  theme_minimal()  +\n  theme(axis.title.y.left = element_text(angle = 0, vjust = 0.5)) +\n  theme(axis.title.y.right = element_text(angle = 0, vjust = 0.5), axis.ticks = element_blank()) +\n  labs(title = \"s values given k and t\", subtitle=\"T=10\")+ coord_fixed()\n\np2 &lt;- res |&gt; \n  ggplot(aes(x=s,y=t+1))+\n  geom_point(shape=0, color = \"#21908C\") +\n  scale_color_viridis_d(option = \"D\") +\n  scale_x_continuous(name = \"s\", breaks = 1:T) +\n  scale_y_continuous(\n    sec.axis = ggplot2::sec_axis(transform = ~.-1, breaks = 1:(T), name = \"t\")\n  ) + #lims(x=c(1,10), y = c(1,10))+\n  theme_minimal() +\n  theme(axis.title.y.left = element_blank()\n        , axis.ticks.y.left=element_blank(), axis.text.y.left=element_blank()) +\n  theme(axis.title.y.right = element_text(angle = 0, vjust = 0.5), axis.ticks = element_blank()) +\n  labs(title = \"t-s indices\", subtitle=\"T=10\") + coord_fixed()\n  \np1 + p2 + plot_annotation(tag_levels = 'A', tag_prefix = \"(\", , tag_suffix = \")\")\n\n\n\n\n\n\n\n\n\nNote that the inner sum on tt gives s−1s-1 pairs, so that the total number of pairs is\n∑s=2T(s−1)=1+2+3+⋯(T−1)=T(T−1)2\n\\sum_{s=2}^T (s-1) = 1+2+3+\\cdots (T-1) = \\frac{T(T-1)}{2}\n\nor half of all T2T^2 index pairs (t,s)(t,s) less the diagonal (s,s)(s,s). From figure (B) we see that it sums the lower diagonal of all index pair values, less the diagonal.\nThis sum also counts the number of unique pairs (s,t)(s,t) where 2≤s≤T2\\le s\\le T and 1≤t&lt;s1\\le t&lt;s which is equivalent to counting the number of pairs where 1≤t&lt;s≤T1 \\le t &lt; s \\le T\n\n\nterm 4:\nThe fourth term is −1T2∑k=1T−1∑t=1T−katbt+k-\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}a_{t}b_{t+k} and again we will use the change of variables s=t+ks=t+k.\nWith the change of variable, and with ss now the second index, the inner sum goes from s=t+1s=t+1 to s=Ts=T and the outer sum goes from t=1t=1 to t=T−1t=T-1. The (t,s)(t,s) indices used to sum −1T2∑t=1T−1∑s=t+1Tatbs-\\frac{1}{T^{2}}\\sum_{t=1}^{T-1}\\sum_{s=t+1}^{T}a_{t}b_{s} are shown in figure (B) below (for T=10T=10).\nIn fact the atbt+ka_{t}b_{t+k} terms are the transpose of the terms at+kbta_{t+k}b_{t} of figure (B) as can be seen in figure (C).\n\n\nCode\np3 &lt;- res |&gt; \n  ggplot(aes(x=t,y=s))+\n  geom_point(shape=0, color = \"#21908C\") +\n  scale_color_viridis_d(option = \"D\") +\n  scale_x_continuous(name = \"t\", breaks = 1:T) +\n  scale_y_continuous(\n    sec.axis = ggplot2::sec_axis(transform = ~., breaks = 2:(T), name = \"s\")\n  ) + #lims(x=c(1,10), y = c(1,10))+\n  theme_minimal() +\n  theme(axis.title.y.left = element_blank(), axis.ticks.y.left=element_blank(), axis.text.y.left=element_blank()) +\n  theme(axis.title.y.right = element_text(angle = 0, vjust = 0.5), axis.ticks = element_blank()) +\n  labs(title = \"t-s indices\", subtitle=\"T=10\") + coord_fixed()\n\np3 + plot_annotation(tag_levels = list('C'), tag_prefix = \"(\", , tag_suffix = \")\")\n\n\n\n\n\n\n\n\n\nSo per terms 3 & 4, each cross-product (aibj,i≠ja_i b_j,\\,i\\ne j) appears exactly once with a negative sign, for a total of T2−TT^2-T terms (since each of terms 3 & 4 has T(T−1)2\\frac{T(T-1)}{2} terms.\nSo we are missing a set of TT diagonal terms from the total T2T^2 terms.\nbut the sum of the first two terms is T−1T2∑t=1Tatbt\\frac{T-1}{T^2}\\sum_{t=1}^{T}a_t b_t, which provides the missing diagonal, so we have:\nT−1T2∑t=1Tatbt−1T2∑s=1T∑t=1,t≠sTatbs=1T∑t=1Tatbt−1T2∑s=1T∑t=1Tatbs=1T∑t=1Tatbt−1T∑s=1Tbs×1T∑t=1Tat=1T∑t=1Tatbt−b‾a‾\n\\begin{align*}\n\\frac{T-1}{T^{2}}\\sum_{t=1}^{T}a_{t}b_{t}-\\frac{1}{T^{2}}\\sum_{s=1}^{T}\\sum_{t=1,\\,t\\ne s}^{T}a_{t}b_{s} & =\\\\\n\\frac{1}{T}\\sum_{t=1}^{T}a_{t}b_{t}-\\frac{1}{T^{2}}\\sum_{s=1}^{T}\\sum_{t=1}^{T}a_{t}b_{s} & =\\\\\n\\frac{1}{T}\\sum_{t=1}^{T}a_{t}b_{t}-\\frac{1}{T}\\sum_{s=1}^{T}b_{s}\\times\\frac{1}{T}\\sum_{t=1}^{T}a_{t} & =\\\\\n\\frac{1}{T}\\sum_{t=1}^{T}a_{t}b_{t}-\\bar{b}\\bar{a}\n\\end{align*}\n\n\n\ncompute:\n\n\nCode\nset.seed(8740); T = 20\na &lt;- 1 + rnorm(n = T)\nb &lt;- a + 4 + rnorm(n = T, sd = 2)\n\n# calculate using difference equation\ncov_diff &lt;- \n  1:(T-1) |&gt; purrr::map_vec(\n    (\\(k){ ((dplyr::lead(a,k) - a) * (dplyr::lead(b,k) - b)) |&gt; sum(na.rm=TRUE) })\n  ) |&gt; sum(na.rm=TRUE) / (T*T)\n\n# summarize results\ntibble::tibble(x = a, y = b) |&gt; \n  dplyr::mutate(x = x - mean(x), y = y - mean(y), prod = x*y) |&gt; \n  dplyr::summarize(pop_cov = mean(prod), smpl_cov = sum(prod)/(dplyr::n()-1) ) |&gt; \n  tibble::add_column(\n    calc_cov = cov(a,b) # built in covariance \n    , cov_diff = cov_diff\n  ) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"Covariances\", subtitle = stringr::str_glue(\"T={T}\")) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nCovariances\n\n\nT=20\n\n\npop_cov\nsmpl_cov\ncalc_cov\ncov_diff\n\n\n\n\n0.9179426\n0.9662554\n0.9662554\n0.9179426\n\n\n\n\n\n\n\n1T2∑k=1T−1∑t=1T−k(at+k−at)(bt+k−bt)\\frac{1}{T^{2}}\\sum_{k=1}^{T-1}\\sum_{t=1}^{T-k}\\left(a_{t+k}-a_{t}\\right)\\left(b_{t+k}-b_{t}\\right)\nAnd the same expression hold for variance calculations: the population variance, calculated as the mean of the square of centered values, can also be calculated using all the differences between values.\n\n\nCode\na_var &lt;- (a-mean(a))^2 |&gt; mean()\n\n# calculate using difference equation\nvar_diff &lt;- \n  1:(T-1) |&gt; purrr::map_vec(\n    (\\(k){ ((dplyr::lead(a,k) - a)^2) |&gt; sum(na.rm=TRUE) })\n  ) |&gt; sum(na.rm=TRUE) / (T*T)\n\n# summarize results\ntibble::tibble(\"population variance\" = a_var, \"by difference calculation\" = var_diff) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"Variances\", subtitle = stringr::str_glue(\"variable a | T={T}\")) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nVariances\n\n\nvariable a | T=20\n\n\npopulation variance\nby difference calculation\n\n\n\n\n0.6150733\n0.6150733"
  },
  {
    "objectID": "supplemental/covariance_decomposition.html#implication-for-regression-coefficients",
    "href": "supplemental/covariance_decomposition.html#implication-for-regression-coefficients",
    "title": "Covariance Decomposition",
    "section": "Implication for regression coefficients",
    "text": "Implication for regression coefficients\nFrom FWL we know we can write our regression coefficient of interest as the ratio of a covariance over a variance, and so we can also express our coefficient in terms of differences.\nYit=αi+λt+δ⋅Dit;i∈{0,1},t∈{0,1}Δ1Yit=Δ1λt+δ⋅Δ1DitYi10−Yi00=λ(1−0)+δ(0−0)Yi11−Yi01=λ(1−0)+δ(1−0)\n\\begin{align*}\nY_{it} & =\\alpha_{i}+\\lambda_{t}+\\delta\\cdot D_{it};\\;i\\in\\left\\{ 0,1\\right\\} ,\\,t\\in\\left\\{ 0,1\\right\\} \\\\\n\\Delta_{1}Y_{it} & =\\Delta_{1}\\lambda_{t}+\\delta\\cdot\\Delta_{1}D_{it}\\\\\nY_{i1}^{0}-Y_{i0}^{0} & =\\lambda\\left(1-0\\right)+\\delta\\left(0-0\\right)\\\\\nY_{i1}^{1}-Y_{i0}^{1} & =\\lambda\\left(1-0\\right)+\\delta\\left(1-0\\right)\n\\end{align*}\n\n\n\nCode\n# TWFE Example with 40 samples\n# Setting seed for reproducibility\nset.seed(123)\n\n# Parameters\nn_units &lt;- 280        # Number of units/entities\nn_periods &lt;- 2      # Number of time periods\ntotal_samples &lt;- n_units * n_periods  # Total observations (40)\n\n# Unit and time fixed effects\nunit_effects &lt;- rnorm(n_units, mean = 5, sd = 2)  # Unit-specific effects\ntime_effects &lt;- rnorm(n_periods, mean = 0, sd = 1)  # Time-specific effects\nepsilon      &lt;- rnorm(total_samples, mean = 0, sd = 1)  # Error term\n\n# True treatment effect\ntrue_effect &lt;- 2.5\n\ndf &lt;- tibble::tibble(\n  unit = rep(1:n_units, each = n_periods)\n  , time = rep(1:n_periods, times = n_units)\n  , treatment =\n    dplyr::case_when((unit &gt;= n_units/2) & (time == 2) ~ 1, TRUE ~ 0)\n  , y = unit_effects[unit] +                  # Unit fixed effects\n    time_effects[time] +                      # Time fixed effects\n    true_effect * treatment +                 # Treatment effect\n    epsilon \n  , unit_effect = unit_effects[unit]\n  , time_effect = time_effects[time]\n)\n\n# panel data estimators\ntwfe_model &lt;- plm::plm(y ~ treatment, \n                 data = df |&gt; as.data.frame(), \n                 index = c(\"unit\", \"time\"), \n                 model = \"within\", \n                 effect = \"twoways\")\n\n# linear model\ntwfe_lm_model &lt;- lm(y ~ 1 + time + treatment, data = df)\n# lm(y ~ 1 + factor(unit) + factor(time) + factor(treatment), data = df)\n\n# plm::plm(y ~ + treatment, data = df, index = c(\"unit\", \"time\"), model = \"random\", effect = \"twoways\")\n\n# Display model results\ng &lt;- twfe_model |&gt; broom::tidy() |&gt; dplyr::mutate(model = 'twfe_plm_model', .before = 1) |&gt; \n  dplyr::bind_rows(\n    twfe_lm_model |&gt; broom::tidy() |&gt; dplyr::mutate(model = 'twfe_lm_model', .before = 1)\n  ) |&gt; dplyr::group_by(model) |&gt; \n  gt::gt(\"term\") |&gt; \n  gt::fmt_number(columns = -c(model,term), decimals = 3) |&gt; \n  gt::tab_header(title = \"TWFE Estimates\", subtitle = \"using lm and plm\") |&gt; \n  gtExtras::gt_theme_espn()\n\ndf_plot_data &lt;- df |&gt; dplyr::group_by(unit) |&gt; \n  dplyr::group_map(\n    .f = \n      ~dplyr::mutate(\n        .x, treated = dplyr::case_when(1 %in% treatment ~ \"treated\", TRUE ~ \"control\" )\n      )\n    , .keep = TRUE\n  ) |&gt; \n  dplyr::bind_rows() |&gt; dplyr::group_by(time, treated) |&gt; \n  dplyr::summarize(mean_y = mean(y))\n\n# p1 &lt;- df_plot_data |&gt; \n#   ggplot(aes(x = time, y = mean_y, group = treated, color = treated)) +\n#   geom_line(linewidth = 1) +\n#   geom_point(size = 3) +\n#   labs(title = \"Difference-in-Differences Visual\",\n#        subtitle = paste(\"True treatment effect =\", true_effect),\n#        x = \"Time Period\", y = \"Mean Outcome\", color = \"Group\") +\n#   scale_x_continuous(breaks = c(1, 2)) +\n#   theme_minimal() +\n#   theme(legend.position = \"none\")\n  \n\n# Plot group means over time\n# p1 &lt;- ggplot(plot_data, aes(x = time, y = mean_y, group = group, color = group)) +\n#   geom_line(linewidth = 1) +\n#   geom_point(size = 3) +\n#   labs(title = \"Difference-in-Differences Visual\",\n#        subtitle = paste(\"True treatment effect =\", true_effect),\n#        x = \"Time Period\", y = \"Mean Outcome\", color = \"Group\") +\n#   scale_x_continuous(breaks = c(1, 2)) +\n#   theme_minimal()\n\n# # Plot individual unit trajectories\n# p2 &lt;- df |&gt; dplyr::group_by(unit) |&gt; \n#   dplyr::group_map(\n#     .f = ~dplyr::mutate(.x, treated = dplyr::case_when(1 %in% treatment ~ \"treated\", TRUE ~ \"control\" ))\n#     , .keep = TRUE\n#   ) |&gt; \n#   dplyr::bind_rows() |&gt; \n#   rsample::group_initial_split(group=unit, prop = 0.2, strata = treated) |&gt; \n#   rsample::training() |&gt; \n#   ggplot(aes(x = time, y = y, group = unit, \n#                color = treated)) + # factor(ifelse(unit &lt;= T/2, \"Control\", \"Treated\"))\n#   geom_line(alpha = 0.3,  ) +\n#   geom_point() +\n#   # plot_data, aes(x = time, y = mean_y, group = group, color = group)) +\n#   labs(title = \"Individual Unit Trajectories\",\n#        x = \"Time Period\", y = \"Outcome\", color = \"Group\") +\n#   scale_x_continuous(breaks = c(1, 2)) +\n#   #theme_set(theme_bw(base_size = 12) + theme(legend.position = \"top\"))\n#   theme_minimal()\n\np3 &lt;- df |&gt; dplyr::group_by(unit) |&gt; \n  dplyr::group_map(\n    .f = ~dplyr::mutate(.x, treated = dplyr::case_when(1 %in% treatment ~ \"treated\", TRUE ~ \"control\" ))\n    , .keep = TRUE\n  ) |&gt; \n  dplyr::bind_rows() |&gt; \n  rsample::group_initial_split(group=unit, prop = 0.2, strata = treated) |&gt; \n  rsample::training() |&gt; \n  ggplot(aes(x = time, y = y, group = unit, color = treated)) + \n  geom_line(alpha = 0.8, linetype=3 ) +\n  geom_point(alpha = 0.8, shape = 15) +\n  geom_line(\n    data=df_plot_data, aes(x = time, y = mean_y, group = treated, color = treated), linewidth = 1\n  ) +\n  labs(x = \"Time Period\", y = \"Outcome\", color = \"Group\") +\n  scale_x_continuous(breaks = c(1, 2)) +\n  theme_minimal() +\n  theme(legend.title = element_blank()) \n\np3 + wrap_table(g, panel=\"full\", space = \"free_y\") + plot_annotation(\n  title = 'Individual Unit Trajectories (dotted), Mean Values (solid), and Model Estimates',\n  subtitle = \n    stringr::str_glue(\"True treatment effect = {true_effect} | {n_units} units | {n_periods} periods\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate means by group and time\ngroup_means &lt;- stats::aggregate(y ~ treatment + time, data = df, FUN = mean) |&gt; \n  dplyr::mutate(group = dplyr::case_when(treatment == 1 ~ \"Treated\", TRUE ~ \"Control\"))\n\ngroup_means |&gt; tidyr::pivot_wider(names_from = group, values_from = y) |&gt; \n  tibble::add_row(treatment = 1, time = 1, .before = 3) |&gt; \n  dplyr::mutate(\n    Treated = dplyr::case_when(dplyr::row_number() == 3 ~ mean(df$y[df$unit %in% 11:20 & df$time == 1]), TRUE ~ Treated)\n    , Treated = dplyr::lead(Treated,2)) |&gt; \n  dplyr::reframe(dplyr::across(.cols=Control:Treated, .fns = ~dplyr::lead(.x)-.x)) |&gt; \n  tidyr::drop_na() |&gt; \n  dplyr::mutate(diff = Treated - Control) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals = 3) |&gt; \n  gt::tab_header(title = \"Manually calculated difference-in-differences\") |&gt; \n  gt::cols_label(\n    Control = \"control mean diffs\",\n    Treated = \"treated mean diffs\",\n    diff = \"Treated - Control\"\n  ) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nManually calculated difference-in-differences\n\n\ncontrol mean diffs\ntreated mean diffs\nTreated - Control\n\n\n\n\n2.266\n4.537\n2.271\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated by differences\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.131\n0.123\n17.298\n0.000\n\n\ntreatment_diff\n2.743\n0.174\n15.798\n0.000\n\n\n\n\n\n\n\n\n# TWFE Example with 40 samples, 3 time periods, treatment in period 2 only\n# Setting seed for reproducibility\n# TWFE Example with 100 units, 3 time periods, treatment in period 2 only\n# Setting seed for reproducibility\nset.seed(123)\n\n# Parameters\nn_units &lt;- 100       # Number of units/entities\nn_periods &lt;- 3       # Number of time periods\ntotal_samples &lt;- n_units * n_periods  # Total observations (300)\n\n# Create balanced panel data\ncreate_twfe_data &lt;- function() {\n  # Create unit and time indices\n  unit_id &lt;- rep(1:n_units, each = n_periods)\n  time_id &lt;- rep(1:n_periods, times = n_units)\n  \n  # Unit and time fixed effects\n  unit_effects &lt;- rnorm(n_units, mean = 5, sd = 2)  # Unit-specific effects\n  time_effects &lt;- c(0, 0.5, 1)  # Time trend\n  \n  # Assign treatment (only in period 2)\n  # Half of the units are treated, but only in period 2\n  treated_units &lt;- 51:100  # Units 51-100 get treated in period 2 only\n  treatment &lt;- sapply(1:length(unit_id), function(i) {\n    unit_id[i] %in% treated_units && time_id[i] == 2\n  }) * 1\n  \n  # True treatment effect\n  true_effect &lt;- 2.5\n  \n  # Generate outcome with unit fixed effects, time fixed effects, and treatment effect\n  epsilon &lt;- rnorm(total_samples, mean = 0, sd = 1)  # Error term\n  y &lt;- unit_effects[unit_id] +                       # Unit fixed effects\n       time_effects[time_id] +                       # Time fixed effects\n       true_effect * treatment +                     # Treatment effect\n       epsilon                                       # Error term\n  \n  # Create dataframe\n  df &lt;- data.frame(\n    unit = unit_id,\n    time = time_id,\n    treatment = treatment,\n    y = y,\n    unit_effect = unit_effects[unit_id],\n    time_effect = time_effects[time_id]\n  )\n  \n  # Add treatment group indicator\n  df$group &lt;- ifelse(df$unit %in% treated_units, \"Treated\", \"Control\")\n  \n  return(list(df = df, true_effect = true_effect))\n}\n\n# Generate the data and store\ntwfe_data &lt;- create_twfe_data()\ndf &lt;- twfe_data$df\ntrue_effect &lt;- twfe_data$true_effect\n\n# Basic data summary\nprint(paste(\"Total observations:\", nrow(df)))\n\n[1] \"Total observations: 300\"\n\nprint(paste(\"Number of units:\", length(unique(df$unit))))\n\n[1] \"Number of units: 100\"\n\nprint(paste(\"Number of time periods:\", length(unique(df$time))))\n\n[1] \"Number of time periods: 3\"\n\nprint(paste(\"True treatment effect:\", true_effect))\n\n[1] \"True treatment effect: 2.5\"\n\n# Treatment summary\ntreatment_summary &lt;- table(df$group, df$time, df$treatment)\nprint(\"Treatment pattern:\")\n\n[1] \"Treatment pattern:\"\n\nprint(treatment_summary)\n\n, ,  = 0\n\n         \n           1  2  3\n  Control 50 50 50\n  Treated 50  0 50\n\n, ,  = 1\n\n         \n           1  2  3\n  Control  0  0  0\n  Treated  0 50  0\n\n# Calculate group means by treatment status and time\ngroup_means &lt;- aggregate(y ~ group + time, data = df, FUN = mean)\nprint(\"Group means by group and time period:\")\n\n[1] \"Group means by group and time period:\"\n\nprint(group_means)\n\n    group time        y\n1 Control    1 4.983993\n2 Treated    1 5.302649\n3 Control    2 5.439735\n4 Treated    2 8.558159\n5 Control    3 6.059079\n6 Treated    3 6.194647\n\n# Calculate treatment effect using a regression\nlibrary(plm)\ntwfe_model &lt;- plm(y ~ treatment, \n                 data = df, \n                 index = c(\"unit\", \"time\"), \n                 model = \"within\", \n                 effect = \"twoways\")\n\n# Display model results\nprint(\"TWFE Model Results:\")\n\n[1] \"TWFE Model Results:\"\n\nsummary_result &lt;- summary(twfe_model)\nprint(summary_result)\n\nTwoways effects Within Model\n\nCall:\nplm(formula = y ~ treatment, data = df, effect = \"twoways\", model = \"within\", \n    index = c(\"unit\", \"time\"))\n\nBalanced Panel: n = 100, T = 3, N = 300\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-2.337710 -0.536378 -0.036149  0.540570  2.411136 \n\nCoefficients:\n          Estimate Std. Error t-value  Pr(&gt;|t|)    \ntreatment  2.89131    0.25286  11.434 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    349.25\nResidual Sum of Squares: 209.93\nR-Squared:      0.39893\nAdj. R-Squared: 0.087716\nF-statistic: 130.749 on 1 and 197 DF, p-value: &lt; 2.22e-16\n\n# Calculate manual DiD estimate for period 2\nget_did_estimate &lt;- function(df) {\n  # Pre-period difference (time=1)\n  pre_diff &lt;- mean(df$y[df$group == \"Treated\" & df$time == 1]) - \n              mean(df$y[df$group == \"Control\" & df$time == 1])\n  \n  # Treatment period difference (time=2)\n  treat_diff &lt;- mean(df$y[df$group == \"Treated\" & df$time == 2]) - \n                mean(df$y[df$group == \"Control\" & df$time == 2])\n  \n  # DiD estimate\n  did_est &lt;- treat_diff - pre_diff\n  \n  return(did_est)\n}\n\nmanual_did &lt;- get_did_estimate(df)\nprint(paste(\"Manual DiD estimate:\", round(manual_did, 4)))\n\n[1] \"Manual DiD estimate: 2.7998\"\n\n# Calculate event study estimates\nevent_study_model &lt;- lm(y ~ factor(unit) + factor(time) + \n                           group:factor(time), data = df)\n\n# Extract coefficients for group:time interactions\nes_coefs &lt;- coef(event_study_model)[grepl(\"group.*:factor\\\\(time\\\\)\", names(coef(event_study_model)))]\nprint(\"Event study coefficients (relative to period 1):\")\n\n[1] \"Event study coefficients (relative to period 1):\"\n\nprint(es_coefs)\n\nnamed numeric(0)\n\n# Visualize the data\nlibrary(ggplot2)\n\n# Plot group means over time\nggplot(group_means, aes(x = time, y = y, group = group, color = group)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  annotate(\"rect\", xmin = 1.5, xmax = 2.5, ymin = -Inf, ymax = Inf, \n           alpha = 0.2, fill = \"lightblue\") +\n  annotate(\"text\", x = 2, y = max(group_means$y), \n           label = \"Treatment Period\", vjust = -1) +\n  labs(title = \"Effect of Treatment in Period 2 Only\",\n       subtitle = paste(\"True treatment effect =\", true_effect),\n       x = \"Time Period\", y = \"Mean Outcome\", color = \"Group\") +\n  scale_x_continuous(breaks = 1:3) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n# Calculate and plot the distribution of treatment effects\ndf_period2 &lt;- df[df$time == 2,]\ndf_period1 &lt;- df[df$time == 1,]\n\n# Merge period 1 data to period 2\ndf_period2$y_pre &lt;- df_period1$y[match(df_period2$unit, df_period1$unit)]\ndf_period2$y_diff &lt;- df_period2$y - df_period2$y_pre\n\n# Plot distribution of changes by group\nggplot(df_period2, aes(x = y_diff, fill = group)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(xintercept = mean(df_period2$y_diff[df_period2$group == \"Treated\"]), \n             color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean(df_period2$y_diff[df_period2$group == \"Control\"]), \n             color = \"blue\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Changes from Period 1 to 2\",\n       subtitle = paste(\"Mean difference (Treated - Control):\", \n                       round(mean(df_period2$y_diff[df_period2$group == \"Treated\"]) - \n                             mean(df_period2$y_diff[df_period2$group == \"Control\"]), 4)),\n       x = \"Change in Outcome\", y = \"Density\", fill = \"Group\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Plot a subset of individual trajectories (20 units for clarity)\nsampled_units &lt;- c(sample(1:50, 10), sample(51:100, 10))\ndf_subset &lt;- df[df$unit %in% sampled_units,]\n\nggplot(df_subset, aes(x = time, y = y, group = unit, \n                     color = group)) +\n  geom_line(alpha = 0.7) +\n  geom_point(aes(shape = factor(treatment)), size = 3) +\n  scale_shape_manual(values = c(16, 8), name = \"Treated\") +\n  annotate(\"rect\", xmin = 1.5, xmax = 2.5, ymin = -Inf, ymax = Inf, \n           alpha = 0.1, fill = \"lightblue\") +\n  labs(title = \"Sample of Individual Unit Trajectories\",\n       subtitle = \"Treatment occurs only in period 2 for treated units\",\n       x = \"Time Period\", y = \"Outcome\", color = \"Group\") +\n  scale_x_continuous(breaks = 1:3) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Perform placebo test comparing period 1 to period 3 changes (no treatment in either)\ndf_period3 &lt;- df[df$time == 3,]\ndf_period1$y_post &lt;- df_period3$y[match(df_period1$unit, df_period3$unit)]\ndf_period1$y_diff &lt;- df_period1$y_post - df_period1$y\n\nplacebo_diff &lt;- mean(df_period1$y_diff[df_period1$group == \"Treated\"]) - \n                mean(df_period1$y_diff[df_period1$group == \"Control\"])\n\nprint(paste(\"Placebo test (period 1 to 3 difference):\", round(placebo_diff, 4)))\n\n[1] \"Placebo test (period 1 to 3 difference): -0.1831\"\n\n# Test for heterogeneous treatment effects\ndf_treated_p2 &lt;- df[df$treatment == 1,]\ndf_treated_p2$unit_effect_quantile &lt;- cut(df_treated_p2$unit_effect, \n                                         breaks = quantile(df_treated_p2$unit_effect, probs = seq(0, 1, 0.25)),\n                                         labels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"), \n                                         include.lowest = TRUE)\n\n# Calculate treatment effect by unit effect quantile\ndf_treated_p1 &lt;- df[df$group == \"Treated\" & df$time == 1,]\ndf_treated_p1$unit_effect_quantile &lt;- cut(df_treated_p1$unit_effect, \n                                         breaks = quantile(df_treated_p1$unit_effect, probs = seq(0, 1, 0.25)),\n                                         labels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"), \n                                         include.lowest = TRUE)\n\ndf_treated_p2$y_pre &lt;- df_treated_p1$y[match(df_treated_p2$unit, df_treated_p1$unit)]\ndf_treated_p2$effect_estimate &lt;- df_treated_p2$y - df_treated_p2$y_pre\n\n# Get control trend\ncontrol_trend &lt;- mean(df$y[df$group == \"Control\" & df$time == 2]) - \n                mean(df$y[df$group == \"Control\" & df$time == 1])\n\ndf_treated_p2$effect_adjusted &lt;- df_treated_p2$effect_estimate - control_trend\n\nhet_effects &lt;- aggregate(effect_adjusted ~ unit_effect_quantile, \n                        data = df_treated_p2, FUN = mean)\n\nprint(\"Heterogeneous treatment effects by unit effect quantile:\")\n\n[1] \"Heterogeneous treatment effects by unit effect quantile:\"\n\nprint(het_effects)\n\n  unit_effect_quantile effect_adjusted\n1                   Q1        2.525934\n2                   Q2        2.870579\n3                   Q3        2.820147\n4                   Q4        2.989424\n\n# Plot heterogeneous effects\nggplot(het_effects, aes(x = unit_effect_quantile, y = effect_adjusted)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_hline(yintercept = true_effect, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Heterogeneous Treatment Effects\",\n       subtitle = \"By Unit Effect Quantile\",\n       x = \"Unit Effect Quantile\", y = \"Estimated Treatment Effect\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Check power of the test\npower_analysis &lt;- function(df, n_simulations = 1000) {\n  # Extract model parameters\n  sigma &lt;- sd(residuals(twfe_model))\n  effect_sizes &lt;- seq(0, 5, 0.5)\n  \n  # Store power results\n  power_results &lt;- data.frame(\n    effect_size = effect_sizes,\n    power = NA\n  )\n  \n  for (i in 1:length(effect_sizes)) {\n    # Track significant results\n    significant_results &lt;- 0\n    \n    for (j in 1:n_simulations) {\n      # Create simulated data with this effect size\n      df_sim &lt;- df\n      df_sim$y &lt;- df_sim$unit_effect + df_sim$time_effect + \n                 effect_sizes[i] * df_sim$treatment + \n                 rnorm(nrow(df_sim), mean = 0, sd = sigma)\n      \n      # Run the model\n      model_sim &lt;- plm(y ~ treatment, \n                      data = df_sim, \n                      index = c(\"unit\", \"time\"), \n                      model = \"within\", \n                      effect = \"twoways\")\n      \n      # Check if result is significant at 5% level\n      p_value &lt;- summary(model_sim)$coefficients[1, \"Pr(&gt;|t|)\"]\n      if (p_value &lt; 0.05) {\n        significant_results &lt;- significant_results + 1\n      }\n    }\n    \n    # Calculate power\n    power_results$power[i] &lt;- significant_results / n_simulations\n  }\n  \n  return(power_results)\n}\n\n# Run a smaller number of simulations for demonstration\npower_results &lt;- power_analysis(df, n_simulations = 100)\nprint(\"Power analysis results:\")\n\n[1] \"Power analysis results:\"\n\nprint(power_results)\n\n   effect_size power\n1          0.0  0.05\n2          0.5  0.68\n3          1.0  1.00\n4          1.5  1.00\n5          2.0  1.00\n6          2.5  1.00\n7          3.0  1.00\n8          3.5  1.00\n9          4.0  1.00\n10         4.5  1.00\n11         5.0  1.00\n\n# Plot power curve\nggplot(power_results, aes(x = effect_size, y = power)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = true_effect, linetype = \"dashed\", color = \"blue\") +\n  labs(title = \"Power Analysis\",\n       subtitle = \"Power to detect different effect sizes\",\n       x = \"Effect Size\", y = \"Power\") +\n  theme_minimal()"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review.",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have nn observations. Let the ithi^{th} be (xi1,…,xip,yi)(x_{i1}, \\ldots, x_{ip}, y_i), such that xi1,…,xipx_{i1}, \\ldots, x_{ip} are the explanatory variables (predictors) and yiy_i is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\ny=β0+β1x1+…+βpxp(1)\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \n \\qquad(1)\nWe can write the response for the ithi^{th} observation as shown in Equation 2.\nyi=β0+β1xi1+…+βpxip+ϵi(2)\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i \n \\qquad(2)\nsuch that ϵi\\epsilon_i is the amount yiy_i deviates from μ{y|xi1,…,xip}\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}, the mean response for a given combination of explanatory variables. We assume each ϵi∼N(0,σ2)\\epsilon_i \\sim N(0,\\sigma^2), where σ2\\sigma^2 is a constant variance for the distribution of the response yy for any combination of explanatory variables x1,…,xpx_1, \\ldots, x_p.",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n𝐘=[y1y2⋮yn]𝐗=[x11x12…x1px21x22…x2p⋮⋮⋱⋮xn1xn2…xnp]𝛃=[β0β1⋮βp]𝛜=[ϵ1ϵ2⋮ϵn](3)\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} \n\\hspace{1cm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{1cm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} \n\\hspace{1cm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n \\qquad(3)\nThus,\n𝐘=𝐗𝛃+𝛜\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n𝐘̂=𝐗𝛃̂𝐞=𝐘−𝐗𝛃̂(4)\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{1cm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n \\qquad(4)",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that 𝛃̂\\hat{\\boldsymbol{\\beta}} can be written as the following:\n𝛃̂=(𝐗T𝐗)−1𝐗T𝐘(5)\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n \\qquad(5)\nCombining Equation 4 and Equation 5, we can write 𝐘̂\\hat{\\mathbf{Y}} as the following:\n𝐘̂=𝐗𝛃̂=𝐗(𝐗T𝐗)−1𝐗T𝐘(6)\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n \\qquad(6)\nWe define the hat matrix as an n×nn \\times n matrix of the form 𝐇=𝐗(𝐗T𝐗)−1𝐗T\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T. Thus Equation 6 becomes\n𝐘̂=𝐇𝐘(7)\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n \\qquad(7)\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, hiih_{ii} is a measure of how far the values of the predictor variables for the ithi^{th} observation, xi1,xi2,…,xipx_{i1}, x_{i2}, \\ldots, x_{ip}, are from the mean values of the predictor variables, x‾1,x‾2,…,x‾p\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p. In the case of simple linear regression, the ithi^{th} diagonal, hiih_{ii}, can be written as\nhii=1n+(xi−x‾)2∑j=1n(xj−x‾)2\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n0≤hii≤10 \\leq h_ii \\leq 1\n∑i=1nhii=p+1\\sum\\limits_{i=1}^{n} h_{ii} = p+1, where pp is the number of predictor variables in the model.\nThe mean hat value is h‾=∑i=1nhiin=p+1n\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}.\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than 2(p+1)n\\frac{2(p+1)}{n} are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation 4 using Equation 7.\n𝐞=𝐘−𝐘̂=𝐘−𝐇𝐘=(1−𝐇)𝐘(8)\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n \\qquad(8)\nNote that the identity matrix and hat matrix are idempotent, i.e. 𝐈𝐈=𝐈\\mathbf{I}\\mathbf{I} = \\mathbf{I}, 𝐇𝐇=𝐇\\mathbf{H}\\mathbf{H} = \\mathbf{H}. Thus, (𝐈−𝐇)(\\mathbf{I} - \\mathbf{H}) is also idempotent. These matrices are also symmetric. Using these properties and Equation 8, we have that the variance-covariance matrix of the residuals 𝐞\\boldsymbol{e}, is\nVar(𝐞)=𝐞𝐞T=(1−𝐇)Var(𝐘)T(1−𝐇)T=(1−𝐇)σ̂2(1−𝐇)T=σ̂2(1−𝐇)(1−𝐇)=σ̂2(1−𝐇)(9)\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n \\qquad(9)\nwhere σ̂2=∑i=1nei2n−p−1\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1} is the estimated regression variance. Thus, the variance of the ithi^{th} residual is Var(ei)=σ̂2(1−hii)Var(e_i) = \\hat{\\sigma}^2(1-h_{ii}). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage.",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the ithi^{th} standardized residual takes the form\nstd.resi=ei−E(ei)SE(ei)\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\nThe expected value of the residuals is 0, i.e. E(ei)=0E(e_i) = 0. From Equation 9), the standard error of the residual is SE(ei)=σ̂1−hiiSE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}. Therefore,\nstd.resi=eiσ̂1−hii(10)\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n \\qquad(10)",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the ithi^{th} observation can be written as\nDi=(𝐘̂−𝐘̂(i))T(𝐘̂−𝐘̂(i))(p+1)σ̂(11)\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n \\qquad(11)\nwhere 𝐘̂(i)\\hat{\\mathbf{Y}}_{(i)} is the vector of predicted values from the model fitted when the ithi^{th} observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Equation 12 below is mathematically equivalent to Equation 11.\nDi=1p+1std.resi2[hii(1−hii)]=ei2(p+1)σ̂2(1−hii)[hii(1−hii)](12)\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n \\qquad(12)",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (β1\\beta_1) and intercept (β0\\beta_0). We obtain the estimates, β̂1\\hat{\\beta}_1 and β̂0\\hat{\\beta}_0 by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\nSSR=∑i=1n[yi−ŷi]2=[yi−(β̂0+β̂1xi)]2=[yi−β̂0−β̂1xi]2(1)\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n \\qquad(1)\nRecall that we can find the values of β̂1\\hat{\\beta}_1 and β̂0\\hat{\\beta}_0 that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of β̂1\\hat{\\beta}_1 and β̂0\\hat{\\beta}_0 that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n∂SSR∂β̂1=−2∑i=1nxi(yi−β̂0−β̂1xi)∂SSR∂β̂0=−2∑i=1n(yi−β̂0−β̂1xi)(2)\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n \\qquad(2)\nThe derivation of deriving β̂0\\hat{\\beta}_0 is shown in Equation 3.\n∂SSR∂β̂0=−2∑i=1n(yi−β̂0−β̂1xi)=0⇒−∑i=1n(yi+β̂0+β̂1xi)=0⇒−∑i=1nyi+nβ̂0+β̂1∑i=1nxi=0⇒nβ̂0=∑i=1nyi−β̂1∑i=1nxi⇒β̂0=1n(∑i=1nyi−β̂1∑i=1nxi)⇒β̂0=y‾−β̂1x‾(3)\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n \\qquad(3)\nThe derivation of β̂1\\hat{\\beta}_1 using the β̂0\\hat{\\beta}_0 we just derived is shown in Equation 4.\n∂SSR∂β̂1=−2∑i=1nxi(yi−β̂0−β̂1xi)=0⇒−∑i=1nxiyi+β̂0∑i=1nxi+β̂1∑i=1nxi2=0(Fill in β̂0)⇒−∑i=1nxiyi+(y‾−β̂1x‾)∑i=1nxi+β̂1∑i=1nxi2=0⇒(y‾−β̂1x‾)∑i=1nxi+β̂1∑i=1nxi2=∑i=1nxiyi⇒y‾∑i=1nxi−β̂1x‾∑i=1nxi+β̂1∑i=1nxi2=∑i=1nxiyi⇒ny‾x‾−β̂1nx‾2+β̂1∑i=1nxi2=∑i=1nxiyi⇒β̂1∑i=1nxi2−β̂1nx‾2=∑i=1nxiyi−ny‾x‾⇒β̂1(∑i=1nxi2−nx‾2)=∑i=1nxiyi−ny‾x‾β̂1=∑i=1nxiyi−ny‾x‾∑i=1nxi2−nx‾2(4)\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n \\qquad(4)\nTo write β̂1\\hat{\\beta}_1 in a form that’s more recognizable, we will use the following:\n∑xiyi−ny‾x‾=∑(x−x‾)(y−y‾)=(n−1)Cov(x,y)(5)\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n \\qquad(5)\n∑xi2−nx‾2−∑(x−x‾)2=(n−1)sx2(6)\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n \\qquad(6)\nwhere Cov(x,y)\\text{Cov}(x,y) is the covariance of xx and yy, and sx2s_x^2 is the sample variance of xx (sxs_x is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\nβ̂1=∑i=1nxiyi−ny‾x‾∑i=1nxi2−nx‾2=∑i=1n(x−x‾)(y−y‾)∑i=1n(x−x‾)2=(n−1)Cov(x,y)(n−1)sx2=Cov(x,y)sx2(7)\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n \\qquad(7)\nThe correlation between xx and yy is r=Cov(x,y)sxsyr = \\frac{\\text{Cov}(x,y)}{s_x s_y}. Thus, Cov(x,y)=rsxsy\\text{Cov}(x,y) = r s_xs_y. Plugging this into Equation 7, we have\nβ̂1=Cov(x,y)sx2=rsysxsx2=rsysx(8)\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n \\qquad(8)",
    "crumbs": [
      "Supplemental notes",
      "SLR derivations"
    ]
  },
  {
    "objectID": "supplemental/exam_questions.html",
    "href": "supplemental/exam_questions.html",
    "title": "Exam-like questions",
    "section": "",
    "text": "Each quiz, midterm and the final exam will consist of two parts:\n\nPart 1 - Conceptual: Simple questions designed to evaluate your familiarity with the written course notes.\nPart 2 - Applied: Data analysis in RStudio (like a usual lab, but simpler).",
    "crumbs": [
      "Supplemental notes",
      "Sample exam questions"
    ]
  },
  {
    "objectID": "supplemental/exam_questions.html#notes-on-quizes-midterms-and-the-final-exam",
    "href": "supplemental/exam_questions.html#notes-on-quizes-midterms-and-the-final-exam",
    "title": "Exam-like questions",
    "section": "",
    "text": "Each quiz, midterm and the final exam will consist of two parts:\n\nPart 1 - Conceptual: Simple questions designed to evaluate your familiarity with the written course notes.\nPart 2 - Applied: Data analysis in RStudio (like a usual lab, but simpler).",
    "crumbs": [
      "Supplemental notes",
      "Sample exam questions"
    ]
  },
  {
    "objectID": "supplemental/exam_questions.html#conceptual-questions",
    "href": "supplemental/exam_questions.html#conceptual-questions",
    "title": "Exam-like questions",
    "section": "Conceptual Questions",
    "text": "Conceptual Questions\nHere are examples of the conceptual questions that might be asked:\nWhat is the primary difference between inner_join() and left_join()? a) inner_join() keeps all rows from both tables, left_join() keeps all rows from the left table b) inner_join() keeps only matching rows, left_join() keeps all rows from the left table c) inner_join() is faster, left_join() is more accurate d) There is no difference\nIn the context of tidy data, which of the following is NOT a principle? a) Every column is a measurement b) Every row is an observation c) Every cell is a single value d) Every dataset has multiple tables\nWhat does the step_dummy() function in recipes do? a) Remove duplicate rows b) Create dummy variables for categorical predictors c) Impute missing values d) Scale numeric variables\nWhich of the following is NOT a step in the typical Tidymodels workflow? a) Data splitting b) Model specification c) Model training d) Database querying",
    "crumbs": [
      "Supplemental notes",
      "Sample exam questions"
    ]
  },
  {
    "objectID": "supplemental/exam_questions.html#applied-questions",
    "href": "supplemental/exam_questions.html#applied-questions",
    "title": "Exam-like questions",
    "section": "Applied Questions",
    "text": "Applied Questions\nHere are examples of the applied questions that might be asked:\nUsing the ggplot2::diamonds dataset. Using tidyverse functions: a) Create a new variable called ‘size’ that is the product of x, y, and z. b) Filter out any diamonds with a price greater than $10,000 or less than $500. c) Fit a multiple regression model predicting price based on carat, cut, and size. d) Use broom::augment() to add residuals and fitted values to your dataset.\nUsing the datasets::mtcars dataset, create a linear regression model to predict mpg based on wt and hp. Use the tidyverse and broom packages to: a) Create a tibble with only the mpg, wt, and hp columns. b) Fit the linear model. c) Extract the model coefficients and their p-values into a tidy tibble.",
    "crumbs": [
      "Supplemental notes",
      "Sample exam questions"
    ]
  },
  {
    "objectID": "supplemental/decompositions.html",
    "href": "supplemental/decompositions.html",
    "title": "Decompositions",
    "section": "",
    "text": "When faced with a gap in mean outcomes between two groups, researchers frequently examine how much of the gap can be explained by differences in observable characteristics.\nThe simple approach is to estimate the pooled regression including an indicator variable for group membership as well as the other observable characteristics, interpreting the coefficient on the group indicator as the unexplained component.\nThe Oaxaca-Blinder (O-B) decomposition represents an alternative approach\n\n\nConsider a categorical (or dummy) variable dd that splits our dataset into two groups.\nIn this case we can run regressions of the form y=Xβ+ϵy=X\\beta+\\epsilon to estimate the the mean difference between groups, as follows\n𝔼[y0]=𝔼[X(0)]β0;group d=0𝔼[y1]=𝔼[X(1)]β1;group d=1\n\\begin{align*}\n\\mathbb{E}\\left[y^{0}\\right] & =\\mathbb{E}\\left[X^{(0)}\\right]\\beta_{0};\\;\\text{group }d=0\\\\\n\\mathbb{E}\\left[y^{1}\\right] & =\\mathbb{E}\\left[X^{(1)}\\right]\\beta_{1};\\;\\text{group }d=1\n\\end{align*}\n\nAlternatively\ny‾0=X‾0β0;group d=0y‾1=X‾1β1;group d=1\n\\begin{align*}\n\\bar{y}_0 & =\\bar{X}_0\\beta_{0};\\;\\text{group }d=0\\\\\n\\bar{y}_1 & =\\bar{X}_1\\beta_{1};\\;\\text{group }d=1\n\\end{align*}\n\nThen the mean difference in outcomes is:\n𝔼[y1]−𝔼[y0]=𝔼[X(1)]β1−𝔼[X(0)]β0=(𝔼[X(1)]−𝔼[X(0)])β1+𝔼[X(0)](β1−β0)=(𝔼[X(1)]−𝔼[X(0)])β0−𝔼[X(1)](β0−β1)\n\\begin{align*}\n\\mathbb{E}\\left[y^{1}\\right]-\\mathbb{E}\\left[y^{0}\\right] & =\\mathbb{E}\\left[X^{(1)}\\right]\\beta_{1}-\\mathbb{E}\\left[X^{(0)}\\right]\\beta_{0}\\\\\n & =\\left(\\mathbb{E}\\left[X^{(1)}\\right]-\\mathbb{E}\\left[X^{(0)}\\right]\\right)\\beta_{1}+\\mathbb{E}\\left[X^{(0)}\\right]\\left(\\beta_{1}-\\beta_{0}\\right)\\\\\n & =\\left(\\mathbb{E}\\left[X^{(1)}\\right]-\\mathbb{E}\\left[X^{(0)}\\right]\\right)\\beta_{0}-\\mathbb{E}\\left[X^{(1)}\\right]\\left(\\beta_{0}-\\beta_{1}\\right)\n\\end{align*}\n\nAlternatively\ny‾1−y‾0=X‾1β1−X‾0β0=(X‾1−X‾0)β1+X‾0(β1−β0)=(X‾1−X‾0)β0−X‾1(β0−β1)(1)\n\\begin{align*}\n\\bar{y}_1-\\bar{y}_0 & =\\bar{X}_1\\beta_{1}-\\bar{X}_0\\beta_{0}\\\\\n & =\\left(\\bar{X}_1-\\bar{X}_0\\right)\\beta_{1}+\\bar{X}_0\\left(\\beta_{1}-\\beta_{0}\\right)\\\\\n & =\\left(\\bar{X}_1-\\bar{X}_0\\right)\\beta_{0}-\\bar{X}_1\\left(\\beta_{0}-\\beta_{1}\\right)\n\\end{align*}\n \\qquad(1)\nWhere:\n(X‾1−X‾0)β1\\left( \\bar{X}_1-\\bar{X}_0\\right)\\beta_{1} is the “explained” component (differences in characteristics), and X‾0(β1−β0)\\bar{X}_0\\left(\\beta_{1}-\\beta_{0}\\right)) is the “unexplained” component (differences in returns to characteristics), where X‾0\\bar{X}_0 is the baseline (this decomposition is not unique, as you can see from the third line of Equation 1)\nand we define\nGap1=X‾0(β1−β0)Gap0=X‾1(β0−β1)GapOLS=δd;where y=δ0+δdd+δ1X+ϵ\n\\begin{align*}\n\\text{Gap}^{1} & =\\bar{X}_0\\left(\\beta_{1}-\\beta_{0}\\right)\\\\\n\\text{Gap}^{0} & =\\bar{X}_1\\left(\\beta_{0}-\\beta_{1}\\right)\\\\\n\\text{Gap}^{\\text{OLS}} & =\\delta_{d};\\;\\text{where }y=\\delta_{0}+\\delta_{d}d+\\delta_{1}X+\\epsilon\n\\end{align*}\n where the strong assumptions that the model is properly specified and that coefficients are equal across groups, a sensible definition of the population unexplained gap is δd\\delta_d.\nExample\ndata:\n\n# https://cran.r-project.org/web/packages/oaxaca/vignettes/oaxaca.pdf\n# https://www.worldbank.org/content/dam/Worldbank/document/HDN/Health/HealthEquityCh12.pdf\n# https://pmc.ncbi.nlm.nih.gov/articles/PMC8343972/\n# https://www.sciencedirect.com/science/article/abs/pii/S0169721811004072\n# https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2528391\n# https://link.springer.com/article/10.1186/s12982-021-00100-9\n# https://giacomovagni.com/blog/2023/oaxaca/\n# https://journals.sagepub.com/doi/pdf/10.1177/1536867X0800800401\n# https://ocw.mit.edu/courses/14-662-labor-economics-ii-spring-2015/resources/mit14_662s15_lecnotes1/\n\n# Load HMDA data\ndata('PSID1982', package = \"AER\")\nhmda &lt;- PSID1982\n\n# data(\"nswcps\", package = \"hettreatreg\")\n\n# Prepare data for analysis\n# Looking at income differences between racial groups\nlending_data &lt;- hmda |&gt; \n  dplyr::mutate(\n    minority = factor(race != \"white\"),\n    log_income = log(income)\n  ) |&gt; \n  dplyr::select(log_income, minority, education, hrat, ccred, mcred, pubrec)\n\n\n# Fit separate regressions\nmodel_a &lt;- lm(wage ~ education + experience, data = dat |&gt; dplyr::filter(union=='yes'))\nmodel_b &lt;- lm(wage ~ education + experience, data = dat |&gt; dplyr::filter(union=='no'))\n\n# Get mean characteristics (including intercept)\nX_mean_a &lt;- c(1, colMeans( dat |&gt; dplyr::filter(union=='yes') |&gt; dplyr::select(education, experience) ) )\nX_mean_b &lt;- c(1, colMeans( dat |&gt; dplyr::filter(union=='no') |&gt; dplyr::select(education, experience) ) )\n\n# Get coefficients\nbeta_a &lt;- coef(model_a)\nbeta_b &lt;- coef(model_b)\n\n# Calculate decomposition\ntibble::tibble(\n  explained = sum((X_mean_a - X_mean_b) * beta_a)\n  , unexplained = sum(X_mean_b * (beta_a - beta_b))\n  , total_gap &lt;- explained + unexplained\n)\n\n# Perform Oaxaca-Blinder decomposition\ndecomp &lt;- oaxaca::oaxaca(wage ~ education + experience | union, \n                 data = dat |&gt; dplyr::mutate(union = dplyr::case_when(union=='yes'~0, TRUE ~1)))\n\nWe an define another measure Gapp\\text{Gap}^{p} as\ny‾1−y‾0=(X‾1−X‾0)β̂p+GappGapp=X‾1(β̂1−β̂p)+X‾0(β̂p−β̂0)\n\\begin{align*}\n\\bar{y}_1-\\bar{y}_0 & =\\left(\\bar{X}_1-\\bar{X}_0\\right)\\hat{\\beta}^{p}+\\text{Gap}^p\\\\\n\\text{Gap}^p & =\\bar{X}_1\\left(\\hat{\\beta}_1-\\hat{\\beta}^{p}\\right)+\\bar{X}_0\\left(\\hat{\\beta}^{p}-\\hat{\\beta}^{0}\\right)\n\\end{align*}\n\nwhere β̂p\\hat{\\beta}^{p} is the coefficient from the pooled regression of yy on XX.\nAn O-B unexplained gap can always be written as the difference in overall mean outcomes minus the difference in predicted mean outcomes, and both of these differences can be denoted by linear projections.\nA general expression for an O-B unexplained gap is\nGap=[y‾1−y‾0]−[β̂(x‾1−x‾0)]=b(y|d)−b(β̂x|d)(2)\n\\begin{align*}\n\\text{Gap} & =\\left[\\bar{y}_{1}-\\bar{y}_{0}\\right]-\\left[\\hat{\\beta}\\left(\\bar{x}_{1}-\\bar{x}_{0}\\right)\\right]\\\\\n & =\\text{b}(y\\vert d)-\\text{b}(\\hat{\\beta}x\\vert d)\n\\end{align*}\n \\qquad(2)\nwhere β̂\\hat{\\beta} is a coefficient computed from sample data and b(z|w)\\text{b}(z\\vert w) is the slope from a regression of zz on ww and an intercept.\nSo equation Equation 2 gives Gap1\\text{Gap}^{1} when β̂\\hat{\\beta} is the slope coefficient of a regression of yy on xx using the data from group 1, and gives Gap0\\text{Gap}^{0} when β̂\\hat{\\beta} is the slope coefficient using data from group 01.\nGap=b(y|d)−b(β̂x|d)=cov(d,y)var(d)−β̂cov(x,d)var(d)=cov(d,β0+βdd+βxx)+ϵvar(d)−β̂cov(d,x)var(d)=βd+βxcov(d,x)var(d)−β̂cov(d,x)var(d)→βd,when βx=β̂(3)\n\\begin{align*}\n\\text{Gap} & =\\text{b}(y\\vert d)-\\text{b}(\\hat{\\beta}x\\vert d)\\\\\n & =\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\hat{\\beta}\\frac{\\text{cov}\\left(x,d\\right)}{\\text{var}(d)}\\\\\n & =\\frac{\\text{cov}\\left(d,\\beta_{0}+\\beta_{d}d+\\beta_{x}x\\right)+\\epsilon}{\\text{var}(d)}-\\hat{\\beta}\\frac{\\text{cov}\\left(d,x\\right)}{\\text{var}(d)}\\\\\n & =\\beta_{d}+\\beta_{x}\\frac{\\text{cov}\\left(d,x\\right)}{\\text{var}(d)}-\\hat{\\beta}\\frac{\\text{cov}\\left(d,x\\right)}{\\text{var}(d)}\\\\\n & \\rightarrow\\beta_{d},\\;\\text{when }\\beta_{x}=\\hat{\\beta}\n\\end{align*}\n \\qquad(3)\nimplying that all the gap expressions are equivalent, except for Gapp\\text{Gap}^{p}. The difference arises because in the pooled regression used to compute Gapp\\text{Gap}^p, dd is a omitted variable.\nGapp=b(y|d)−b(xb(y|x)|d)=cov(d,y)var(d)−cov(d,[cov(x,y)/var(x)])var(d)=cov(d,y)var(d)−cov(d,x)var(d)×cov(x,y)var(x)=1var(d)(cov(d,y)−cov(d,x)cov(x,y)var(x))\n\\begin{align*}\n\\text{Gap}^{p} & =\\text{b}(y\\vert d)-\\text{b}(x\\text{b}(y\\vert x)\\vert d)\\\\\n & =\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\frac{\\text{cov}\\left(d,\\left[\\text{cov}\\left(x,y\\right)/\\text{var}\\left(x\\right)\\right]\\right)}{\\text{var}(d)}\\\\\n & =\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\frac{\\text{cov}\\left(d,x\\right)}{\\text{var}(d)}\\times\\frac{\\text{cov}\\left(x,y\\right)}{\\text{var}(x)}\\\\\n & =\\frac{1}{\\text{var}(d)}\\left(\\text{cov}\\left(d,y\\right)-\\frac{\\text{cov}\\left(d,x\\right)\\text{cov}\\left(x,y\\right)}{\\text{var}(x)}\\right)\n\\end{align*}\n\nCompare this to the GapOLS\\text{Gap}^{\\text{OLS}}, defining z̃(w)\\tilde{z}(w) as the component of zz that is orthogonal to ww in the population (so that z̃(w)=z−wb(z|w)\\tilde{z}(w)=z-w\\text{b}(z\\vert w), alternatively z̃(w)=z−wcov(z,w)var(w)\\tilde{z}(w)=z-w\\frac{\\text{cov}(z,w)}{\\text{var}(w)})2\nGapOLS=δd=cov(d̃(x),ỹ(x))var(d̃(x));per FWL=cov(d,ỹ(x))var(d̃(x))−cov(x,ỹ(x))var(d̃(x))×cov(d,x)var(x);per definition of d̃(x)=cov(d,ỹ(x))var(d̃(x));per definition of ỹ(x),cov(x,ỹ(x))=0=1var(d̃(x))(cov(d,y)−cov(d,x)cov(x,y)var(x))\n\\begin{align*}\n\\text{Gap}^{\\text{OLS}} & =\\delta_{d}\\\\\n & =\\frac{\\text{cov}(\\tilde{d}(x),\\tilde{y}(x))}{\\text{var}(\\tilde{d}(x))};\\;\\text{per FWL}\\\\\n & =\\frac{\\text{cov}(d,\\tilde{y}(x))}{\\text{var}(\\tilde{d}(x))}-\\frac{\\text{cov}(x,\\tilde{y}(x))}{\\text{var}(\\tilde{d}(x))}\\times\\frac{\\text{cov}(d,x)}{\\text{var}(x)};\\;\\text{per definition of }\\tilde{d}(x)\\\\\n & =\\frac{\\text{cov}(d,\\tilde{y}(x))}{\\text{var}(\\tilde{d}(x))};\\;\\text{per definition of }\\tilde{y}(x),\\,\\text{cov}(x,\\tilde{y}(x))=0\\\\\n & =\\frac{1}{\\text{var}(\\tilde{d}(x))}\\left(\\text{cov}(d,y)-\\text{cov}(d,x)\\frac{\\text{cov}(x,y)}{\\text{var}(x)}\\right)\n\\end{align*}\n\nand so Gapp=var(d̃(x))var(d)GapOLS\\text{Gap}^{p}=\\frac{\\text{var}(\\tilde{d}(x))}{\\text{var}(d)}\\text{Gap}^{\\text{OLS}}\n\n\nThe law of total variance, or variance decomposition formula is\nvar(X)=𝔼[var(X|Y)]+var(𝔼[X|Y])\n\\text{var}(X)=\\mathbb{E}[\\text{var}(X\\vert Y)]+\\text{var}(\\mathbb{E}[X\\vert Y])\n where the first term is the between-group variance and the second term is the within-group variance.\nIn our problem, the variance decomposition is\nvar(x)=(1−π)(var(x|d=0)+(𝔼[x|d=0]−𝔼[x])2)+π(var(x|d=1)+(𝔼[x|d=1]−𝔼[x])2)\n\\begin{align*}\n\\text{var}(x) & =\\left(1-\\pi\\right)\\left(\\text{var}(x\\vert d=0)+\\left(\\mathbb{E}\\left[x\\vert d=0\\right]-\\mathbb{E}\\left[x\\right]\\right)^{2}\\right)\\\\\n & +\\pi\\left(\\text{var}(x\\vert d=1)+\\left(\\mathbb{E}\\left[x\\vert d=1\\right]-\\mathbb{E}\\left[x\\right]\\right)^{2}\\right)\n\\end{align*}\n but we have\n𝔼[x]=𝔼[𝔼[x|y]]=π𝔼[x|d=1]+(1−π)𝔼[x|d=0]\n\\mathbb{E}\\left[x\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[x\\vert y\\right]\\right]=\\pi\\mathbb{E}\\left[x\\vert d=1\\right] + (1-\\pi)\\mathbb{E}\\left[x\\vert d=0\\right]\n substituting the RHS for 𝔼[x]\\mathbb{E}\\left[x\\right] we have\nvar(x)=(1−π)(var(x|d=0)+(𝔼[x|d=1]−𝔼[x|d=0])2π2)+π(var(x|d=1)+(𝔼[x|d=1]−𝔼[x|d=0])2(1−π)2)\n\\begin{align*}\n\\text{var}(x) & =\\left(1-\\pi\\right)\\left(\\text{var}(x\\vert d=0)+\\left(\\mathbb{E}\\left[x\\vert d=1\\right]-\\mathbb{E}\\left[x\\vert d=0\\right]\\right)^{2}\\pi^{2}\\right)\\\\\n & +\\pi\\left(\\text{var}(x\\vert d=1)+\\left(\\mathbb{E}\\left[x\\vert d=1\\right]-\\mathbb{E}\\left[x\\vert d=0\\right]\\right)^{2}(1-\\pi)^{2}\\right)\n\\end{align*}\n\nbut we have 𝔼[x|d=1]−𝔼[x|d=0]=cov(x,d)/var(d)\\mathbb{E}\\left[x\\vert d=1\\right]-\\mathbb{E}\\left[x\\vert d=0\\right]=\\text{cov}(x,d)/\\text{var}(d), and var(d)=π(1−π)\\text{var}(d)=\\pi(1-\\pi), so\nvar(x)=(1−π)var(x|d=0)+πvar(x|d=1)+cov(d,x)2var(d)2×[(1−π)π2+π(1−π)2]=(1−π)var(x|d=0)+πvar(x|d=1)+cov(d,x)2var(d)2×[(1−π)var(d)+πvar(d)]=(1−π)var(x|d=0)+πvar(x|d=1)+cov(d,x)2var(d)\n\\begin{align*}\n\\text{var}(x) & =\\left(1-\\pi\\right)\\text{var}(x\\vert d=0)+\\pi\\text{var}(x\\vert d=1)+\\frac{\\text{cov}(d,x)^{2}}{\\text{var}(d)^{2}}\\times\\left[(1-\\pi)\\pi^{2}+\\pi(1-\\pi)^{2}\\right]\\\\\n & =\\left(1-\\pi\\right)\\text{var}(x\\vert d=0)+\\pi\\text{var}(x\\vert d=1)+\\frac{\\text{cov}(d,x)^{2}}{\\text{var}(d)^{2}}\\times\\left[(1-\\pi)\\text{var}(d)+\\pi\\text{var}(d)\\right]\\\\\n & =\\left(1-\\pi\\right)\\text{var}(x\\vert d=0)+\\pi\\text{var}(x\\vert d=1)+\\frac{\\text{cov}(d,x)^{2}}{\\text{var}(d)}\n\\end{align*}\n by the same logic\ncov(x,y)=(1−π)cov(x,y|d=0)+πcov(x,y|d=1)+cov(d,x)cov(d,y)var(d)\n\\text{cov}(x,y)=\\left(1-\\pi\\right)\\text{cov}(x,y\\vert d=0)+\\pi\\text{cov}(x,y\\vert d=1)+\\frac{\\text{cov}(d,x)\\text{cov}(d,y)}{\\text{var}(d)}\n Now writing GapOLS=w1Gap1+w0Gap0\\text{Gap}^{\\text{OLS}}=w_1\\text{Gap}^1 + w_0\\text{Gap}^0 with w1+w0=1w_1+w_0=1.\n\n\n\nBased on Equation 3 we have\nGap1=cov(d,y)var(d)−cov(x,d)var(d)×cov(x,y|d=1)var(x|d=1)\n\\text{Gap}^1=\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\frac{\\text{cov}\\left(x,d\\right)}{\\text{var}(d)}\\times\\frac{\\text{cov}\\left(x,y\\vert d=1\\right)}{\\text{var}(x\\vert d=1)}\n\nand\nGap0=cov(d,y)var(d)−cov(x,d)var(d)×cov(x,y|d=0)var(x|d=0)\n\\text{Gap}^0=\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\frac{\\text{cov}\\left(x,d\\right)}{\\text{var}(d)}\\times\\frac{\\text{cov}\\left(x,y\\vert d=0\\right)}{\\text{var}(x\\vert d=0)}\n\n\n\n\n\nConsider the regression y=β0+βdd+βxx+ϵy = \\beta_0 + \\beta_d d + \\beta_x x + \\epsilon. Per the Frisch-Waugh theorem, if\nGiven the propensity score P(x)=cov(d,x)var(x)xP(x) = \\frac{\\text{cov}(d,x)}{\\text{var}(x)}x and the regression y=β0+βdd+βxx+ϵy = \\beta_0 + \\beta_d d + \\beta_x x + \\epsilon then, per the Frisch-Waugh theorem the coefficient βd\\beta_d is equal to the coefficient from regressing\ny−cov(P(x),y)var(P(x))P(x) on d−cov(P(x),d)var(P(x))P(x)\ny-\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))}P(x)\\text{ on }d-\\frac{\\text{cov}(P(x),d)}{\\text{var}(P(x))}P(x)\n but cov(P(x),d)=cov(cov(d,x)var(x)x,d)=(cov(d,x))2var(x)\\text{cov}(P(x),d) = \\text{cov}(\\frac{\\text{cov}(d,x)}{\\text{var}(x)}x,d)=\\frac{\\left(\\text{cov}(d,x)\\right)^2}{\\text{var}(x)}, so d−cov(P(x),d)var(P(x))P(x)d-\\frac{\\text{cov}(P(x),d)}{\\text{var}(P(x))}P(x)\nP(x)⊤P(x)=cov(d,x)var(x)x⊤xcov(y,x)var(x)=(cov(d,x))2var(x)P(x)^\\top P(x)=\\frac{\\text{cov}(d,x)}{\\text{var}(x)}x^\\top x\\frac{\\text{cov}(y,x)}{\\text{var}(x)}=\\frac{\\left(\\text{cov}(d,x)\\right)^2}{\\text{var}(x)}\nP(x)(P(x)⊤P(x))−1=xcov(d,x)var(x)×var(x)(cov(d,x))2P(x)\\left(P(x)^\\top P(x)\\right)^{-1}=x\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\times\\frac{\\text{var}(x)}{\\left(\\text{cov}(d,x)\\right)^2}\nP(x)(P(x)⊤P(x))−1cov(P(x),y)=xcov(d,x)var(x)×var(x)(cov(d,x))2×cov(d,x)var(x)cov(x,y)=xcov(x,y)var(x)P(x)\\left(P(x)^\\top P(x)\\right)^{-1}\\text{cov(P(x),y)}=x\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\times\\frac{\\text{var}(x)}{\\left(\\text{cov}(d,x)\\right)^2}\\times\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\text{cov}(x,y)=x\\frac{\\text{cov}(x,y)}{\\text{var}(x)}\nand the coefficient due regressing on the propensity score is the same as the coefficient due regressing on the covariates.\nCan P(x)=xcov(d,x)var(x)P(x) = x\\frac{\\text{cov}(d,x)}{\\text{var}(x)} replace xx in a regression?\n\nThe regression y=βxx+ϵy=\\beta_x x+\\epsilon can be written as y=xcov(x,y)var(x)+ϵy=x\\frac{\\text{cov}(x,y)}{\\text{var}(x)}+\\epsilon, and\nthe regression y=βP(x)P(x)+ϵy=\\beta_{P(x)} P(x)+\\epsilon can be written as y=P(x)cov(P(x),y)var(P(x))+ϵy=P(x)\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))}+\\epsilon, but\nP(x)cov(P(x),y)var(P(x))=P(x)(P(x)⊤P(x))−1cov(P(x),y)P(x)\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))} = P(x)\\left(P(x)^\\top P(x)\\right)^{-1}\\text{cov(P(x),y)}, and P(x)(P(x)⊤P(x))−1cov(P(x),y)=xcov(d,x)var(x)×var(x)(cov(d,x))2×cov(d,x)var(x)cov(x,y)P(x)\\left(P(x)^{\\top}P(x)\\right)^{-1}\\text{cov(P(x),y)}=x\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\times\\frac{\\text{var}(x)}{\\left(\\text{cov}(d,x)\\right)^{2}}\\times\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\text{cov}(x,y)\n\nsince P(x)⊤P(x)=cov(d,x)var(x)x⊤xcov(d,x)var(x)=(cov(d,x))2var(x)P(x)^\\top P(x)=\\frac{\\text{cov}(d,x)}{\\text{var}(x)}x^\\top x\\frac{\\text{cov}(d,x)}{\\text{var}(x)}=\\frac{\\left(\\text{cov}(d,x)\\right)^2}{\\text{var}(x)}, and\ncov(P(x),y)=cov(cov(d,x)var(x)x,y)=cov(d,x)var(x)cov(x,y)\\text{cov}(P(x),y) = \\text{cov}(\\frac{\\text{cov}(d,x)}{\\text{var}(x)}x,y)=\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\text{cov}(x,y)\nso P(x)cov(P(x),y)var(P(x))=xcov(x,y)var(x)P(x)\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))} = x\\frac{\\text{cov}(x,y)}{\\text{var}(x)} and they are exchangeable.\n\nIt follows that in the regression y=β0+βdd+βxx+ϵy = \\beta_0 + \\beta_d d + \\beta_x x + \\epsilon, with the propensity score P(x)=cov(d,x)var(x)xP(x) = \\frac{\\text{cov}(d,x)}{\\text{var}(x)}x, then per the Frisch-Waugh theorem the coefficient βd\\beta_d is equal to the coefficient from regressing\ny−cov(P(x),y)var(P(x))P(x) on d−P(x)\ny-\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))}P(x)\\text{ on }d-P(x)"
  },
  {
    "objectID": "supplemental/decompositions.html#oaxaca-blinder-decomposition",
    "href": "supplemental/decompositions.html#oaxaca-blinder-decomposition",
    "title": "Decompositions",
    "section": "",
    "text": "Consider a categorical (or dummy) variable dd that splits our dataset into two groups.\nIn this case we can run regressions of the form y=Xβ+ϵy=X\\beta+\\epsilon to estimate the the mean difference between groups, as follows\n𝔼[y0]=𝔼[X(0)]β0;group d=0𝔼[y1]=𝔼[X(1)]β1;group d=1\n\\begin{align*}\n\\mathbb{E}\\left[y^{0}\\right] & =\\mathbb{E}\\left[X^{(0)}\\right]\\beta_{0};\\;\\text{group }d=0\\\\\n\\mathbb{E}\\left[y^{1}\\right] & =\\mathbb{E}\\left[X^{(1)}\\right]\\beta_{1};\\;\\text{group }d=1\n\\end{align*}\n\nAlternatively\ny‾0=X‾0β0;group d=0y‾1=X‾1β1;group d=1\n\\begin{align*}\n\\bar{y}_0 & =\\bar{X}_0\\beta_{0};\\;\\text{group }d=0\\\\\n\\bar{y}_1 & =\\bar{X}_1\\beta_{1};\\;\\text{group }d=1\n\\end{align*}\n\nThen the mean difference in outcomes is:\n𝔼[y1]−𝔼[y0]=𝔼[X(1)]β1−𝔼[X(0)]β0=(𝔼[X(1)]−𝔼[X(0)])β1+𝔼[X(0)](β1−β0)=(𝔼[X(1)]−𝔼[X(0)])β0−𝔼[X(1)](β0−β1)\n\\begin{align*}\n\\mathbb{E}\\left[y^{1}\\right]-\\mathbb{E}\\left[y^{0}\\right] & =\\mathbb{E}\\left[X^{(1)}\\right]\\beta_{1}-\\mathbb{E}\\left[X^{(0)}\\right]\\beta_{0}\\\\\n & =\\left(\\mathbb{E}\\left[X^{(1)}\\right]-\\mathbb{E}\\left[X^{(0)}\\right]\\right)\\beta_{1}+\\mathbb{E}\\left[X^{(0)}\\right]\\left(\\beta_{1}-\\beta_{0}\\right)\\\\\n & =\\left(\\mathbb{E}\\left[X^{(1)}\\right]-\\mathbb{E}\\left[X^{(0)}\\right]\\right)\\beta_{0}-\\mathbb{E}\\left[X^{(1)}\\right]\\left(\\beta_{0}-\\beta_{1}\\right)\n\\end{align*}\n\nAlternatively\ny‾1−y‾0=X‾1β1−X‾0β0=(X‾1−X‾0)β1+X‾0(β1−β0)=(X‾1−X‾0)β0−X‾1(β0−β1)(1)\n\\begin{align*}\n\\bar{y}_1-\\bar{y}_0 & =\\bar{X}_1\\beta_{1}-\\bar{X}_0\\beta_{0}\\\\\n & =\\left(\\bar{X}_1-\\bar{X}_0\\right)\\beta_{1}+\\bar{X}_0\\left(\\beta_{1}-\\beta_{0}\\right)\\\\\n & =\\left(\\bar{X}_1-\\bar{X}_0\\right)\\beta_{0}-\\bar{X}_1\\left(\\beta_{0}-\\beta_{1}\\right)\n\\end{align*}\n \\qquad(1)\nWhere:\n(X‾1−X‾0)β1\\left( \\bar{X}_1-\\bar{X}_0\\right)\\beta_{1} is the “explained” component (differences in characteristics), and X‾0(β1−β0)\\bar{X}_0\\left(\\beta_{1}-\\beta_{0}\\right)) is the “unexplained” component (differences in returns to characteristics), where X‾0\\bar{X}_0 is the baseline (this decomposition is not unique, as you can see from the third line of Equation 1)\nand we define\nGap1=X‾0(β1−β0)Gap0=X‾1(β0−β1)GapOLS=δd;where y=δ0+δdd+δ1X+ϵ\n\\begin{align*}\n\\text{Gap}^{1} & =\\bar{X}_0\\left(\\beta_{1}-\\beta_{0}\\right)\\\\\n\\text{Gap}^{0} & =\\bar{X}_1\\left(\\beta_{0}-\\beta_{1}\\right)\\\\\n\\text{Gap}^{\\text{OLS}} & =\\delta_{d};\\;\\text{where }y=\\delta_{0}+\\delta_{d}d+\\delta_{1}X+\\epsilon\n\\end{align*}\n where the strong assumptions that the model is properly specified and that coefficients are equal across groups, a sensible definition of the population unexplained gap is δd\\delta_d.\nExample\ndata:\n\n# https://cran.r-project.org/web/packages/oaxaca/vignettes/oaxaca.pdf\n# https://www.worldbank.org/content/dam/Worldbank/document/HDN/Health/HealthEquityCh12.pdf\n# https://pmc.ncbi.nlm.nih.gov/articles/PMC8343972/\n# https://www.sciencedirect.com/science/article/abs/pii/S0169721811004072\n# https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2528391\n# https://link.springer.com/article/10.1186/s12982-021-00100-9\n# https://giacomovagni.com/blog/2023/oaxaca/\n# https://journals.sagepub.com/doi/pdf/10.1177/1536867X0800800401\n# https://ocw.mit.edu/courses/14-662-labor-economics-ii-spring-2015/resources/mit14_662s15_lecnotes1/\n\n# Load HMDA data\ndata('PSID1982', package = \"AER\")\nhmda &lt;- PSID1982\n\n# data(\"nswcps\", package = \"hettreatreg\")\n\n# Prepare data for analysis\n# Looking at income differences between racial groups\nlending_data &lt;- hmda |&gt; \n  dplyr::mutate(\n    minority = factor(race != \"white\"),\n    log_income = log(income)\n  ) |&gt; \n  dplyr::select(log_income, minority, education, hrat, ccred, mcred, pubrec)\n\n\n# Fit separate regressions\nmodel_a &lt;- lm(wage ~ education + experience, data = dat |&gt; dplyr::filter(union=='yes'))\nmodel_b &lt;- lm(wage ~ education + experience, data = dat |&gt; dplyr::filter(union=='no'))\n\n# Get mean characteristics (including intercept)\nX_mean_a &lt;- c(1, colMeans( dat |&gt; dplyr::filter(union=='yes') |&gt; dplyr::select(education, experience) ) )\nX_mean_b &lt;- c(1, colMeans( dat |&gt; dplyr::filter(union=='no') |&gt; dplyr::select(education, experience) ) )\n\n# Get coefficients\nbeta_a &lt;- coef(model_a)\nbeta_b &lt;- coef(model_b)\n\n# Calculate decomposition\ntibble::tibble(\n  explained = sum((X_mean_a - X_mean_b) * beta_a)\n  , unexplained = sum(X_mean_b * (beta_a - beta_b))\n  , total_gap &lt;- explained + unexplained\n)\n\n# Perform Oaxaca-Blinder decomposition\ndecomp &lt;- oaxaca::oaxaca(wage ~ education + experience | union, \n                 data = dat |&gt; dplyr::mutate(union = dplyr::case_when(union=='yes'~0, TRUE ~1)))\n\nWe an define another measure Gapp\\text{Gap}^{p} as\ny‾1−y‾0=(X‾1−X‾0)β̂p+GappGapp=X‾1(β̂1−β̂p)+X‾0(β̂p−β̂0)\n\\begin{align*}\n\\bar{y}_1-\\bar{y}_0 & =\\left(\\bar{X}_1-\\bar{X}_0\\right)\\hat{\\beta}^{p}+\\text{Gap}^p\\\\\n\\text{Gap}^p & =\\bar{X}_1\\left(\\hat{\\beta}_1-\\hat{\\beta}^{p}\\right)+\\bar{X}_0\\left(\\hat{\\beta}^{p}-\\hat{\\beta}^{0}\\right)\n\\end{align*}\n\nwhere β̂p\\hat{\\beta}^{p} is the coefficient from the pooled regression of yy on XX.\nAn O-B unexplained gap can always be written as the difference in overall mean outcomes minus the difference in predicted mean outcomes, and both of these differences can be denoted by linear projections.\nA general expression for an O-B unexplained gap is\nGap=[y‾1−y‾0]−[β̂(x‾1−x‾0)]=b(y|d)−b(β̂x|d)(2)\n\\begin{align*}\n\\text{Gap} & =\\left[\\bar{y}_{1}-\\bar{y}_{0}\\right]-\\left[\\hat{\\beta}\\left(\\bar{x}_{1}-\\bar{x}_{0}\\right)\\right]\\\\\n & =\\text{b}(y\\vert d)-\\text{b}(\\hat{\\beta}x\\vert d)\n\\end{align*}\n \\qquad(2)\nwhere β̂\\hat{\\beta} is a coefficient computed from sample data and b(z|w)\\text{b}(z\\vert w) is the slope from a regression of zz on ww and an intercept.\nSo equation Equation 2 gives Gap1\\text{Gap}^{1} when β̂\\hat{\\beta} is the slope coefficient of a regression of yy on xx using the data from group 1, and gives Gap0\\text{Gap}^{0} when β̂\\hat{\\beta} is the slope coefficient using data from group 01.\nGap=b(y|d)−b(β̂x|d)=cov(d,y)var(d)−β̂cov(x,d)var(d)=cov(d,β0+βdd+βxx)+ϵvar(d)−β̂cov(d,x)var(d)=βd+βxcov(d,x)var(d)−β̂cov(d,x)var(d)→βd,when βx=β̂(3)\n\\begin{align*}\n\\text{Gap} & =\\text{b}(y\\vert d)-\\text{b}(\\hat{\\beta}x\\vert d)\\\\\n & =\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\hat{\\beta}\\frac{\\text{cov}\\left(x,d\\right)}{\\text{var}(d)}\\\\\n & =\\frac{\\text{cov}\\left(d,\\beta_{0}+\\beta_{d}d+\\beta_{x}x\\right)+\\epsilon}{\\text{var}(d)}-\\hat{\\beta}\\frac{\\text{cov}\\left(d,x\\right)}{\\text{var}(d)}\\\\\n & =\\beta_{d}+\\beta_{x}\\frac{\\text{cov}\\left(d,x\\right)}{\\text{var}(d)}-\\hat{\\beta}\\frac{\\text{cov}\\left(d,x\\right)}{\\text{var}(d)}\\\\\n & \\rightarrow\\beta_{d},\\;\\text{when }\\beta_{x}=\\hat{\\beta}\n\\end{align*}\n \\qquad(3)\nimplying that all the gap expressions are equivalent, except for Gapp\\text{Gap}^{p}. The difference arises because in the pooled regression used to compute Gapp\\text{Gap}^p, dd is a omitted variable.\nGapp=b(y|d)−b(xb(y|x)|d)=cov(d,y)var(d)−cov(d,[cov(x,y)/var(x)])var(d)=cov(d,y)var(d)−cov(d,x)var(d)×cov(x,y)var(x)=1var(d)(cov(d,y)−cov(d,x)cov(x,y)var(x))\n\\begin{align*}\n\\text{Gap}^{p} & =\\text{b}(y\\vert d)-\\text{b}(x\\text{b}(y\\vert x)\\vert d)\\\\\n & =\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\frac{\\text{cov}\\left(d,\\left[\\text{cov}\\left(x,y\\right)/\\text{var}\\left(x\\right)\\right]\\right)}{\\text{var}(d)}\\\\\n & =\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\frac{\\text{cov}\\left(d,x\\right)}{\\text{var}(d)}\\times\\frac{\\text{cov}\\left(x,y\\right)}{\\text{var}(x)}\\\\\n & =\\frac{1}{\\text{var}(d)}\\left(\\text{cov}\\left(d,y\\right)-\\frac{\\text{cov}\\left(d,x\\right)\\text{cov}\\left(x,y\\right)}{\\text{var}(x)}\\right)\n\\end{align*}\n\nCompare this to the GapOLS\\text{Gap}^{\\text{OLS}}, defining z̃(w)\\tilde{z}(w) as the component of zz that is orthogonal to ww in the population (so that z̃(w)=z−wb(z|w)\\tilde{z}(w)=z-w\\text{b}(z\\vert w), alternatively z̃(w)=z−wcov(z,w)var(w)\\tilde{z}(w)=z-w\\frac{\\text{cov}(z,w)}{\\text{var}(w)})2\nGapOLS=δd=cov(d̃(x),ỹ(x))var(d̃(x));per FWL=cov(d,ỹ(x))var(d̃(x))−cov(x,ỹ(x))var(d̃(x))×cov(d,x)var(x);per definition of d̃(x)=cov(d,ỹ(x))var(d̃(x));per definition of ỹ(x),cov(x,ỹ(x))=0=1var(d̃(x))(cov(d,y)−cov(d,x)cov(x,y)var(x))\n\\begin{align*}\n\\text{Gap}^{\\text{OLS}} & =\\delta_{d}\\\\\n & =\\frac{\\text{cov}(\\tilde{d}(x),\\tilde{y}(x))}{\\text{var}(\\tilde{d}(x))};\\;\\text{per FWL}\\\\\n & =\\frac{\\text{cov}(d,\\tilde{y}(x))}{\\text{var}(\\tilde{d}(x))}-\\frac{\\text{cov}(x,\\tilde{y}(x))}{\\text{var}(\\tilde{d}(x))}\\times\\frac{\\text{cov}(d,x)}{\\text{var}(x)};\\;\\text{per definition of }\\tilde{d}(x)\\\\\n & =\\frac{\\text{cov}(d,\\tilde{y}(x))}{\\text{var}(\\tilde{d}(x))};\\;\\text{per definition of }\\tilde{y}(x),\\,\\text{cov}(x,\\tilde{y}(x))=0\\\\\n & =\\frac{1}{\\text{var}(\\tilde{d}(x))}\\left(\\text{cov}(d,y)-\\text{cov}(d,x)\\frac{\\text{cov}(x,y)}{\\text{var}(x)}\\right)\n\\end{align*}\n\nand so Gapp=var(d̃(x))var(d)GapOLS\\text{Gap}^{p}=\\frac{\\text{var}(\\tilde{d}(x))}{\\text{var}(d)}\\text{Gap}^{\\text{OLS}}\n\n\nThe law of total variance, or variance decomposition formula is\nvar(X)=𝔼[var(X|Y)]+var(𝔼[X|Y])\n\\text{var}(X)=\\mathbb{E}[\\text{var}(X\\vert Y)]+\\text{var}(\\mathbb{E}[X\\vert Y])\n where the first term is the between-group variance and the second term is the within-group variance.\nIn our problem, the variance decomposition is\nvar(x)=(1−π)(var(x|d=0)+(𝔼[x|d=0]−𝔼[x])2)+π(var(x|d=1)+(𝔼[x|d=1]−𝔼[x])2)\n\\begin{align*}\n\\text{var}(x) & =\\left(1-\\pi\\right)\\left(\\text{var}(x\\vert d=0)+\\left(\\mathbb{E}\\left[x\\vert d=0\\right]-\\mathbb{E}\\left[x\\right]\\right)^{2}\\right)\\\\\n & +\\pi\\left(\\text{var}(x\\vert d=1)+\\left(\\mathbb{E}\\left[x\\vert d=1\\right]-\\mathbb{E}\\left[x\\right]\\right)^{2}\\right)\n\\end{align*}\n but we have\n𝔼[x]=𝔼[𝔼[x|y]]=π𝔼[x|d=1]+(1−π)𝔼[x|d=0]\n\\mathbb{E}\\left[x\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[x\\vert y\\right]\\right]=\\pi\\mathbb{E}\\left[x\\vert d=1\\right] + (1-\\pi)\\mathbb{E}\\left[x\\vert d=0\\right]\n substituting the RHS for 𝔼[x]\\mathbb{E}\\left[x\\right] we have\nvar(x)=(1−π)(var(x|d=0)+(𝔼[x|d=1]−𝔼[x|d=0])2π2)+π(var(x|d=1)+(𝔼[x|d=1]−𝔼[x|d=0])2(1−π)2)\n\\begin{align*}\n\\text{var}(x) & =\\left(1-\\pi\\right)\\left(\\text{var}(x\\vert d=0)+\\left(\\mathbb{E}\\left[x\\vert d=1\\right]-\\mathbb{E}\\left[x\\vert d=0\\right]\\right)^{2}\\pi^{2}\\right)\\\\\n & +\\pi\\left(\\text{var}(x\\vert d=1)+\\left(\\mathbb{E}\\left[x\\vert d=1\\right]-\\mathbb{E}\\left[x\\vert d=0\\right]\\right)^{2}(1-\\pi)^{2}\\right)\n\\end{align*}\n\nbut we have 𝔼[x|d=1]−𝔼[x|d=0]=cov(x,d)/var(d)\\mathbb{E}\\left[x\\vert d=1\\right]-\\mathbb{E}\\left[x\\vert d=0\\right]=\\text{cov}(x,d)/\\text{var}(d), and var(d)=π(1−π)\\text{var}(d)=\\pi(1-\\pi), so\nvar(x)=(1−π)var(x|d=0)+πvar(x|d=1)+cov(d,x)2var(d)2×[(1−π)π2+π(1−π)2]=(1−π)var(x|d=0)+πvar(x|d=1)+cov(d,x)2var(d)2×[(1−π)var(d)+πvar(d)]=(1−π)var(x|d=0)+πvar(x|d=1)+cov(d,x)2var(d)\n\\begin{align*}\n\\text{var}(x) & =\\left(1-\\pi\\right)\\text{var}(x\\vert d=0)+\\pi\\text{var}(x\\vert d=1)+\\frac{\\text{cov}(d,x)^{2}}{\\text{var}(d)^{2}}\\times\\left[(1-\\pi)\\pi^{2}+\\pi(1-\\pi)^{2}\\right]\\\\\n & =\\left(1-\\pi\\right)\\text{var}(x\\vert d=0)+\\pi\\text{var}(x\\vert d=1)+\\frac{\\text{cov}(d,x)^{2}}{\\text{var}(d)^{2}}\\times\\left[(1-\\pi)\\text{var}(d)+\\pi\\text{var}(d)\\right]\\\\\n & =\\left(1-\\pi\\right)\\text{var}(x\\vert d=0)+\\pi\\text{var}(x\\vert d=1)+\\frac{\\text{cov}(d,x)^{2}}{\\text{var}(d)}\n\\end{align*}\n by the same logic\ncov(x,y)=(1−π)cov(x,y|d=0)+πcov(x,y|d=1)+cov(d,x)cov(d,y)var(d)\n\\text{cov}(x,y)=\\left(1-\\pi\\right)\\text{cov}(x,y\\vert d=0)+\\pi\\text{cov}(x,y\\vert d=1)+\\frac{\\text{cov}(d,x)\\text{cov}(d,y)}{\\text{var}(d)}\n Now writing GapOLS=w1Gap1+w0Gap0\\text{Gap}^{\\text{OLS}}=w_1\\text{Gap}^1 + w_0\\text{Gap}^0 with w1+w0=1w_1+w_0=1.\n\n\n\nBased on Equation 3 we have\nGap1=cov(d,y)var(d)−cov(x,d)var(d)×cov(x,y|d=1)var(x|d=1)\n\\text{Gap}^1=\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\frac{\\text{cov}\\left(x,d\\right)}{\\text{var}(d)}\\times\\frac{\\text{cov}\\left(x,y\\vert d=1\\right)}{\\text{var}(x\\vert d=1)}\n\nand\nGap0=cov(d,y)var(d)−cov(x,d)var(d)×cov(x,y|d=0)var(x|d=0)\n\\text{Gap}^0=\\frac{\\text{cov}\\left(d,y\\right)}{\\text{var}(d)}-\\frac{\\text{cov}\\left(x,d\\right)}{\\text{var}(d)}\\times\\frac{\\text{cov}\\left(x,y\\vert d=0\\right)}{\\text{var}(x\\vert d=0)}"
  },
  {
    "objectID": "supplemental/decompositions.html#an-aside",
    "href": "supplemental/decompositions.html#an-aside",
    "title": "Decompositions",
    "section": "",
    "text": "Consider the regression y=β0+βdd+βxx+ϵy = \\beta_0 + \\beta_d d + \\beta_x x + \\epsilon. Per the Frisch-Waugh theorem, if\nGiven the propensity score P(x)=cov(d,x)var(x)xP(x) = \\frac{\\text{cov}(d,x)}{\\text{var}(x)}x and the regression y=β0+βdd+βxx+ϵy = \\beta_0 + \\beta_d d + \\beta_x x + \\epsilon then, per the Frisch-Waugh theorem the coefficient βd\\beta_d is equal to the coefficient from regressing\ny−cov(P(x),y)var(P(x))P(x) on d−cov(P(x),d)var(P(x))P(x)\ny-\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))}P(x)\\text{ on }d-\\frac{\\text{cov}(P(x),d)}{\\text{var}(P(x))}P(x)\n but cov(P(x),d)=cov(cov(d,x)var(x)x,d)=(cov(d,x))2var(x)\\text{cov}(P(x),d) = \\text{cov}(\\frac{\\text{cov}(d,x)}{\\text{var}(x)}x,d)=\\frac{\\left(\\text{cov}(d,x)\\right)^2}{\\text{var}(x)}, so d−cov(P(x),d)var(P(x))P(x)d-\\frac{\\text{cov}(P(x),d)}{\\text{var}(P(x))}P(x)\nP(x)⊤P(x)=cov(d,x)var(x)x⊤xcov(y,x)var(x)=(cov(d,x))2var(x)P(x)^\\top P(x)=\\frac{\\text{cov}(d,x)}{\\text{var}(x)}x^\\top x\\frac{\\text{cov}(y,x)}{\\text{var}(x)}=\\frac{\\left(\\text{cov}(d,x)\\right)^2}{\\text{var}(x)}\nP(x)(P(x)⊤P(x))−1=xcov(d,x)var(x)×var(x)(cov(d,x))2P(x)\\left(P(x)^\\top P(x)\\right)^{-1}=x\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\times\\frac{\\text{var}(x)}{\\left(\\text{cov}(d,x)\\right)^2}\nP(x)(P(x)⊤P(x))−1cov(P(x),y)=xcov(d,x)var(x)×var(x)(cov(d,x))2×cov(d,x)var(x)cov(x,y)=xcov(x,y)var(x)P(x)\\left(P(x)^\\top P(x)\\right)^{-1}\\text{cov(P(x),y)}=x\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\times\\frac{\\text{var}(x)}{\\left(\\text{cov}(d,x)\\right)^2}\\times\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\text{cov}(x,y)=x\\frac{\\text{cov}(x,y)}{\\text{var}(x)}\nand the coefficient due regressing on the propensity score is the same as the coefficient due regressing on the covariates.\nCan P(x)=xcov(d,x)var(x)P(x) = x\\frac{\\text{cov}(d,x)}{\\text{var}(x)} replace xx in a regression?\n\nThe regression y=βxx+ϵy=\\beta_x x+\\epsilon can be written as y=xcov(x,y)var(x)+ϵy=x\\frac{\\text{cov}(x,y)}{\\text{var}(x)}+\\epsilon, and\nthe regression y=βP(x)P(x)+ϵy=\\beta_{P(x)} P(x)+\\epsilon can be written as y=P(x)cov(P(x),y)var(P(x))+ϵy=P(x)\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))}+\\epsilon, but\nP(x)cov(P(x),y)var(P(x))=P(x)(P(x)⊤P(x))−1cov(P(x),y)P(x)\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))} = P(x)\\left(P(x)^\\top P(x)\\right)^{-1}\\text{cov(P(x),y)}, and P(x)(P(x)⊤P(x))−1cov(P(x),y)=xcov(d,x)var(x)×var(x)(cov(d,x))2×cov(d,x)var(x)cov(x,y)P(x)\\left(P(x)^{\\top}P(x)\\right)^{-1}\\text{cov(P(x),y)}=x\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\times\\frac{\\text{var}(x)}{\\left(\\text{cov}(d,x)\\right)^{2}}\\times\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\text{cov}(x,y)\n\nsince P(x)⊤P(x)=cov(d,x)var(x)x⊤xcov(d,x)var(x)=(cov(d,x))2var(x)P(x)^\\top P(x)=\\frac{\\text{cov}(d,x)}{\\text{var}(x)}x^\\top x\\frac{\\text{cov}(d,x)}{\\text{var}(x)}=\\frac{\\left(\\text{cov}(d,x)\\right)^2}{\\text{var}(x)}, and\ncov(P(x),y)=cov(cov(d,x)var(x)x,y)=cov(d,x)var(x)cov(x,y)\\text{cov}(P(x),y) = \\text{cov}(\\frac{\\text{cov}(d,x)}{\\text{var}(x)}x,y)=\\frac{\\text{cov}(d,x)}{\\text{var}(x)}\\text{cov}(x,y)\nso P(x)cov(P(x),y)var(P(x))=xcov(x,y)var(x)P(x)\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))} = x\\frac{\\text{cov}(x,y)}{\\text{var}(x)} and they are exchangeable.\n\nIt follows that in the regression y=β0+βdd+βxx+ϵy = \\beta_0 + \\beta_d d + \\beta_x x + \\epsilon, with the propensity score P(x)=cov(d,x)var(x)xP(x) = \\frac{\\text{cov}(d,x)}{\\text{var}(x)}x, then per the Frisch-Waugh theorem the coefficient βd\\beta_d is equal to the coefficient from regressing\ny−cov(P(x),y)var(P(x))P(x) on d−P(x)\ny-\\frac{\\text{cov}(P(x),y)}{\\text{var}(P(x))}P(x)\\text{ on }d-P(x)"
  },
  {
    "objectID": "supplemental/decompositions.html#footnotes",
    "href": "supplemental/decompositions.html#footnotes",
    "title": "Decompositions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nunder the data generation process described by y=δ0+δdd+δ1X+ϵy=\\delta_{0}+\\delta_{d}d+\\delta_{1}X+\\epsilon↩︎\nNote: d̃(x)=d−P(x)=d−xcov(x,d)var(x)\\tilde{d}(x)=d-P(x)=d-x\\frac{\\text{cov}(x,d)}{\\text{var}(x)}, the linear propensity, and ỹ(x)=y−xcov(y,x)var(x)\\tilde{y}(x)=y-x\\frac{\\text{cov}(y,x)}{\\text{var}(x)}, so the coefficient δd\\delta_d is cov(d̃(x),ỹ(x))var(d̃(x))\\frac{\\text{cov}(\\tilde{d}(x),\\tilde{y}(x))}{\\text{var}(\\tilde{d}(x))}.↩︎"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review.",
    "crumbs": [
      "Supplemental notes",
      "Model selection criteria"
    ]
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of 𝛃\\boldsymbol{\\beta} and σ\\sigma",
    "text": "Maximum Likelihood Estimation of 𝛃\\boldsymbol{\\beta} and σ\\sigma\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet 𝐘\\mathbf{Y} be n×1n \\times 1 matrix of responses, 𝐗\\mathbf{X}, the n×(p+1)n \\times (p+1) matrix of predictors, and 𝛃\\boldsymbol{\\beta}, (p+1)×1(p+1) \\times 1 matrix of coefficients. If the multiple linear regression model is correct then,\n𝐘∼N(𝐗𝛃,σ2)(1)\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n \\qquad(1)\nWhen we do linear regression, our goal is to estimate the unknown parameters 𝛃\\boldsymbol{\\beta} and σ2\\sigma^2 from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating 𝛃\\boldsymbol{\\beta} and σ2\\sigma^2 is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\nL(𝐗,𝐘|𝛃,σ2)=∏i=1n(2πσ2)−12exp{−12σ2∑i=1n(Yi−𝐗i𝛃)T(Yi−𝐗i𝛃)}(2)\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n \\qquad(2)\nwhere YiY_i is the ithi^{th} response and 𝐗i\\mathbf{X}_i is the vector of predictors for the ithi^{th} observation. One approach estimating 𝛃\\boldsymbol{\\beta} and σ2\\sigma^2 is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\nlogL(𝐗,𝐘|𝛃,σ2)=∑i=1n−12log(2πσ2)−12σ2∑i=1n(Yi−𝐗i𝛃)T(Yi−𝐗i𝛃)=−n2log(2πσ2)−12σ2(𝐘−𝐗𝛃)T(𝐘−𝐗𝛃)(3)\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n \\qquad(3)\n\nThe maximum likelihood estimate of 𝛃\\boldsymbol{\\beta} and σ2\\sigma^2 are 𝛃̂=(𝐗T𝐗)−1𝐗T𝐘σ̂2=1n(𝐘−𝐗𝛃)T(𝐘−𝐗𝛃)=1nRSS(4)\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{1cm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n \\qquad(4)\nwhere RSSRSS is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of σ2\\sigma^2 we typically use RSSn−p−1\\frac{RSS}{n-p-1}. This is because the maximum likelihood estimate of σ2\\sigma^2 in Equation 4 is a biased estimator of σ2\\sigma^2. When nn is much larger than the number of predictors pp, then the differences in these two estimates are trivial.",
    "crumbs": [
      "Supplemental notes",
      "Model selection criteria"
    ]
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is is a model selection criterion developed by Hirotugu Akaike that aims to estimate the relative quality of different models while penalizing for model complexity. Here is the original paper on AIC concept by Akaike – A New Look at the Statistical Modeling Identification. The purpose of AIC is to find a model that maximizes the likelihood of the data while taking into account the number of parameters used. The formula for AIC is as follows:\nAIC=−2logL+2(p+1)(5)\nAIC = -2 \\log L + 2(p+1)\n \\qquad(5)\nwhere logL\\log L is the log-likelihood which measures how well the model fits the data. The term p+1p+1 represents the number of parameters in the model, including the intercept and any additional predictors. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\nAIC=−2logL+2(p+1)=−2[−n2log(2πσ2)−12σ2(𝐘−𝐗𝛃)T(𝐘−𝐗𝛃)]+2(p+1)=nlog(2πRSSn)+1RSS/nRSS=nlog(2π)+nlog(RSS)−nlog(n)+2(p+1)(6)\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n \\qquad(6)",
    "crumbs": [
      "Supplemental notes",
      "Model selection criteria"
    ]
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\nSimilar to AIC, the Bayesian Information Criterion (BIC) is another model selection criterion that considers both model fit and complexity. BIC is based on Bayesian principles and provides a more stronger penalty for model complexity compared to AIC. Gideon Schwarz’s foundational paper on BIC is titled “Estimating the Dimension of a Model” and was published in 1978. The formula for BIC is as follows:\nBIC=−2logL+(p+1)logn(7)\nBIC = -2 \\log L + (p+1) \\log n\n \\qquad(7)\nIn the formula, the terms logL\\log L and p+1p+1 have the same meaning as in AIC. Additionally, the term logn\\log n represents the logarithm of the sample size (nn). The logn\\log n term in BIC introduces a stronger penalty for model complexity compared to AIC, as the penalty term scales with the sample size.\nThe main difference between AIC and BIC lies in the penalty term for model complexity. While AIC penalizes complexity to some extent with the term 2(p+1)2 (p+1), BIC’s penalty increases logarithmically with the sample size, resulting in a more pronounced penalty. Therefore, BIC tends to favor simpler models compared to AIC, promoting a more parsimonious approach to model selection.",
    "crumbs": [
      "Supplemental notes",
      "Model selection criteria"
    ]
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.1.4     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 0.1.4 ──\n\n\n✓ broom        0.7.10         ✓ rsample      0.1.1     \n✓ dials        0.0.10         ✓ tune         0.1.6     \n✓ infer        1.0.1.9000     ✓ workflows    0.2.4     \n✓ modeldata    0.1.1          ✓ workflowsets 0.1.0     \n✓ parsnip      0.1.7          ✓ yardstick    0.0.9     \n✓ recipes      0.2.0          \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 × 9\n   .rownames body_mass_g flipper_length_… .fitted  .resid    .hat .sigma .cooksd\n   &lt;chr&gt;           &lt;int&gt;            &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# … with 332 more rows, and 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "scratch/elasticity.html",
    "href": "scratch/elasticity.html",
    "title": "Elasticity Calculations",
    "section": "",
    "text": "NI provided the R code for their elasticity analysis (expand the code below) along with the output as a *.csv file.\n\n\noriginal code used to calculate elasticity\nrequire(dplyr, quietly = TRUE)\nrequire(data.table, quietly = TRUE)\n\n# setwd(here::here(\"dat\"))\n\nprice_volume_data &lt;- fread(here::here(\"scratch/dat\",\"price_data.csv\"))\n\nprice_volume_data$`Volume Weighted Price` &lt;- as.integer(price_volume_data$`Volume Weighted Price`)\nprice_volume_data$Volume &lt;- as.integer(price_volume_data$Volume)\n\n\n# Retrieve Regression elements\n\n#PN List\nPNs &lt;- out &lt;- lapply(unique(price_volume_data$`Part Number`), function(b){\n  sub_df &lt;- subset(price_volume_data, `Part Number` == b)\n return(unique(sub_df$`Part Number`))\n})\n\nPNs &lt;- do.call(rbind,PNs)\nPNs &lt;- as.data.frame(PNs)\nPNs &lt;- cbind(PNs , 1:42)\ncolnames(PNs) &lt;- c('Part Number', 'List ID')\n\n#Elasticity Coefficients\n\ncoef_elasticty &lt;- lapply(unique(price_volume_data$`Part Number`), function(b){\n  sub_df &lt;- subset(price_volume_data, `Part Number` == b)\n  m &lt;- lm(log(Volume) ~ log(`Volume Weighted Price`), data = sub_df)\n  coef(m)\n  \n})\n\ncoef_elasticty &lt;- do.call(rbind, coef_elasticty)\ncoef_elasticty &lt;- as.data.frame(coef_elasticty)\ncoef_elasticty &lt;- cbind(coef_elasticty , 1:42)\ncolnames(coef_elasticty) &lt;- c('Intercept', 'Slope - log(Price)' , 'List ID')\n\n\n# P Value\n\npvalue &lt;- lapply(unique(price_volume_data$`Part Number`), function(b){\n  sub_df &lt;- subset(price_volume_data, `Part Number` == b)\n  m &lt;- lm(log(Volume) ~ log(`Volume Weighted Price`), data = sub_df)\n  lmp &lt;- function (modelobject) {\n    if (class(modelobject) != \"lm\") stop(\"Not an object of class 'lm' \")\n    f &lt;- summary(modelobject)$fstatistic\n    p &lt;- pf(f[1],f[2],f[3],lower.tail=F)\n    attributes(p) &lt;- NULL\n    return(p)\n  }\n  lmp(m)\n })\n\npvalue &lt;- do.call(rbind, pvalue)\npvalue &lt;- as.data.frame(pvalue)\npvalue &lt;- cbind(pvalue , 1:42)\ncolnames(pvalue) &lt;- c('p-value', 'List ID')\n\n# R Squared\n\nr_squared &lt;- lapply(unique(price_volume_data$`Part Number`), function(b){\n  sub_df &lt;- subset(price_volume_data, `Part Number` == b)\n  m &lt;- lm(log(Volume) ~ log(`Volume Weighted Price`), data = sub_df)\n  summary(m)$r.squared\n})\n\nr_squared &lt;- do.call(rbind, r_squared)\nr_squared &lt;- as.data.frame(r_squared)\nr_squared &lt;- cbind(r_squared , 1:42)\ncolnames(r_squared) &lt;- c('R-squared', 'List ID')\n\n#Combined Dataset\n\nelasticity_data &lt;- left_join(PNs , coef_elasticty , by = 'List ID')\nelasticity_data &lt;- left_join(elasticity_data , pvalue , by = 'List ID')\nelasticity_data &lt;- left_join(elasticity_data , r_squared , by = 'List ID')\n\n# fwrite(elasticity_data , \"Price Elastcity Regression Results.csv\")\n\n\n\n\nCode\ndata.table::data.table(elasticity_data)  |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(\n    title = \"Estimated elasticities\"\n    , subtitle = \"lm(log(Volume) ~ log(`Volume Weighted Price`))\"\n  ) |&gt; \n  gt::fmt_number(columns = -c(`Part Number`, `List ID`), decimals = 3) |&gt; \n#  gt::data_color(columns = `Slope - log(Price)`) |&gt; \n  gt::tab_style(\n    style = gt::cell_fill(color = \"lightgreen\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = `Slope - log(Price)`,\n      rows = `Slope - log(Price)` &gt;= 0\n    )\n  ) |&gt;\n  gt::tab_style(\n    style = gt::cell_fill(color = \"red\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = `Slope - log(Price)`,\n      rows = `Slope - log(Price)` &lt; 0\n    ) \n  )|&gt; \n  gtExtras::gt_theme_espn()\n\n\nI was able to replicate their calculation, streamlining their code a bit and providing a bit more functionality (expand the code below).\n\n\nPSL re-write of elasticity code\n# this is a re-write that is somewhat less verbose\n\n# Load and clean the data   \nfile_dir &lt;- \"../../data/AWS Source/Price Elasticity Analysis/\"\nprice_volume_data &lt;- \n  readr::read_csv( here::here(\"dat\",\"price_data.csv\"), show_col_types = FALSE ) %&gt;% \n  dplyr::mutate(\n    `Volume Weighted Price` = as.integer(`Volume Weighted Price`)\n    , Volume = as.integer(Volume)\n  )\n\n# Retrieve Regression elements\n\nPNs &lt;- out &lt;- price_volume_data %&gt;% \n  dplyr::distinct(`Part Number`) %&gt;% \n  tibble::rowid_to_column(\"List ID\")\n\n#Elasticity Coefficients\n\nPNs %&gt;% \n  dplyr::mutate(\n    coef = \n      purrr::map(\n        `Part Number`\n        ,\\(x){\n          # fit the linear model\n          m &lt;- price_volume_data %&gt;% dplyr::filter(`Part Number` == x) %&gt;% \n            lm(log(Volume) ~ log(`Volume Weighted Price`), data = .)\n          \n          m %&gt;% \n            # pull out the statistics from the fit object\n            stats::coef() %&gt;% \n            as.list() %&gt;% \n            # make into a tibble\n            tibble::as_tibble() %&gt;% \n            # add other stats as separate columns\n            tibble::add_column(\n              'p-value' = m %&gt;% \n                (\\(modelobject){\n                  if (class(modelobject) != \"lm\") stop(\"Not an object of class 'lm' \")\n                  # pull the F statistics\n                  f &lt;- summary(modelobject)$fstatistic\n                  # extract the stats we want (per the original NI code)\n                  p &lt;- stats::pf(f[1],f[2],f[3],lower.tail=F)\n                  attributes(p) &lt;- NULL\n                  return(p)\n                })\n                # include the R^2 stat\n                , 'R-squared' = summary(m)$r.squared\n            )\n        }\n      )\n  ) %&gt;% \n  tidyr::unnest(coef) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(\n    title = \"Estimated elasticities\"\n    , subtitle = \"lm(log(Volume) ~ log(`Volume Weighted Price`))\"\n  ) |&gt; \n  gt::fmt_number(columns = -c(`Part Number`, `List ID`), decimals = 3) |&gt; \n  gt::tab_style(\n    style = gt::cell_fill(color = \"lightgreen\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = `log(\\`Volume Weighted Price\\`)`,\n      rows = `log(\\`Volume Weighted Price\\`)` &gt;= 0\n    )\n  ) |&gt;\n  gt::tab_style(\n    style = gt::cell_fill(color = \"red\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = `log(\\`Volume Weighted Price\\`)`,\n      rows = `log(\\`Volume Weighted Price\\`)` &lt; 0\n    ) \n  )|&gt; \n  gtExtras::gt_theme_espn()\n\n\nThe orifinal code approached the problem by running a regular regression of log(price) against the log of the number of orders (by part number), for a small set of part numbers.\nHowever, a regular regression as used in this model assumes that the data generating process is Gaussian, which may not be the case. What is more problematic is that there are no constraints on the regression coefficients, which may lead to positive elasticity estimates; positive elasticities are impossible per economic theory.\nNote that there are quite a few positive elasticities (18 of 42).\nI next extended my code to provide a look at the data, plotting both the data and the fitted elasticities.\nA sample is shown below (the green points are along the fitted demand curve).\n\n\nCode extracting PSL elasticity results + plots\nprice_volume_data &lt;- \n  readr::read_csv( here::here(\"dat\",\"price_data.csv\"), show_col_types = FALSE) %&gt;% \n  dplyr::mutate(\n    `Volume Weighted Price` = as.integer(`Volume Weighted Price`)\n    , Volume = as.integer(Volume)\n  )\n\n# Retrieve Regression elements\n\n# Part Number List\nPNs &lt;- out &lt;- price_volume_data %&gt;% \n  dplyr::distinct(`Part Number`) %&gt;% \n  tibble::rowid_to_column(\"List ID\")\n\nfoo &lt;- PNs %&gt;% \n  dplyr::mutate(\n    coef = \n      purrr::map(\n        `Part Number`\n        ,\\(x){\n          m &lt;- price_volume_data %&gt;% \n            dplyr::filter(`Part Number` == x) %&gt;% \n            dplyr::mutate(\n              `Volume Weighted Price` = `Volume Weighted Price`/mean(`Volume Weighted Price`)\n            ) %&gt;% \n            lm(log(Volume) ~ log(`Volume Weighted Price`), data = .)\n          \n          m %&gt;% \n            stats::coef() %&gt;% \n            as.list() %&gt;% \n            tibble::as_tibble() %&gt;% \n            tibble::add_column(\n              'p-value' = m %&gt;% \n                (\\(modelobject){\n                  if (class(modelobject) != \"lm\") stop(\"Not an object of class 'lm' \")\n                  f &lt;- summary(modelobject)$fstatistic\n                  p &lt;- stats::pf(f[1],f[2],f[3],lower.tail=F)\n                  attributes(p) &lt;- NULL\n                  return(p)\n                })\n              , 'R-squared' = summary(m)$r.squared\n            )\n        }\n      )\n    , plot =\n      purrr::map2(\n        `Part Number`\n        , coef\n        , \\(x,y){\n          dat1 &lt;- price_volume_data %&gt;% \n            dplyr::filter(`Part Number` == x) %&gt;% \n            dplyr::mutate(mean_price = mean(`Volume Weighted Price`))\n          \n          dat2 &lt;- tibble::tibble(\n            price = seq(\n              from = min(dat1$`Volume Weighted Price`)\n              , to = max(dat1$`Volume Weighted Price`)\n              , by = 2\n            )\n          ) %&gt;% \n            dplyr::mutate(\n              predicted_volume = exp(y$`(Intercept)` + y$`log(\\`Volume Weighted Price\\`)` * log(price/dat1$mean_price[1]))\n            )\n          \n          dat1 %&gt;% ggplot( aes(x =`Volume Weighted Price`, y = Volume) ) +\n            geom_point() +\n            geom_line(\n              data = dat2\n              , aes(x = price,y = predicted_volume)\n              , linetype = \"dotted\"\n              , linewidth = 1.5\n              , color = 'green'\n            ) +\n            scale_y_log10() + \n            scale_x_log10() +\n            ggtitle( stringr::str_glue(\"Part {x}\") ) +\n            theme_minimal(base_size = 12)\n        }\n        \n      )\n  ) \n\np1 &lt;- foo %&gt;% \n  dplyr::slice(5) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\np2 &lt;- foo %&gt;% \n  dplyr::slice(1) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\np3 &lt;- foo %&gt;% \n  dplyr::slice(4) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\np4 &lt;- foo %&gt;% \n  dplyr::slice(3) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\n\n(p1 + p2)/(p3 + p4)"
  },
  {
    "objectID": "scratch/elasticity.html#elasticity-as-calculated-by-ni",
    "href": "scratch/elasticity.html#elasticity-as-calculated-by-ni",
    "title": "Elasticity Calculations",
    "section": "",
    "text": "NI provided the R code for their elasticity analysis (expand the code below) along with the output as a *.csv file.\n\n\noriginal code used to calculate elasticity\nrequire(dplyr, quietly = TRUE)\nrequire(data.table, quietly = TRUE)\n\n# setwd(here::here(\"dat\"))\n\nprice_volume_data &lt;- fread(here::here(\"scratch/dat\",\"price_data.csv\"))\n\nprice_volume_data$`Volume Weighted Price` &lt;- as.integer(price_volume_data$`Volume Weighted Price`)\nprice_volume_data$Volume &lt;- as.integer(price_volume_data$Volume)\n\n\n# Retrieve Regression elements\n\n#PN List\nPNs &lt;- out &lt;- lapply(unique(price_volume_data$`Part Number`), function(b){\n  sub_df &lt;- subset(price_volume_data, `Part Number` == b)\n return(unique(sub_df$`Part Number`))\n})\n\nPNs &lt;- do.call(rbind,PNs)\nPNs &lt;- as.data.frame(PNs)\nPNs &lt;- cbind(PNs , 1:42)\ncolnames(PNs) &lt;- c('Part Number', 'List ID')\n\n#Elasticity Coefficients\n\ncoef_elasticty &lt;- lapply(unique(price_volume_data$`Part Number`), function(b){\n  sub_df &lt;- subset(price_volume_data, `Part Number` == b)\n  m &lt;- lm(log(Volume) ~ log(`Volume Weighted Price`), data = sub_df)\n  coef(m)\n  \n})\n\ncoef_elasticty &lt;- do.call(rbind, coef_elasticty)\ncoef_elasticty &lt;- as.data.frame(coef_elasticty)\ncoef_elasticty &lt;- cbind(coef_elasticty , 1:42)\ncolnames(coef_elasticty) &lt;- c('Intercept', 'Slope - log(Price)' , 'List ID')\n\n\n# P Value\n\npvalue &lt;- lapply(unique(price_volume_data$`Part Number`), function(b){\n  sub_df &lt;- subset(price_volume_data, `Part Number` == b)\n  m &lt;- lm(log(Volume) ~ log(`Volume Weighted Price`), data = sub_df)\n  lmp &lt;- function (modelobject) {\n    if (class(modelobject) != \"lm\") stop(\"Not an object of class 'lm' \")\n    f &lt;- summary(modelobject)$fstatistic\n    p &lt;- pf(f[1],f[2],f[3],lower.tail=F)\n    attributes(p) &lt;- NULL\n    return(p)\n  }\n  lmp(m)\n })\n\npvalue &lt;- do.call(rbind, pvalue)\npvalue &lt;- as.data.frame(pvalue)\npvalue &lt;- cbind(pvalue , 1:42)\ncolnames(pvalue) &lt;- c('p-value', 'List ID')\n\n# R Squared\n\nr_squared &lt;- lapply(unique(price_volume_data$`Part Number`), function(b){\n  sub_df &lt;- subset(price_volume_data, `Part Number` == b)\n  m &lt;- lm(log(Volume) ~ log(`Volume Weighted Price`), data = sub_df)\n  summary(m)$r.squared\n})\n\nr_squared &lt;- do.call(rbind, r_squared)\nr_squared &lt;- as.data.frame(r_squared)\nr_squared &lt;- cbind(r_squared , 1:42)\ncolnames(r_squared) &lt;- c('R-squared', 'List ID')\n\n#Combined Dataset\n\nelasticity_data &lt;- left_join(PNs , coef_elasticty , by = 'List ID')\nelasticity_data &lt;- left_join(elasticity_data , pvalue , by = 'List ID')\nelasticity_data &lt;- left_join(elasticity_data , r_squared , by = 'List ID')\n\n# fwrite(elasticity_data , \"Price Elastcity Regression Results.csv\")\n\n\n\n\nCode\ndata.table::data.table(elasticity_data)  |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(\n    title = \"Estimated elasticities\"\n    , subtitle = \"lm(log(Volume) ~ log(`Volume Weighted Price`))\"\n  ) |&gt; \n  gt::fmt_number(columns = -c(`Part Number`, `List ID`), decimals = 3) |&gt; \n#  gt::data_color(columns = `Slope - log(Price)`) |&gt; \n  gt::tab_style(\n    style = gt::cell_fill(color = \"lightgreen\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = `Slope - log(Price)`,\n      rows = `Slope - log(Price)` &gt;= 0\n    )\n  ) |&gt;\n  gt::tab_style(\n    style = gt::cell_fill(color = \"red\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = `Slope - log(Price)`,\n      rows = `Slope - log(Price)` &lt; 0\n    ) \n  )|&gt; \n  gtExtras::gt_theme_espn()\n\n\nI was able to replicate their calculation, streamlining their code a bit and providing a bit more functionality (expand the code below).\n\n\nPSL re-write of elasticity code\n# this is a re-write that is somewhat less verbose\n\n# Load and clean the data   \nfile_dir &lt;- \"../../data/AWS Source/Price Elasticity Analysis/\"\nprice_volume_data &lt;- \n  readr::read_csv( here::here(\"dat\",\"price_data.csv\"), show_col_types = FALSE ) %&gt;% \n  dplyr::mutate(\n    `Volume Weighted Price` = as.integer(`Volume Weighted Price`)\n    , Volume = as.integer(Volume)\n  )\n\n# Retrieve Regression elements\n\nPNs &lt;- out &lt;- price_volume_data %&gt;% \n  dplyr::distinct(`Part Number`) %&gt;% \n  tibble::rowid_to_column(\"List ID\")\n\n#Elasticity Coefficients\n\nPNs %&gt;% \n  dplyr::mutate(\n    coef = \n      purrr::map(\n        `Part Number`\n        ,\\(x){\n          # fit the linear model\n          m &lt;- price_volume_data %&gt;% dplyr::filter(`Part Number` == x) %&gt;% \n            lm(log(Volume) ~ log(`Volume Weighted Price`), data = .)\n          \n          m %&gt;% \n            # pull out the statistics from the fit object\n            stats::coef() %&gt;% \n            as.list() %&gt;% \n            # make into a tibble\n            tibble::as_tibble() %&gt;% \n            # add other stats as separate columns\n            tibble::add_column(\n              'p-value' = m %&gt;% \n                (\\(modelobject){\n                  if (class(modelobject) != \"lm\") stop(\"Not an object of class 'lm' \")\n                  # pull the F statistics\n                  f &lt;- summary(modelobject)$fstatistic\n                  # extract the stats we want (per the original NI code)\n                  p &lt;- stats::pf(f[1],f[2],f[3],lower.tail=F)\n                  attributes(p) &lt;- NULL\n                  return(p)\n                })\n                # include the R^2 stat\n                , 'R-squared' = summary(m)$r.squared\n            )\n        }\n      )\n  ) %&gt;% \n  tidyr::unnest(coef) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(\n    title = \"Estimated elasticities\"\n    , subtitle = \"lm(log(Volume) ~ log(`Volume Weighted Price`))\"\n  ) |&gt; \n  gt::fmt_number(columns = -c(`Part Number`, `List ID`), decimals = 3) |&gt; \n  gt::tab_style(\n    style = gt::cell_fill(color = \"lightgreen\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = `log(\\`Volume Weighted Price\\`)`,\n      rows = `log(\\`Volume Weighted Price\\`)` &gt;= 0\n    )\n  ) |&gt;\n  gt::tab_style(\n    style = gt::cell_fill(color = \"red\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = `log(\\`Volume Weighted Price\\`)`,\n      rows = `log(\\`Volume Weighted Price\\`)` &lt; 0\n    ) \n  )|&gt; \n  gtExtras::gt_theme_espn()\n\n\nThe orifinal code approached the problem by running a regular regression of log(price) against the log of the number of orders (by part number), for a small set of part numbers.\nHowever, a regular regression as used in this model assumes that the data generating process is Gaussian, which may not be the case. What is more problematic is that there are no constraints on the regression coefficients, which may lead to positive elasticity estimates; positive elasticities are impossible per economic theory.\nNote that there are quite a few positive elasticities (18 of 42).\nI next extended my code to provide a look at the data, plotting both the data and the fitted elasticities.\nA sample is shown below (the green points are along the fitted demand curve).\n\n\nCode extracting PSL elasticity results + plots\nprice_volume_data &lt;- \n  readr::read_csv( here::here(\"dat\",\"price_data.csv\"), show_col_types = FALSE) %&gt;% \n  dplyr::mutate(\n    `Volume Weighted Price` = as.integer(`Volume Weighted Price`)\n    , Volume = as.integer(Volume)\n  )\n\n# Retrieve Regression elements\n\n# Part Number List\nPNs &lt;- out &lt;- price_volume_data %&gt;% \n  dplyr::distinct(`Part Number`) %&gt;% \n  tibble::rowid_to_column(\"List ID\")\n\nfoo &lt;- PNs %&gt;% \n  dplyr::mutate(\n    coef = \n      purrr::map(\n        `Part Number`\n        ,\\(x){\n          m &lt;- price_volume_data %&gt;% \n            dplyr::filter(`Part Number` == x) %&gt;% \n            dplyr::mutate(\n              `Volume Weighted Price` = `Volume Weighted Price`/mean(`Volume Weighted Price`)\n            ) %&gt;% \n            lm(log(Volume) ~ log(`Volume Weighted Price`), data = .)\n          \n          m %&gt;% \n            stats::coef() %&gt;% \n            as.list() %&gt;% \n            tibble::as_tibble() %&gt;% \n            tibble::add_column(\n              'p-value' = m %&gt;% \n                (\\(modelobject){\n                  if (class(modelobject) != \"lm\") stop(\"Not an object of class 'lm' \")\n                  f &lt;- summary(modelobject)$fstatistic\n                  p &lt;- stats::pf(f[1],f[2],f[3],lower.tail=F)\n                  attributes(p) &lt;- NULL\n                  return(p)\n                })\n              , 'R-squared' = summary(m)$r.squared\n            )\n        }\n      )\n    , plot =\n      purrr::map2(\n        `Part Number`\n        , coef\n        , \\(x,y){\n          dat1 &lt;- price_volume_data %&gt;% \n            dplyr::filter(`Part Number` == x) %&gt;% \n            dplyr::mutate(mean_price = mean(`Volume Weighted Price`))\n          \n          dat2 &lt;- tibble::tibble(\n            price = seq(\n              from = min(dat1$`Volume Weighted Price`)\n              , to = max(dat1$`Volume Weighted Price`)\n              , by = 2\n            )\n          ) %&gt;% \n            dplyr::mutate(\n              predicted_volume = exp(y$`(Intercept)` + y$`log(\\`Volume Weighted Price\\`)` * log(price/dat1$mean_price[1]))\n            )\n          \n          dat1 %&gt;% ggplot( aes(x =`Volume Weighted Price`, y = Volume) ) +\n            geom_point() +\n            geom_line(\n              data = dat2\n              , aes(x = price,y = predicted_volume)\n              , linetype = \"dotted\"\n              , linewidth = 1.5\n              , color = 'green'\n            ) +\n            scale_y_log10() + \n            scale_x_log10() +\n            ggtitle( stringr::str_glue(\"Part {x}\") ) +\n            theme_minimal(base_size = 12)\n        }\n        \n      )\n  ) \n\np1 &lt;- foo %&gt;% \n  dplyr::slice(5) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\np2 &lt;- foo %&gt;% \n  dplyr::slice(1) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\np3 &lt;- foo %&gt;% \n  dplyr::slice(4) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\np4 &lt;- foo %&gt;% \n  dplyr::slice(3) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\n\n(p1 + p2)/(p3 + p4)"
  },
  {
    "objectID": "scratch/elasticity.html#a-bayesian-approach-to-calculating-elasticity",
    "href": "scratch/elasticity.html#a-bayesian-approach-to-calculating-elasticity",
    "title": "Elasticity Calculations",
    "section": "A Bayesian approach to calculating elasticity",
    "text": "A Bayesian approach to calculating elasticity\n\nProblem statement\nSince elasticity is defined as the percentage change in volume (ΔV/V\\Delta V/V) for a given percentage change in price (Δp/p\\Delta p/p), with elasticity parameter β\\beta we write:\nΔVV=β×Δpp∂VV=β×∂pp∂log(V)=β×∂log(p)(1)\n\\begin{align*}\n\\frac{\\Delta V}{V} & = \\beta\\times\\frac{\\Delta p}{p} \\\\\n\\frac{\\partial V}{V} & = \\beta\\times\\frac{\\partial p}{p} \\\\\n\\partial\\log(V) & = \\beta\\times\\partial\\log(p)\n\\end{align*}\n \\qquad(1)\nThis is the justification for the log-log regression model, and this model has solution V=KpβV = Kp^\\beta, where KK is a constant. As written, the value of KK is either the volume when p=1p=1 which may or may not be useful, or it is the volume when β=0\\beta=0, which is uninteresting. To make the interpretation of the constant KK more useful, the model can be written as\n∂log(V)=β×∂log(p/pbaseline);V=K(ppbaseline)β\n\\partial\\log(V) = \\beta\\times\\partial\\log(p/p_{\\text{baseline}});\\qquad V = K\\left(\\frac{p}{p_{\\text{baseline}}}\\right)^{\\beta}\n\nin which case the constant is interpreted as the volume when the price equals the baseline price; the elasticity parameter β\\beta is unchanged.\n\n\n\n\n\n\nClosing the loop on the derivation\n\n\n\nIf V=KpβV = Kp^\\beta then log(V)=log(K)+βlog(p)\\log(V) = \\log(K) + \\beta\\log(p), and ∂log(V)/∂log(p)=β\\partial\\log(V)/\\partial\\log(p) = \\beta as in the last line of equation (Equation 1).\n\n\nIn this version of the problem there are only two parameters, the constant log(K)\\log(K) (aka the intercept in the log-log plot of volume vs price plot) and the elasticity β\\beta, the slope of the log-log plot.\n\n\nBayesian model\nThe Bayesian model for this problem is (to within a scaling constant)\nπ[(K,β)|V]=π[V|(K,β)]×π[K]×π[β](2)\n\\begin{align*}\n\\pi\\left[\\left.\\left(K,\\beta\\right)\\right|V\\right] =\\pi\\left[\\left.V\\right|\\left(K,\\beta\\right)\\right]\\times\\pi\\left[K\\right]\\times\\pi\\left[\\beta\\right] \\\\\n\\end{align*}\n \\qquad(2)\nIn words: the joint probability of the parameters given the observed volume data is equal to (to within a scaling constant) the probability of the observed volume data given the parameters, times the prior probabilities of the parameters. In practice we refer to the probabilities as likelihoods, and use log-likelihoods in equation (Equation 2) to avoid numerical problems arising from the product of small probabilities.\nThe key choice we need to make in the Bayesian model is the form of the likelihood function for the observed volumes given the parameters. This is a model describing how the observed volume data is generated given the parameters.\nSince the volume data is units sold (i.e. integers), we have several options for the likelihood function (e.g. Poisson, Negative Binomial, Binomial, mixture models of various sorts), but the Poisson model is the simplest.\nThe Poisson model of the data has a single, positive, real-valued rate parameter λ\\lambda which represents the units sold per unit time (a rate), so we can choose:\nλ=explog(K)+βlog(p)⇒log(λ)=log(K)+βlog(p)\n\\begin{align*}\n\\lambda = \\exp^{\\log(K) + \\beta\\log(p)}\\Rightarrow \\log(\\lambda) = \\log(K) + \\beta\\log(p)\n\\end{align*}\n which gives us the log-log model equivalent to the NI model, with the crucial difference that we have additionally chosen a model for the data-generating process: a Poisson process with parameter λ\\lambda.\nThe Stan language program implementing this model is given below.\n\n\n\n\n\n\nStan functions\n\n\n\nIn the Stan code below, the functions normal_lpdf and cauchy_lpdf implement the Normal and Cauchy log-probability density functions respectively, while poisson_lpmf implements the Poisson log-probability mass function and poisson_rng implements a Poisson random number generator.\n\n\nThe prior distributions for the intercept (log(K)\\log(K)) and elasticity (β\\beta) parameters are Normal and half-Cauchy respectively, where the elasticity prior constrains the parameter to be negative, as required.\n\n\nPoisson model implemented in the Stan language\ndata {\n  /* Dimensions */\n  int&lt;lower=1&gt; N; // rows\n\n  /* price vector (integer) */\n  array[N] real P;\n  \n  /* demand vector (integer) */\n  array[N] int&lt;lower=0&gt; Y;\n\n  /* hyperparameters*/\n  real&lt;lower=0&gt; s;       // scale parameter for intercept prior\n  real&lt;lower=0&gt; e_scale; // scale parameter for elasticity prior\n}\n\nparameters {\n  real &lt;upper=0&gt; elasticity;      // elasticities variable &lt; 0 \n  real intercept;                 // intercepts variable\n\n}\n\ntransformed parameters {\n  array[N] real log_lambda;       // log volume for likelihoods\n  \n  for (i in 1:N){\n    log_lambda[i] = intercept + elasticity * P[i];\n  }\n  \n}\n\nmodel {\n  /* Priors on the parameters */\n  target += normal_lpdf(intercept  | 0, s);\n  target += cauchy_lpdf(elasticity | 0, e_scale);\n\n  /* Conditional log-likelihoods for each observed volume */\n  for (i in 1 : N) {\n    target += poisson_lpmf(Y[i] | exp(log_lambda[i]) );\n  }\n}\n\ngenerated quantities {\n  array[N] int&lt;lower=-1&gt; y_new;  // estimate volumes\n  vector[N] log_lik;             // compute log-likelihood for this model\n  for (i in 1 : N) {\n      y_new[i]   = poisson_rng( exp(log_lambda[i]) );\n      log_lik[i] = poisson_lpmf(Y[i] | exp(log_lambda[i]) );\n  }\n}\n\n\nIn practice we would run the model using different likelihood functions representing different data generating processes, and then pick the one that is ‘best’ according to some metric. I’ve only used the Poisson likelihood here.\n\n\nRunning Stan code\nThere are several interfaces to Stan that are available in R (e.g. RStan, CmdStanR, RStanArm, and brms). I use CmdStanR in the code below.\nThis code transforms the data into the form required by Stan, then samples from the posterior distributions for the parameters using Stan. These samples are then saved, and the posterior distributions are plotted.\n\n\nR Code to run the Stan model\n# this is a re-write of the NI model using Stan\n\n# load packages\nrequire(magrittr)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(webshot2)\nlibrary(patchwork)\nrequire(ggplot2)\n\n# parameters and checks\ncheck_cmdstan_toolchain(fix = TRUE, quiet = TRUE)\ncolor_scheme_set(\"brightblue\")\ncmdstan_path()\ncmdstan_version()\n\n# set parameters\noptions(mc.cores = parallel::detectCores())\nrstan::rstan_options(auto_write = TRUE)\niterations &lt;- 2000\n\n# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ DATA\n\n# (1) read the price-volume data ----\n\nprice_volume_data &lt;- \n  readr::read_csv( here::here(\"dat\",\"price_data.csv\"), show_col_types = FALSE) %&gt;% \n  dplyr::mutate(\n    `Volume Weighted Price` = as.integer(`Volume Weighted Price`)\n    , Volume = as.integer(Volume)\n  ) %&gt;% \n  dplyr::mutate(\n    month = \n      lubridate::ymd( paste0(`Month of Price Date`,'-01') ) %&gt;% \n      lubridate::month()\n    , .after = `Month of Price Date`\n  ) %&gt;% \n  dplyr::group_by(`Part Number`) %&gt;% \n  tidyr::nest(data = -c(`Part Number`)) %&gt;% \n  dplyr::ungroup()\n\n# (2) put the price-volume data in the form required by Stan ----\nprice_volume_data %&lt;&gt;% \n  dplyr::mutate(\n    stan_data = \n      purrr::map(\n        data\n        , (\\(x){\n          dat &lt;- x %&gt;% \n            # note that the mean price is the baseline\n            dplyr::select(volume = Volume, price = `Volume Weighted Price`) %&gt;% \n            dplyr::mutate(price = log( price/mean(price) ) )\n          \n          print(dim(dat))\n          # format the data\n          list(N = nrow(dat)\n               , P = dat$price\n               , Y   = dat$volume\n               , e_scale = 5\n               , s = 10\n          )\n        }) \n      )\n  )\n\n# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ MODEL\n\n# (3) compile model ----\n\n# set directory for the results\nmod_canonical_dir &lt;- here::here(\"results/stan/\")\n\n# compile the model\nmod_canonical &lt;-\n  cmdstanr::cmdstan_model(\n    # stan_file points to the Stan surce code file\n    stan_file= here::here(\"stan\",\"cmdstan_example_NI_03.stan\")\n    , stanc_options = list(\"O1\")\n  )\n\n# (4) fit model by calling Stan through CmnStanR ----\n\n# splt the dataframe by part number \nstan_data_lst &lt;- price_volume_data %&gt;% split(~`Part Number`)\n\nfits_lst &lt;- purrr::map(\n  stan_data_lst\n  # run the fit and save the results (returns NA if the fitting has a problem)\n  , purrr::possibly(\n    (\\(x){\n      # extract the Stan data\n      dat &lt;- x %&gt;% dplyr::select(stan_data) %&gt;% dplyr::pull(stan_data) %&gt;% purrr::pluck(1)\n      # use variational analysis to initialize the parameter values (this can be dropped)\n      mod_canonical_vb &lt;- mod_canonical$variational(\n        data = dat\n        , seed = 123\n        , iter = 20000\n        , algorithm=\"fullrank\"\n        , output_samples = 1000)\n      # get the summary from variational estimate\n      mod_canonical_vb_summary &lt;- mod_canonical_vb$summary()\n      # pull the initial estimates\n      inits &lt;- list(\n        \"elasticity\"=NULL, \"intercept\"=NULL\n      ) %&gt;% purrr::imap(\n        (\\(x,y)\n         mod_canonical_vb_summary %&gt;%\n           dplyr::filter(stringr::str_starts(variable, y)) %&gt;%\n           dplyr::pull(mean)\n        )\n      )\n      # use the compiled model to sample the posterior parameer distributions\n      fit_under_100K &lt;- mod_canonical$sample(\n        data = dat,\n        init = list(inits,inits,inits,inits),\n        seed = 123,\n        chains = 4,\n        parallel_chains = 4,\n        iter_warmup = 1000,\n        iter_sampling = 2000,\n        max_treedepth = 15,\n        refresh = 100 # print update every 100 iters\n      )\n    }\n    )\n    , otherwise = NA\n  )\n)\n\n# (5) extract the fit values for the parameter values ----\nresults_tbl &lt;- fits_lst %&gt;% \n  purrr::imap(\n    (\\(x,y)\n     if (class(x)[1] == 'logical'){\n       NA\n     }else{\n       posterior::summarise_draws( x ) %&gt;% \n         dplyr::filter(variable %in% c('elasticity', 'intercept')) %&gt;% \n         dplyr::mutate(part = y, .before = 1)\n     }\n    )\n  ) %&gt;% \n  dplyr::bind_rows()\n\nresults_tbl |&gt;\n  dplyr::filter(variable != 'intercept') |&gt; \n  dplyr::rename('model parameter' = variable) |&gt; \n  dplyr::select(-c(rhat,ess_bulk,ess_tail)) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(\n    title = \"Estimated elasticities\"\n    , subtitle = \"Bayesian model\"\n  ) |&gt; \n  gt::fmt_number(columns = -c(part, `model parameter`), decimals = 3) |&gt; \n  gt::tab_style(\n    style = gt::cell_fill(color = \"lightgreen\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = c(mean,median),\n      rows = `model parameter` == 'elasticity' & mean &gt;= 0\n    )\n  ) |&gt;\n  gt::tab_style(\n    style = gt::cell_fill(color = \"red\", alpha = 0.5),\n    locations = gt::cells_body(\n      columns = c(mean,median),\n      rows = `model parameter` == 'elasticity' & mean &lt; 0\n    ) \n  ) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ POST PROCESSING\n\n# (6) save the fit data for use later ----\nsuffix = \"elasticity_2024\"\nsummary_lst &lt;- purrr::imap(\n  fits_lst\n  , (\\(x,y){\n    # save the results of the fit for each product\n    x$save_output_files(\n      dir = mod_canonical_dir\n      , basename = paste0(\"output_files_\",suffix,\"_\",y)\n      , timestamp = TRUE\n      , random = TRUE\n    )\n    # convert the cmdstanr output to stan fit objects\n    stanfit_ni &lt;- rstan::read_stan_csv(x$output_files())\n    # save the stan fit objects\n    stanfit_ni %&gt;% saveRDS(paste0(mod_canonical_dir,\"stanfit_\",suffix,\"_\",y,\".rds\"))\n    # return the fit result\n    x$summary(\n      variables = c(\"elasticity\", \"intercept\", \"lp__\")\n    ) %&gt;%\n      dplyr::mutate(part = y, .before = 1) %&gt;%\n      dplyr::filter( stringr::str_starts(variable, 'elasticity')) %&gt;%\n      dplyr::select(part,variable,mean, median) %&gt;%\n      as.list()\n  }\n  )\n) %&gt;%\n  dplyr::bind_rows() %&gt;%\n  tidyr::nest('summary' = variable:median) %&gt;%\n  # save the aggregated results to a file\n  saveRDS(\n    stringr::str_glue(\n      here::here(\"results/stan/summary_ni.rds\")\n    )\n  )\n\n\n# (7) use the saved fit data to plot the posteriod distributions of the parameters ----\nprice_volume_data_plot &lt;- price_volume_data %&gt;% \n  dplyr::mutate(\n    plot =\n      purrr::map(\n        `Part Number`\n        , (\\(y){\n          print(y)\n          temp_fit &lt;- readRDS( here::here(stringr::str_glue('results/stan/stanfit_{suffix}_{y}.rds')) )\n          posterior &lt;- as.array(temp_fit)\n          params &lt;- \n            rstan::summary(temp_fit)$summary[1:2,1] %&gt;% scales::number(accuracy=0.01)\n          \n          bayesplot::mcmc_hex(posterior, pars = c(\"elasticity\",\"intercept\")) + \n            ggplot2::geom_hline(yintercept = rstan::summary(temp_fit)$summary[1:2,1][2]) +\n            ggplot2::geom_vline(xintercept = rstan::summary(temp_fit)$summary[1:2,1][1]) +\n            ggplot2::labs(\n              title = stringr::str_glue(\"Elasticity for product {y}\")\n              , subtitle = \n                stringr::str_glue(\"elasticity = {params[1]}, intercept = {params[2]}\")\n            ) +\n            ggplot2::theme_minimal(base_size = 18)\n        }\n        )\n      )\n  )\n\np1 &lt;- price_volume_data_plot %&gt;% \n  dplyr::slice(5) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\np2 &lt;- price_volume_data_plot %&gt;% \n  dplyr::slice(1) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\np3 &lt;- price_volume_data_plot %&gt;% \n  dplyr::slice(4) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\np4 &lt;- price_volume_data_plot %&gt;% \n  dplyr::slice(3) %&gt;% \n  dplyr::pull(plot) %&gt;% \n  purrr::pluck(1)\n\n(p1 + p2)/(p3 + p4)\n\n\nThe last section of the code uses the saved results of the model fits to plot the posterior distributions of the parameters, i.e. π[(K,β)|V]\\pi\\left[\\left.\\left(K,\\beta\\right)\\right|V\\right], the joint probabilities of the parameters given the observed data for the volume.\nAn example is shown below for 4 of the 42 products fitted. The point estimates reported for each parameter equal the mean of the corresponding posterior distribution (black lines).\n\n\n\nPosterior parameter distributions"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html",
    "href": "exams/BSMM_8740_quiz_1_solutions.html",
    "title": "BSMM-quiz-1",
    "section": "",
    "text": "# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  ggplot2, magrittr, tidymodels, tidyverse, rsample, broom, recipes, parsnip, modeldata\n)\n\n# set the efault theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#packages",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#packages",
    "title": "BSMM-quiz-1",
    "section": "",
    "text": "# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  ggplot2, magrittr, tidymodels, tidyverse, rsample, broom, recipes, parsnip, modeldata\n)\n\n# set the efault theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-1",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-1",
    "title": "BSMM-quiz-1",
    "section": "Q-1",
    "text": "Q-1\nIs this data a tidy dataset?\n\n\n\n\n\n\n\n\n\n\n\nRegion\n&lt; $1M\n$1 - $5M\n$5 - $10M\n$10 - $100M\n&gt; $100M\n\n\n\n\nN America\n$50M\n$324M\n$1045M\n$941M\n$1200M\n\n\nEMEA\n$10M\n$121M\n$77M\n$80M\n$0M\n\n\n\nDelete the wrong answer:\n\n\n\n\n\n\nSOLUTION: the answer is No\n\n\n\nThe values of categories appear as column names, while the corresponding dollar values are spread out across the tables. The Tidy version would have three columns - Region, income_range, and dollar value so that all measurements that go together can be found in a row.\nThis could be achieved by transforming the original table using tidyr::pivot_longer()\n\n\n\n# original table\ndat &lt;- tibble::tibble(\n  Region = c('N America', 'EMEA')\n  , '&lt; $1M' = c('$50M', '$10M')\n  , '$1 - $5M' = c('$324M', '$121M')\n  , '$5 - $10M' = c('$1045M', '$77M')\n  , '$10 - $100M' = c('$941M', '$80M')\n  , '&gt; $100M' = c('$1200M', '$0M')\n) \ndat |&gt; gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nRegion\n&lt; $1M\n$1 - $5M\n$5 - $10M\n$10 - $100M\n&gt; $100M\n\n\n\n\nN America\n$50M\n$324M\n$1045M\n$941M\n$1200M\n\n\nEMEA\n$10M\n$121M\n$77M\n$80M\n$0M\n\n\n\n\n\n\n\n\n# transformed table\ndat |&gt; \n  tidyr::pivot_longer(-Region, names_to = \"range\", values_to = \"$ amount\")|&gt; \n  gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nRegion\nrange\n$ amount\n\n\n\n\nN America\n&lt; $1M\n$50M\n\n\nN America\n$1 - $5M\n$324M\n\n\nN America\n$5 - $10M\n$1045M\n\n\nN America\n$10 - $100M\n$941M\n\n\nN America\n&gt; $100M\n$1200M\n\n\nEMEA\n&lt; $1M\n$10M\n\n\nEMEA\n$1 - $5M\n$121M\n\n\nEMEA\n$5 - $10M\n$77M\n\n\nEMEA\n$10 - $100M\n$80M\n\n\nEMEA\n&gt; $100M\n$0M"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-2",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-2",
    "title": "BSMM-quiz-1",
    "section": "Q-2",
    "text": "Q-2\nWhich resampling method from the resample:: package randomly partitions the data into V sets of roughly equal size?\n\n\n\n\n\n\nSOLUTION: The answer is V-fold cross-validation\n\n\n\nV-fold cross-validation (also known as k-fold cross-validation) randomly splits the data into V groups of roughly equal size (called “folds”). In the tidyverse you create a dataset for V-fold cross-validation using rsample::vfold_cv() ."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-3",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-3",
    "title": "BSMM-quiz-1",
    "section": "Q-3",
    "text": "Q-3\nIf I join the two tables below as follows:\ndplyr::????_join(employees, departments, by = \"department_id\")\nwhich type of join would include employee_name == Moe Syzslak?\n\n\n\n\n\n\nSOLUTION: the answer is left_join\n\n\n\nInner join will return all the rows with common department ID, and since Moe Syzslak has a NA department ID, with no match in the department ID table,his name won’t appear in the result of the join.\nRight join will return the departments table and all the rows of employees with department ID in common with the departments table, and since Moe Syzslak has a NA department ID, with no match in the department ID table,his name won’t appear in the result of the join.\nLeft join will return the employees table and all the rows of departments with department ID in common with the employees table. Since Moe Syzslak appears in the employees table his name will appear in the result of the join. Since\nSee the code below.\n\n\n\ninner\nleft\nright\nall of the above\n\nDelete the incorrect answers.\nemployees - This table contains each employee’s ID, name, and department ID.\n\n\n\nid\nemployee_name\ndepartment_id\n\n\n\n\n1\nHomer Simpson\n4\n\n\n2\nNed Flanders\n1\n\n\n3\nBarney Gumble\n5\n\n\n4\nClancy Wiggum\n3\n\n\n5\nMoe Syzslak\nNA\n\n\n\ndepartments - This table contains each department’s ID and name.\n\n\n\ndepartment_id\ndepartment_name\n\n\n\n\n1\nSales\n\n\n2\nEngineering\n\n\n3\nHuman Resources\n\n\n4\nCustomer Service\n\n\n5\nResearch And Development\n\n\n\ntbl1 &lt;- tibble::tribble(\n~id , ~employee_name,   ~department_id\n,1  ,'Homer Simpson'    ,4\n,2  ,'Ned Flanders'   ,1\n,3  ,'Barney Gumble'    ,5\n,4  ,'Clancy Wiggum'    ,3\n,5  ,'Moe Syzslak'    ,NA\n)\n\ntbl2 &lt;- tibble::tribble(\n~department_id  ,~department_name\n,1  ,\"Sales\"\n,2  ,\"Engineering\"\n,3  ,\"Human Resources\"\n,4  ,\"Customer Service\"\n,5  ,\"Research And Development\"\n)\n\n# left_join\ndplyr::left_join(tbl1,tbl2,by = \"department_id\") |&gt; \n  gt::gt() |&gt; gt::tab_header(title = \"Left Join\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nLeft Join\n\n\nid\nemployee_name\ndepartment_id\ndepartment_name\n\n\n\n\n1\nHomer Simpson\n4\nCustomer Service\n\n\n2\nNed Flanders\n1\nSales\n\n\n3\nBarney Gumble\n5\nResearch And Development\n\n\n4\nClancy Wiggum\n3\nHuman Resources\n\n\n5\nMoe Syzslak\nNA\nNA\n\n\n\n\n\n\n\n\n# right_join\ndplyr::right_join(tbl1,tbl2,by = \"department_id\") |&gt; \n  gt::gt() |&gt; gt::tab_header(title = \"Right Join\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nRight Join\n\n\nid\nemployee_name\ndepartment_id\ndepartment_name\n\n\n\n\n1\nHomer Simpson\n4\nCustomer Service\n\n\n2\nNed Flanders\n1\nSales\n\n\n3\nBarney Gumble\n5\nResearch And Development\n\n\n4\nClancy Wiggum\n3\nHuman Resources\n\n\nNA\nNA\n2\nEngineering\n\n\n\n\n\n\n\n\n# inner_join\ndplyr::inner_join(tbl1,tbl2,by = \"department_id\") |&gt; \n  gt::gt() |&gt; gt::tab_header(title = \"Inner Join\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nInner Join\n\n\nid\nemployee_name\ndepartment_id\ndepartment_name\n\n\n\n\n1\nHomer Simpson\n4\nCustomer Service\n\n\n2\nNed Flanders\n1\nSales\n\n\n3\nBarney Gumble\n5\nResearch And Development\n\n\n4\nClancy Wiggum\n3\nHuman Resources"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-4",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-4",
    "title": "BSMM-quiz-1",
    "section": "Q-4",
    "text": "Q-4\nRecall that the first step of a decision-tree regression model will divide the space of predictors into 2 parts and estimate constant prediction values for each part. For a single predictor, the result of the first step estimates the outcome as:\nŷ=∑i=12ci×I(x∈Ri)\n\\hat{y} =\\sum_{i=1}^{2}c_i\\times I_{(x\\in R_i)}\nsuch that\nSSE={∑i∈R1(yi−ci)2+∑i∈R2(yi−ci)2}\n\\text{SSE}=\\left\\{ \\sum_{i\\in R_{1}}\\left(y_{i}-c_{i}\\right)^{2}+\\sum_{i\\in R_{2}}\\left(y_{i}-c_{i}\\right)^{2}\\right\\} \n\nis minimized.\nOn the first split of a decision tree regression model for the following data:\n\n\n\n\n\nThe first two regions that partition xx will be (Delete the wrong answer(s) below):\n\n\n\n\n\n\nSOLUTION: the answer is [0,1/2] and (1/2, 2/2]\n\n\n\nSince the decision tree is minimizing the SSE at each split, you want to minimize the range (max-min) of y values in each split. You can find a cic_i value to minimize the SSE within each split, but a wider range of yy values will have a larger SSE than a smaller range of yy values, due to the squares, and so the splits should have equal ranges.\nSince it looks like yi=xi+eiy_i = x_i + e_i (where eie_i is an error term), equal x ranges will give equal y ranges, so the split should be [0,1/2] and (1/2, 2/2]."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-5",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-5",
    "title": "BSMM-quiz-1",
    "section": "Q-5",
    "text": "Q-5\nIn an ordinary linear regression, regressing the outcome yy on a single predictor xx, the regression coefficient can be estimated as:\n\n\n\n\n\n\nSOLUTION:\n\n\n\nIn class we showed that the regression coefficient can be estimated as\ncovar(x,y)var(x)\n\\displaystyle\\frac{\\text{covar(x,y)}}{\\text{var(x)}}"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q6",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q6",
    "title": "BSMM-quiz-1",
    "section": "Q6",
    "text": "Q6\nWrite code to determine the number of species of penguin in the dataset. How many are there?\n\n\n\n\n\n\nSOLUTION: there are 3 penguin species in the dataset\n\n\n\n\npalmerpenguins::penguins |&gt; \n  dplyr::distinct(species)\n\n# A tibble: 3 × 1\n  species  \n  &lt;fct&gt;    \n1 Adelie   \n2 Gentoo   \n3 Chinstrap\n\n\n\n# == OR ==\npalmerpenguins::penguins$species |&gt;  \n  unique() |&gt; length()\n\n[1] 3"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q7",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q7",
    "title": "BSMM-quiz-1",
    "section": "Q7",
    "text": "Q7\nExecute the following code to read sales data from a csv file.\n\n# read sales data\nsales_dat &lt;-\n  readr::read_csv(\"data/sales_data_sample.csv\", show_col_types = FALSE) |&gt;\n  janitor::clean_names() |&gt; \n  dplyr::mutate(\n    orderdate = lubridate::as_date(orderdate, format = \"%m/%d/%Y %H:%M\")\n    , orderdate = lubridate::year(orderdate)\n  )\n\nDescribe what the group_by step does in the code below, and complete the code to produce a sales summary by year, i.e. a data.frame where productline and orderdate are the columns (one column for each year), while each year column contains the sales for each productline that year.\n\n  sales_dat |&gt; \n    dplyr::group_by(orderdate, productline) |&gt; \n    dplyr::summarize( sales = sum(___) ) |&gt; \n    tidyr::pivot_wider(names_from = ___, values_from = ___)\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nthe result of the group_by step is: order first by the values of the orderdate column, and then, within each orderdate value, order the rows by the values of the productline column.\nthe sales summary table produced by the code is given below\n\n\n# executed code\nsales_dat |&gt; \n    dplyr::group_by(orderdate, productline) |&gt; \n    dplyr::summarize( sales = sum(sales) ) |&gt; \n    tidyr::pivot_wider(names_from = orderdate, values_from = sales)\n\n# A tibble: 7 × 4\n  productline        `2003`   `2004`  `2005`\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 Classic Cars     1484785. 1762257. 672573.\n2 Motorcycles       370896.  560545. 234948.\n3 Planes            272258.  502672. 200074.\n4 Ships             244821.  341438. 128178.\n5 Trains             72802.  116524.  36917.\n6 Trucks and Buses  420430.  529303. 178057.\n7 Vintage Cars      650988.  911424. 340739."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q8",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q8",
    "title": "BSMM-quiz-1",
    "section": "Q8",
    "text": "Q8\nFor the data below, it is expected that the response variable yy can be described by the independent variables x1x1 and x2x2. This implies that the parameters of the following model should be estimated and tested per the model:\ny=β0+β1x1+β2x2+ϵ,ϵ∼𝒩(0,σ2)\ny = \\beta_0 + \\beta_1x1 + \\beta_2x2 + \\epsilon, \\epsilon ∼ \\mathcal{N}(0, \\sigma^2)\n\n\ndat &lt;- tibble::tibble(\n  x1=c(0.58, 0.86, 0.29, 0.20, 0.56, 0.28, 0.08, 0.41, 0.22, 0.35, 0.59, 0.22, 0.26, 0.12, 0.65, 0.70, 0.30\n        , 0.70, 0.39, 0.72, 0.45, 0.81, 0.04, 0.20, 0.95)\n  , x2=c(0.71, 0.13, 0.79, 0.20, 0.56, 0.92, 0.01, 0.60, 0.70, 0.73, 0.13, 0.96, 0.27, 0.21, 0.88, 0.30\n        , 0.15, 0.09, 0.17, 0.25, 0.30, 0.32, 0.82, 0.98, 0.00)\n  , y=c(1.45, 1.93, 0.81, 0.61, 1.55, 0.95, 0.45, 1.14, 0.74, 0.98, 1.41, 0.81, 0.89, 0.68, 1.39, 1.53\n        , 0.91, 1.49, 1.38, 1.73, 1.11, 1.68, 0.66, 0.69, 1.98)\n)\n\nCalculate the parameter estimates ( β̂0\\hat{\\beta}_0, β̂1\\hat{\\beta}_1, and β̂2\\hat{\\beta}_2); in addition find the usual 95% confidence intervals for β0\\beta_0, β1\\beta_1, β2\\beta_2.\n\n\n\n\n\n\nSOLUTION:\n\n\n\nUsing broom::tidy(conf.int = TRUE) with a regression model:\n\n# your code goes here\nfit_Q8 &lt;- lm(y ~ ., data = dat)\nfit_Q8 |&gt; broom::tidy(conf.int = TRUE) |&gt; \n  gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.433547115\n0.06598300\n6.57058782\n1.313239e-06\n0.2967067\n0.5703875\n\n\nx1\n1.652993451\n0.09524539\n17.35510141\n2.525004e-14\n1.4554666\n1.8505203\n\n\nx2\n0.003944875\n0.07485382\n0.05270105\n9.584457e-01\n-0.1512924\n0.1591822"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q9",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q9",
    "title": "BSMM-quiz-1",
    "section": "Q9",
    "text": "Q9\nUsing the .resid column created by broom::augment(___, dat) , calculate σ̂2\\hat{\\sigma}^2.\n\n\n\n\n\n\nSOLUTION: the variance of the residual is 0.0116\n\n\n\n\nbroom::augment(fit_Q8, dat) |&gt; \n  dplyr::pull(.resid) |&gt; \n  var()\n\n[1] 0.01164646"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q10",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q10",
    "title": "BSMM-quiz-1",
    "section": "Q10",
    "text": "Q10\nDoes the following code train a model on the full training set of the modeldata::ames housing dataset and then evaluate the model using a test set?\n\nIs any step missing?\nWhen the recipe is baked and prepped, do you think all categories will be converted to dummy variables and all numeric predictors will be normalized?\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# Load the ames housing dataset\ndata(ames)\n\n# Create an initial split of the data\nset.seed(123)\names_split &lt;- initial_split(ames, prop = 0.8, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\n# Create a recipe\names_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt;\n  step_log(Sale_Price, base = 10) |&gt;  \n  step_dummy(all_nominal_predictors()) |&gt;  \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())  \n\n\n# Create a workflow\names_workflow &lt;- workflow() |&gt;\n  add_recipe(ames_recipe) |&gt;\n  add_model(lm_spec)\n\n# Fit the model and evaluate on the test set\names_fit &lt;- ames_workflow |&gt; last_fit(ames_split)\n\n# View the metrics\names_fit |&gt; collect_metrics()\n\n\nIf you run the code chunk above you will get an error error, “‘lm_spec’ is missing”. A workflow requires a pre-processing step (by formula or recipe) and a model specification. That is what is missing here.\nLooking at the data in the ames dataset, there are 40 factor variables and 34 numeric variables.\n\n\nskim_data &lt;- ames |&gt; skimr::skim()\nskim_data$skim_type |&gt; table()\n\n\n factor numeric \n     40      34 \n\n\nThe author of the recipe (see below) likely wanted the factor variables to be converted to dummy variables, and the numeric variables to be normalized.\nHowever, the order of the steps in the recipe turns the factor variables into dummy variables before the numeric variable were normalized, and dummy variables are numeric. Given the order of the steps, all the dummy variables will be normalized and all variables will be normalized numeric variables.\nNo dummy variables remain after the bake operation!\n\names_recipe |&gt; prep() |&gt; \n  bake(new_data = ames) |&gt; \n  dplyr::glimpse()\n\nRows: 2,930\nColumns: 273\n$ Lot_Frontage                                          &lt;dbl&gt; 2.46645944, 0.66…\n$ Lot_Area                                              &lt;dbl&gt; 2.553049311, 0.1…\n$ Year_Built                                            &lt;dbl&gt; -0.39216369, -0.…\n$ Year_Remod_Add                                        &lt;dbl&gt; -1.17030684, -1.…\n$ Mas_Vnr_Area                                          &lt;dbl&gt; 0.05161557, -0.5…\n$ BsmtFin_SF_1                                          &lt;dbl&gt; -0.97323408, 0.8…\n$ BsmtFin_SF_2                                          &lt;dbl&gt; -0.2948976, 0.54…\n$ Bsmt_Unf_SF                                           &lt;dbl&gt; -0.26264166, -0.…\n$ Total_Bsmt_SF                                         &lt;dbl&gt; 0.066772012, -0.…\n$ First_Flr_SF                                          &lt;dbl&gt; 1.24235505, -0.6…\n$ Second_Flr_SF                                         &lt;dbl&gt; -0.7768908, -0.7…\n$ Gr_Liv_Area                                           &lt;dbl&gt; 0.3012894541, -1…\n$ Bsmt_Full_Bath                                        &lt;dbl&gt; 1.0688790, -0.82…\n$ Bsmt_Half_Bath                                        &lt;dbl&gt; -0.2447658, -0.2…\n$ Full_Bath                                             &lt;dbl&gt; -1.0350779, -1.0…\n$ Half_Bath                                             &lt;dbl&gt; -0.7505082, -0.7…\n$ Bedroom_AbvGr                                         &lt;dbl&gt; 0.1586909, -1.04…\n$ Kitchen_AbvGr                                         &lt;dbl&gt; -0.2142401, -0.2…\n$ TotRms_AbvGrd                                         &lt;dbl&gt; 0.3483003, -0.91…\n$ Fireplaces                                            &lt;dbl&gt; 2.1485023, -0.92…\n$ Garage_Cars                                           &lt;dbl&gt; 0.3157983, -1.00…\n$ Garage_Area                                           &lt;dbl&gt; 0.269501338, 1.2…\n$ Wood_Deck_SF                                          &lt;dbl&gt; 0.9247677, 0.372…\n$ Open_Porch_SF                                         &lt;dbl&gt; 0.21567955, -0.7…\n$ Enclosed_Porch                                        &lt;dbl&gt; -0.364656, -0.36…\n$ Three_season_porch                                    &lt;dbl&gt; -0.1027822, -0.1…\n$ Screen_Porch                                          &lt;dbl&gt; -0.2849706, 1.83…\n$ Pool_Area                                             &lt;dbl&gt; -0.06118388, -0.…\n$ Misc_Val                                              &lt;dbl&gt; -0.08608582, -0.…\n$ Mo_Sold                                               &lt;dbl&gt; -0.42445939, -0.…\n$ Year_Sold                                             &lt;dbl&gt; 1.666407, 1.6664…\n$ Longitude                                             &lt;dbl&gt; 0.9003850, 0.900…\n$ Latitude                                              &lt;dbl&gt; 1.0642009, 1.008…\n$ Sale_Price                                            &lt;dbl&gt; 5.332438, 5.0211…\n$ MS_SubClass_One_Story_1945_and_Older                  &lt;dbl&gt; -0.2198252, -0.2…\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    &lt;dbl&gt; -0.04135376, -0.…\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    &lt;dbl&gt; -0.08027027, -0.…\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      &lt;dbl&gt; -0.3211104, -0.3…\n$ MS_SubClass_Two_Story_1946_and_Newer                  &lt;dbl&gt; -0.4996264, -0.4…\n$ MS_SubClass_Two_Story_1945_and_Older                  &lt;dbl&gt; -0.2066988, -0.2…\n$ MS_SubClass_Two_and_Half_Story_All_Ages               &lt;dbl&gt; -0.08549097, -0.…\n$ MS_SubClass_Split_or_Multilevel                       &lt;dbl&gt; -0.2066988, -0.2…\n$ MS_SubClass_Split_Foyer                               &lt;dbl&gt; -0.1283979, -0.1…\n$ MS_SubClass_Duplex_All_Styles_and_Ages                &lt;dbl&gt; -0.2055737, -0.2…\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              &lt;dbl&gt; -0.2707322, -0.2…\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           &lt;dbl&gt; -0.02066363, -0.…\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              &lt;dbl&gt; -0.2187562, -0.2…\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          &lt;dbl&gt; -0.07469546, -0.…\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; -0.1399371, -0.1…\n$ MS_Zoning_Residential_High_Density                    &lt;dbl&gt; -0.09509974, 10.…\n$ MS_Zoning_Residential_Low_Density                     &lt;dbl&gt; 0.5420306, -1.84…\n$ MS_Zoning_Residential_Medium_Density                  &lt;dbl&gt; -0.4344558, -0.4…\n$ MS_Zoning_A_agr                                       &lt;dbl&gt; -0.02066363, -0.…\n$ MS_Zoning_C_all                                       &lt;dbl&gt; -0.09735866, -0.…\n$ MS_Zoning_I_all                                       &lt;dbl&gt; -0.02922903, -0.…\n$ Street_Pave                                           &lt;dbl&gt; 0.06868034, 0.06…\n$ Alley_No_Alley_Access                                 &lt;dbl&gt; 0.2661636, 0.266…\n$ Alley_Paved                                           &lt;dbl&gt; -0.1648677, -0.1…\n$ Lot_Shape_Slightly_Irregular                          &lt;dbl&gt; 1.4134589, -0.70…\n$ Lot_Shape_Moderately_Irregular                        &lt;dbl&gt; -0.1607238, -0.1…\n$ Lot_Shape_Irregular                                   &lt;dbl&gt; -0.07174967, -0.…\n$ Land_Contour_HLS                                      &lt;dbl&gt; -0.2066988, -0.2…\n$ Land_Contour_Low                                      &lt;dbl&gt; -0.1430754, -0.1…\n$ Land_Contour_Lvl                                      &lt;dbl&gt; 0.3401764, 0.340…\n$ Utilities_NoSeWa                                      &lt;dbl&gt; -0.02066363, -0.…\n$ Utilities_NoSewr                                      &lt;dbl&gt; -0.02922903, -0.…\n$ Lot_Config_CulDSac                                    &lt;dbl&gt; -0.2577909, -0.2…\n$ Lot_Config_FR2                                        &lt;dbl&gt; -0.1793294, -0.1…\n$ Lot_Config_FR3                                        &lt;dbl&gt; -0.07174967, -0.…\n$ Lot_Config_Inside                                     &lt;dbl&gt; -1.6286599, 0.61…\n$ Land_Slope_Mod                                        &lt;dbl&gt; -0.2187562, -0.2…\n$ Land_Slope_Sev                                        &lt;dbl&gt; -0.07174967, -0.…\n$ Neighborhood_College_Creek                            &lt;dbl&gt; -0.3186783, -0.3…\n$ Neighborhood_Old_Town                                 &lt;dbl&gt; -0.287611, -0.28…\n$ Neighborhood_Edwards                                  &lt;dbl&gt; -0.2679979, -0.2…\n$ Neighborhood_Somerset                                 &lt;dbl&gt; -0.2596688, -0.2…\n$ Neighborhood_Northridge_Heights                       &lt;dbl&gt; -0.2384008, -0.2…\n$ Neighborhood_Gilbert                                  &lt;dbl&gt; -0.2423742, -0.2…\n$ Neighborhood_Sawyer                                   &lt;dbl&gt; -0.2302931, -0.2…\n$ Neighborhood_Northwest_Ames                           &lt;dbl&gt; -0.2166052, -0.2…\n$ Neighborhood_Sawyer_West                              &lt;dbl&gt; -0.2155231, -0.2…\n$ Neighborhood_Mitchell                                 &lt;dbl&gt; -0.2010204, -0.2…\n$ Neighborhood_Brookside                                &lt;dbl&gt; -0.1904408, -0.1…\n$ Neighborhood_Crawford                                 &lt;dbl&gt; -0.1928346, -0.1…\n$ Neighborhood_Iowa_DOT_and_Rail_Road                   &lt;dbl&gt; -0.1916409, -0.1…\n$ Neighborhood_Timberland                               &lt;dbl&gt; -0.1550442, -0.1…\n$ Neighborhood_Northridge                               &lt;dbl&gt; -0.1607238, -0.1…\n$ Neighborhood_Stone_Brook                              &lt;dbl&gt; -0.1351039, -0.1…\n$ Neighborhood_South_and_West_of_Iowa_State_University  &lt;dbl&gt; -0.1157946, -0.1…\n$ Neighborhood_Clear_Creek                              &lt;dbl&gt; -0.1213469, -0.1…\n$ Neighborhood_Meadow_Village                           &lt;dbl&gt; -0.113887, -0.11…\n$ Neighborhood_Briardale                                &lt;dbl&gt; -0.1079726, -0.1…\n$ Neighborhood_Bloomington_Heights                      &lt;dbl&gt; -0.09956823, -0.…\n$ Neighborhood_Veenker                                  &lt;dbl&gt; -0.09041895, -0.…\n$ Neighborhood_Northpark_Villa                          &lt;dbl&gt; -0.09278786, -0.…\n$ Neighborhood_Blueste                                  &lt;dbl&gt; -0.05853314, -0.…\n$ Neighborhood_Greens                                   &lt;dbl&gt; -0.05066948, -0.…\n$ Neighborhood_Green_Hills                              &lt;dbl&gt; -0.02922903, -0.…\n$ Neighborhood_Landmark                                 &lt;dbl&gt; -0.02066363, -0.…\n$ Condition_1_Feedr                                     &lt;dbl&gt; -0.2462976, 4.05…\n$ Condition_1_Norm                                      &lt;dbl&gt; 0.403473, -2.477…\n$ Condition_1_PosA                                      &lt;dbl&gt; -0.08549097, -0.…\n$ Condition_1_PosN                                      &lt;dbl&gt; -0.1176728, -0.1…\n$ Condition_1_RRAe                                      &lt;dbl&gt; -0.09735866, -0.…\n$ Condition_1_RRAn                                      &lt;dbl&gt; -0.1301046, -0.1…\n$ Condition_1_RRNe                                      &lt;dbl&gt; -0.05066948, -0.…\n$ Condition_1_RRNn                                      &lt;dbl&gt; -0.05474101, -0.…\n$ Condition_2_Feedr                                     &lt;dbl&gt; -0.0654701, -0.0…\n$ Condition_2_Norm                                      &lt;dbl&gt; 0.09509974, 0.09…\n$ Condition_2_PosA                                      &lt;dbl&gt; -0.02066363, -0.…\n$ Condition_2_PosN                                      &lt;dbl&gt; -0.02922903, -0.…\n$ Condition_2_RRAe                                      &lt;dbl&gt; -0.02066363, -0.…\n$ Condition_2_RRAn                                      &lt;dbl&gt; -0.02066363, -0.…\n$ Condition_2_RRNn                                      &lt;dbl&gt; -0.02922903, -0.…\n$ Bldg_Type_TwoFmCon                                    &lt;dbl&gt; -0.1415143, -0.1…\n$ Bldg_Type_Duplex                                      &lt;dbl&gt; -0.2055737, -0.2…\n$ Bldg_Type_Twnhs                                       &lt;dbl&gt; -0.1880208, -0.1…\n$ Bldg_Type_TwnhsE                                      &lt;dbl&gt; -0.3029889, -0.3…\n$ House_Style_One_and_Half_Unf                          &lt;dbl&gt; -0.08549097, -0.…\n$ House_Style_One_Story                                 &lt;dbl&gt; 0.983694, 0.9836…\n$ House_Style_SFoyer                                    &lt;dbl&gt; -0.1689205, -0.1…\n$ House_Style_SLvl                                      &lt;dbl&gt; -0.2166052, -0.2…\n$ House_Style_Two_and_Half_Fin                          &lt;dbl&gt; -0.05474101, -0.…\n$ House_Style_Two_and_Half_Unf                          &lt;dbl&gt; -0.08549097, -0.…\n$ House_Style_Two_Story                                 &lt;dbl&gt; -0.6547801, -0.6…\n$ Overall_Cond_Poor                                     &lt;dbl&gt; -0.06209708, -0.…\n$ Overall_Cond_Fair                                     &lt;dbl&gt; -0.1351039, -0.1…\n$ Overall_Cond_Below_Average                            &lt;dbl&gt; -0.1892341, -0.1…\n$ Overall_Cond_Average                                  &lt;dbl&gt; 0.876672, -1.140…\n$ Overall_Cond_Above_Average                            &lt;dbl&gt; -0.4700736, 2.12…\n$ Overall_Cond_Good                                     &lt;dbl&gt; -0.3875958, -0.3…\n$ Overall_Cond_Very_Good                                &lt;dbl&gt; -0.232341, -0.23…\n$ Overall_Cond_Excellent                                &lt;dbl&gt; -0.113887, -0.11…\n$ Roof_Style_Gable                                      &lt;dbl&gt; -1.9639943, 0.50…\n$ Roof_Style_Gambrel                                    &lt;dbl&gt; -0.07753179, -0.…\n$ Roof_Style_Hip                                        &lt;dbl&gt; 2.0815862, -0.48…\n$ Roof_Style_Mansard                                    &lt;dbl&gt; -0.06209708, -0.…\n$ Roof_Style_Shed                                       &lt;dbl&gt; -0.04135376, -0.…\n$ Roof_Matl_CompShg                                     &lt;dbl&gt; 0.1157946, 0.115…\n$ Roof_Matl_Membran                                     &lt;dbl&gt; -0.02066363, -0.…\n$ Roof_Matl_Metal                                       &lt;dbl&gt; -0.02066363, -0.…\n$ Roof_Matl_Tar.Grv                                     &lt;dbl&gt; -0.08292059, -0.…\n$ Roof_Matl_WdShake                                     &lt;dbl&gt; -0.05853314, -0.…\n$ Roof_Matl_WdShngl                                     &lt;dbl&gt; -0.04135376, -0.…\n$ Exterior_1st_AsphShn                                  &lt;dbl&gt; -0.02922903, -0.…\n$ Exterior_1st_BrkComm                                  &lt;dbl&gt; -0.0462448, -0.0…\n$ Exterior_1st_BrkFace                                  &lt;dbl&gt; 5.7382892, -0.17…\n$ Exterior_1st_CemntBd                                  &lt;dbl&gt; -0.2155231, -0.2…\n$ Exterior_1st_HdBoard                                  &lt;dbl&gt; -0.4267943, -0.4…\n$ Exterior_1st_ImStucc                                  &lt;dbl&gt; -0.02066363, -0.…\n$ Exterior_1st_MetalSd                                  &lt;dbl&gt; -0.4232945, -0.4…\n$ Exterior_1st_Plywood                                  &lt;dbl&gt; -0.288480, -0.28…\n$ Exterior_1st_PreCast                                  &lt;dbl&gt; -0.02066363, -0.…\n$ Exterior_1st_Stone                                    &lt;dbl&gt; -0.02066363, -0.…\n$ Exterior_1st_Stucco                                   &lt;dbl&gt; -0.1195232, -0.1…\n$ Exterior_1st_VinylSd                                  &lt;dbl&gt; -0.7345379, 1.36…\n$ Exterior_1st_Wd.Sdng                                  &lt;dbl&gt; -0.3991715, -0.3…\n$ Exterior_1st_WdShing                                  &lt;dbl&gt; -0.1461515, -0.1…\n$ Exterior_2nd_AsphShn                                  &lt;dbl&gt; -0.03580575, -0.…\n$ Exterior_2nd_Brk.Cmn                                  &lt;dbl&gt; -0.09041895, -0.…\n$ Exterior_2nd_BrkFace                                  &lt;dbl&gt; -0.1249191, -0.1…\n$ Exterior_2nd_CBlock                                   &lt;dbl&gt; -0.02066363, -0.…\n$ Exterior_2nd_CmentBd                                  &lt;dbl&gt; -0.2155231, -0.2…\n$ Exterior_2nd_HdBoard                                  &lt;dbl&gt; -0.4070422, -0.4…\n$ Exterior_2nd_ImStucc                                  &lt;dbl&gt; -0.07469546, -0.…\n$ Exterior_2nd_MetalSd                                  &lt;dbl&gt; -0.4232945, -0.4…\n$ Exterior_2nd_Other                                    &lt;dbl&gt; -0.02066363, -0.…\n$ Exterior_2nd_Plywood                                  &lt;dbl&gt; 3.0517540, -0.32…\n$ Exterior_2nd_PreCast                                  &lt;dbl&gt; -0.02066363, -0.…\n$ Exterior_2nd_Stone                                    &lt;dbl&gt; -0.0462448, -0.0…\n$ Exterior_2nd_Stucco                                   &lt;dbl&gt; -0.1195232, -0.1…\n$ Exterior_2nd_VinylSd                                  &lt;dbl&gt; -0.7283491, 1.37…\n$ Exterior_2nd_Wd.Sdng                                  &lt;dbl&gt; -0.3854073, -0.3…\n$ Exterior_2nd_Wd.Shng                                  &lt;dbl&gt; -0.1689205, -0.1…\n$ Mas_Vnr_Type_BrkFace                                  &lt;dbl&gt; -0.6634428, -0.6…\n$ Mas_Vnr_Type_CBlock                                   &lt;dbl&gt; -0.02066363, -0.…\n$ Mas_Vnr_Type_None                                     &lt;dbl&gt; -1.2199205, 0.81…\n$ Mas_Vnr_Type_Stone                                    &lt;dbl&gt; 3.2366522, -0.30…\n$ Exter_Cond_Fair                                       &lt;dbl&gt; -0.1550442, -0.1…\n$ Exter_Cond_Good                                       &lt;dbl&gt; -0.3393949, -0.3…\n$ Exter_Cond_Poor                                       &lt;dbl&gt; -0.02922903, -0.…\n$ Exter_Cond_Typical                                    &lt;dbl&gt; 0.3890515, 0.389…\n$ Foundation_CBlock                                     &lt;dbl&gt; 1.1411815, 1.141…\n$ Foundation_PConc                                      &lt;dbl&gt; -0.8927767, -0.8…\n$ Foundation_Slab                                       &lt;dbl&gt; -0.1367326, -0.1…\n$ Foundation_Stone                                      &lt;dbl&gt; -0.06868034, -0.…\n$ Foundation_Wood                                       &lt;dbl&gt; -0.02922903, -0.…\n$ Bsmt_Cond_Fair                                        &lt;dbl&gt; -0.1975475, -0.1…\n$ Bsmt_Cond_Good                                        &lt;dbl&gt; 4.9442976, -0.20…\n$ Bsmt_Cond_No_Basement                                 &lt;dbl&gt; -0.1728887, -0.1…\n$ Bsmt_Cond_Poor                                        &lt;dbl&gt; -0.0462448, -0.0…\n$ Bsmt_Cond_Typical                                     &lt;dbl&gt; -2.8539373, 0.35…\n$ Bsmt_Exposure_Gd                                      &lt;dbl&gt; 3.0742999, -0.32…\n$ Bsmt_Exposure_Mn                                      &lt;dbl&gt; -0.3055009, -0.3…\n$ Bsmt_Exposure_No                                      &lt;dbl&gt; -1.3381530, 0.74…\n$ Bsmt_Exposure_No_Basement                             &lt;dbl&gt; -0.1767779, -0.1…\n$ BsmtFin_Type_1_BLQ                                    &lt;dbl&gt; 3.0895747, -0.32…\n$ BsmtFin_Type_1_GLQ                                    &lt;dbl&gt; -0.6481321, -0.6…\n$ BsmtFin_Type_1_LwQ                                    &lt;dbl&gt; -0.2333596, -0.2…\n$ BsmtFin_Type_1_No_Basement                            &lt;dbl&gt; -0.1728887, -0.1…\n$ BsmtFin_Type_1_Rec                                    &lt;dbl&gt; -0.3291359, 3.03…\n$ BsmtFin_Type_1_Unf                                    &lt;dbl&gt; -0.6342108, -0.6…\n$ BsmtFin_Type_2_BLQ                                    &lt;dbl&gt; -0.1491694, -0.1…\n$ BsmtFin_Type_2_GLQ                                    &lt;dbl&gt; -0.1176728, -0.1…\n$ BsmtFin_Type_2_LwQ                                    &lt;dbl&gt; -0.1780576, 5.61…\n$ BsmtFin_Type_2_No_Basement                            &lt;dbl&gt; -0.1741936, -0.1…\n$ BsmtFin_Type_2_Rec                                    &lt;dbl&gt; -0.194022, -0.19…\n$ BsmtFin_Type_2_Unf                                    &lt;dbl&gt; 0.4183756, -2.38…\n$ Heating_GasA                                          &lt;dbl&gt; 0.1249191, 0.124…\n$ Heating_GasW                                          &lt;dbl&gt; -0.09278786, -0.…\n$ Heating_Grav                                          &lt;dbl&gt; -0.05853314, -0.…\n$ Heating_OthW                                          &lt;dbl&gt; -0.02066363, -0.…\n$ Heating_Wall                                          &lt;dbl&gt; -0.05066948, -0.…\n$ Heating_QC_Fair                                       &lt;dbl&gt; 5.2487339, -0.19…\n$ Heating_QC_Good                                       &lt;dbl&gt; -0.4379219, -0.4…\n$ Heating_QC_Poor                                       &lt;dbl&gt; -0.03580575, -0.…\n$ Heating_QC_Typical                                    &lt;dbl&gt; -0.6441494, 1.55…\n$ Central_Air_Y                                         &lt;dbl&gt; 0.265243, 0.2652…\n$ Electrical_FuseF                                      &lt;dbl&gt; -0.1351039, -0.1…\n$ Electrical_FuseP                                      &lt;dbl&gt; -0.05066948, -0.…\n$ Electrical_Mix                                        &lt;dbl&gt; -0.02066363, -0.…\n$ Electrical_SBrkr                                      &lt;dbl&gt; 0.3088293, 0.308…\n$ Electrical_Unknown                                    &lt;dbl&gt; -0.02066363, -0.…\n$ Functional_Maj2                                       &lt;dbl&gt; -0.06209708, -0.…\n$ Functional_Min1                                       &lt;dbl&gt; -0.1506577, -0.1…\n$ Functional_Min2                                       &lt;dbl&gt; -0.1621157, -0.1…\n$ Functional_Mod                                        &lt;dbl&gt; -0.1119485, -0.1…\n$ Functional_Sal                                        &lt;dbl&gt; -0.02922903, -0.…\n$ Functional_Sev                                        &lt;dbl&gt; -0.02922903, -0.…\n$ Functional_Typ                                        &lt;dbl&gt; 0.2814762, 0.281…\n$ Garage_Type_Basment                                   &lt;dbl&gt; -0.1176728, -0.1…\n$ Garage_Type_BuiltIn                                   &lt;dbl&gt; -0.2587311, -0.2…\n$ Garage_Type_CarPort                                   &lt;dbl&gt; -0.07174967, -0.…\n$ Garage_Type_Detchd                                    &lt;dbl&gt; -0.5966212, -0.5…\n$ Garage_Type_More_Than_Two_Types                       &lt;dbl&gt; -0.08549097, -0.…\n$ Garage_Type_No_Garage                                 &lt;dbl&gt; -0.2423742, -0.2…\n$ Garage_Finish_No_Garage                               &lt;dbl&gt; -0.244342, -0.24…\n$ Garage_Finish_RFn                                     &lt;dbl&gt; -0.6229746, -0.6…\n$ Garage_Finish_Unf                                     &lt;dbl&gt; -0.8406517, 1.18…\n$ Garage_Cond_Fair                                      &lt;dbl&gt; -0.1550442, -0.1…\n$ Garage_Cond_Good                                      &lt;dbl&gt; -0.06209708, -0.…\n$ Garage_Cond_No_Garage                                 &lt;dbl&gt; -0.244342, -0.24…\n$ Garage_Cond_Poor                                      &lt;dbl&gt; -0.07469546, -0.…\n$ Garage_Cond_Typical                                   &lt;dbl&gt; 0.3154172, 0.315…\n$ Paved_Drive_Partial_Pavement                          &lt;dbl&gt; 6.9116757, -0.14…\n$ Paved_Drive_Paved                                     &lt;dbl&gt; -3.1286491, 0.31…\n$ Pool_QC_Fair                                          &lt;dbl&gt; -0.02922903, -0.…\n$ Pool_QC_Good                                          &lt;dbl&gt; -0.03580575, -0.…\n$ Pool_QC_No_Pool                                       &lt;dbl&gt; 0.0654701, 0.065…\n$ Pool_QC_Typical                                       &lt;dbl&gt; -0.02066363, -0.…\n$ Fence_Good_Wood                                       &lt;dbl&gt; -0.1975475, -0.1…\n$ Fence_Minimum_Privacy                                 &lt;dbl&gt; -0.3548348, 2.81…\n$ Fence_Minimum_Wood_Wire                               &lt;dbl&gt; -0.06209708, -0.…\n$ Fence_No_Fence                                        &lt;dbl&gt; 0.4902687, -2.03…\n$ Misc_Feature_Gar2                                     &lt;dbl&gt; -0.04135376, -0.…\n$ Misc_Feature_None                                     &lt;dbl&gt; 0.1855737, 0.185…\n$ Misc_Feature_Othr                                     &lt;dbl&gt; -0.03580575, -0.…\n$ Misc_Feature_Shed                                     &lt;dbl&gt; -0.1741936, -0.1…\n$ Misc_Feature_TenC                                     &lt;dbl&gt; -0.02066363, -0.…\n$ Sale_Type_Con                                         &lt;dbl&gt; -0.04135376, -0.…\n$ Sale_Type_ConLD                                       &lt;dbl&gt; -0.09278786, -0.…\n$ Sale_Type_ConLI                                       &lt;dbl&gt; -0.05474101, -0.…\n$ Sale_Type_ConLw                                       &lt;dbl&gt; -0.0462448, -0.0…\n$ Sale_Type_CWD                                         &lt;dbl&gt; -0.06868034, -0.…\n$ Sale_Type_New                                         &lt;dbl&gt; -0.2893471, -0.2…\n$ Sale_Type_Oth                                         &lt;dbl&gt; -0.05066948, -0.…\n$ Sale_Type_VWD                                         &lt;dbl&gt; -0.02066363, -0.…\n$ Sale_Type_WD.                                         &lt;dbl&gt; 0.3890515, 0.389…\n$ Sale_Condition_AdjLand                                &lt;dbl&gt; -0.0654701, -0.0…\n$ Sale_Condition_Alloca                                 &lt;dbl&gt; -0.09735866, -0.…\n$ Sale_Condition_Family                                 &lt;dbl&gt; -0.1266697, -0.1…\n$ Sale_Condition_Normal                                 &lt;dbl&gt; 0.4619311, 0.461…\n$ Sale_Condition_Partial                                &lt;dbl&gt; -0.292798, -0.29…\n\n\nThe code used in this question was written by chatGPT. Use these tools with caution."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html",
    "href": "exams/BSMM_8740_quiz_2_solutions.html",
    "title": "2024-quiz-2",
    "section": "",
    "text": "Log in to your github account and then go to the GitHub organization for the course and find the 2024-quiz-2-[your github username] repository to complete the quiz.\nCreate an R project using your 2024-quiz-2-[your github username] repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the 2024-quiz-2.qmd file in your project.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)\n\n\n\n\n\n\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  ggplot2, magrittr, tidymodels, tidyverse, rsample, broom, recipes, parsnip, tidysmd\n)\n\n# set the efault theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))\n\n\n\n\nQuiz 2 will be released on Wednesday, November 20, and is designed to be completed in 30 minutes.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Simple questions designed to evaluate your familiarity with the written course notes.\nPart 2 - Applied: Data analysis in RStudio (like a usual lab, but simpler).\n\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to that:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor. For example, you may not communicate with other students or post/solicit help on the internet, email, chat, or via any other method of communication. No phones are allowed.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\n\n\n\n\n\nYour answers should be typed in the document below.\nMake sure you save and commit any changes and push the changes to the course repository before the end of the quiz.\nOnce the quiz has ended, the contents of your repository will be pulled for grading. This will happen only once, so no changes made after the end of the quiz will be recorded."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#packages",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#packages",
    "title": "2024-quiz-2",
    "section": "",
    "text": "# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  ggplot2, magrittr, tidymodels, tidyverse, rsample, broom, recipes, parsnip, tidysmd\n)\n\n# set the efault theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#overview",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#overview",
    "title": "2024-quiz-2",
    "section": "",
    "text": "Quiz 2 will be released on Wednesday, November 20, and is designed to be completed in 30 minutes.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Simple questions designed to evaluate your familiarity with the written course notes.\nPart 2 - Applied: Data analysis in RStudio (like a usual lab, but simpler).\n\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#academic-integrity",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#academic-integrity",
    "title": "2024-quiz-2",
    "section": "",
    "text": "By taking this exam, you pledge to that:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#rules-notes",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#rules-notes",
    "title": "2024-quiz-2",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor. For example, you may not communicate with other students or post/solicit help on the internet, email, chat, or via any other method of communication. No phones are allowed.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#submission",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#submission",
    "title": "2024-quiz-2",
    "section": "",
    "text": "Your answers should be typed in the document below.\nMake sure you save and commit any changes and push the changes to the course repository before the end of the quiz.\nOnce the quiz has ended, the contents of your repository will be pulled for grading. This will happen only once, so no changes made after the end of the quiz will be recorded."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#point-each",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#point-each",
    "title": "2024-quiz-2",
    "section": "1 point each:",
    "text": "1 point each:"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#q-1",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#q-1",
    "title": "2024-quiz-2",
    "section": "Q-1",
    "text": "Q-1\nWhat is the key assumption of difference-in-differences (DID) estimation.\n\n\n\n\n\n\nSOLUTION :\n\n\n\nDelete the incorrect answers.\n\nparallel trends"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#q-2",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#q-2",
    "title": "2024-quiz-2",
    "section": "Q-2",
    "text": "Q-2\nWhat is the primary purpose of a randomized controlled trial (RCT)?\n\n\n\n\n\n\nSOLUTION :\n\n\n\nDelete the incorrect answers.\n\nTo establish a causal relationship between two variables"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#q-3",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#q-3",
    "title": "2024-quiz-2",
    "section": "Q-3",
    "text": "Q-3\nControlling for a confounding variable in an analysis always guarantees that the observed association between two variables is causal.\n\n\n\n\n\n\nSOLUTION :\n\n\n\nDelete the incorrect answers.\n\nit’s complicated"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#q-4",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#q-4",
    "title": "2024-quiz-2",
    "section": "Q-4",
    "text": "Q-4\nWhat is the primary purpose of sensitivity analysis in causal inference?\n\n\n\n\n\n\nSOLUTION :\n\n\n\nDelete the incorrect answers.\n\nTo assess the robustness of results to unobserved confounding"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#q-5",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#q-5",
    "title": "2024-quiz-2",
    "section": "Q-5",
    "text": "Q-5\nWhich type of bias occurs when the treatment and control groups differ systematically in their pre-treatment characteristics?\n\n\n\n\n\n\nSOLUTION :\n\n\n\nDelete the incorrect answers.\n\nSelection bias"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#q-6",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#q-6",
    "title": "2024-quiz-2",
    "section": "Q-6",
    "text": "Q-6\nYou are asked to evaluate the effectiveness of a retail marketing campaign. The marketing team has provided the following data:\n\n(X1): a measure of customer engagement on social media\n(X2): a measure of customer satisfaction\nTreatment (D): a new marketing campaign targeting store visits\nOutcome (Y): store sales\n\nThe data below is representative of the marketing data you have:\n\nset.seed(8740)\nn &lt;- 500\ndat &lt;- tibble::tibble(\n  D = rbinom(n, 1, 0.5)\n  , Y = 2.0 * D + rnorm(n)\n  , X1 = 4.0 * Y + rnorm(n)\n  , X2 = 2.0 * Y + 3.0 * X1 + rnorm(n)\n)\n\nCalculate the causal effect of the marketing campaign on sales using one of the methods described in lecture.\n\n\n\n\n\n\nSOLUTION : (1 point)\n\n\n\n\n# please show your work to calculate the causal effect of the marketing campaign on sales.\nlm(Y ~ D, data = dat)\n\n\nCall:\nlm(formula = Y ~ D, data = dat)\n\nCoefficients:\n(Intercept)            D  \n   -0.07093      2.02653  \n\n\nThe causal effect of the marketing campaign on sales is 2.0"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_2_solutions.html#q-7",
    "href": "exams/BSMM_8740_quiz_2_solutions.html#q-7",
    "title": "2024-quiz-2",
    "section": "Q-7",
    "text": "Q-7\nCalculate the Average Treatment Effect (ATE) using inverse probability weighting (IPW) with the following data:\n\nOutcome (Y): student_score\nTreatment (D): tutoring_program\nCovariates/adjustment set (X): previous_grade, study_hours, parent_education\n\n\nset.seed(123)\nn &lt;- 1000\ndat &lt;-tibble::tibble(\n  previous_grade = rnorm(n, mean = 75, sd = 10),\n  study_hours = rpois(n, lambda = 10),\n  parent_education = sample(c(\"HS\", \"College\", \"Graduate\"), n, replace = TRUE),\n  tutoring_program = rbinom(n, 1, 0.4),\n  student_score = 5 * tutoring_program + 0.5 * previous_grade+ 2 * study_hours + rnorm(n, 0, 5)\n)\n\nExecute your code to confirm that it is doing what you expect.\n\n\n\n\n\n\nSOLUTION (7-1): (1 point)\n\n\n\nShow all your work\n\n# create the model predicting the treatment\nps_model &lt;- glm(tutoring_program ~ previous_grade + study_hours + parent_education,\n                family = binomial(), data = dat)\n\n# predict the treatment likelihood and calculate the IPW\ndat_aug &lt;- \n  ps_model |&gt; broom::augment(data = dat, type.predict = \"response\") |&gt; \n  dplyr::mutate(ipw = tutoring_program/.fitted + (1-tutoring_program)/(1-.fitted))\n\n# use weighted regression with the IPW to estimate the causal effect\nlm(student_score ~ tutoring_program, data = dat_aug, weights = ipw)\n\n\nCall:\nlm(formula = student_score ~ tutoring_program, data = dat_aug, \n    weights = ipw)\n\nCoefficients:\n     (Intercept)  tutoring_program  \n          57.397             4.834  \n\n\n\n\nCalculate the standardized mean differences of the adjustment set variables before and after the IP weighting.\n\n\n\n\n\n\nSOLUTION (7-2): (1 point)\n\n\n\n\nplot_df &lt;- tidysmd::tidy_smd(\n  dat_aug,\n  c(previous_grade, study_hours, parent_education),\n  .group = tutoring_program,\n  .wts = ipw\n)\n\nplot_df |&gt; ggplot(\n  aes(x = abs(smd), y = variable,\n    group = method, color = method\n  )\n) + tidysmd::geom_love()\n\n\n\n\n\n\n\n\n\n\nWhat is the remaining analysis step that should be performed to support your ATE estimate?\n\n\n\n\n\n\nSOLUTION (7-3): (1 point)\n\n\n\nWe need to check that the ranges of the IPWs overlap (positivity check)\n\n\n\n\n\n\n\n\nSOLUTION (7-4): (1 point)\n\n\n\n\nggplot(dat_aug, aes(x = ipw, fill = factor(tutoring_program))) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Propensity Score Distribution by Treatment Status\")\n\n\n\n\n\n\n\n\nThe IPWs fail the positivity check."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "BSMM 8740 Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus.\nClass section\n002\n\n\nClass meetings\nWednesdays, 11:30 AM to 2:20 PM - Odette Building 507\n\n\nInstructor\nDr. Lou Odette\n\n\nCourse Website\nhttps://brightspace.uwindsor.ca\n\n\nTextbook\nThe Elements of Statistical Learning - Data Mining, Inference, and Prediction. Second Edition by Hastie, Tibshirani, and Firedman. Available for free at https://web.stanford.edu/~hastie/ElemStatLearn/\n\n\n\n\n\n\n\n\n\n\n\nOffice hours\nThursday 2:00 PM\nOffice\nZoom\n\n\nTelephone\n-\nEmail\nlodette@uwindsor.ca\n\n\nAcademic Director\nDr. Brent Furneaux\nEmail\nbrent.furneaux@uwindsor.ca:\n\n\nProgram Administrator\nTBD\nEmail\n\n\n\nStudent Experience Coordinator\nSamantha DesRosiers\nEmail\nSamantha.Desrosiers@uwindsor.ca\n\n\nCareer Advisor Coordinator\nClementa Stan\nEmail\ncstan@uwindsor.ca\n\n\nGraduate Secretary\nLisa Power\nEmail\nlisa.power@uwindsor.ca",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#recording",
    "href": "course-syllabus.html#recording",
    "title": "BSMM 8740 Syllabus",
    "section": "Recording",
    "text": "Recording\nRecording or reproduction of class sessions in whole or in part in any format including audio, video, or photographic format is not permitted without prior written permission from the course instructor or presenter. In addition, course materials cannot be shared, distributed, emailed, posted online, or otherwise disseminated or communicated in any form to any other person (including fellow classmates) unless written consent has first been obtained from the instructor or presenter. Course materials include but are not limited to slides, instructor notes, assignment instructions, audio and video recordings of course lectures, and audio and video recordings of software demonstrations.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#calendar-description",
    "href": "course-syllabus.html#calendar-description",
    "title": "BSMM 8740 Syllabus",
    "section": "Calendar Description",
    "text": "Calendar Description\nThis course is the exploration of an analytical framework for method selection and model building to help students develop professional capability in data-based techniques of data analytics. A focus will be placed on comparing and selecting appropriate methodology to conduct advanced statistical analysis and on building predictive modeling in order to create a competitive advantage in business operations with efficient analytical methods and data modeling.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-objectives-expected-outcomes",
    "href": "course-syllabus.html#learning-objectives-expected-outcomes",
    "title": "BSMM 8740 Syllabus",
    "section": "Learning Objectives & Expected Outcomes",
    "text": "Learning Objectives & Expected Outcomes\nThe general objectives of this course are to:\n\nDescribe the concepts and issues associated with analytical framework for method selection and model building\nDescribe the assumptions, limitations, and advantages of various statistical techniques for building predictive models\nDevelop an understanding of various data analytics algorithms\nDemonstrate a capacity for interpersonal interactions",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#master-of-management-competencies",
    "href": "course-syllabus.html#master-of-management-competencies",
    "title": "BSMM 8740 Syllabus",
    "section": "Master of Management Competencies",
    "text": "Master of Management Competencies\n\n\n\n\n\n\n\n\nProgram Competencies\nCourse Competencies\nTested by\n\n\n\n\nApply an evidence-based decision model to evaluate and recommend the best available alternative to resolve an international business problem.\nApply an evidence-based decision model to evaluate and recommend the best available alternative to resolve an international business problem.\nLab Assessments\n\n\nAnalyze both qualitative and quantitative data and findings, distinguishing and evaluating their relevance to the resolution of international business issues.\nAnalyze both qualitative and quantitative data and findings, distinguishing and evaluating their relevance to the resolution of international business issues.\nQuizzes, Midterm Examination, and Final Examination",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "BSMM 8740 Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nWhile there is no official textbook for the course, the following textbooks are useful references for the class material, and additional suggested reading for each lecture can be found in the weekly materials.\n\nThe Elements of Statistical Learning Learning - Data Mining, Inference, and Prediction by Hastie, Tibshirani, and Firedman.\nr Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin\nTidy modeling with R by Max Kuhn and Julia Silge\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-content",
    "href": "course-syllabus.html#course-content",
    "title": "BSMM 8740 Syllabus",
    "section": "Course Content",
    "text": "Course Content\n\n\n\nDate\nTopic\n\n\n\n\nSep 11, 2024\nThe Tidyverse, EDA & Git\n\n\nSep 18, 2024\nThe Recipes Package\n\n\nSep 25, 2024\nRegression Methods\n\n\nOct 02, 2024\nThe TidyModels Package\n\n\nOct 09, 2024\nClassification & Clustering Methods\n\n\nOct 23, 2024\nTime Series Methods\n\n\nOct 30, 2024\nCausality: DAGs\n\n\nNov 06, 2024\nCausality: Methods\n\n\nNov 13, 2024\nMonte Carlo Methods\n\n\nNov 20, 2024\nBayesian Methods\n\n\nNov 27, 2024\nAdvanced Topics\n\n\nDec 04, 2024\nFinal Exam",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#key-dates-for-examsassignments1",
    "href": "course-syllabus.html#key-dates-for-examsassignments1",
    "title": "BSMM 8740 Syllabus",
    "section": "Key Dates For Exams/Assignments1",
    "text": "Key Dates For Exams/Assignments1\n\n\n\nDate\nExam/Assignment\n\n\n\n\nOct 09, 2024\nQuiz1\n\n\nNov 06, 2024\nMidterm Examination\n\n\nNov 20, 2024\nQuiz 2\n\n\nDec 04, 2024\nFinal Examination",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "BSMM 8740 Syllabus",
    "section": "Grading",
    "text": "Grading\n\n\n\n\n%\n\n\n\n\nQuizzes\n20\n\n\nLab Assessments\n30\n\n\nMidterm Examination\n25\n\n\nFinal Examination\n25\n\n\nTOTAL\n100",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading-scale-policies",
    "href": "course-syllabus.html#grading-scale-policies",
    "title": "BSMM 8740 Syllabus",
    "section": "Grading Scale Policies",
    "text": "Grading Scale Policies\nAll course work is to be marked and final grades submitted using the 100% scale beginning September 1, 2013. In accordance with the Senate resolution, instructors are to submit whole numbers (e.g., 88, 76, etc.) as percentages. The following University-wide grade descriptors are in effect and will be printed on the back of transcripts:\n\n\n\nLetter Grade\nPercentage Range\n\n\n\n\nA+\n90-100\n\n\nA\n85-89.9\n\n\nA-\n80-84.9\n\n\nB+\n77-79.9\n\n\nB\n73-76.9\n\n\nB-\n70-72.9\n\n\nC+\n67-69.9\n\n\nC\n63-66.9\n\n\nC-\n60-62.9\n\n\nF\n0-59.9",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#examassignment-descriptions",
    "href": "course-syllabus.html#examassignment-descriptions",
    "title": "BSMM 8740 Syllabus",
    "section": "Exam/Assignment Descriptions",
    "text": "Exam/Assignment Descriptions\nThe use of generative artificial intelligence (AI) tools is strictly prohibited in all course assessments unless explicitly indicated otherwise in guidelines provided by the instructor for an assessment. This includes the use of ChatGPT, Google Gemini, Claude, Jenni, Github Co-pilot, DaLL-E, Midjourney, and all other tools that provide artificial intelligence capabilities. When the use of generative AI is permitted this use must be acknowledged and cited following citation instructions given in the assessment guidelines. Use of generative AI outside of assessment guidelines or without required citation will constitute academic misconduct and may be subject to discipline under Bylaw 31: Academic Integrity. It is the student’s responsibility to be clear concerning constraints on the use of generative AI for each assessment and to comply with these constraints.\n\nQuizzes\nThe quizzes can consist of true/false, multiple choice, short answer, and essay questions from all material covered before the date of the quiz. When writing quizzes, you must abide by University of Windsor policies governing plagiarism and academic integrity. Quiz submissions may be subjected to review by automated tools to verify their originality.\n\n\nLab Assessments\nLab assessments will require learners to demonstrate the ability to apply methods and techniques to machine learning problems using the R language and explain the steps they followed to solve a problem. Assessments should be started in person during lab sessions. Deadlines for each lab assessment will be posted.\n\n\nMidterm Examination\nThe midterm exam can consist of true/false, multiple choice, short answer, and essay questions from all material covered before the date of the mid-term exam. When writing this exam, you must abide by University of Windsor policies governing plagiarism and academic integrity. Exam submissions may be subjected to review by automated tools to verify their originality.\nFinal Examination\nThe final exam can consist of true/false, multiple choice, short answer, and essay questions covering all course material, including material discussed during lab sessions. When writing this exam, you must abide by University of Windsor policies governing plagiarism and academic integrity. Exam submissions may be subjected to review by automated tools to verify their originality.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#odette-school-of-business-course-policies",
    "href": "course-syllabus.html#odette-school-of-business-course-policies",
    "title": "BSMM 8740 Syllabus",
    "section": "Odette School Of Business Course Policies",
    "text": "Odette School Of Business Course Policies\nPlease refer to the Odette School of Business Course Policies document for specific information on the following subjects. This Course Policies document is available electronically on each course website, on the Brightspace Master of Management Program page at https://brightspace.uwindsor.ca/d2l/le/content/136263/viewContent/654482/View?ou=136263 and also in paper form outside each Area Secretary’s office on the 4th floor of the Odette building. (Adopted Fall 2009).",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#master-of-management-program-etiquette",
    "href": "course-syllabus.html#master-of-management-program-etiquette",
    "title": "BSMM 8740 Syllabus",
    "section": "Master Of Management Program Etiquette",
    "text": "Master Of Management Program Etiquette\nThe Master of Management program is a culturally inclusive program where it is expected that students, faculty, and staff will recognize, appreciate, and benefit from diversity so as to enhance the learning experience. Promoting a culturally inclusive learning environment encourages individuals to collaborate and develop intercultural respect. The following outlines the protocol for Master of Management students while they are at the University of Windsor:\n\nAll students will communicate in English at all times. It is important for students to continually improve language skills and be inclusive of others from different backgrounds. \nStudents will demonstrate respectful behavior toward their peers and professors, regardless of culture, language, values, beliefs, or ideas.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#secondary-data-use-evaluation-focus-groups-and-interviews",
    "href": "course-syllabus.html#secondary-data-use-evaluation-focus-groups-and-interviews",
    "title": "BSMM 8740 Syllabus",
    "section": "Secondary Data Use, Evaluation, Focus Groups And Interviews",
    "text": "Secondary Data Use, Evaluation, Focus Groups And Interviews\nThis course will be evaluated as part of internal or external quality assurance processes and reporting requirements to funding agencies and as research data for scholarly use. As a student in this course your online student data will be used for evaluating the course delivery and your engagement in the various aspects of the course. This will only occur after final grades have been submitted and approved so it will no effect on your grade. This course data provides information about your individual course usage and activity during the time that you are enrolled in the course. Your anonymized, aggregated data may also be used in the future in reports, articles or presentations.\nDuring the final week of the course you may also be invited to participate in further research about the course. If you decide to participate you may be asked to fill out anonymous online questionnaires that solicit your impressions about the course design and student learning in the course.  The survey participation is voluntary and no questions of a personal nature will be asked. Your participation will have no effect on your grade and your instructor will not know who participated in the surveys.\nFinally, at the end of the survey you may also be asked if you want to participate in a focus group or interviews after final grades have been assigned to gather yours and other student opinions about specific course delivery methods and technologies used.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#commitment-to-student-wellness",
    "href": "course-syllabus.html#commitment-to-student-wellness",
    "title": "BSMM 8740 Syllabus",
    "section": "Commitment To Student Wellness:",
    "text": "Commitment To Student Wellness:\nFeeling Overwhelmed?\nFrom time to time, students face obstacles that can affect academic performance. If you experience difficulties and need help, it is important to reach out to someone.\nFor help addressing mental or physical health concerns on campus, contact (519) 253-3000: \n\nStudent Health Services at ext. 7002 (http://www.uwindsor.ca/studenthealthservices/)\nStudent Counselling Centre at ext. 4616 (http://www.uwindsor.ca/studentcounselling/)\nPeer Support Centre at ext. 4551\n\n24 Hour Support is Available\n\nMy Student Support Program (MySSP) is an immediate and fully confidential 24/7 mental health support that can be accessed for free through chat, online, and telephone. This service is available to all University of Windsor students and offered in over 30 languages. Call: 1-844-451-9700, visit https://keepmesafe.myissp.com/  or download the My SSP app: Apple App Store/Google Play.\n\nA full list of on- and off-campus resources is available at  http://www.uwindsor.ca/wellness.\nShould you need to request alternative accommodation contact your Instructor, Program Administrator, or Director.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#lectures-and-labs",
    "href": "course-syllabus.html#lectures-and-labs",
    "title": "BSMM 8740 Syllabus",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nThe goal of both the lectures and the labs is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to attend all lecture and lab sessions and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature [application exercises] that will be graded. In addition to application exercises will be periodic activities help build a learning community. These will be short, fun activities that will help everyone in the class connect throughout the semester.\nYou are expected to make use of the provided GitHub repository as your central code source platform. Commits to this repository will be used as a metric (one of several) of each team member’s relative contribution for each project.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#five-tips-for-success",
    "href": "course-syllabus.html#five-tips-for-success",
    "title": "BSMM 8740 Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. Your TAs and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TAs, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you’re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework and lab.The earlier you start, the better. It’s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon’t procrastinate. If something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, and eventually you won’t know where to begin asking questions. Don’t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours, and let me help you identify a good (re)starting point.\n\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. On individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 3 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThere is no late work accepted for application exercises, since these are designed to help you prepare for labs and homework.\nThe late work policy for exams will be provided with the exam instructions.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email Dr. Lou Odette and our head TA TODO before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns this semester. All course lectures will be recorded and available to enrolled students after class. If you miss a lecture, make sure to watch the recording and review the material before the next class session. Lab time is dedicated to working on your lab assignments and collaborating with your teammates on your project. If you miss a lab session, make sure to communicate with your team about how you can make up your contribution. Given the technologies we use in the course, this is straightforward to do asynchronously. If you know you’re going to miss a lab session and you’re feeling well enough to do so, notify your teammates ahead of time. Overall these policies are put in place to ensure communication between team members, respect for each others’ time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the university’s top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have symptoms related to COVID-19, have had a known exposure to COVID-19, or have tested positive for COVID-19. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously or watching a recording of the class.\nNote: If you’ve read this far in the syllabus, email me a picture of your pet if you have one or your favourite meme!",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-academic-dates",
    "href": "course-syllabus.html#important-academic-dates",
    "title": "BSMM 8740 Syllabus",
    "section": "Important Academic Dates",
    "text": "Important Academic Dates\n\n\n\nFALL 2024\n\n\n\n\n\nSeptember 2\nLabour Day (University closed)\n\n\nSeptember 5\nall 2024 classes begin\n\n\nSeptember 18\nAdd/Drop Date\n\n\nOctober 3\nFinancial Drop Date\n\n\nOctober 14\nThanksgiving Day (University closed)\n\n\nOctober 12 - 20\nReading Week (no classes)\n\n\nNovember 13\nVoluntary Withdrawal Date\n\n\nDecember 4\nClasses end\n\n\nDecember 7\nFinal Exams Begin\n\n\nDecember 18\nFinal Exams End\n\n\nDecember 19\nAlternate Final Exam Day\n\n\nDecember 23 – January 1\nUniversity closed – Winter holiday",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "BSMM 8740 Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLab assessments will be part of each class session.↩︎",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  }
]