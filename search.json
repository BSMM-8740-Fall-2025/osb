[
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If youâ€™re having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading youâ€™ll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if itâ€™s been resolved. If thereâ€™s a deadline coming up soon, post on the course forum to let us know that thereâ€™s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We donâ€™t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what youâ€™ve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. Theyâ€™ll be able to help diagnose the issue."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Fri, Feb 18\nProposal due Fri, Mar 18\nDraft report due Fri, Apr 8\nPeer review due Fri, Apr 15\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30"
  },
  {
    "objectID": "project-description.html#timeline",
    "href": "project-description.html#timeline",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Fri, Feb 18\nProposal due Fri, Mar 18\nDraft report due Fri, Apr 8\nPeer review due Fri, Apr 15\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30"
  },
  {
    "objectID": "project-description.html#introduction",
    "href": "project-description.html#introduction",
    "title": "Project description",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your groupâ€™s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio, and all components of the project must be reproducible (with the exception of the presentation).\n\nLogistics\nYou will work on the project with your lab groups.\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis\nA GitHub repository corresponding to your report\nSlides + a video presentation\nFormal peer review on another teamâ€™s project"
  },
  {
    "objectID": "project-description.html#topic-ideas",
    "href": "project-description.html#topic-ideas",
    "title": "Project description",
    "section": "Topic ideas",
    "text": "Topic ideas\nIdentify 2-3 data sets youâ€™re interested in potentially using for the final project. If youâ€™re unsure where to find data, you can use the list of potential data sources in the Tips + Resources section as a starting point. It may also help to think of topics youâ€™re interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help you be successful in the project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by the teaching team.\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as â€œnameâ€, â€œsocial security numberâ€, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g.Â â€œstate abbreviationâ€ and â€œstate nameâ€), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\nPlease ask a member of the teaching team if youâ€™re unsure whether your data set meets the criteria.\nFor each data set, include the following:\n\nIntroduction and data\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data)\nDescribe the observations and the general characteristics being measured in the data\n\n\n\nResearch question\n\nDescribe a research question youâ€™re interested in answering using this data.\n\n\n\nGlimpse of data\n\nUse the glimpse function to provide an overview of each data set\n\nSubmit the PDF of the topic ideas to Gradescope. Mark all pages associated with each data set."
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Project description",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is to help you think about your analysis strategy early.\nInclude the following in the proposal:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter youâ€™re investigating\nthe motivation for your research question (citing any relevant literature)\nthe general research question you wish to explore\nyour hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data set you wish to explore. This includes\n\ndescription of the observations in the data set,\ndescription of how the data was originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the response variable.\nVisualization and summary statistics for the response variable.\nList of variables that will be considered as predictors\nRegression model technique (multiple linear regression and logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of your project repo, in the data folder. Link to this file from your proposal writeup.\n\n\nSubmission\nPush all of your final changes to the GitHub repo, and submit the PDF of your proposal to Gradescope.\n\n\nProposal grading\n\n\n\nTotal\n10 pts\n\n\n\n\nIntroduction\n3 pts\n\n\nData description\n2 pts\n\n\nAnalysis plan\n4 pts\n\n\nData dictionary\n1 pts\n\n\n\nEach component will be graded as follows:\n\nMeets expectations (full credit): All required elements are completed and are accurate. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nClose to expectations (half credit): There are some elements missing and/or inaccurate. There are some issues with formatting.\nDoes not meet expectations (no credit): Major elements missing. Work is not neatly formatted and would not be presentable in a professional setting."
  },
  {
    "objectID": "project-description.html#draft-report",
    "href": "project-description.html#draft-report",
    "title": "Project description",
    "section": "Draft report",
    "text": "Draft report\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA wonâ€™t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model youâ€™re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model."
  },
  {
    "objectID": "project-description.html#peer-review",
    "href": "project-description.html#peer-review",
    "title": "Project description",
    "section": "Peer review",
    "text": "Peer review\nCritically reviewing othersâ€™ work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teamsâ€™s projects to review. Each team should push their draft to their GitHub repo by the due date. One lab in the following week will be dedicated to the peer review, and all reviews will be due by the end of that lab session.\nDuring the peer review process, you will be provided read-only access to your partner teamsâ€™ GitHub repos. Provide your review in the form of GitHub issues to your partner teamâ€™s GitHub repo using the issue template provided. The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner teamâ€™s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\n\nPairings\n\nSection 1 - M 1:45PM - 3:00PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\nchaa_chaa_chaa\nyay_stats\nstat_over_flow\n\n\ndekk\nchaa_chaa_chaa\nyay_stats\n\n\neight\ndekk\nchaa_chaa_chaa\n\n\nhousecats\neight\ndekk\n\n\nkrafthouse\nhousecats\neight\n\n\nrrawr\nkrafthouse\nhousecats\n\n\nstat_over_flow\nrrawr\nkrafthouse\n\n\nyay_stats\nstat_over_flow\nrrawr\n\n\n\n\n\nSection 2 - M 3:30PM - 4:45PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\na_plus_plus_plus\nwe_r\ntina\n\n\npredictors\na_plus_plus_plus\nwe_r\n\n\nsixers\npredictors\na_plus_plus_plus\n\n\nsoy_nuggets\nsixers\npredictors\n\n\ntina\nsoy_nuggets\nsixers\n\n\nwe_r\ntina\nsoy_nuggets\n\n\n\n\n\nSection 3 - M 5:15PM - 6:30PM\n\n\n\n\n\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\ndown_to_earth_goats\nthe_three_musketeers\nteam_five\n\n\nginger_and_stats\ndown_to_earth_goats\nthe_three_musketeers\n\n\npineapple_wedge_and_diced_papaya\nginger_and_stats\ndown_to_earth_goats\n\n\nstatchelorettes\npineapple_wedge_and_diced_papaya\nginger_and_stats\n\n\nstatisix\nstatchelorettes\npineapple_wedge_and_diced_papaya\n\n\nstats_squad\nstatisix\nstatchelorettes\n\n\nteam_five\nstats_squad\nstatisix\n\n\nthe_three_musketeers\nteam_five\nstats_squad\n\n\n\n\n\n\nProcess and questions\nSpend ~30 mins to review each teamâ€™s project.\n\nFind your team name on the Reviewer 1 and Reviewer 2 columns.\nFor each of the columns, find the name of the team to review in the Team being reviewed column. You should already have access to this teamâ€™s repo.\nOpen the repo of the team youâ€™re reviewing, read their project draft, and browser around the rest of their repo.\nThen, go to the Issues tab in that repo, click on New issue, and click on Get started for the Peer review issue. Fill out this issue, answering the following questions:\n\nPeer review by: [NAME OF TEAM DOING THE REVIEW]\nNames of team members that participated in this review: [FULL NAMES OF TEAM MEMBERS DOING THE REVIEW]\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nIs there anything that is unclear from the proposal?\nProvide constructive feedback on how the team might be able to improve their project. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?"
  },
  {
    "objectID": "project-description.html#written-report",
    "href": "project-description.html#written-report",
    "title": "Project description",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\nBefore you finalize your write up, make sure the printing of code chunks is off with the option echo = FALSE.\nYou will submit the PDF of your final report on Gradescope.\nThe PDF you submit must match the files in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\nClick here for a PDF of the written report rubric.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA wonâ€™t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model youâ€™re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the modelâ€™s predictive power is thoroughly assessed.\n\n\n\nDiscussion + Conclusion\nIn this section youâ€™ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages."
  },
  {
    "objectID": "project-description.html#video-presentation-slides",
    "href": "project-description.html#video-presentation-slides",
    "title": "Project description",
    "section": "Video presentation + slides",
    "text": "Video presentation + slides\n\nSlides\nIn addition to the written report, your team will also create presentation slides and record a video presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nFor submission, convert these slides to a .pdf document, and submit the PDF of the slides on Gradescope.\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\nVideo presentation\nFor the video presentation, you can speak over your slide deck, similar to the lecture content videos. The video presentation must be no longer than 8 minutes. It is fine if the video is shorter than 8 minutes, but it cannot exceed 8 minutes. You may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire, then embed the video in an new discussion post on Conversations.\n\nTo upload your video to Warpwire:\n\nClick the Warpwire tab in the course Sakai site.\nClick the â€œ+â€ and select â€œUpload filesâ€.\nLocate the video on your computer and click to upload.\nOnce youâ€™ve uploaded the video to Warpwire, click to share the video and copy the videoâ€™s URL. You will need this when you post the video in the discussion forum.\n\n\n\nTo post the video to the discussion forum\n\nClick the Presentations tab in the course Sakai site.\nClick the Presentations topic.\nClick â€œStart a new conversationâ€.\nMake the title â€œYour Team Name: Project Titleâ€. For example, â€œTeaching Team: Our Awesome Presentationâ€.\nClick the Warpwire icon (between the table and shopping cart icons).\nSelect your video, then click â€œInsert 1 item.â€ This will embed your video in the conversation.\nUnder the video, paste the URL to your video.\nYouâ€™re done!"
  },
  {
    "objectID": "project-description.html#presentation-comments",
    "href": "project-description.html#presentation-comments",
    "title": "Project description",
    "section": "Presentation comments",
    "text": "Presentation comments\nEach student will be assigned 2 presentations to watch. Your viewing assignments will be posted later in the semester.\nWatch the groupâ€™s video, then click â€œReplyâ€ to post a question for the group. You may not post a question thatâ€™s already been asked on the discussion thread. Additionally, the question should be (i) substantive (i.e.Â it shouldnâ€™t be â€œWhy did you use a bar plot instead of a pie chartâ€?), (ii) demonstrate your understanding of the content from the course, and (iii) relevant to that groupâ€™s specific presentation, i.e demonstrating that youâ€™ve watched the presentation.\nThis portion of the project will be assessed individually.\n\nPairings\nFind your team name in the first column, watch videos from teams in the second column and leave comments.\n\n\n\n\n\n\n\n\nReviewer\nFirst video to review\nSecond video to review\n\n\n\n\nGinger and Stats\nEight\nWe R\n\n\nKrafthouse\nGinger and Stats\nEight\n\n\nSoy Nuggets\nKrafthouse\nGinger and Stats\n\n\nDown To Earth Goats\nSoy Nuggets\nKrafthouse\n\n\nA+++\nDown To Earth Goats\nSoy Nuggets\n\n\nTeam Five\nA+++\nDown To Earth Goats\n\n\nRrawr\nTeam Five\nA+++\n\n\nHousecats\nRrawr\nTeam Five\n\n\nDekk\nHousecats\nRrawr\n\n\nStat OverFlow\nDekk\nHousecats\n\n\nThe Three Musketeers\nStat OverFlow\nDekk\n\n\nPredictors\nThe Three Musketeers\nStat OverFlow\n\n\nStats Squad\nPredictors\nThe Three Musketeers\n\n\nStatisix\nStats Squad\nPredictors\n\n\nSixers\nStatisix\nStats Squad\n\n\nYay Stats\nSixers\nStatisix\n\n\nTINA\nYay Stats\nSixers\n\n\nStatchelorettes\nTINA\nYay Stats\n\n\nPineapple Wedge and Diced Papaya\nStatchelorettes\nTINA\n\n\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\nStatchelorettes\n\n\nWe R\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\n\n\nEight\nWe R\nChaa Chaa Chaa"
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project description",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/previous-work: Folder that contains the topic-ideas and project-proposal files.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-description.html#peer-teamwork-evaluation",
    "href": "project-description.html#peer-teamwork-evaluation",
    "title": "Project description",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.\nIf you have concerns with the teamwork and/or contribution from any team members, please email me by the project video deadline. You only need to email me if you have concerns. Otherwise, I will assume everyone on the team equally contributed and will receive full credit for the teamwork portion of the grade."
  },
  {
    "objectID": "project-description.html#overall-grading",
    "href": "project-description.html#overall-grading",
    "title": "Project description",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n5 pts\n\n\nProject proposal\n10 pts\n\n\nPeer review\n10 pts\n\n\nWritten report\n40 pts\n\n\nSlides + video presentation\n20 pts\n\n\nReproducibility + organization\n5 pts\n\n\nVideo comments\n5 pts\n\n\nPeer teamwork evaluation\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\nLate work policy\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyoneâ€™s office hours here.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that canâ€™t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Brightspace Discussions as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Brightspace Discussions), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr.Â Lou Odette at lodette@uwindsor.ca.\nIf there is a question thatâ€™s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include â€œBSMM-8740â€ in the subject line. Barring extenuating circumstances, I will respond to BSMM-8740 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at UWindsor, and the university offers resources to support students in managing daily stress and self-care. Uwindsor offers resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nMySSP: 1-844-451-9700, MySSP\noff-campus resources\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.1 â”€â”€\n\n\nâœ“ ggplot2 3.3.5     âœ“ purrr   0.3.4\nâœ“ tibble  3.1.6     âœ“ dplyr   1.0.7\nâœ“ tidyr   1.1.4     âœ“ stringr 1.4.0\nâœ“ readr   2.1.1     âœ“ forcats 0.5.1\n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 0.1.4 â”€â”€\n\n\nâœ“ broom        0.7.10         âœ“ rsample      0.1.1     \nâœ“ dials        0.0.10         âœ“ tune         0.1.6     \nâœ“ infer        1.0.1.9000     âœ“ workflows    0.2.4     \nâœ“ modeldata    0.1.1          âœ“ workflowsets 0.1.0     \nâœ“ parsnip      0.1.7          âœ“ yardstick    0.0.9     \nâœ“ recipes      0.2.0          \n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\nâ€¢ Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 Ã— 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 Ã— 9\n   .rownames body_mass_g flipper_length_â€¦ .fitted  .resid    .hat .sigma .cooksd\n   &lt;chr&gt;           &lt;int&gt;            &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# â€¦ with 332 more rows, and 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaikeâ€™s Information Criterion (AIC) and Schwarzâ€™s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review.",
    "crumbs": [
      "Supplemental notes",
      "Model selection criteria"
    ]
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of ğ›ƒ\\boldsymbol{\\beta} and Ïƒ\\sigma",
    "text": "Maximum Likelihood Estimation of ğ›ƒ\\boldsymbol{\\beta} and Ïƒ\\sigma\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet ğ˜\\mathbf{Y} be nÃ—1n \\times 1 matrix of responses, ğ—\\mathbf{X}, the nÃ—(p+1)n \\times (p+1) matrix of predictors, and ğ›ƒ\\boldsymbol{\\beta}, (p+1)Ã—1(p+1) \\times 1 matrix of coefficients. If the multiple linear regression model is correct then,\nğ˜âˆ¼N(ğ—ğ›ƒ,Ïƒ2)(1)\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n \\qquad(1)\nWhen we do linear regression, our goal is to estimate the unknown parameters ğ›ƒ\\boldsymbol{\\beta} and Ïƒ2\\sigma^2 from EquationÂ 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating ğ›ƒ\\boldsymbol{\\beta} and Ïƒ2\\sigma^2 is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using EquationÂ 1, we will write the likelihood function for linear regression as\nL(ğ—,ğ˜|ğ›ƒ,Ïƒ2)=âˆi=1n(2Ï€Ïƒ2)âˆ’12exp{âˆ’12Ïƒ2âˆ‘i=1n(Yiâˆ’ğ—iğ›ƒ)T(Yiâˆ’ğ—iğ›ƒ)}(2)\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n \\qquad(2)\nwhere YiY_i is the ithi^{th} response and ğ—i\\mathbf{X}_i is the vector of predictors for the ithi^{th} observation. One approach estimating ğ›ƒ\\boldsymbol{\\beta} and Ïƒ2\\sigma^2 is to find the values of those parameters that maximize the likelihood in EquationÂ 2, i.e.Â maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e.Â the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\nlogL(ğ—,ğ˜|ğ›ƒ,Ïƒ2)=âˆ‘i=1nâˆ’12log(2Ï€Ïƒ2)âˆ’12Ïƒ2âˆ‘i=1n(Yiâˆ’ğ—iğ›ƒ)T(Yiâˆ’ğ—iğ›ƒ)=âˆ’n2log(2Ï€Ïƒ2)âˆ’12Ïƒ2(ğ˜âˆ’ğ—ğ›ƒ)T(ğ˜âˆ’ğ—ğ›ƒ)(3)\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n \\qquad(3)\n\nThe maximum likelihood estimate of ğ›ƒ\\boldsymbol{\\beta} and Ïƒ2\\sigma^2 are ğ›ƒÌ‚=(ğ—Tğ—)âˆ’1ğ—Tğ˜ÏƒÌ‚2=1n(ğ˜âˆ’ğ—ğ›ƒ)T(ğ˜âˆ’ğ—ğ›ƒ)=1nRSS(4)\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{1cm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n \\qquad(4)\nwhere RSSRSS is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of Ïƒ2\\sigma^2 we typically use RSSnâˆ’pâˆ’1\\frac{RSS}{n-p-1}. This is because the maximum likelihood estimate of Ïƒ2\\sigma^2 in EquationÂ 4 is a biased estimator of Ïƒ2\\sigma^2. When nn is much larger than the number of predictors pp, then the differences in these two estimates are trivial.",
    "crumbs": [
      "Supplemental notes",
      "Model selection criteria"
    ]
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaikeâ€™s Information Criterion (AIC) is is a model selection criterion developed by Hirotugu Akaike that aims to estimate the relative quality of different models while penalizing for model complexity. Here is the original paper on AIC concept by Akaike â€“ A New Look at the Statistical Modeling Identification. The purpose of AIC is to find a model that maximizes the likelihood of the data while taking into account the number of parameters used. The formula for AIC is as follows:\nAIC=âˆ’2logL+2(p+1)(5)\nAIC = -2 \\log L + 2(p+1)\n \\qquad(5)\nwhere logL\\log L is the log-likelihood which measures how well the model fits the data. The term p+1p+1 represents the number of parameters in the model, including the intercept and any additional predictors. This is the general form of AIC that can be applied to a variety of models, but for now, letâ€™s focus on AIC for mutliple linear regression.\nAIC=âˆ’2logL+2(p+1)=âˆ’2[âˆ’n2log(2Ï€Ïƒ2)âˆ’12Ïƒ2(ğ˜âˆ’ğ—ğ›ƒ)T(ğ˜âˆ’ğ—ğ›ƒ)]+2(p+1)=nlog(2Ï€RSSn)+1RSS/nRSS=nlog(2Ï€)+nlog(RSS)âˆ’nlog(n)+2(p+1)(6)\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n \\qquad(6)",
    "crumbs": [
      "Supplemental notes",
      "Model selection criteria"
    ]
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\nSimilar to AIC, the Bayesian Information Criterion (BIC) is another model selection criterion that considers both model fit and complexity. BIC is based on Bayesian principles and provides a more stronger penalty for model complexity compared to AIC. Gideon Schwarzâ€™s foundational paper on BIC is titled â€œEstimating the Dimension of a Modelâ€ and was published in 1978. The formula for BIC is as follows:\nBIC=âˆ’2logL+(p+1)logn(7)\nBIC = -2 \\log L + (p+1) \\log n\n \\qquad(7)\nIn the formula, the terms logL\\log L and p+1p+1 have the same meaning as in AIC. Additionally, the term logn\\log n represents the logarithm of the sample size (nn). The logn\\log n term in BIC introduces a stronger penalty for model complexity compared to AIC, as the penalty term scales with the sample size.\nThe main difference between AIC and BIC lies in the penalty term for model complexity. While AIC penalizes complexity to some extent with the term 2(p+1)2 (p+1), BICâ€™s penalty increases logarithmically with the sample size, resulting in a more pronounced penalty. Therefore, BIC tends to favor simpler models compared to AIC, promoting a more parsimonious approach to model selection.",
    "crumbs": [
      "Supplemental notes",
      "Model selection criteria"
    ]
  },
  {
    "objectID": "supplemental/exam_questions.html",
    "href": "supplemental/exam_questions.html",
    "title": "Exam-like questions",
    "section": "",
    "text": "Each quiz, midterm and the final exam will consist of two parts:\n\nPart 1 - Conceptual: Simple questions designed to evaluate your familiarity with the written course notes.\nPart 2 - Applied: Data analysis in RStudio (like a usual lab, but simpler).",
    "crumbs": [
      "Supplemental notes",
      "Sample exam questions"
    ]
  },
  {
    "objectID": "supplemental/exam_questions.html#notes-on-quizes-midterms-and-the-final-exam",
    "href": "supplemental/exam_questions.html#notes-on-quizes-midterms-and-the-final-exam",
    "title": "Exam-like questions",
    "section": "",
    "text": "Each quiz, midterm and the final exam will consist of two parts:\n\nPart 1 - Conceptual: Simple questions designed to evaluate your familiarity with the written course notes.\nPart 2 - Applied: Data analysis in RStudio (like a usual lab, but simpler).",
    "crumbs": [
      "Supplemental notes",
      "Sample exam questions"
    ]
  },
  {
    "objectID": "supplemental/exam_questions.html#conceptual-questions",
    "href": "supplemental/exam_questions.html#conceptual-questions",
    "title": "Exam-like questions",
    "section": "Conceptual Questions",
    "text": "Conceptual Questions\nHere are examples of the conceptual questions that might be asked:\nWhat is the primary difference between inner_join() and left_join()? a) inner_join() keeps all rows from both tables, left_join() keeps all rows from the left table b) inner_join() keeps only matching rows, left_join() keeps all rows from the left table c) inner_join() is faster, left_join() is more accurate d) There is no difference\nIn the context of tidy data, which of the following is NOT a principle? a) Every column is a measurement b) Every row is an observation c) Every cell is a single value d) Every dataset has multiple tables\nWhat does the step_dummy() function in recipes do? a) Remove duplicate rows b) Create dummy variables for categorical predictors c) Impute missing values d) Scale numeric variables\nWhich of the following is NOT a step in the typical Tidymodels workflow? a) Data splitting b) Model specification c) Model training d) Database querying",
    "crumbs": [
      "Supplemental notes",
      "Sample exam questions"
    ]
  },
  {
    "objectID": "supplemental/exam_questions.html#applied-questions",
    "href": "supplemental/exam_questions.html#applied-questions",
    "title": "Exam-like questions",
    "section": "Applied Questions",
    "text": "Applied Questions\nHere are examples of the applied questions that might be asked:\nUsing the ggplot2::diamonds dataset. Using tidyverse functions: a) Create a new variable called â€˜sizeâ€™ that is the product of x, y, and z. b) Filter out any diamonds with a price greater than $10,000 or less than $500. c) Fit a multiple regression model predicting price based on carat, cut, and size. d) Use broom::augment() to add residuals and fitted values to your dataset.\nUsing the datasets::mtcars dataset, create a linear regression model to predict mpg based on wt and hp. Use the tidyverse and broom packages to: a) Create a tibble with only the mpg, wt, and hp columns. b) Fit the linear model. c) Extract the model coefficients and their p-values into a tidy tibble.",
    "crumbs": [
      "Supplemental notes",
      "Sample exam questions"
    ]
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (Î²1\\beta_1) and intercept (Î²0\\beta_0). We obtain the estimates, Î²Ì‚1\\hat{\\beta}_1 and Î²Ì‚0\\hat{\\beta}_0 by finding the values that minimize the sum of squared residuals, as shown in EquationÂ 1.\nSSR=âˆ‘i=1n[yiâˆ’yÌ‚i]2=[yiâˆ’(Î²Ì‚0+Î²Ì‚1xi)]2=[yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi]2(1)\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n \\qquad(1)\nRecall that we can find the values of Î²Ì‚1\\hat{\\beta}_1 and Î²Ì‚0\\hat{\\beta}_0 that minimize /eq-ssr by taking the partial derivatives of EquationÂ 1 and setting them to 0. Thus, the values of Î²Ì‚1\\hat{\\beta}_1 and Î²Ì‚0\\hat{\\beta}_0 that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in EquationÂ 2.\nâˆ‚SSRâˆ‚Î²Ì‚1=âˆ’2âˆ‘i=1nxi(yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi)âˆ‚SSRâˆ‚Î²Ì‚0=âˆ’2âˆ‘i=1n(yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi)(2)\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n \\qquad(2)\nThe derivation of deriving Î²Ì‚0\\hat{\\beta}_0 is shown in EquationÂ 3.\nâˆ‚SSRâˆ‚Î²Ì‚0=âˆ’2âˆ‘i=1n(yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi)=0â‡’âˆ’âˆ‘i=1n(yi+Î²Ì‚0+Î²Ì‚1xi)=0â‡’âˆ’âˆ‘i=1nyi+nÎ²Ì‚0+Î²Ì‚1âˆ‘i=1nxi=0â‡’nÎ²Ì‚0=âˆ‘i=1nyiâˆ’Î²Ì‚1âˆ‘i=1nxiâ‡’Î²Ì‚0=1n(âˆ‘i=1nyiâˆ’Î²Ì‚1âˆ‘i=1nxi)â‡’Î²Ì‚0=yâ€¾âˆ’Î²Ì‚1xâ€¾(3)\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n \\qquad(3)\nThe derivation of Î²Ì‚1\\hat{\\beta}_1 using the Î²Ì‚0\\hat{\\beta}_0 we just derived is shown in EquationÂ 4.\nâˆ‚SSRâˆ‚Î²Ì‚1=âˆ’2âˆ‘i=1nxi(yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi)=0â‡’âˆ’âˆ‘i=1nxiyi+Î²Ì‚0âˆ‘i=1nxi+Î²Ì‚1âˆ‘i=1nxi2=0(Fill in Î²Ì‚0)â‡’âˆ’âˆ‘i=1nxiyi+(yâ€¾âˆ’Î²Ì‚1xâ€¾)âˆ‘i=1nxi+Î²Ì‚1âˆ‘i=1nxi2=0â‡’(yâ€¾âˆ’Î²Ì‚1xâ€¾)âˆ‘i=1nxi+Î²Ì‚1âˆ‘i=1nxi2=âˆ‘i=1nxiyiâ‡’yâ€¾âˆ‘i=1nxiâˆ’Î²Ì‚1xâ€¾âˆ‘i=1nxi+Î²Ì‚1âˆ‘i=1nxi2=âˆ‘i=1nxiyiâ‡’nyâ€¾xâ€¾âˆ’Î²Ì‚1nxâ€¾2+Î²Ì‚1âˆ‘i=1nxi2=âˆ‘i=1nxiyiâ‡’Î²Ì‚1âˆ‘i=1nxi2âˆ’Î²Ì‚1nxâ€¾2=âˆ‘i=1nxiyiâˆ’nyâ€¾xâ€¾â‡’Î²Ì‚1(âˆ‘i=1nxi2âˆ’nxâ€¾2)=âˆ‘i=1nxiyiâˆ’nyâ€¾xâ€¾Î²Ì‚1=âˆ‘i=1nxiyiâˆ’nyâ€¾xâ€¾âˆ‘i=1nxi2âˆ’nxâ€¾2(4)\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n \\qquad(4)\nTo write Î²Ì‚1\\hat{\\beta}_1 in a form thatâ€™s more recognizable, we will use the following:\nâˆ‘xiyiâˆ’nyâ€¾xâ€¾=âˆ‘(xâˆ’xâ€¾)(yâˆ’yâ€¾)=(nâˆ’1)Cov(x,y)(5)\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n \\qquad(5)\nâˆ‘xi2âˆ’nxâ€¾2âˆ’âˆ‘(xâˆ’xâ€¾)2=(nâˆ’1)sx2(6)\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n \\qquad(6)\nwhere Cov(x,y)\\text{Cov}(x,y) is the covariance of xx and yy, and sx2s_x^2 is the sample variance of xx (sxs_x is the sample standard deviation).\nThus, applying EquationÂ 5 and EquationÂ 6, we have\nÎ²Ì‚1=âˆ‘i=1nxiyiâˆ’nyâ€¾xâ€¾âˆ‘i=1nxi2âˆ’nxâ€¾2=âˆ‘i=1n(xâˆ’xâ€¾)(yâˆ’yâ€¾)âˆ‘i=1n(xâˆ’xâ€¾)2=(nâˆ’1)Cov(x,y)(nâˆ’1)sx2=Cov(x,y)sx2(7)\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n \\qquad(7)\nThe correlation between xx and yy is r=Cov(x,y)sxsyr = \\frac{\\text{Cov}(x,y)}{s_x s_y}. Thus, Cov(x,y)=rsxsy\\text{Cov}(x,y) = r s_xs_y. Plugging this into EquationÂ 7, we have\nÎ²Ì‚1=Cov(x,y)sx2=rsysxsx2=rsysx(8)\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n \\qquad(8)",
    "crumbs": [
      "Supplemental notes",
      "SLR derivations"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cookâ€™s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review.",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have nn observations. Let the ithi^{th} be (xi1,â€¦,xip,yi)(x_{i1}, \\ldots, x_{ip}, y_i), such that xi1,â€¦,xipx_{i1}, \\ldots, x_{ip} are the explanatory variables (predictors) and yiy_i is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in EquationÂ 1.\ny=Î²0+Î²1x1+â€¦+Î²pxp(1)\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \n \\qquad(1)\nWe can write the response for the ithi^{th} observation as shown in EquationÂ 2.\nyi=Î²0+Î²1xi1+â€¦+Î²pxip+Ïµi(2)\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i \n \\qquad(2)\nsuch that Ïµi\\epsilon_i is the amount yiy_i deviates from Î¼{y|xi1,â€¦,xip}\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}, the mean response for a given combination of explanatory variables. We assume each Ïµiâˆ¼N(0,Ïƒ2)\\epsilon_i \\sim N(0,\\sigma^2), where Ïƒ2\\sigma^2 is a constant variance for the distribution of the response yy for any combination of explanatory variables x1,â€¦,xpx_1, \\ldots, x_p.",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the EquationÂ 1 and EquationÂ 2 using matrix notation. Let\nğ˜=[y1y2â‹®yn]ğ—=[x11x12â€¦x1px21x22â€¦x2pâ‹®â‹®â‹±â‹®xn1xn2â€¦xnp]ğ›ƒ=[Î²0Î²1â‹®Î²p]ğ›œ=[Ïµ1Ïµ2â‹®Ïµn](3)\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} \n\\hspace{1cm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{1cm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} \n\\hspace{1cm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n \\qquad(3)\nThus,\nğ˜=ğ—ğ›ƒ+ğ›œ\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\nğ˜Ì‚=ğ—ğ›ƒÌ‚ğ=ğ˜âˆ’ğ—ğ›ƒÌ‚(4)\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{1cm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n \\qquad(4)",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that ğ›ƒÌ‚\\hat{\\boldsymbol{\\beta}} can be written as the following:\nğ›ƒÌ‚=(ğ—Tğ—)âˆ’1ğ—Tğ˜(5)\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n \\qquad(5)\nCombining EquationÂ 4 and EquationÂ 5, we can write ğ˜Ì‚\\hat{\\mathbf{Y}} as the following:\nğ˜Ì‚=ğ—ğ›ƒÌ‚=ğ—(ğ—Tğ—)âˆ’1ğ—Tğ˜(6)\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n \\qquad(6)\nWe define the hat matrix as an nÃ—nn \\times n matrix of the form ğ‡=ğ—(ğ—Tğ—)âˆ’1ğ—T\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T. Thus EquationÂ 6 becomes\nğ˜Ì‚=ğ‡ğ˜(7)\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n \\qquad(7)\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, hiih_{ii} is a measure of how far the values of the predictor variables for the ithi^{th} observation, xi1,xi2,â€¦,xipx_{i1}, x_{i2}, \\ldots, x_{ip}, are from the mean values of the predictor variables, xâ€¾1,xâ€¾2,â€¦,xâ€¾p\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p. In the case of simple linear regression, the ithi^{th} diagonal, hiih_{ii}, can be written as\nhii=1n+(xiâˆ’xâ€¾)2âˆ‘j=1n(xjâˆ’xâ€¾)2\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n0â‰¤hiiâ‰¤10 \\leq h_ii \\leq 1\nâˆ‘i=1nhii=p+1\\sum\\limits_{i=1}^{n} h_{ii} = p+1, where pp is the number of predictor variables in the model.\nThe mean hat value is hâ€¾=âˆ‘i=1nhiin=p+1n\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}.\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than 2(p+1)n\\frac{2(p+1)}{n} are considered to be high leverage points, i.e.Â outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from EquationÂ 4 using EquationÂ 7.\nğ=ğ˜âˆ’ğ˜Ì‚=ğ˜âˆ’ğ‡ğ˜=(1âˆ’ğ‡)ğ˜(8)\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n \\qquad(8)\nNote that the identity matrix and hat matrix are idempotent, i.e.Â ğˆğˆ=ğˆ\\mathbf{I}\\mathbf{I} = \\mathbf{I}, ğ‡ğ‡=ğ‡\\mathbf{H}\\mathbf{H} = \\mathbf{H}. Thus, (ğˆâˆ’ğ‡)(\\mathbf{I} - \\mathbf{H}) is also idempotent. These matrices are also symmetric. Using these properties and EquationÂ 8, we have that the variance-covariance matrix of the residuals ğ\\boldsymbol{e}, is\nVar(ğ)=ğğT=(1âˆ’ğ‡)Var(ğ˜)T(1âˆ’ğ‡)T=(1âˆ’ğ‡)ÏƒÌ‚2(1âˆ’ğ‡)T=ÏƒÌ‚2(1âˆ’ğ‡)(1âˆ’ğ‡)=ÏƒÌ‚2(1âˆ’ğ‡)(9)\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n \\qquad(9)\nwhere ÏƒÌ‚2=âˆ‘i=1nei2nâˆ’pâˆ’1\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1} is the estimated regression variance. Thus, the variance of the ithi^{th} residual is Var(ei)=ÏƒÌ‚2(1âˆ’hii)Var(e_i) = \\hat{\\sigma}^2(1-h_{ii}). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage.",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the ithi^{th} standardized residual takes the form\nstd.resi=eiâˆ’E(ei)SE(ei)\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\nThe expected value of the residuals is 0, i.e.Â E(ei)=0E(e_i) = 0. From EquationÂ 9), the standard error of the residual is SE(ei)=ÏƒÌ‚1âˆ’hiiSE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}. Therefore,\nstd.resi=eiÏƒÌ‚1âˆ’hii(10)\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n \\qquad(10)",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cookâ€™s Distance",
    "text": "Cookâ€™s Distance\nCookâ€™s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cookâ€™s distance for the ithi^{th} observation can be written as\nDi=(ğ˜Ì‚âˆ’ğ˜Ì‚(i))T(ğ˜Ì‚âˆ’ğ˜Ì‚(i))(p+1)ÏƒÌ‚(11)\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n \\qquad(11)\nwhere ğ˜Ì‚(i)\\hat{\\mathbf{Y}}_{(i)} is the vector of predicted values from the model fitted when the ithi^{th} observation is deleted. Cookâ€™s Distance can be calculated without deleting observations one at a time, since EquationÂ 12 below is mathematically equivalent to EquationÂ 11.\nDi=1p+1std.resi2[hii(1âˆ’hii)]=ei2(p+1)ÏƒÌ‚2(1âˆ’hii)[hii(1âˆ’hii)](12)\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n \\qquad(12)",
    "crumbs": [
      "Supplemental notes",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra.",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have nn observations. Let the ithi^{th} be (xi1,â€¦,xip,yi)(x_{i1}, \\ldots, x_{ip}, y_i), such that xi1,â€¦,xipx_{i1}, \\ldots, x_{ip} are the explanatory variables (predictors) and yiy_i is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in EquationÂ 1.\ny=Î²0+Î²1x1+â€¦+Î²pxp(1)\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \n \\qquad(1)\nWe can write the response for the ithi^{th} observation as shown in EquationÂ 2\nyi=Î²0+Î²1xi1+â€¦+Î²pxip+Ïµi(2)\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i \n \\qquad(2)\nsuch that Ïµi\\epsilon_i is the amount yiy_i deviates from Î¼{y|xi1,â€¦,xip}\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}, the mean response for a given combination of explanatory variables. We assume each Ïµiâˆ¼N(0,Ïƒ2)\\epsilon_i \\sim N(0,\\sigma^2), where Ïƒ2\\sigma^2 is a constant variance for the distribution of the response yy for any combination of explanatory variables x1,â€¦,xpx_1, \\ldots, x_p.",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the EquationÂ 1 and EquationÂ 2 using matrix notation. Let\nğ˜=[y1y2â‹®yn]ğ—=[x11x12â€¦x1px21x22â€¦x2pâ‹®â‹®â‹±â‹®xn1xn2â€¦xnp]ğ›ƒ=[Î²0Î²1â‹®Î²p]ğ›œ=[Ïµ1Ïµ2â‹®Ïµn](3)\n\\begin{align*}\\mathbf{Y}=\\left[\\begin{matrix}y_{1}\\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{matrix}\\right]\\hspace{1cm}\\mathbf{X}=\\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p}\\\\\nx_{21} & x_{22} & \\dots & x_{2p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np}\n\\end{bmatrix}\\hspace{1cm}\\boldsymbol{\\beta}=\\begin{bmatrix}\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{p}\n\\end{bmatrix}\\hspace{1cm}\\boldsymbol{\\epsilon}=\\begin{bmatrix}\\epsilon_{1}\\\\\n\\epsilon_{2}\\\\\n\\vdots\\\\\n\\epsilon_{n}\n\\end{bmatrix}\\end{align*}\n \\qquad(3)\nThus,\nğ˜=ğ—ğ›ƒ+ğ›œ\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\nğ˜Ì‚=ğ—ğ›ƒÌ‚ğ=ğ˜âˆ’ğ—ğ›ƒÌ‚(4)\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{1cm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n \\qquad(4)",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, ğ›ƒÌ‚\\hat{\\boldsymbol{\\beta}} that minimizes\nâˆ‘i=1nei2=ğTğ=(ğ˜âˆ’ğ—ğ›ƒÌ‚)T(ğ˜âˆ’ğ—ğ›ƒÌ‚)(5)\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n \\qquad(5)\nwhere ğT\\mathbf{e}^T, the transpose of the matrix ğ\\mathbf{e}.\n(ğ˜âˆ’ğ—ğ›ƒÌ‚)T(ğ˜âˆ’ğ—ğ›ƒÌ‚)=(ğ˜Tğ˜âˆ’ğ˜Tğ—ğ›ƒÌ‚âˆ’(ğ›ƒÌ‚Tğ—Tğ˜+ğ›ƒÌ‚Tğ—Tğ—ğ›ƒÌ‚)(6)\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} - \n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n \\qquad(6)\nNote that (ğ˜ğ“ğ—ğ›ƒÌ‚)T=ğ›ƒÌ‚Tğ—Tğ˜(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}. Since these are both constants (i.e.Â 1Ã—11\\times 1 vectors), ğ˜ğ“ğ—ğ›ƒÌ‚=ğ›ƒÌ‚Tğ—Tğ˜\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}. Thus, EquationÂ 6 becomes\nğ˜Tğ˜âˆ’2ğ—Tğ›ƒÌ‚Tğ˜+ğ›ƒÌ‚Tğ—Tğ—ğ›ƒÌ‚\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\nSince we want to find the ğ›ƒÌ‚\\hat{\\boldsymbol{\\beta}} that minimizes EquationÂ 5, will find the value of ğ›ƒÌ‚\\hat{\\boldsymbol{\\beta}} such that the derivative with respect to ğ›ƒÌ‚\\hat{\\boldsymbol{\\beta}} is equal to 0.\nâˆ‚ğTğâˆ‚ğ›ƒÌ‚=âˆ‚âˆ‚ğ›ƒÌ‚(ğ˜Tğ˜âˆ’2ğ—Tğ›ƒÌ‚Tğ˜+ğ›ƒÌ‚Tğ—Tğ—ğ›ƒÌ‚)=0â‡’âˆ’2ğ—Tğ˜+2ğ—Tğ—ğ›ƒÌ‚=0â‡’2ğ—Tğ˜=2ğ—Tğ—ğ›ƒÌ‚â‡’ğ—Tğ˜=ğ—Tğ—ğ›ƒÌ‚â‡’(ğ—Tğ—)âˆ’1ğ—Tğ˜=(ğ—Tğ—)âˆ’1ğ—Tğ—ğ›ƒÌ‚â‡’(ğ—Tğ—)âˆ’1ğ—Tğ˜=ğˆğ›ƒÌ‚(7)\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n \\qquad(7)\nThus, the estimate of the model coefficients is ğ›ƒÌ‚=(ğ—Tğ—)âˆ’1ğ—Tğ˜\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}.",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\nE[ğ›œğ›œT]=Ïƒ2IE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\nğ›ƒÌ‚=ğ›ƒ+(ğ—Tğ—)âˆ’1Ïµ\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\n\nFirst, we will show that E[ğ›œğ›œT]=Ïƒ2IE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\nE[ğ›œğ›œT]=E[Ïµ1Ïµ2â€¦Ïµn][Ïµ1Ïµ2â‹®Ïµn]=E[Ïµ12Ïµ1Ïµ2â€¦Ïµ1ÏµnÏµ2Ïµ1Ïµ22â€¦Ïµ2Ïµnâ‹®â‹®â‹±â‹®ÏµnÏµ1ÏµnÏµ2â€¦Ïµn2]=[E[Ïµ12]E[Ïµ1Ïµ2]â€¦E[Ïµ1Ïµn]E[Ïµ2Ïµ1]E[Ïµ22]â€¦E[Ïµ2Ïµn]â‹®â‹®â‹±â‹®E[ÏµnÏµ1]E[ÏµnÏµ2]â€¦E[Ïµn2]](8)\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\ \n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2 \n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n \\qquad(8)\nRecall, the regression assumption that the errors Ïµiâ€²s\\epsilon_i's are Normally distributed with mean 0 and variance Ïƒ2\\sigma^2. Thus, E(Ïµi2)=Var(Ïµi)=Ïƒ2E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2 for all ii. Additionally, recall the regression assumption that the errors are uncorrelated, i.e.Â E(ÏµiÏµj)=Cov(Ïµi,Ïµj)=0E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0 for all i,ji,j. Using these assumptions, we can write EquationÂ 8 as\nE[ğ›œğ›œT]=[Ïƒ20â€¦00Ïƒ2â€¦0â‹®â‹®â‹±â‹®00â€¦Ïƒ2]=Ïƒ2ğˆ(9)\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n \\qquad(9)\nwhere ğˆ\\mathbf{I} is the nÃ—nn \\times n identity matrix.\nNext, we show that ğ›ƒÌ‚=ğ›ƒ+(ğ—Tğ—)âˆ’1Ïµ\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon.\nRecall that the ğ›ƒÌ‚=(ğ—Tğ—)âˆ’1ğ—Tğ˜\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} and ğ˜=ğ—ğ›ƒ+ğ›œ\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}. Then,\nğ›ƒÌ‚=(ğ—Tğ—)âˆ’1ğ—Tğ˜=(ğ—Tğ—)âˆ’1ğ—T(ğ—ğ›ƒ+ğ›œ)=ğ›ƒ+(ğ—Tğ—)âˆ’1ğ—Tğ›œ(10)\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n \\qquad(10)\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is E[(ğ›ƒÌ‚âˆ’ğ›ƒ)(ğ›ƒÌ‚âˆ’ğ›ƒ)T]E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\nE[(ğ›ƒÌ‚âˆ’ğ›ƒ)(ğ›ƒÌ‚âˆ’ğ›ƒ)T]=E[(ğ›ƒ+(ğ—Tğ—)âˆ’1ğ—Tğ›œâˆ’ğ›ƒ)(ğ›ƒ+(ğ—Tğ—)âˆ’1ğ—Tğ›œâˆ’ğ›ƒ)T]=E[(ğ—Tğ—)âˆ’1ğ—Tğ›œğ›œTğ—(ğ—Tğ—)âˆ’1]=(ğ—Tğ—)âˆ’1ğ—TE[ğ›œğ›œT]ğ—(ğ—Tğ—)âˆ’1=(ğ—Tğ—)âˆ’1ğ—T(Ïƒ2ğˆ)ğ—(ğ—Tğ—)âˆ’1=Ïƒ2ğˆ(ğ—Tğ—)âˆ’1ğ—Tğ—(ğ—Tğ—)âˆ’1=Ïƒ2ğˆ(ğ—Tğ—)âˆ’1=Ïƒ2(ğ—Tğ—)âˆ’1(11)\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n \\qquad(11)",
    "crumbs": [
      "Supplemental notes",
      "MLR matrix notation"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html",
    "href": "supplemental/omitted_variables.html",
    "title": "Omitted variable bias, FWL",
    "section": "",
    "text": "This fact will about covariance estimation be useful for the following discussion:\nâˆ‘i(xiâˆ’xâ€¾)(yiâˆ’yâ€¾)=âˆ‘i(xiâˆ’xâ€¾)yiâˆ’âˆ‘i(xiâˆ’xâ€¾)yâ€¾=âˆ‘i(xiâˆ’xâ€¾)yiâˆ’yâ€¾âˆ‘i(xiâˆ’xâ€¾)=âˆ‘i(xiâˆ’xâ€¾)yi\n\\begin{align*}\n\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right) & =\\sum_{i}\\left(x_{i}-\\bar{x}\\right)y_{i}-\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\bar{y}\\\\\n & =\\sum_{i}\\left(x_{i}-\\bar{x}\\right)y_{i}-\\bar{y}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\\\\n & =\\sum_{i}\\left(x_{i}-\\bar{x}\\right)y_{i}\n\\end{align*}\n and by the same argument âˆ‘i(xiâˆ’xâ€¾)(yiâˆ’yâ€¾)=âˆ‘i(yiâˆ’yâ€¾)xi\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)=\\sum_{i}\\left(y_{i}-\\bar{y}\\right)x_{i} and âˆ‘i(xiâˆ’xâ€¾)(xiâˆ’xâ€¾)=âˆ‘i(xiâˆ’xâ€¾)xi\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)=\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#olr",
    "href": "supplemental/omitted_variables.html#olr",
    "title": "Omitted variable bias, FWL",
    "section": "OLR",
    "text": "OLR\nPer our population model\ny=Î²0+Î²1x+u\ny = \\beta_0 + \\beta_1x + u\n and so our sample regression is\nyi=Î²0+Î²1xi+ui(1)\ny_i = \\beta_0 + \\beta_1x_i + u_i\n \\qquad(1) where the index ii identifies each sample, and our prediction is yiÌ‚=Î²0Ì‚+Î²1Ì‚xi\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i. with residuals ui=yiâˆ’yiÌ‚u_i=y_i-\\hat{y_i}.\nWe know that the OLS formula for the regression coefficient Î²1\\beta_1 is\nÎ²1Ì‚=Cov(xi,yi)Var(xi)=âˆ‘i(xiâˆ’xâ€¾)(yiâˆ’yâ€¾)âˆ‘i(xiâˆ’xâ€¾)(xiâˆ’xâ€¾)=âˆ‘i(xiâˆ’xâ€¾)yiâˆ‘i(xiâˆ’xâ€¾)xi\n\\begin{align*}\n\\hat{\\beta_{1}} & =\\frac{\\text{Cov}\\left(x_{i},y_{i}\\right)}{\\text{Var}\\left(x_{i}\\right)}\\\\\n & =\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)}\\\\\n & =\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)y_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\n\\end{align*}\n\nbut per our model EquationÂ 1 we can write\nÎ²1Ì‚=âˆ‘i(xiâˆ’xâ€¾)(Î²0+Î²1xi+ui)âˆ‘i(xiâˆ’xâ€¾)xi=Î²0âˆ‘i(xiâˆ’xâ€¾)+Î²1âˆ‘i(xiâˆ’xâ€¾)xi+âˆ‘i(xiâˆ’xâ€¾)uiâˆ‘i(xiâˆ’xâ€¾)xi==Î²1+âˆ‘i(xiâˆ’xâ€¾)uiâˆ‘i(xiâˆ’xâ€¾)xi\n\\begin{align*}\n\\hat{\\beta_{1}} & =\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(\\beta_{0}+\\beta_{1}x_{i}+u_{i}\\right)}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\\\\\n & =\\frac{\\beta_{0}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)+\\beta_{1}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}+\\sum_{i}\\left(x_{i}-\\bar{x}\\right)u_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}=\\\\\n & =\\beta_{1}+\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)u_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\n\\end{align*}\n\nand so, our estimate Î²1Ì‚\\hat{\\beta_1} is equal to the population parameter Î²\\beta IF the noise (i.e.Â residuals) is uncorrelated with our predictor.\nThe assumption that the residual term is uncorrelated with our predictor is one of the assumptions we used in setting up ordinary linear regression.",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#omitted-variable-bias-ovb",
    "href": "supplemental/omitted_variables.html#omitted-variable-bias-ovb",
    "title": "Omitted variable bias, FWL",
    "section": "Omitted variable bias (OVB)",
    "text": "Omitted variable bias (OVB)\nBut what if the residuals are not uncorrelated with the predictor? For example what if our population model was really\ny=Î²0+Î²1x+Î²2z+u\ny = \\beta_0 + \\beta_1x + \\beta_2z + u\n with unobserved variable zz, but our estimates are yiÌ‚=Î²0Ì‚+Î²1Ì‚xi\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i, then\nÎ²1Ì‚=âˆ‘i(xiâˆ’xâ€¾)(Î²0+Î²1xi+Î²2zi+ui)âˆ‘i(xiâˆ’xâ€¾)xi=Î²0âˆ‘i(xiâˆ’xâ€¾)+Î²1âˆ‘i(xiâˆ’xâ€¾)xi+Î²2âˆ‘i(xiâˆ’xâ€¾)zi+âˆ‘i(xiâˆ’xâ€¾)uiâˆ‘i(xiâˆ’xâ€¾)xi=ğ”¼[Î²1Ì‚|x]=Î²1+Î²2âˆ‘i(xiâˆ’xâ€¾)ziâˆ‘i(xiâˆ’xâ€¾)xi\n\\begin{align*}\n\\hat{\\beta_{1}} & =\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(\\beta_{0}+\\beta_{1}x_{i}+\\beta_2z_{i}+u_{i}\\right)}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\\\\\n & =\\frac{\\beta_{0}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)+\\beta_{1}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}+\\beta_{2}\\sum_{i}\\left(x_{i}-\\bar{x}\\right)z_{i}+\\sum_{i}\\left(x_{i}-\\bar{x}\\right)u_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}=\\\\\n \\mathbb{E}[\\hat{\\beta_{1}}|x] &=\\beta_{1}+\\beta_{2}\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)z_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}\n\\end{align*}\n and the bias in our estimate of Î²1\\beta_1 is âˆ‘i(xiâˆ’xâ€¾)ziâˆ‘i(xiâˆ’xâ€¾)xi\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)z_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)x_{i}}",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#ovb-in-action",
    "href": "supplemental/omitted_variables.html#ovb-in-action",
    "title": "Omitted variable bias, FWL",
    "section": "OVB in action:",
    "text": "OVB in action:\nIn this example we will simulate what happens with linearly dependent predictors.\nWe have seen the data below in lab-4, where\n\ny=demand1y = \\text{demand1}\nx=price1,Î²1=âˆ’0.5x = \\text{price1},\\;\\beta_1=-0.5\nz=unobserved1,Î²2=âˆ’1z = \\text{unobserved1},\\;\\beta_2=-1\n\n\n&gt; set.seed(1966)\n&gt; \n&gt; dat1 &lt;- tibble::tibble(\n+   unobserved1 = rnorm(500)\n+   , price1 = 10 + unobserved1 + rnorm(500)\n+   , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))\n+ )\n\nWithout including the unobserved variable, the fit is\n\n&gt; fit1 &lt;- lm(demand1 ~ price1, data = dat1)\n&gt; broom::tidy(fit1)\n\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   27.8      0.387       71.8 5.65e-265\n2 price1        -0.989    0.0384     -25.8 9.44e- 94\n\n\nand this fit estimates Î²1Ì‚=âˆ’0.99&lt;âˆ’0.5\\hat{\\beta_1}=-0.99&lt;-0.5 so it is incorrect & biased. Checking using the covariance formula:\n\n&gt; cov(dat1$demand1, dat1$price1)/var(dat1$price1)\n\n[1] -0.9893123\n\n\nBut we know we have an unobserved variable (and we know Î²2=âˆ’1\\beta_2=-1) so we can correct the bias.\n\n&gt; # biased estimate\n&gt; biased_est &lt;- broom::tidy(fit1) %&gt;% \n+   dplyr::filter(term == 'price1') %&gt;% \n+   dplyr::pull(estimate)\n&gt; \n&gt; # bias\n&gt; bias &lt;- -cov(dat1$unobserved1, dat1$price1)/var(dat1$price1)\n&gt; \n&gt; #corrected estimate\n&gt; biased_est - bias\n\n[1] -0.4841394\n\n\n\n&gt; lm(demand1 ~ price1 + unobserved1, data = dat1) %&gt;% \n+   broom::tidy()\n\n# A tibble: 3 Ã— 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   22.4      0.443      50.5  1.10e-197\n2 price1        -0.443    0.0444     -9.98 1.74e- 21\n3 unobserved1   -1.08     0.0638    -17.0  3.21e- 51",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#frischwaughlovell-theorem",
    "href": "supplemental/omitted_variables.html#frischwaughlovell-theorem",
    "title": "Omitted variable bias, FWL",
    "section": "Frischâ€“Waughâ€“Lovell theorem",
    "text": "Frischâ€“Waughâ€“Lovell theorem\n\nFWL or decomposition theorem:\nWhen estimating a model of the form\ny=Î²0+Î²1x1+Î²1x2+u\ny = \\beta_0 + \\beta_1x_1 + \\beta_1x_2 + u\n\nthen the following estimators of Î²1\\beta_1 are equivalent\n\nthe OLS estimator obtained by regressing yy on x1x_1 and x2x_2\nthe OLS estimator obtained by regressing yy on xÌŒ1\\check{x}_1\n\nwhere xÌŒ1\\check{x}_1 is the residual from the regression of x1x_1 on x2x_2\n\nthe OLS estimator obtained by regressing yÌŒ\\check{y} on xÌŒ1\\check{x}_1\n\nwhere yÌŒ\\check{y} is the residual from the regression of yy on x2x_2\n\n\n\n\nInterpretation:\nThe Frisch-Waugh-Lowell theorem is telling us that there are multiple ways to estimate a single regression coefficient. One possibility is to run the full regression of yy on xx, as usual.\nHowever, we can also regress x1x_1 on x2x_2, take the residuals, and regress yy only those residuals. The first part of this process is sometimes referred to as partialling-out (or orthogonalization, or residualization) of x1x_1 with respect to x2x_2. The idea is that we are isolating the variation in x1x_1 that is independent of (orthogonal to) x2x_2. Note that x2x_2 can be also be multi-dimensional (i.e.Â include multiple variables and not just one).\nWhy would one ever do that?\nThis seems like a way more complicated procedure. Instead of simply doing the regression in 1 step, now we need to do 2 or even 3 steps. Itâ€™s not intuitive at all. The main advantage comes from the fact that we have reduced a multivariate regression to a univariate one, making more tractable and more intuitive.\n\n\nExample1\nUsing the data from OVB example:\n\n&gt; # partial out unobserved1 (predictor) from price1 (predictor)\n&gt; fit_price &lt;- lm(price1 ~ unobserved1, data = dat1)\n&gt; \n&gt; # regress demand1 (outcome) on price residuals\n&gt; lm(\n+   demand1 ~ price_resid\n+   , data = tibble::tibble(demand1 = dat1$demand1, price_resid = fit_price$residuals)\n+ ) %&gt;% \n+   broom::tidy()\n\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)   17.9      0.0830    216.   0          \n2 price_resid   -0.443    0.0828     -5.35 0.000000135\n\n\n\n&gt; # partial out unobserved1 (predictor) from demand1 (outcome)\n&gt; fit_demand &lt;- lm(demand1 ~ unobserved1, data = dat1)\n&gt; \n&gt; # regress demand residuals on price residuals\n&gt; lm(\n+   demand_resid ~ price_resid\n+   , data = tibble::tibble(demand_resid = fit_demand$residuals, price_resid = fit_price$residuals)\n+ ) %&gt;% \n+   broom::tidy()\n\n# A tibble: 2 Ã— 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  1.37e-16    0.0445  3.08e-15 1.00e+ 0\n2 price_resid -4.43e- 1    0.0444 -9.99e+ 0 1.59e-21",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "supplemental/omitted_variables.html#partial-identification",
    "href": "supplemental/omitted_variables.html#partial-identification",
    "title": "Omitted variable bias, FWL",
    "section": "Partial Identification",
    "text": "Partial Identification\nIn our linear regression models we include errors (aka noise) in the model of the outcome\nyi=Î²0+Î²1xi+ui\ny_i = \\beta_0 + \\beta_1x_i + u_i\n\nwhere each sample uiu_i is drawn from a Gaussian distributions with zero mean and variance Ïƒ2\\sigma^2.\nWhat if we are also uncertain about xix_i? In particular what if instead of xix_i we can only use xÌƒi=xi+wi\\tilde{x}_i=x_i + w_i where\nCov(w,x)=Cov(w,u)=ğ”¼[w]=0\n\\text{Cov}\\left(w,x\\right)=\\text{Cov}\\left(w,u\\right)=\\mathbb{E}\\left[w\\right]=0\n\ni.e.Â xÌƒi\\tilde{x}_i reflects measurement error in the predictor xix_i, where the measurement error has zero mean and is independent of xx and uu (e.g.Â the measurement error is an independent Gaussian).\nIn this case ğ”¼[xÌƒ]=ğ”¼[x+w]=ğ”¼[x]\\mathbb{E}\\left[\\tilde{x}\\right]=\\mathbb{E}\\left[x+w\\right]=\\mathbb{E}\\left[x\\right] and\nCov(xÌƒ,y)=Cov(x+w,y)=Cov(x,y)+Cov(w,y)=Cov(x,y)+Cov(w,Î²0+Î²1x+u)=Cov(x,y)+Cov(w,u)+Î²1Cov(w,x)=Cov(x,y)\n\\begin{align*}\n\\text{Cov}\\left(\\tilde{x},y\\right) & =\\text{Cov}\\left(x+w,y\\right)=\\text{Cov}\\left(x,y\\right)+\\text{Cov}\\left(w,y\\right)\\\\\n & =\\text{Cov}\\left(x,y\\right)+\\text{Cov}\\left(w,\\beta_{0}+\\beta_{1}x+u\\right)\\\\\n & =\\text{Cov}\\left(x,y\\right)+\\text{Cov}\\left(w,u\\right)+\\beta_{1}\\text{Cov}\\left(w,x\\right)\\\\\n & =\\text{Cov}\\left(x,y\\right)\n\\end{align*}\n\nhowever Var(xÌƒ)=Var(x+w)â‰¥Var(x)\\text{Var}\\left(\\tilde{x}\\right)=\\text{Var}\\left(x+w\\right)\\ge\\text{Var}\\left(x\\right) so\nCov(xÌƒ,y)Var(xÌƒ)=Cov(x,y)Var(x)+Var(w)=Cov(x,y)/Var(x)1+Var(w)/Var(x)=Î²11+Var(w)/Var(x)\n\\begin{align*}\n\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)} & =\\frac{\\text{Cov}\\left(x,y\\right)}{\\text{Var}\\left(x\\right)+\\text{Var}\\left(w\\right)}\\\\\n & =\\frac{\\text{Cov}\\left(x,y\\right)/\\text{Var}\\left(x\\right)}{1+\\text{Var}\\left(w\\right)/\\text{Var}\\left(x\\right)}\\\\\n & =\\frac{\\beta_{1}}{1+\\text{Var}\\left(w\\right)/\\text{Var}\\left(x\\right)}\n\\end{align*}\n and since Var(w)/Var(x)\\text{Var}\\left(w\\right)/\\text{Var}\\left(x\\right) is non-negative Cov(xÌƒ,y)Var(xÌƒ)\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)} has the same sign as Î²1\\beta_1 and our data gives us a lower bound for Î²1\\beta_1:\n|Cov(xÌƒ,y)Var(xÌƒ)|â‰¤|Î²1|\n\\left|\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\\right|\\le\\left|\\beta_{1}\\right|\n\nIf we reverse the regression (regress xÌƒ\\tilde{x} on yy)\nCov(xÌƒ,y)Var(y)=Cov(x,y)Î²12Var(x)+Var(u)=Î²1Var(x)Î²12Var(x)+Var(u)\n\\begin{align*}\n\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(y\\right)} & =\\frac{\\text{Cov}\\left(x,y\\right)}{\\beta_{1}^{2}\\text{Var}\\left(x\\right)+\\text{Var}\\left(u\\right)}\\\\\n & =\\frac{\\beta_{1}\\text{Var}\\left(x\\right)}{\\beta_{1}^{2}\\text{Var}\\left(x\\right)+\\text{Var}\\left(u\\right)}\n\\end{align*}\n and taking the reciprocal:\nVar(y)Cov(xÌƒ,y)=Î²1+Var(u)ğ›ƒğŸğ•ğšğ«(ğ±)=Î²1[1+Var(u)ğ›ƒğŸğŸğ•ğšğ«(ğ±)]\n\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}=\\beta_{1}+\\frac{\\text{Var}\\left(u\\right)}{\\mathbf{\\beta_{1}\\text{Var}\\left(x\\right)}}=\\beta_{1}\\left[1+\\frac{\\text{Var}\\left(u\\right)}{\\mathbf{\\beta_{1}^{2}\\text{Var}\\left(x\\right)}}\\right]\n and the factor in the brackets is greater than 1 and as bofore Cov(xÌƒ,y)Var(xÌƒ)\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)} has the same sign as Î²1\\beta_1 so\n|Var(y)Cov(xÌƒ,y)|â‰¥|Î²1|\n\\left|\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}\\right|\\ge\\left|\\beta_{1}\\right|\n\nSome terminology:\n\na bound is sharp if it cannot be improved (under our assumptions)\na bound is tight if it is short enough to be useful in practice\n\nWe have sharp bounds for Î²1\\beta_1 under measurement error:\n|Var(y)Cov(xÌƒ,y)|â‰¥|Î²1|â‰¥|Cov(xÌƒ,y)Var(xÌƒ)|(2)\n\\left|\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}\\right|\\ge\\left|\\beta_{1}\\right|\\ge\\left|\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\\right|\n \\qquad(2)\nWith respect to how tight the bounds are, let rr denote the correlation between xÌƒ\\tilde{x} and yy, then\nr2=Cov(xÌƒ,y)2Var(xÌƒ)Var(y)=Cov(xÌƒ,y)Var(xÌƒ)Ã—Cov(xÌƒ,y)Var(y)r2Ã—Var(y)Cov(xÌƒ,y)=Cov(xÌƒ,y)Var(xÌƒ)\n\\begin{align*}\nr^{2} & =\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)^{2}}{\\text{Var}\\left(\\tilde{x}\\right)\\text{Var}\\left(y\\right)}=\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\\times\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(y\\right)}\\\\\nr^{2}\\times\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)} & =\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\n\\end{align*}\n\nand the the width of the bounds in EquationÂ 2 is\nwidth=|Var(y)Cov(xÌƒ,y)âˆ’Cov(xÌƒ,y)Var(xÌƒ)|=(1âˆ’r2)|Var(y)Cov(xÌƒ,y)|\n\\text{width}=\\left|\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}-\\frac{\\text{Cov}\\left(\\tilde{x},y\\right)}{\\text{Var}\\left(\\tilde{x}\\right)}\\right|=\\left(1-r^{2}\\right)\\left|\\frac{\\text{Var}\\left(y\\right)}{\\text{Cov}\\left(\\tilde{x},y\\right)}\\right|\n\nSo the bounds for Î²1\\beta_1 are tighter when xÌƒ\\tilde{x} and yy are strongly correlated.\nsee ovb",
    "crumbs": [
      "Supplemental notes",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_08.html#recap-of-last-week",
    "title": "Causality Part 2",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced DAGs as a method to represent our causal assumptions and infer how we might estimate the causal effects of interest.\nWe worked through a method to estimate causal effects using IPW."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#this-week",
    "href": "slides/BSMM_8740_lec_08.html#this-week",
    "title": "Causality Part 2",
    "section": "This week",
    "text": "This week\n\nWe will look at other methods used to estimate causal effects, along with methods used for special situations."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\nLast week we used IPW to create a pseudopopulation where, for every confounder level, the numbers of treated and untreated were balanced.\nIPW requires us to build a model to predict the treatment, depending on the confounders (assuming we have data for all the confounders)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-1",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-1",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\nRepeating our prior process, first we calculate the ipw weights and estimate the ATE using the weights.\n\n\nATE estimate by IPW\ndat_ &lt;- causalworkshop::net_data |&gt; dplyr::mutate(net = as.numeric(net))\n\npropensity_model &lt;- glm(\n  net ~ income + health + temperature\n  , data = dat_, family = binomial()\n)\n\nnet_data_wts &lt;- propensity_model |&gt;\n  broom::augment(newdata = dat_, type.predict = \"response\") |&gt;\n  # .fitted is the value predicted by the model\n  # for a given observation\n  dplyr::mutate(wts = propensity::wt_ate(.fitted, .exposure = net))\n  # alternatively:\n  # dplyr::mutate(\n  #   ipw = dplyr::case_when(net == 1 ~ 1/.fitted, TRUE ~ 1/(1-.fitted))\n  #   , wts_alt = (net / .fitted) + ((1 - net) / (1 - .fitted) )\n  # )\n\nnet_data_wts |&gt;\n  lm(malaria_risk ~ net, data = _, weights = wts) |&gt;\n  broom::tidy(conf.int = TRUE)\n\n\n# A tibble: 2 Ã— 7\n  term   estimate std.error statistic  p.value conf.low\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 (Inteâ€¦     42.7     0.442      96.7 0            41.9\n2 net       -12.5     0.624     -20.1 5.50e-81    -13.8\n# â„¹ 1 more variable: conf.high &lt;dbl&gt;"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-2",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-2",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nUsing propensity::wt_ate calculates unstabilized weights by default. In the last lecture we looked at the distribution of weights and decided they were stable enough, because none of the weights were too big or too small.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLetâ€™s explore the question of IPW stabilization a bit more."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-3",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-3",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\nStabilized weights\n\nThe IP weights (given covariate set \\(L\\)) are \\(W^A=1/\\mathbb{E}[D|L]\\). Because the denominator can be very close to \\(0\\) or \\(1\\) the estimates using these weights can be unstable.\nStabilized weights are often used to address this, e.g.Â the function propensity::wt_ate with stabilize = TRUE multiplies the weights by the mean of the treatment so in this case \\(SW^A=\\mathbb{E}[D]/\\mathbb{E}[D|L]\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-4",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-4",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\nV-Stabilized weights\nBaseline covariates (\\(V\\subset L\\)) are also used to stabilize IP weights: \\(SW^A(V)=\\mathbb{E}[D|V]/\\mathbb{E}[D|L]\\).\n\nNote the the variables \\(V\\) need to be included in the numerator and the denominator (as part of \\(L\\))\nV-stabilization results in IP weights that are more stabilized than the ones without V."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-5",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-5",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nWe know that IPW weighting should create a pseudo-population that make the covariates more balanced by treatment. In the last lecture we checked this using histograms. Here we check this via the statistics of the data:\n\n\nunweightedweighted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n0\nN = 1,298\n1\n1\nN = 454\n1\nOverall\nN = 1,752\n1\n\n\n\n\nincome\n880 (751, 1,010)\n954 (806, 1,081)\n893 (765, 1,031)\n\n\nhealth\n49 (35, 61)\n54 (40, 68)\n50 (37, 64)\n\n\ntemperature\n23.9 (21.0, 27.1)\n23.5 (20.6, 26.9)\n23.8 (20.9, 27.1)\n\n\nmalaria_risk\n41 (32, 54)\n26 (20, 32)\n36 (28, 50)\n\n\n\n1\nMedian (Q1, Q3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n0\nN = 1,751\n1\n1\nN = 1,760\n1\nOverall\nN = 3,511\n1\n\n\n\n\nincome\n892 (767, 1,025)\n892 (755, 1,039)\n892 (761, 1,031)\n\n\nhealth\n50 (36, 63)\n49 (37, 64)\n50 (37, 64)\n\n\ntemperature\n23.9 (20.9, 27.1)\n23.8 (20.8, 27.3)\n23.8 (20.8, 27.2)\n\n\nmalaria_risk\n40 (31, 53)\n28 (23, 34)\n33 (26, 45)\n\n\n\n1\nMedian (Q1, Q3)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-6",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-6",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nNext we use bootstrapping to generate proper confidence intervals for the effect, first by creating a function to generate (unstabilized) ipw effect estimates for each bootstrap split:\n\n\nCode\nfit_ipw &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  if(\"rsplit\" %in% class(split)){\n    .df &lt;- rsample::analysis(split)\n  }else if(\"data.frame\" %in% class(split)){\n    .df &lt;- split\n  }\n\n  # fit propensity score model\n  propensity_model &lt;- glm(\n    net ~ income + health + temperature,\n    data = .df,\n    family = binomial()\n  )\n\n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\n  # fit correctly bootstrapped ipw model\n  lm(malaria_risk ~ net, data = .df, weights = wts) |&gt;\n    broom::tidy()\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-7",
    "href": "slides/BSMM_8740_lec_08.html#inverse-probability-weighting-7",
    "title": "Causality Part 2",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nNext we use our function to bootstrap proper confidence intervals:\n\n\n\nCode\n# create bootstrap samples\nbootstrapped_net_data &lt;- rsample::bootstraps(\n  dat_,\n  times = 1000,\n  # required to calculate CIs later\n  apparent = TRUE\n)\n\n# create ipw and fit each bootstrap sample\nresults &lt;- bootstrapped_net_data |&gt;\n  dplyr::mutate(\n    ipw_fits = purrr::map(splits, fit_ipw))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#regression-adjustment",
    "href": "slides/BSMM_8740_lec_08.html#regression-adjustment",
    "title": "Causality Part 2",
    "section": "Regression Adjustment",
    "text": "Regression Adjustment\n\nIPW estimates rely on a model to predict the treatment using covariate/confounder values. We know that we can also predict the effect of treatment by building a model to predict the outcome using a regression model, regressing the effect on the treatment and covariate/confounder values.\n\n\n\nCode\noutcome_model &lt;- glm(\n  malaria_risk ~ net + income + health + temperature + insecticide_resistance +\n    I(health^2) + I(temperature^2) + I(income^2),\n  data = dat_\n)\n\noutcome_model |&gt; broom::tidy(conf.int = TRUE)\n\n\n# A tibble: 9 Ã— 7\n  term  estimate std.error statistic   p.value conf.low\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intâ€¦  1.08e+2   3.55e+0    30.5   1.89e-164  1.01e+2\n2 net   -1.24e+1   2.30e-1   -53.8   0         -1.28e+1\n3 incoâ€¦ -1.65e-1   4.58e-3   -36.1   3.48e-213 -1.74e-1\n4 healâ€¦  2.01e-1   2.82e-2     7.13  1.44e- 12  1.46e-1\n5 tempâ€¦  7.69e-1   2.62e-1     2.93  3.42e-  3  2.55e-1\n6 inseâ€¦  2.19e-1   6.89e-3    31.8   4.97e-175  2.05e-1\n7 I(heâ€¦ -6.60e-4   2.65e-4    -2.49  1.28e-  2 -1.18e-3\n8 I(teâ€¦  5.01e-3   5.46e-3     0.919 3.58e-  1 -5.68e-3\n9 I(inâ€¦  5.02e-5   2.50e-6    20.1   1.04e- 80  4.53e-5\n# â„¹ 1 more variable: conf.high &lt;dbl&gt;"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#regression-adjustment-1",
    "href": "slides/BSMM_8740_lec_08.html#regression-adjustment-1",
    "title": "Causality Part 2",
    "section": "Regression Adjustment",
    "text": "Regression Adjustment\n\nAnd we can also bootstrap the regression adjustment estimates to get confidence intervals, first by creating the estimation function, then generating bootstrapped estimates, like we did with IPWs:\n\n\n\nCode\nfit_reg &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  if(\"rsplit\" %in% class(split)){\n    .df &lt;- rsample::analysis(split)\n  }else if(\"data.frame\" %in% class(split)){\n    .df &lt;- split\n  }\n\n  # fit outcome model\n  glm(malaria_risk ~ net + income + health + temperature + insecticide_resistance +\n      I(health^2) + I(temperature^2) + I(income^2), data = .df\n    )|&gt;\n    broom::tidy()\n}\n\nboth_results &lt;- results |&gt;\n  dplyr::mutate(\n    reg_fits = purrr::map(splits, fit_reg))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment",
    "href": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment",
    "title": "Causality Part 2",
    "section": "IPW vs Regression Adjustment",
    "text": "IPW vs Regression Adjustment\n\nWe can compare the results:\n\n\n\nCode\nboth_results_dat &lt;- both_results |&gt;\n  dplyr::mutate(\n    reg_estimate = purrr::map_dbl(\n      reg_fits,\n      # pull the `estimate` for net for each fit\n      \\(.fit) .fit |&gt;\n        dplyr::filter(term == \"net\") |&gt;\n        dplyr::pull(estimate)\n    )\n    , ipw_estimate = purrr::map_dbl(\n      ipw_fits,\n      # pull the `estimate` for net for each fit\n      \\(.fit) .fit |&gt;\n        dplyr::filter(term == \"net\") |&gt;\n        dplyr::pull(estimate)\n    )\n  )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-1",
    "href": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-1",
    "title": "Causality Part 2",
    "section": "IPW vs Regression Adjustment",
    "text": "IPW vs Regression Adjustment\n\nWe can plot the results:\n\n\n\nCode\ndat &lt;- both_results_dat |&gt; \n  dplyr::select(reg_estimate, ipw_estimate) |&gt; \n  tidyr::pivot_longer(cols=everything(), names_to = \"method\", values_to = \"effect estimate\")\n\ndat |&gt;\n  dplyr::mutate(method=factor(method)) |&gt;\n  ggplot( bins = 50, alpha = .5\n    , aes(\n      `effect estimate`, after_stat(density) , colour = method, fill = method\n      )\n  ) +\n  geom_histogram(alpha = 0.2, position = 'identity') +\n  geom_density(alpha = 0.2) +\n  theme(legend.text=element_text(size=10), legend.title = element_blank())"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-2",
    "href": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-2",
    "title": "Causality Part 2",
    "section": "IPW vs Regression Adjustment",
    "text": "IPW vs Regression Adjustment\n\nWe can compare the means and the confidence intervals of the regression adjustment and IPW effect estimates:\n\n\nCode\ndat |&gt; \n  dplyr::group_by(method) |&gt; \n  dplyr::summarise(\n  mean = mean(`effect estimate`)\n  , lower_ci = quantile(`effect estimate`, 0.025)\n  , upper_ci = quantile(`effect estimate`, 0.975)\n)\n\n\n# A tibble: 2 Ã— 4\n  method        mean lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 ipw_estimate -12.6    -13.4    -11.7\n2 reg_estimate -12.4    -12.9    -11.8\n\n\nNote that each mean ate estimate is within the CIs of the other ate mean estimate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-3",
    "href": "slides/BSMM_8740_lec_08.html#ipw-vs-regression-adjustment-3",
    "title": "Causality Part 2",
    "section": "IPW vs Regression Adjustment",
    "text": "IPW vs Regression Adjustment\n\nThere is a package (boot) that performs cross-validation and CI estimation at the same time:\n\n\nCode\n# bootstrap (1000 times) using the fit_reg function\nboot_out_reg &lt;- boot::boot(\n  data = causalworkshop::net_data |&gt; dplyr::mutate(net = as.numeric(net))\n  , R = 1000\n  , sim = \"ordinary\"\n  , statistic =\n    (\\(x,y){ # x is the data, y is a vector of row numbers for the bootstrap sample\n      fit_reg(x[y,]) |&gt;\n        dplyr::filter(term == \"net\") |&gt;\n        dplyr::pull(estimate)\n    })\n)\n# calculate the CIs\nCIs &lt;- boot_out_reg |&gt;\n  boot::boot.ci(L = boot::empinf(boot_out_reg, index=1L, type=\"jack\"))\n\ntibble::tibble(CI = c(\"lower\", \"upper\"), normal = CIs$normal[-1], basic = CIs$basic[-(1:3)]\n               , percent = CIs$percent[-(1:3)])\n\n\n# A tibble: 2 Ã— 4\n  CI    normal basic percent\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 lower  -12.9 -12.9   -12.9\n2 upper  -11.8 -11.8   -11.8"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\nDonâ€™t Put All your Eggs in One Basket\n\nWeâ€™ve learned how to use linear regression and propensity score weighting to estimate \\(E[Y|D=1] - E[Y|D=0] | X\\). But which one should we use and when?\nWhen in doubt, just use both! Doubly Robust Estimation is a way of combining propensity score and linear regression in a way you donâ€™t have to rely on either of them."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-1",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-1",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\nThe estimator is as follows.\n\nThe estimator is as follows.\n\\[\n\\hat{ATE} = \\frac{1}{N}\\sum \\bigg( \\dfrac{D_i(Y_i - \\hat{\\mu_1}(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg) - \\frac{1}{N}\\sum \\bigg( \\dfrac{(1-D_i)(Y_i - \\hat{\\mu_0}(X_i))}{1-\\hat{P}(X_i)} + \\hat{\\mu_0}(X_i) \\bigg)\n\\]\nwhere\n\n\\(\\hat{P}(x)\\) is an estimation of the propensity score (using logistic regression, for example),\n\\(\\hat{\\mu_1}(x)\\) is an estimation of (using linear regression, for example), and is an estimation of \\(E[Y|X, D=1]\\).\n\nAs you might have already guessed, the first part of the doubly robust estimator estimates \\(E[Y^1]\\) and the second part estimates \\(E[Y^0]\\).\nLetâ€™s examine the first part, as all the intuition will also apply to the second part by analogy."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-2",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-2",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nFirst letâ€™s examine the code. Note we use a recipe with the regression to add the polynomial confounders for each\n\n\nCode\nD &lt;- \"net\"\nY &lt;- \"malaria_risk\"\nX &lt;- paste0(c('income', 'health', 'temperature'),c(rep('_poly_1',3),rep('_poly_2',3)))\n\ndoubly_robust &lt;- function(df, X, D, Y){\n  ps &lt;- # propensity score\n    as.formula(paste(D, \" ~ \", paste(X, collapse= \"+\"))) |&gt;\n    stats::glm( data = df, family = binomial() ) |&gt;\n    broom::augment(type.predict = \"response\", data = df) |&gt;\n    dplyr::pull(.fitted)\n  \n  lin_frml &lt;- formula(paste(Y, \" ~ \", paste(X, collapse= \"+\")))\n  \n  idx &lt;- df[,D] %&gt;% dplyr::pull(1) == 0\n  mu0 &lt;- # mean response D == 0\n    lm(lin_frml, data = df[idx,]) %&gt;% \n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt;\n    dplyr::pull(.fitted)\n  \n  idx &lt;- df[,D] %&gt;% dplyr::pull(1) == 1\n  mu1 &lt;- # mean response D == 1\n    lm(lin_frml, data = df[idx,]) |&gt;  \n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt; \n    dplyr::pull(.fitted)\n  \n  # convert treatment factor to integer | recast as vectors\n  d &lt;- df[,D] %&gt;% dplyr::pull(1) |&gt; as.character() |&gt; as.numeric()\n  y &lt;- df[,Y] %&gt;% dplyr::pull(1)\n  \n  mean( d*(y - mu1)/ps + mu1 ) -\n    mean(( 1-d)*(y - mu0)/(1-ps) + mu0 )\n}\n\ndoubly_robust_rec &lt;- causalworkshop::net_data |&gt; \n  dplyr::mutate(net = as.numeric(net)) |&gt; \n  recipes::recipe(malaria_risk ~ net + income + health + temperature) |&gt; \n  recipes::step_poly(income, health, temperature) |&gt; \n  recipes::prep() \n\ndoubly_robust_dat &lt;- doubly_robust_rec |&gt; recipes::bake(new_data=NULL)\n  \ndoubly_robust_dat |&gt; \n  doubly_robust(X, D, Y)\n\n\n[1] -12.9"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-3",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-3",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nOnce again, we can use bootstrap to construct confidence intervals.\n\n\n\nCode\nall_results_dat &lt;- both_results_dat |&gt;\n  dplyr::mutate(\n    dre_estimate = \n      purrr::map_dbl(\n        splits\n        , (\\(x){\n            doubly_robust_rec |&gt; \n              recipes::bake(new_data = rsample::analysis(x)) |&gt; \n              doubly_robust(X, D, Y)\n        })\n      )\n  )\n\nall_results_dat\n\n\n# Bootstrap sampling with apparent sample \n# A tibble: 1,001 Ã— 7\n   splits             id            ipw_fits reg_fits\n   &lt;list&gt;             &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;  \n 1 &lt;split [1752/648]&gt; Bootstrap0001 &lt;tibble&gt; &lt;tibble&gt;\n 2 &lt;split [1752/630]&gt; Bootstrap0002 &lt;tibble&gt; &lt;tibble&gt;\n 3 &lt;split [1752/650]&gt; Bootstrap0003 &lt;tibble&gt; &lt;tibble&gt;\n 4 &lt;split [1752/638]&gt; Bootstrap0004 &lt;tibble&gt; &lt;tibble&gt;\n 5 &lt;split [1752/648]&gt; Bootstrap0005 &lt;tibble&gt; &lt;tibble&gt;\n 6 &lt;split [1752/645]&gt; Bootstrap0006 &lt;tibble&gt; &lt;tibble&gt;\n 7 &lt;split [1752/642]&gt; Bootstrap0007 &lt;tibble&gt; &lt;tibble&gt;\n 8 &lt;split [1752/651]&gt; Bootstrap0008 &lt;tibble&gt; &lt;tibble&gt;\n 9 &lt;split [1752/640]&gt; Bootstrap0009 &lt;tibble&gt; &lt;tibble&gt;\n10 &lt;split [1752/635]&gt; Bootstrap0010 &lt;tibble&gt; &lt;tibble&gt;\n# â„¹ 991 more rows\n# â„¹ 3 more variables: reg_estimate &lt;dbl&gt;,\n#   ipw_estimate &lt;dbl&gt;, dre_estimate &lt;dbl&gt;"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-4",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-4",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nLooking at the histogram/distributions of all the estimates:\n\n\n\nCode\ndat &lt;- all_results_dat |&gt; \n  dplyr::select(reg_estimate, ipw_estimate, dre_estimate) |&gt; \n  tidyr::pivot_longer(cols=everything(), names_to = \"method\", values_to = \"effect estimate\")\n\ndat |&gt;\n  dplyr::mutate(method=factor(method)) |&gt;\n  ggplot( bins = 50, alpha = .5\n    , aes(\n      `effect estimate`, after_stat(density) , colour = method, fill = method\n      )\n  ) +\n  geom_histogram(alpha = 0.2, position = 'identity') +\n  geom_density(alpha = 0.2) +\n  theme(legend.text=element_text(size=10), legend.title = element_blank())"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-5",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-5",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nConfidence intervals, including the dre are:\n\n\n\nCode\ndat |&gt; \n  dplyr::group_by(method) |&gt; \n  dplyr::summarise(\n  mean = mean(`effect estimate`)\n  , lower_ci = quantile(`effect estimate`, 0.025)\n  , upper_ci = quantile(`effect estimate`, 0.975)\n)\n\n\n# A tibble: 3 Ã— 4\n  method        mean lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 dre_estimate -12.9    -13.5    -12.3\n2 ipw_estimate -12.6    -13.4    -11.7\n3 reg_estimate -12.4    -12.9    -11.8\n\n\n\nAll methods are consistent."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-6",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-6",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nThe doubly robust estimator is called doubly robust because it only requires one of the models, \\(\\hat{P}(x)\\) or \\(\\hat{\\mu}(x)\\), to be correctly specified.\nTo see this, take the first part that estimates \\(E[Y^1]\\) and take a good look at it.\n\\[\n\\hat{E}[Y^1] = \\frac{1}{N}\\sum \\bigg( \\dfrac{D_i(Y_i - \\hat{\\mu_1}(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg)\n\\]\nAssume that \\(\\hat{\\mu_1}(x)\\) is correct. If the propensity score model is wrong, we wouldnâ€™t need to worry. Because if \\(\\hat{\\mu_1}(x)\\) is correct, then \\(E[D_i(Y_i - \\hat{\\mu_1}(X_i))]=0\\). That is because the multiplication by \\(T_i\\) selects only the treated and the residual of \\(\\hat{\\mu_1}\\) on the treated have, by definition, mean zero.\nThis causes the whole thing to reduce to \\(\\hat{\\mu_1}(X_i)\\), which is correctly estimated \\(E[Y^1]\\) by assumption. So, you see, that by being correct, \\(\\hat{\\mu_1}(X_i)\\) wipes out the relevance of the propensity score model. We can apply the same reasoning to understand the estimator of \\(E[Y^0]\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-7",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-7",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nReplace the propensity score by a random uniform variable that goes from 0.1 to 0.9 (we donâ€™t want very small weights to blow up the propensity score variance). Since this is random, there is no way it is a good propensity score model, but the doubly robust estimator still manages to produce an estimation that is very close to when the propensity score was estimated with logistic regression.\n\n\nCode\ndoubly_robust_bad_ipw &lt;- function(df, X, D, Y){\n  ps &lt;- runif(dim(df)[1], 0.1, 0.9) # wrong propensity score \n  \n  lin_frml &lt;- formula(paste(Y, \" ~ \", paste(X, collapse= \"+\")))\n  \n  idx &lt;- df[,D] %&gt;% dplyr::pull(1) == 0\n  mu0 &lt;- # mean response D == 0\n    lm(lin_frml, data = df[idx,]) %&gt;%\n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt;\n    dplyr::pull(.fitted)\n  \n  idx &lt;- df[,D] %&gt;% dplyr::pull(1) == 1\n  mu1 &lt;- # mean response D == 1\n    lm(lin_frml, data = df[idx,]) |&gt;\n    broom::augment(type.predict = \"response\", newdata = df[,X]) |&gt;\n    dplyr::pull(.fitted)\n  \n  # convert treatment factor to integer | recast as vectors\n  d &lt;- df[,D] %&gt;% dplyr::pull(1) |&gt; as.character() |&gt; as.numeric()\n  y &lt;- df[,Y] %&gt;% dplyr::pull(1)\n  \n  mean( d*(y - mu1)/ps + mu1 ) -\n    mean(( 1-d)*(y - mu0)/(1-ps) + mu0 )\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-8",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-8",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\n\n\nCode\nall_results_dat &lt;- all_results_dat |&gt;\n  dplyr::mutate(\n    dre_bad_ipw_estimate = \n      purrr::map_dbl(\n        splits\n        , (\\(x){\n          # rsample::analysis(x) |&gt; \n          #   # dplyr::mutate(net = as.numeric(net)) |&gt; # not needed\n          #   recipes::bake(new_data = rsample::analysis(x)) |&gt; \n          doubly_robust_rec |&gt; \n            recipes::bake(new_data = rsample::analysis(x)) |&gt; \n            doubly_robust_bad_ipw(X, D, Y)\n        })\n      )\n  )\n\ndat &lt;- all_results_dat |&gt; \n  dplyr::select(reg_estimate, ipw_estimate, dre_estimate, dre_bad_ipw_estimate) |&gt; \n  tidyr::pivot_longer(cols=everything(), names_to = \"method\", values_to = \"effect estimate\")\n\ndat |&gt;\n  dplyr::mutate(method=factor(method)) |&gt;\n  ggplot( bins = 50, alpha = .5\n    , aes(\n      `effect estimate`, after_stat(density) , colour = method, fill = method\n      )\n  ) +\n  geom_histogram(alpha = 0.2, position = 'identity') +\n  geom_density(alpha = 0.2) +\n  theme(legend.text=element_text(size=10), legend.title = element_blank())"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-9",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-9",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\n\nCode\ndat |&gt; \n  dplyr::group_by(method) |&gt; \n  dplyr::summarise(\n  mean = mean(`effect estimate`)\n  , lower_ci = quantile(`effect estimate`, 0.025)\n  , upper_ci = quantile(`effect estimate`, 0.975)\n)\n\n\n# A tibble: 4 Ã— 4\n  method                mean lower_ci upper_ci\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 dre_bad_ipw_estimate -12.9    -13.7    -12.2\n2 dre_estimate         -12.9    -13.5    -12.3\n3 ipw_estimate         -12.6    -13.4    -11.7\n4 reg_estimate         -12.4    -12.9    -11.8"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-10",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-10",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nMessing up the propensity score yields slightly different ATEs, but not by much. This covers the case that the propensity model is wrong but the outcome model is correct. Now letâ€™s again take a good look at the first part of the estimator, rearranging some terms:\n\\[\n\\begin{align*}\n\\hat{E}[Y^{1}] & =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}(Y_{i}-\\hat{\\mu_{1}}(X_{i}))}{\\hat{P}(X_{i})}+\\hat{\\mu_{1}}(X_{i})\\bigg)\\\\\n& =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}Y_{i}}{\\hat{P}(X_{i})}-\\dfrac{D_{i}\\hat{\\mu_{1}}(X_{i})}{\\hat{P}(X_{i})}+\\hat{\\mu_{1}}(X_{i})\\bigg)\\\\\n& =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}Y_{i}}{\\hat{P}(X_{i})}-\\bigg(\\dfrac{D_{i}}{\\hat{P}(X_{i})}-1\\bigg)\\hat{\\mu_{1}}(X_{i})\\bigg)\\\\\n& =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}Y_{i}}{\\hat{P}(X_{i})}-\\bigg(\\dfrac{D_{i}-\\hat{P}(X_{i})}{\\hat{P}(X_{i})}\\bigg)\\hat{\\mu_{1}}(X_{i})\\bigg)\n\\end{align*}\n\\]\nAssume that the propensity score \\(\\hat{P}(X_i)\\) is correctly specified. In this case, \\(E[D_i - \\hat{P}(X_i)]=0\\), which wipes out the part dependent on \\(\\hat{\\mu_1}(X_i)\\). This makes the doubly robust estimator reduce to the propensity score weighting estimator \\(\\frac{D_iY_i}{\\hat{P}(X_i)}\\), which is correct by assumption. So, even if the \\(\\hat{\\mu_1}(X_i)\\) is wrong, the estimator will still be correct, provided that the propensity score is correctly specified."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-11",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-11",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nMessing up the propensity score yields slightly different ATEs, but not by much. This covers the case that the propensity model is wrong but the outcome model is correct. Now letâ€™s again take a good look at the first part of the estimator, rearranging some terms:\n\\[\n\\begin{align*}\n\\hat{E}[Y^{1}] & =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}(Y_{i}-\\hat{\\mu_{1}}(X_{i}))}{\\hat{P}(X_{i})}+\\hat{\\mu_{1}}(X_{i})\\bigg) =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}Y_{i}}{\\hat{P}(X_{i})}-\\dfrac{D_{i}\\hat{\\mu_{1}}(X_{i})}{\\hat{P}(X_{i})}+\\hat{\\mu_{1}}(X_{i})\\bigg)\\\\\n& =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}Y_{i}}{\\hat{P}(X_{i})}-\\bigg(\\dfrac{D_{i}}{\\hat{P}(X_{i})}-1\\bigg)\\hat{\\mu_{1}}(X_{i})\\bigg) =\\frac{1}{N}\\sum\\bigg(\\dfrac{D_{i}Y_{i}}{\\hat{P}(X_{i})}-\\bigg(\\dfrac{D_{i}-\\hat{P}(X_{i})}{\\hat{P}(X_{i})}\\bigg)\\hat{\\mu_{1}}(X_{i})\\bigg)\n\\end{align*}\n\\]\nAssume that the propensity score \\(\\hat{P}(X_i)\\) is correctly specified. In this case, \\(E[D_i - \\hat{P}(X_i)]=0\\), which wipes out the part dependent on \\(\\hat{\\mu_1}(X_i)\\). This makes the doubly robust estimator reduce to the propensity score weighting estimator \\(\\frac{D_iY_i}{\\hat{P}(X_i)}\\), which is correct by assumption. So, even if the \\(\\hat{\\mu_1}(X_i)\\) is wrong, the estimator will still be correct, provided that the propensity score is correctly specified."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-12",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-12",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\ndoubly_robust_bad_reg &lt;- function(df, X, D, Y){\n  ps &lt;- # propensity score\n    as.formula(paste(D, \" ~ \", paste(X, collapse= \"+\"))) |&gt;\n    stats::glm( data = df, family = binomial() ) |&gt;\n    broom::augment(type.predict = \"response\", data = df) |&gt;\n    dplyr::pull(.fitted)\n  \n  mu0 &lt;- rnorm(dim(df)[1], 0, 1) # wrong mean response D == 0\n  mu1 &lt;- rnorm(dim(df)[1], 0, 1) # wrong mean response D == 1\n  \n  # convert treatment factor to integer | recast as vectors\n  d &lt;- df[,D] %&gt;% dplyr::pull(1) |&gt; as.character() |&gt; as.numeric()\n  y &lt;- df[,Y] %&gt;% dplyr::pull(1)\n  \n  mean( d*(y - mu1)/ps + mu1 ) -\n    mean(( 1-d)*(y - mu0)/(1-ps) + mu0 )\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-13",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-13",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\n\nCode\nall_results_dat &lt;- all_results_dat |&gt;\n  dplyr::mutate(\n    dre_bad_reg_estimate = \n      purrr::map_dbl(\n        splits\n        , (\\(x){\n          # rsample::analysis(x) |&gt; \n          #   # dplyr::mutate(net = as.numeric(net)) |&gt; # not needed\n          #   recipes::bake(new_data = rsample::analysis(x)) |&gt; \n          doubly_robust_rec |&gt; \n            recipes::bake(new_data = rsample::analysis(x)) |&gt; \n            doubly_robust_bad_reg(X, D, Y)\n        })\n      )\n  )\n\ndat &lt;- all_results_dat |&gt; \n  dplyr::select(\n    reg_estimate, ipw_estimate, dre_estimate, dre_bad_ipw_estimate, dre_bad_reg_estimate\n  ) |&gt; \n  tidyr::pivot_longer(cols=everything(), names_to = \"method\", values_to = \"effect estimate\")\n\ndat |&gt;\n  dplyr::mutate(method=factor(method)) |&gt;\n  ggplot( bins = 50, alpha = .5\n    , aes(\n      `effect estimate`, after_stat(density) , colour = method, fill = method\n      )\n  ) +\n  geom_histogram(alpha = 0.2, position = 'identity') +\n  geom_density(alpha = 0.2) +\n  theme(legend.text=element_text(size=10), legend.title = element_blank())"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-14",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-14",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\nOnce again, we can use bootstrap and see that the variance is just slightly higher.\n\n\nCode\ndat |&gt; \n  dplyr::group_by(method) |&gt; \n  dplyr::summarise(\n  mean = mean(`effect estimate`)\n  , lower_ci = quantile(`effect estimate`, 0.025)\n  , upper_ci = quantile(`effect estimate`, 0.975)\n)\n\n\n# A tibble: 5 Ã— 4\n  method                mean lower_ci upper_ci\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 dre_bad_ipw_estimate -12.9    -13.7    -12.2\n2 dre_bad_reg_estimate -12.8    -13.5    -11.9\n3 dre_estimate         -12.9    -13.5    -12.3\n4 ipw_estimate         -12.6    -13.4    -11.7\n5 reg_estimate         -12.4    -12.9    -11.8"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-15",
    "href": "slides/BSMM_8740_lec_08.html#doubly-robust-estimation-15",
    "title": "Causality Part 2",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nOnce more, messing up the conditional mean model alone yields only slightly different ATE. The magic of doubly robust estimation happens because in causal inference, there are two ways to remove bias from our causal estimates: you either model the treatment mechanism or the outcome mechanism. If either of these models are correct, you are good to go.\nOne caveat is that, in practice, itâ€™s very hard to model precisely either of those. More often, what ends up happening is that neither the propensity score nor the outcome model are 100% correct. They are both wrong, but in different ways. When this happens, it is not exactly settled [1] [2] [3] if itâ€™s better to use a single model or doubly robust estimation. I still like using them because at least it gives me two possibilities of being correct."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\n\nWe know that not accounting for confounders can bias our causal estimates, but it turns out that even after accounting for all confounders, we may still get a biased estimate in finite samples. Many of the properties we tout in statistics rely on large samplesâ€”how â€œlargeâ€ is defined can be opaque. Letâ€™s look at a quick simulation. Here, we have an exposure/treatment, \\(X\\), an outcome, \\(Y\\), and one confounder, \\(Z\\). We will simulate \\(Y\\), which is only dependent on \\(Z\\) (so the true treatment effect is 0), and \\(X\\), which also depends on \\(Z\\).\n\n\\[\n\\begin{align*}\nX &\\sim \\mathscr{N}(0,1)\\\\\nX & = \\mathrm{ifelse}(0.5+Z&gt;0,1,0)\\\\\nY & = Z +  \\mathscr{N}(0,1)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-1",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-1",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\n\n\nCode\nset.seed(928)\nn &lt;- 100\nfinite_sample &lt;- tibble::tibble(\n  # z is normally distributed with a mean: 0 and sd: 1\n  z = rnorm(n),\n  # x is defined from a probit selection model with normally distributed errors\n  x = dplyr::case_when(\n    0.5 + z + rnorm(n) &gt; 0 ~ 1,\n    TRUE ~ 0\n  ),\n  # y is continuous, dependent only on z with normally distrbuted errors\n  y = z + rnorm(n)\n)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-2",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-2",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\n\nIf we fit a propensity score model using the one confounder \\(Z\\) and calculate the weighted estimator, we should get an unbiased result (which in this case would be \\(0\\)).\n\n\n\nCode\n## fit the propensity score model\nfinite_sample_wts &lt;- glm(\n  x ~ z,\n  data = finite_sample,\n  family = binomial(\"probit\")\n) |&gt;\n  broom::augment(newdata = finite_sample, type.predict = \"response\") |&gt;\n  dplyr::mutate(wts = propensity::wt_ate(.fitted, x))\n\nfinite_sample_wts |&gt;\n  dplyr::summarize(\n    effect = sum(y * x * wts) / sum(x * wts) -\n      sum(y * (1 - x) * wts) / sum((1 - x) * wts)\n  )\n\n\n# A tibble: 1 Ã— 1\n  effect\n   &lt;dbl&gt;\n1  0.197"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-3",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-3",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\nOur effect of is pretty far from 0, although itâ€™s hard to know if this is really bias, or something we are just seeing by chance in this particular simulated sample.\nTo explore the potential for finite sample bias, we can rerun this simulation many times at different sample sizes:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-4",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-4",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\nPloting the results:\n\nFinite sample bias present with ATE weights created using correctly specified propensity score model, varying the sample size from n = 50 to n = 10,000"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#finite-sample-bias-5",
    "href": "slides/BSMM_8740_lec_08.html#finite-sample-bias-5",
    "title": "Causality Part 2",
    "section": "Finite Sample Bias",
    "text": "Finite Sample Bias\n\nThis is an example of finite sample bias. Notice here, even when the sample size is quite large (5,000) we still see some bias away from the â€œtrueâ€ effect of 0. It isnâ€™t until a sample size larger than 10,000 that we see this bias disappear.\nEstimands that utilize weights that are unbounded (i.e.Â that theoretically can be infinitely large) are more likely to suffer from finite sample bias. The likelihood of falling into finite sample bias depends on:\n\nthe estimand you have chosen (i.e.Â are the weights bounded?)\nthe distribution of the covariates in the exposed and unexposed groups (i.e.Â is there good overlap? Potential positivity violations, when there is poor overlap, are the regions where weights can become too large)\nthe sample size."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching",
    "href": "slides/BSMM_8740_lec_08.html#matching",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nRegression does an amazing job at controlling for additional variables when we do a test vs control comparison. If we have independence, \\((Y^0, Y^1)\\perp D | X\\), then regression can identify the ATE by controlling for X.\nTo get some intuition about controlling for X, letâ€™s remember the case when all variables X are dummy variables.\nIf that is the case, regression partitions the data into the dummy cells and computes the mean difference between test and control. Effectively we are calculating doing\n\\[\nE[Y|D=1] - E[Y|D=0] | X=x]\n\\]\nwhere \\(x\\) is a dummy cell (all dummies set to 1, for example).\nRegression then combines the estimate in each of the cells to produce a final ATE. The way it does this is by applying weights to the cell proportional to the variance of the treatment on that group."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-1",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nTo give an example, suppose we try to estimate the effect of a drug and I have 6 men and 4 women. The response variable is days hospitalised and I hope my drug can lower that. On men, the true causal effect is -3, so the drug lowers the stay period by 3 days. On women, it is -2.\nTo make matters more interesting, men are much more affected by this illness and stay longer at the hospital. They also get much more of the drug. Only 1 out of the 6 men does not get the drug. On the other hand, women are more resistant to this illness, so they stay less at the hospital. 50% of the women get the drug.\n\n\n\n\n\n\n\n\n\nsex\ndrug\ndays\n\n\n\n\nM\n1\n5\n\n\nM\n1\n5\n\n\nM\n1\n5\n\n\nM\n1\n5\n\n\nM\n1\n5\n\n\nM\n0\n8\n\n\nW\n1\n2\n\n\nW\n0\n4\n\n\nW\n1\n2\n\n\nW\n0\n4"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-2",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nNote that simple comparison of treatment and control yields a negatively biased effect, that is, the drug seems less effective than it truly is. This is expected, since weâ€™ve omitted the sex confounder.\nIn this case, the estimated ATE is smaller than the true one because men get more of the drug and are more affected by the illness.\n\n\n\n\n\n\n\n\n\ndrug\nmean_effect\nATE\n\n\n\n\n0\n5.333\nNA\n\n\n1\n4.143\n-1.19"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-3",
    "href": "slides/BSMM_8740_lec_08.html#matching-3",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nSince the true effect for men is -3 and the true effect for women is -2, the ATE should be\n\n\\[\nATE=\\dfrac{(-3*6) + (-2*4)}{10}=-2.6\n\\]\n\nThis estimate is done by\n\npartitioning the data into confounder cells, in this case, men and women,\nestimating the effect on each cell and\ncombining the estimate with a weighted average, where the weight is the sample size of the cell or covariate group."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-4",
    "href": "slides/BSMM_8740_lec_08.html#matching-4",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nIf we had exactly the same number of men and women in the data, the ATE estimate would be right in the middle of the ATE of the 2 groups, -2.5. Since there are more men than women in our dataset, the ATE estimate is a little bit closer to the menâ€™s ATE.\nThis is called a non-parametric estimate, since it places no assumption on how the data was generated.\nIf we control for sex using regression, we will add the assumption of linearity. Regression will also partition the data into men and women and estimate the effect on both of these groups.\nSo far, so good. However, when it comes to combining the effect on each group, it does not weigh them by the sample size."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-5",
    "href": "slides/BSMM_8740_lec_08.html#matching-5",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nInstead, regression uses weights that are proportional to the variance of the treatment in that group. In our case, the variance of the treatment in men is smaller than in women, since only one man is in the control group.\nTo be exact, the variance of D for men is \\(0.139=1/6*(1 - 1/6)\\) and for women is \\(0.25=2/4*(1 - 2/4)\\).\nSo regression will give a higher weight to women in our example and the ATE will be a bit closer to the womenâ€™s ATE of -2.\n\n\n\n\n\n\n\n\n\nRegression summary (ATE)\n\n\ndays ~ drug + sex\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n7.5455\n0.188\n40.093\n0.000\n\n\ndrug\nâˆ’2.4545\n0.188\nâˆ’13.042\n0.000\n\n\nsexW\nâˆ’3.3182\n0.176\nâˆ’18.849\n0.000"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-6",
    "href": "slides/BSMM_8740_lec_08.html#matching-6",
    "title": "Causality Part 2",
    "section": "Matching",
    "text": "Matching\n\nThe result is more intuitive with dummy variables, but regression also keeps continuous variables constant while estimating the effect.\nAlso, with continuous variables, the ATE will point in the direction where covariates have more variance.\nSo weâ€™ve seen that regression has its idiosyncrasies. It is linear, parametric, likes high variance featuresâ€¦ This can be good or bad, depending on the context.\nBecause of this, itâ€™s important to be aware of other techniques we can use to control for confounders. Not only are they an extra tool in your causal tool belt, but understanding different ways to deal with confounding expands our understanding of the problem.\nFor this reason, I present you now the Subclassification Estimator!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-subclassification",
    "href": "slides/BSMM_8740_lec_08.html#matching-subclassification",
    "title": "Causality Part 2",
    "section": "Matching: subclassification",
    "text": "Matching: subclassification\n\nIn general, if there is some causal effect we want to estimate, but it is hard to do so because of confounding of some variables X, what we need to do is make the treatment vs control comparison within small groups where X is the same.\nIf we have conditional independence \\((Y^0, Y^1)\\perp D | X\\) , then we can write the ATE as follows.\n\n\\[\nATE = \\int(E[Y|X, D=1] - E[Y|X, D=0])dP(x)\n\\]\n\nWhat the integral does is it goes through all the space of the distribution of features X, computes the difference in means for all those tiny spaces and combines everything into the ATE."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-subclassification-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-subclassification-1",
    "title": "Causality Part 2",
    "section": "Matching: subclassification",
    "text": "Matching: subclassification\n\nAnother way to see this is to think about a discrete set of features.\nIn this case, we can say that the features X takes on K different cells \\(\\{X_1, X_2, ..., X_k\\}\\) and what we are doing is computing the treatment effect in each cell and combining them into the ATE.\nIn this discrete case, converting the integral to a sum, we can derive the subclassifications estimator:\n\n\\[\n\\hat{ATE} = \\sum^K_{k=1}(\\bar{Y}^1_k - \\bar{Y}^0_k) * \\dfrac{N_k}{N}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-subclassification-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-subclassification-2",
    "title": "Causality Part 2",
    "section": "Matching: subclassification",
    "text": "Matching: subclassification\n\\[\n\\hat{ATE} = \\sum^K_{k=1}(\\bar{Y}^1_k - \\bar{Y}^0_k) * \\dfrac{N_k}{N}\n\\]\n\nAs you can see, we are computing a local ATE for each cell and combining them using a weighted average, where the weights are the sample size of the cell. In our medicine example above, this would be the first estimate, which gave us âˆ’2.6.\nThe subclassification estimator isnâ€™t used much in practice, because of the curse of dimensionality."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\nThe subclassification estimator gives us a nice intuition of what a causal inference estimator should do, how it should control for confounders.\nThis allows us to explore other kinds of estimators, such as the Matching Estimator."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-1",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nSince some confounder X makes it so that treated and untreated are not initially comparable, we can make them so by matching each treated unit with a similar untreated unit - finding an untreated twin for every treated unit. In making such comparisons, treated and untreated become again comparable.\nAs an example, letâ€™s suppose we are trying to estimate the effect of a trainee program on earnings. Here is what the trainees looks like:\n\n\ntraineesnon trainees\n\n\n\n\nCode\ndata_trainees &lt;- readr::read_csv(\"data/trainees.csv\", show_col_types = FALSE)\n\ndata_trainees %&gt;% \n  dplyr::filter(trainees==1) %&gt;% \n  dplyr::slice_head(n=5) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nunit\ntrainees\nage\nearnings\n\n\n\n\n1\n1\n28\n17700\n\n\n2\n1\n34\n10200\n\n\n3\n1\n29\n14400\n\n\n4\n1\n25\n20800\n\n\n5\n1\n29\n6100\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata_trainees %&gt;% \n  dplyr::filter(trainees==0) %&gt;% \n  dplyr::slice_head(n=5) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nunit\ntrainees\nage\nearnings\n\n\n\n\n20\n0\n43\n20900\n\n\n21\n0\n50\n31000\n\n\n22\n0\n30\n21000\n\n\n23\n0\n27\n9300\n\n\n24\n0\n54\n41100"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-2",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nA simple comparison in means, identifies that the trainees earn less money than those that didnâ€™t go through the program.\n\n\n\nCode\ndata_trainees %&gt;% \n  dplyr::group_by(trainees) %&gt;% \n  dplyr::summarize(mean_effect = mean(earnings)) %&gt;% \n  dplyr::mutate(ATE = mean_effect - dplyr::lag( mean_effect) ) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\ntrainees\nmean_effect\nATE\n\n\n\n\n0\n20724\nNA\n\n\n1\n16426\n-4297\n\n\n\n\n\n\n\n\nHowever, if we look at the data tables, we notice that trainees are much younger than non trainees, which indicates that age is probably a confounder."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-3",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-3",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nWe can use matching on age to try to correct that.\nWe will take unit 1 from the treated and pair it with unit 27, since both are 28 years old. Unit 2 we will pair it with unit 34, unit 3 with unit 37, unit 4 we will pair it with unit 35â€¦ When it comes to unit 5, we need to find someone with age 29 from the non treated, but that is unit 37, which is already paired.\nThis is not a problem, since we can use the same unit multiple times. If more than 1 unit is a match, we can choose randomly between them.\n\n\n\nCode\nunique_on_age &lt;- data_trainees %&gt;% \n  dplyr::filter(trainees == 0) %&gt;% \n  dplyr::distinct(age, .keep_all = TRUE)\n\nmatches &lt;- data_trainees %&gt;% \n  dplyr::filter(trainees == 1) %&gt;% \n  dplyr::left_join(unique_on_age, by = 'age', suffix = c(\"_t_1\", \"_t_0\")) %&gt;% \n  dplyr::mutate(t1_minus_t0 = earnings_t_1 - earnings_t_0) \n\nmatches %&gt;% dplyr::slice_head(n=5) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nunit_t_1\ntrainees_t_1\nage\nearnings_t_1\nunit_t_0\ntrainees_t_0\nearnings_t_0\nt1_minus_t0\n\n\n\n\n1\n1\n28\n17700\n27\n0\n8800\n8900\n\n\n2\n1\n34\n10200\n34\n0\n24200\n-14000\n\n\n3\n1\n29\n14400\n37\n0\n6200\n8200\n\n\n4\n1\n25\n20800\n35\n0\n23300\n-2500\n\n\n5\n1\n29\n6100\n37\n0\n6200\n-100"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-4",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-4",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nIf we take the mean of this last column we get the ATET estimate while controlling for age. Notice how the estimate is now very positive, compared to the previous one where we used a simple difference in means.\n\n\n\nCode\nmatches %&gt;% dplyr::summarize(ATE = mean(t1_minus_t0)) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nATE\n\n\n\n\n2458\n\n\n\n\n\n\n\n\nThis is a contrived example, just to introduce matching."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-5",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-5",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nIn practice, we usually have more than one feature and units donâ€™t match perfectly. In this case, we have to define some measurement of proximity to compare how units are close to each other.\nOne common metric for this is the euclidean norm \\(||X_i - X_j||\\). This difference, however, is not invariant to the scale of the features.\nThis means that features like age, that take values on the tenths, will be much less important when computing this norm compared to features like income, which take the order of hundreds.\nFor this reason, before applying the norm, we need to scale the features so that they are on roughly the same scale."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-6",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-6",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nHaving defined a distance measure, we can now define the match as the nearest neighbour to that sample we wish to match.\nWe can write the matching estimator the following way:\n\n\\[\n\\hat{ATE} = \\frac{1}{N} \\sum^N_{i=1} (2D_i - 1)\\big(Y_i - Y_{jm}(i)\\big)\n\\]\n\nWhere \\(Y_{jm}(i)\\) is the sample from the other treatment group which is most similar to \\(Y_i\\).\nWe do this \\(2T_i - 1\\) to match both ways: treated with controls and controls with the treatment."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-7",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-7",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nTo test this estimator, letâ€™s consider a medicine example.\nOnce again, we want to find the effect of a medication on days until recovery.\nUnfortunately, this effect is confounded by severity, sex and age. We have reasons to believe that patients with more severe conditions have a higher chance of receiving the medicine.\n\n\n\nCode\ndata_med &lt;- readr::read_csv(\"data/medicine_impact_recovery.csv\", show_col_types = FALSE)\n\ndata_med %&gt;% \n  dplyr::slice_head(n=5) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nsex\nage\nseverity\nmedication\nrecovery\n\n\n\n\n0\n35.05\n0.8877\n1\n31\n\n\n1\n41.58\n0.8998\n1\n49\n\n\n1\n28.13\n0.4863\n0\n38\n\n\n1\n36.38\n0.3231\n0\n35\n\n\n0\n25.09\n0.2090\n0\n15"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-8",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-8",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nIf we look at a simple difference in means, \\(E[Y|D=1]-E[Y|D=0]\\), we get that the treatment takes, on average, 16.9 more days to recover than the untreated.\nThis is probably due to confounding, since we donâ€™t expect the medicine to cause harm to the patient.\n\n\n\nCode\ndata_med %&gt;% \n  dplyr::group_by(medication) %&gt;% \n  dplyr::summarize(mean_effect = mean(recovery)) %&gt;% \n  dplyr::mutate(ATE = mean_effect - dplyr::lag( mean_effect) ) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nmedication\nmean_effect\nATE\n\n\n\n\n0\n21.68\nNA\n\n\n1\n38.57\n16.9"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-9",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-9",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nTo correct for this bias, we will control for X using matching.\nFirst, we need to remember to scale our features, otherwise, features like age will have higher importance than features like severity when we compute the distance between points.\nTo do so, we can standardise the features.\n\n\n\nCode\ndata_med %&lt;&gt;% recipes::recipe(recovery ~ .) %&gt;% \n  recipes::update_role(medication, new_role = 'treatment') %&gt;% \n  recipes::step_normalize(recipes::all_predictors()) %&gt;% \n  recipes::prep() %&gt;% \n  recipes::bake(new_data=NULL)\n\ndata_med %&gt;%   \n  dplyr::slice_head(n=5) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nsex\nage\nseverity\nmedication\nrecovery\n\n\n\n\n-0.997\n0.2808\n1.4598\n1\n31\n\n\n1.003\n0.8654\n1.5022\n1\n49\n\n\n1.003\n-0.3387\n0.0578\n0\n38\n\n\n1.003\n0.3995\n-0.5126\n0\n35\n\n\n-0.997\n-0.6105\n-0.9111\n0\n15"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-10",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-10",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nInstead of coding a matching function, we will use the K nearest neighbour algorithm from caret::knnreg.\nThis algorithm makes predictions by finding the nearest data point in an estimation or training set.\nFor matching, we will need 2 of those. One, \\(mt_0\\), will store the untreated points and will find matches in the untreated when asked to do so.\nThe other, \\(mt_1\\), will store the treated point and will find matches in the treated when asked to do so. After this fitting step, we can use these KNN models to make predictions, which will be our matches."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-11",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-11",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\n\nCode\ntreated   &lt;- data_med %&gt;% dplyr::filter(medication==1)\nuntreated &lt;- data_med %&gt;% dplyr::filter(medication==0)\n\nmt0 &lt;- # untreated knn model predicting recovery\n  caret::knnreg(x = untreated %&gt;% dplyr::select(sex,age,severity), y = untreated$recovery, k=1)\nmt1 &lt;- # treated knn model predicting recovery\n  caret::knnreg(x = treated %&gt;% dplyr::select(sex,age,severity), y = treated$recovery, k=1)\n\npredicted &lt;-\n  # combine the treated and untreated matches\n  c(\n    # find matches for the treated looking at the untreated knn model\n    treated %&gt;%\n      tibble::rowid_to_column(\"ID\") %&gt;% \n      split(.$ID) %&gt;% \n      purrr::map(\n        (\\(x){\n          x %&gt;% \n            dplyr::mutate(\n              match = predict( mt0, x[1,c('sex','age','severity')] )\n            )\n        })\n      )\n    # find matches for the untreated looking at the treated knn model\n    , untreated %&gt;%\n        tibble::rowid_to_column(\"ID\") %&gt;% \n        split(.$ID) %&gt;% \n        purrr::map(\n          (\\(x){\n            x %&gt;% \n              dplyr::mutate(\n                match = predict( mt1, x[1,c('sex','age','severity')] )\n              )\n          })\n        )\n  ) %&gt;%\n  # bind the treated and untreated data \n  dplyr::bind_rows()\n\npredicted %&gt;%   \n  dplyr::slice_head(n=5) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(columns = c('sex','age','severity'), decimals = 6) %&gt;%\n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nID\nsex\nage\nseverity\nmedication\nrecovery\nmatch\n\n\n\n\n1\nâˆ’0.996980\n0.280787\n1.459800\n1\n31\n39\n\n\n2\n1.002979\n0.865375\n1.502164\n1\n49\n52\n\n\n3\nâˆ’0.996980\n1.495134\n1.268540\n1\n38\n46\n\n\n4\n1.002979\nâˆ’0.106534\n0.545911\n1\n34\n45\n\n\n5\nâˆ’0.996980\n0.043034\n1.428732\n1\n30\n39"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-estimator-12",
    "href": "slides/BSMM_8740_lec_08.html#matching-estimator-12",
    "title": "Causality Part 2",
    "section": "Matching estimator",
    "text": "Matching estimator\n\nWith the matches, we can now apply the matching estimator formula\n\n\\[\n\\hat{ATE} = \\frac{1}{N} \\sum^N_{i=1} (2D_i - 1)\\big(Y_i - Y_{jm}(i)\\big)\n\\]\n\n\nCode\npredicted %&gt;% \n  dplyr::summarize(\"ATE (est)\" = mean( (2*medication - 1) * (recovery - match) )) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn() |&gt; \n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nATE (est)\n\n\n\n\n-0.9954\n\n\n\n\n\n\n\n\nUsing this sort of matching, we can see that the effect of the medicine is not positive anymore.\nThis means that, controlling for \\(X\\), the medicine reduces the recovery time by about 1 day, on average.\nThis is already a huge improvement on top of the biased estimate that predicted a 16.9 increase in recovery time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nIt turns out the matching estimator as weâ€™ve designed above is biased.\nTo see this, letâ€™s consider the ATET estimator, instead of the ATE, just because it is simpler to write.\nThe intuition will apply to the ATE as well.\n\n\\[\n\\hat{ATET} = \\frac{1}{N_1}\\sum (Y_i - Y_j(i))\n\\]\n\nwhere \\(N_1\\) is the number of treated individuals and \\(Y_j(i)\\) is the untreated match of treated unit i."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-1",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nTo check for bias, what we do is hope we can apply the Central Limit Theorem so that this estimate converges to a normal distribution with mean zero.\n\n\\[\n\\sqrt{N_1}(\\hat{ATET} - ATET)\n\\]\n\nHowever, this doesnâ€™t alway happen. If we define the mean outcome for the untreated given X, \\(\\mu_0(x)=E[Y|X=x, D=0]\\), we will have that\n\n\\[\n\\begin{align*}\n\\mathbb{E}\\left [\\sqrt{N_1}(\\hat{ATET} - ATET)\\right] & = \\\\\n\\mathbb{E}\\left[\\sqrt{N_1}(\\mu_0(X_i) - \\mu_0(X_j(i))\\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-2",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nNow, \\(\\mu_0(X_i) - \\mu_0(X_j(i))\\) is not so simple to understand, so letâ€™s look at it more carefully.\n\n\\(\\mu_0(X_i)\\) is the outcome Y value of a treated unit \\(i\\) had it not been treated.\nIt is the counterfactual outcome \\(Y^0\\) for unit \\(i\\). \\(\\mu_0(X_j(i))\\) is the outcome of the untreated unit \\(j\\) that is the match of unit \\(i\\).\nIt is also the \\(Y^0\\), but for unit \\(j\\) now.\nOnly this time, it is a factual outcome, because \\(j\\) is in the non treated group.\nNow, because \\(j\\) and \\(i\\) are only similar, but not the same, this will likely not be zero. In other words, \\(X_i \\approx X_j\\), so, \\(Y^0_i \\approx Y^0_j\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-3",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-3",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nAs we increase the sample size, there will be more units to match, so the difference between unit \\(i\\) and its match \\(j\\) will also get smaller.\nBut this difference converges to zero slowly.\nAs a result \\(E[\\sqrt{N_1}(\\mu_0(X_i) - \\mu_0(X_j(i)))]\\) may not converge to zero, because the \\(\\sqrt{N_1}\\) grows faster than \\((\\mu_0(X_i) - \\mu_0(X_j(i)))\\) diminishes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-4",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-4",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nBias arises when the matching discrepancies are huge. Fortunately, we know how to correct it. Each observation contributes \\((\\mu_0(X_i) - \\mu_0(X_j(i))\\) to the bias so all we need to do is subtract this quantity from each matching comparison in our estimator.\nTo do so, we can replace \\(\\mu_0(X_j(i))\\) with some sort of estimate of this quantity \\(\\hat{\\mu}_0(X_j(i))\\), which can be obtained with models like linear regression. This updates the ATET estimator to the following equation:\n\n\\[\n\\hat{ATET} = \\frac{1}{N_1}\\sum \\big((Y_i - Y_{j(i)}) - (\\hat{\\mu_0}(X_i) - \\hat{\\mu_0}(X_{j(i)})\\big)\n\\]\n\nwhere \\(\\hat{\\mu_0}(x)\\) is some estimative of \\(E[Y|X, D=0]\\), like a linear regression fitted only on the untreated sample."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-5",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-5",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\n\nCode\nols0 &lt;- lm(recovery ~ sex + age + severity, data = untreated) \nols1 &lt;- lm(recovery ~ sex + age + severity, data = treated) \n\n# find the units that match to the treated\ntreated_match_index &lt;- # RANN::nn2 does Nearest Neighbour Search\n  (RANN::nn2(mt0$learn$X, treated %&gt;% dplyr::select(sex,age,severity), k=1))$nn.idx %&gt;% \n  as.vector()\n\n# find the units that match to the untreated\nuntreated_match_index &lt;- # RANN::nn2 does Nearest Neighbour Search\n  (RANN::nn2(mt1$learn$X, untreated %&gt;% dplyr::select(sex,age,severity), k=1))$nn.idx %&gt;% \n  as.vector()\n\npredicted &lt;- \nc(\n  purrr::map2(\n    .x = \n      treated %&gt;% tibble::rowid_to_column(\"ID\") %&gt;% split(.$ID) \n    , .y = treated_match_index\n    , .f = (\\(x,y){\n          x %&gt;% \n            dplyr::mutate(\n              match = predict( mt0, x[1,c('sex','age','severity')] )\n              , bias_correct =\n                  predict( ols0, x[1,c('sex','age','severity')] ) -\n                  predict( ols0, untreated[y,c('sex','age','severity')] )\n            )\n        })\n  )\n  , purrr::map2(\n    .x = \n      untreated %&gt;% tibble::rowid_to_column(\"ID\") %&gt;% split(.$ID) \n    , .y = untreated_match_index\n    , .f = (\\(x,y){\n          x %&gt;% \n            dplyr::mutate(\n              match = predict( mt1, x[1,c('sex','age','severity')] )\n              , bias_correct =\n                  predict( ols1, x[1,c('sex','age','severity')] ) -\n                  predict( ols1, treated[y,c('sex','age','severity')] )\n            )\n        })\n  )\n) %&gt;% \n  # bind the treated and untreated data \n  dplyr::bind_rows()\n\npredicted %&gt;%   \n  dplyr::slice_head(n=5) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(columns = c('sex','age','severity'), decimals = 6) %&gt;%\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nID\nsex\nage\nseverity\nmedication\nrecovery\nmatch\nbias_correct\n\n\n\n\n1\nâˆ’0.996980\n0.280787\n1.459800\n1\n31\n39\n4.404\n\n\n2\n1.002979\n0.865375\n1.502164\n1\n49\n52\n12.915\n\n\n3\nâˆ’0.996980\n1.495134\n1.268540\n1\n38\n46\n1.871\n\n\n4\n1.002979\nâˆ’0.106534\n0.545911\n1\n34\n45\n-0.497\n\n\n5\nâˆ’0.996980\n0.043034\n1.428732\n1\n30\n39\n2.610"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-6",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-6",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nDoesnâ€™t this defeat the point of matching? If I have to run a linear regression anyway, why donâ€™t I use only that, instead of this complicated model.\nFirst of all, this linear regression that we are fitting doesnâ€™t extrapolate on the treatment dimension to get the treatment effect. Instead, its purpose is just to correct bias.\nLinear regression here is local, in the sense that it doesnâ€™t try to see how the treated would be if it looked like the untreated. It does none of that extrapolation. This is left to the matching part.\nThe meat of the estimator is still the matching component. The point is that OLS is secondary to this estimator."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-7",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-7",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\n\nThe second point is that matching is a non-parametric estimator. It doesnâ€™t assume linearity or any kind of parametric model.\nAs such, it is more flexible than linear regression and can work in situations where linear regression will not, namely, those where non linearity is very strong."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-8",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-8",
    "title": "Causality Part 2",
    "section": "Matching bias",
    "text": "Matching bias\nWith the bias correction formula, I get the following ATE estimation.\n\npredicted %&gt;% \n  dplyr::summarize(\n    \"ATE (est)\" = \n      mean( (2*medication - 1) * (recovery - match - bias_correct) )) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nATE (est)\n\n\n\n\n-7.363"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-ci",
    "href": "slides/BSMM_8740_lec_08.html#matching-ci",
    "title": "Causality Part 2",
    "section": "Matching CI",
    "text": "Matching CI\n\nOf course, we also need to place a confidence interval around this measurement.\nIn practice, we can simply use someone elseâ€™s code and just import a matching estimator. Here is one from the library Matching.\n\n\n\nCode\nrequire(Matching)\n#  See https://www.jsekhon.com for additional documentation.\nc(\"ATE\", \"ATC\", \"ATT\") %&gt;% \n  purrr::map(\n    (\\(x){\n      pairmatching &lt;- Matching::Match(\n        Y = data_med$recovery\n        , Tr = data_med$medication\n        , X = data_med %&gt;% dplyr::select('sex','age','severity')\n        , estimand = x\n        , BiasAdjust = TRUE\n      )\n      tibble::tibble(measure = x, est = pairmatching$est[1,1], se = pairmatching$se)\n    })\n  ) %&gt;% \n  dplyr::bind_rows() %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(columns = c('est','se'), decimals = 3) %&gt;%\n  gt::tab_header(\n    title = \"Treatment Effect Estimates: Matching\"\n    , subtitle = \"using R package Matching\"\n  ) %&gt;% \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nTreatment Effect Estimates: Matching\n\n\nusing R package Matching\n\n\nmeasure\nest\nse\n\n\n\n\nATE\nâˆ’7.708\n0.962\n\n\nATC\nâˆ’6.664\n1.644\n\n\nATT\nâˆ’9.680\n0.123\n\n\n\n\n\n\n\n\nFinally, we can say with confidence that our medicine does indeed lower the time someone spends at the hospital. The ATE estimate is just a little bit lower than our algorithm, due to the difference in tie breaking of matches of knn implementation and the Matching R package."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-again",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-again",
    "title": "Causality Part 2",
    "section": "Matching bias again",
    "text": "Matching bias again\n\nWe saw that matching is biased when the unit and its match are not so similar. But what causes them to be so different?\nAs it turns out, the answer is quite simple and intuitive. It is easy to find people that match on a few characteristics, like sex. But if we add more characteristics, like age, income, city of birth and so on, it becomes harder and harder to find matches. In more general terms, the more features we have, the higher will be the distance between units and their matches.\nThis is not something that hurts only the matching estimator. It ties back to the subclassification estimator we saw earlier. In that contrived medicine example where with man and woman, it was quite easy to build the subclassification estimator. That was because we only had 2 cells: man and woman. But what would happen if we had more?\nLetâ€™s say we have 2 continuous features like age and income and we manage to discretise them into 5 buckets each. This will give us 25 cells, or \\(5^2\\). And what if we had 10 covariates with 3 buckets each? Doesnâ€™t seem like a lot right? Well, this would give us 59049 cells, or \\(3^{10}\\). Itâ€™s easy to see how this can blow out of proportion pretty quickly. This is a phenomena pervasive in all data science, which is called the The Curse of Dimensionality!!!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-again-1",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-again-1",
    "title": "Causality Part 2",
    "section": "Matching bias again",
    "text": "Matching bias again\n\nIn the context of the subclassification estimator, the curse of dimensionality means that it will suffer if we have lots of features.\nLots of features imply multiple cells in X. If there are multiple cells, some of them will have very few data. Some of them might even have only treated or only control, so it wonâ€™t be possible to estimate the ATE there, which would break our estimator.\nIn the matching context, this means that the feature space will be very sparse and units will be very far from each other. This will increase the distance between matches and cause bias problems."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#matching-bias-again-2",
    "href": "slides/BSMM_8740_lec_08.html#matching-bias-again-2",
    "title": "Causality Part 2",
    "section": "Matching bias again",
    "text": "Matching bias again\n\nAs for linear regression, it actually handles this problem quite well.\nWhat it does is project all the features X into a single one, the Y dimension. It then makes treatment and control comparison on that projection.\nSo, in some way, linear regression performs some sort of dimensionality reduction to estimate the ATE. Itâ€™s quite elegant."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nThree Billboards in the South of Brazil\nTo figure out how good billboards were as a marketing channel, A bank placed 3 billboards in the city of Porto Alegre, the capital of the state of Rio Grande do Sul.\nThey wanted to see if that boosted deposits into our savings account.\nRio Grande do Sul is part of the south of Brazil, one of the most developed regions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-1",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-1",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nThree Billboards in the South of Brazil\n\nData was also obtained from another capital from the south, Florianopolis, the capital city of the state of Santa Catarina in Brasil. The idea is that Florianopolis could be used as a control sample to estimate the counterfactual when compared to Porto Alegre. The billboard was placed in Porto Alegre for the entire month of June. The resulting data like this:\n\n\n\nCode\ndata_bboard &lt;- readr::read_csv(\"data/billboard_impact.csv\", show_col_types = FALSE)\n\ndata_bboard %&gt;% \n  dplyr::slice_head(n=5) %&gt;% \n  gt::gt() %&gt;% \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\ndeposits\npoa\njul\n\n\n\n\n42\n1\n0\n\n\n0\n1\n0\n\n\n52\n1\n0\n\n\n119\n1\n0\n\n\n21\n1\n0\n\n\n\n\n\n\n\n\nHere deposits are our outcome variable, the one the bank wishes to increase with the billboards. POA is a dummy variable for the city of Porto Alegre. When it is zero, it means the samples are from Florianopolis. Jul is a dummy for the month of July, or for the post intervention period. When it is zero it refers to samples from May, the pre-intervention period."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-2",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-2",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nLet \\(Y^D(T)\\) be the potential outcome for treatment D on period T. In an ideal world where we have the ability to observe the counterfactual, we would estimate the treatment effect of an intervention the following way:\n\n\\[\n\\hat{ATET} = E[Y^1(1) - Y^0(1)|D=1]\n\\]\n\nIn words, the causal effect is the outcome in the period post intervention in case of a treatment minus the outcome in also in the period after the intervention, but in the case of no treatment. Of course, we canâ€™t measure this because \\(Y^0(1)\\) is counterfactual."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-3",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-3",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\nOne way around this is a before and after comparison.\n\\[\n\\hat{ATET} = E[Y(1)|D=1] - E[Y(0)|D=1]\n\\]\nIn our example, we would compare the average deposits from POA before and after the billboard was placed.\n\n\nCode\npoa_before &lt;- data_bboard |&gt; dplyr::filter(poa==1 & jul==0) |&gt; dplyr::pull(deposits) |&gt; mean()\npoa_after  &lt;- data_bboard |&gt; dplyr::filter(poa==1 & jul==1) |&gt; dplyr::pull(deposits) |&gt; mean()\npoa_after - poa_before\n\n\n[1] 41.05\n\n\nThis estimator is telling us that we should expect deposits to increase R\\(\\$ 41.04\\) after the intervention. But can we trust this?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-4",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-4",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nNotice that \\(E[Y(0)|D=1]=E[Y^0(0)|D=1]\\), the observed outcome for the treated unit before the intervention is equal to the counterfactual outcome for the treated unit also before the intervention. Since we are using this to estimate the counterfactual after the intervention \\(E[Y^0(1)|D=1]\\), this estimation above assumes that \\(E[Y^0(1)|D=1] = E[Y^0(0)|D=1]\\).\nIt is saying that in the case of no intervention, the outcome in the latter period would be the same as the outcome from the starting period. This would obviously be false if your outcome variable follows any kind of trend.\nFor example, if deposits are going up in POA, \\(E[Y^0(1)|D=1] &gt; E[Y^0(0)|D=1]\\), i.e.Â the outcome of the latter period would be greater than that of the starting period even in the absence of the intervention. With a similar argument, if the trend in Y is going down, \\(E[Y^0(1)|D=1] &lt; E[Y^0(0)|D=1]\\). This is to show that this before and after thing is not a great estimator."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-5",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-5",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\nAnother idea is to compare the treated group with an untreated group that didnâ€™t get the intervention:\n\\[\n\\hat{ATET} = E[Y(1)|D=1] - E[Y(1)|D=0]\n\\]\nIn our example, it would be to compare the deposits from POA to that of Florianopolis in the post intervention period.\n\n\nCode\nfl_after  &lt;- data_bboard |&gt; dplyr::filter(poa==0 & jul==1) |&gt; dplyr::pull(deposits) |&gt; mean()\npoa_after - fl_after\n\n\n[1] -119.1"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-6",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-6",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nThis estimator is telling us that the campaign is detrimental and that customers will decrease deposits by R\\(\\$ 119.10\\).\nNotice that \\(E[Y(1)|D=0]=E[Y^0(1)|D=0]\\). And since we are using \\(E[Y(1)|D=0]\\) to estimate the counterfactual for the treated after the intervention, we are assuming we can replace the missing counterfactual like this: \\(E[Y^0(1)|D=0] = E[Y^0(1)|D=1]\\).\nBut notice that this would only be true if both groups have a very similar baseline level. For instance, if Florianopolis has way more deposits than Porto Alegre, this would not be true because \\(E[Y^0(1)|D=0] &gt; E[Y^0(1)|D=1]\\). On the other hand, if the level of deposits are lower in Florianopolis, we would have \\(E[Y^0(1)|D=0] &lt; E[Y^0(1)|D=1]\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-7",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-7",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nIt is saying that in the case of no intervention, the outcome in the latter period would be the same as the outcome from the starting period. This would obviously be false if your outcome variable follows any kind of trend.\nFor example, if deposits are going up in POA, \\(E[Y^0(1)|D=1] &gt; E[Y^0(0)|D=1]\\), i.e.Â the outcome of the latter period would be greater than that of the starting period even in the absence of the intervention.\nWith a similar argument, if the trend in Y is going down, \\(E[Y^0(1)|D=1] &lt; E[Y^0(0)|D=1]\\). This is to show that this before and after thing is not a great estimator."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-8",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-8",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\nAnother idea is to compare the treated group with an untreated group that didnâ€™t get the intervention:\n\\[\n\\hat{ATET} = E[Y(1)|D=1] - E[Y(1)|D=0]\n\\]\nIn our example, it would be to compare the deposits from POA to that of Florianopolis in the post intervention period.\n\n\n[1] -119.1\n\n\nThis estimator is telling us that the campaign is detrimental and that customers will decrease deposits by R$ 119.10."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-9",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-9",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nNotice that \\(E[Y(1)|D=0]=E[Y^0(1)|D=0]\\). And since we are using \\(E[Y(1)|D=0]\\) to estimate the counterfactual for the treated after the intervention, we are assuming we can replace the missing counterfactual like this: \\(E[Y^0(1)|D=0] = E[Y^0(1)|D=1]\\). But notice that this would only be true if both groups have a very similar baseline level.\nFor instance, if Florianopolis has way more deposits than Porto Alegre, this would not be true because \\(E[Y^0(1)|D=0] &gt; E[Y^0(1)|D=1]\\).\nOn the other hand, if the level of deposits are lower in Florianopolis, we would have \\(E[Y^0(1)|D=0] &lt; E[Y^0(1)|D=1]\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-10",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-10",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nAgain, this is not a great idea. To solve this, we can use both space and time comparison. This is the idea of the difference in difference approach. It works by replacing the missing counterfactual the following way:\n\n\\[\n\\begin{align*}\nE[Y^0(1)|D=1] & = E[Y^0(0)|D=1] + \\\\\n& (E[Y^0(1)|D=0] - E[Y^0(0)|D=0])\n\\end{align*}\n\\]\n\nWhat this does is take the treated unit before the intervention and adds a trend component to it, which is estimated using the control \\(E[Y^0(1)|D=0] - E[Y^0(0)|D=0]\\). In words, it is saying that the treated after the intervention, had it not been treated, would look like the treated before the treatment plus a growth factor that is the same as the growth of the control."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-11",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-11",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nIt is important to notice that this assumes that the trends in the treatment and control are the same:\n\n\\[\nE[Y^0(1) âˆ’ Y^0(0)|D=1] = E[Y^0(1) âˆ’ Y^0(0)|D=0]\n\\]\n\nwhere the left hand side is the counterfactual trend. Now, we can replace the estimated counterfactual in the treatment effect definition\n\n\\(E[Y^1(1)|D=1] - E[Y^0(1)|D=1]\\)\n\\[\n\\hat{ATET} = E[Y(1)|D=1] - (E[Y(0)|D=1] + (E[Y(1)|D=0] - E[Y(0)|D=0])\n\\]\nIt gets that name because it gets the difference between the difference between treatment and control after and before the treatment."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-12",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-12",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nHere is what that looks in code.\n\n\n\nCode\nfl_before &lt;- data_bboard |&gt; dplyr::filter(poa==0 & jul==0) |&gt; dplyr::pull(deposits) |&gt; mean()\ndiff_in_diff = (poa_after-poa_before)-(fl_after-fl_before)\ndiff_in_diff\n\n\n[1] 6.525\n\n\n\nDiff-in-Diff is telling us that we should expect deposits to increase by R\\(\\$ 6.52\\) per customer.\nNotice that the assumption that diff-in-diff makes is much more plausible than the other 2 estimators. It just assumes that the growth pattern between the 2 cities are the same. But it doesnâ€™t require them to have the same base level nor does it require the trend to be zero."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-13",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-13",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nTo visualize what diff-in-diff is doing, we can project the growth trend from the untreated into the treated to see the counterfactual, that is, the number of deposits we should expect if there were no intervention.\n\n\n\nThe small difference between the red and the blue dashed lines shows the small treatment effect on Porto Alegre."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-14",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-14",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nBut how much can we trust this estimator? To get standard errors we use a neat trick that uses regression. Specifically, we will estimate the following linear model\n\n\\[\nY_i = \\beta_0 + \\beta_1 POA_i + \\beta_2 Jul_i + \\beta_3 POA_i*Jul_i + e_i\n\\]\n\nNotice that \\(\\beta_0\\) is the baseline of the control. In our case, is the level of deposits in Florianopolis in the month of May.\nIf we turn on the treated city dummy, we get \\(\\beta_1\\). So \\(\\beta_0+\\beta_1\\) is the baseline of Porto Alegre in May, before the intervention, and \\(\\beta_1\\) is the increase of Porto Alegre baseline on top of Florianopolis.\nIf we turn the POA dummy off and turn the July dummy on, we get \\(\\beta_0+\\beta_2\\), which is the level of FlorianÃ³polis in July, after the intervention period. \\(\\beta_2\\) is then the trend of the control, since we add it on top of the baseline to get the level of the control at the period post intervention."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-15",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-15",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nDID Estimator\n\nAs a recap, \\(\\beta_1\\) is the increment we get by going from the control to the treated, \\(\\beta_2\\) is the increment we get by going from the period before to the period after the intervention. Finally, if we turn both dummies on, we get \\(\\beta_3\\). \\(\\beta_0+\\beta_1+\\beta_2+\\beta_3\\) is the level in Porto Alegre after the intervention. So \\(\\beta_3\\) is the incremental impact when you go from May to July and from Florianopolis to POA. In other words, it is the Difference in Difference estimator.\n\n\n\n# A tibble: 4 Ã— 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   172.        2.36     72.6  0        \n2 poa          -126.        4.48    -28.0  1.39e-159\n3 jul            34.5       3.04     11.4  1.43e- 29\n4 poa:jul         6.52      5.73      1.14 2.55e-  1"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-16",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-16",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nNon Parallel Trends\n\nOne obvious problem with Diff-in-Diff is failure to satisfy the parallel trend assumption. If the growth trend from the treated is different from the trend of the control, diff-in-diff will be biased.\nThis is a common problem with non-random data, where the decision to treat a region is based on its potential to respond well to the treatment, or when the treatment is targeted at regions that are not performing very well. In our marketing example we decided to test billboards in Porto Alegre not in order to check the effect of billboards in general. The reason is simply because sales perform poorly there.\nPerhaps online marketing is not working there. In this case, it could be that the growth we would see in Porto Alegre without a billboard would be lower than the growth we observe in other cities. This would cause us to underestimate the effect of the billboard there."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-17",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-17",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nNon Parallel Trends\n\nOne way to check if this is happening is to plot the trend using past periods. For example, letâ€™s suppose POA had a small decreasing trend but Florianopolis was on a steep ascent. In this case, showing periods from before would reveal those trends and we would know Diff-in-Diff is not a reliable estimator."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#difference-in-differences-18",
    "href": "slides/BSMM_8740_lec_08.html#difference-in-differences-18",
    "title": "Causality Part 2",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nNon Parallel Trends\n\nOne final issue that itâ€™s worth mentioning is that you wonâ€™t be able to place confidence intervals around your Diff-in-Diff estimator if you only have aggregated data.\nSay for instance you donâ€™t have data on what each of our customers from FlorianÃ³polis or Porto Alegre did. Instead, you only have the average deposits before and after the intervention for both cities.\nIn this case, you will still be able to estimate the causal effect by Diff-in-Diff, but you wonâ€™t know the variance of it. Thatâ€™s because all the variability in your data got squashed out in aggregation."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\n\nIn the simple Diff-in-Diff setup, we have a treated and a control group (the city of POA and FLN, respectively) and only two periods, a pre-intervention and a post-intervention period. But what would happen if we had more periods? Or more groups?\nThis setup is so common and powerful for causal inference and gets its own name: panel data. A panel is when we have repeated observations of the same unit over multiple periods of time. This is incredibly common in the industry, where companies track user data over multiple weeks and months.\nTo understand how we can leverage such data structure, letâ€™s first continue with our diff-in-diff example, where we wanted to estimate the impact of placing a billboard (treatment) in the city of Porto Alegre (POA). We want to see if that sort of offline marketing strategy can boost the usage of our investment products. Specifically, we want to know how much deposits in our investments account would increase if we placed billboards."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-1",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-1",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\n\nIn the previous section, we motivated the DiD estimator as an imputation strategy of what would have happened to Porto Alegre had we not placed the billboards in it.\nWe said that the counterfactual outcome \\(Y^0\\) for Porto Alegre after the intervention (placing a billboard) could be inputted as the number of deposits in Porto Alegre before the intervention plus a growth factor.\nThis growth factor was estimated in a control city, Florianopolis (FLN)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-2",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-2",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\n\nTo recap some notation, here is how we can estimate this counterfactual outcome\n\n\\[\n\\begin{split}\n\\underbrace{\\widehat{Y^0(1)|D=1}}_{\\substack{\\text{POA outcome after intervention}} \\\\ \\substack{\\text{if no intervention had taken place}}}\n= \\underbrace{Y^0(0)|D=1}_{\\substack{\\text{POA outcome}} \\\\ \\substack{\\text{before intervention}}}\n+ \\big( \\underbrace{Y^0(1)|D=0}_{\\substack{\\text{FLN outcome after}} \\\\ \\substack{\\text{intervention in POA}}}\n- \\underbrace{Y^0(0)|D=0}_{\\substack{\\text{FLN outcome before}} \\\\ \\substack{\\text{intervention in POA}}} \\big)\n\\end{split}\n\\]\n\nwhere \\(t\\) denotes time, \\(D\\) denotes the treatment (since \\(t\\) is taken), \\(Y^D(t)\\) denote the potential outcome for treatment \\(D\\) in period \\(t\\) (for example, \\(Y^0(1)\\) is the outcome under the control in period 1)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-3",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-3",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\n\nNow, if we take that imputed potential outcome, we can recover the treatment effect for POA (ATT) as follows\n\n\\[\n\\begin{split}\n\\widehat{ATT} = \\underbrace{Y^1(1)|D=1}_{\\substack{\\text{POA outcome} \\\\ \\text{after intervention}}} - \\widehat{Y^0(1)|D=1}\n\\end{split}\n\\]\n\nThe effect of placing a billboard in POA is the outcome we saw on POA after placing the billboard minus our estimate of what would have happened if we hadnâ€™t placed the billboard. Recall that the power of DiD comes from the fact that estimating the mentioned counterfactual only requires that the growth deposits in POA matches the growth in deposits in FLW. This is the key parallel trends assumption."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-4",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-4",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nParallel Trends\n\nOne way to see the parallel (or common) trends assumptions is as an independence assumption. Recall that the independence assumption requires that the treatment assignment is independent from the potential outcomes:\n\n\\[\nY^d \\perp  D\n\\]\n\nThis means we donâ€™t give more treatment to units with higher outcome (which would cause upward bias in the effect estimation) or lower outcome (which would cause downward bias).\nIn this example, letâ€™s say the marketing manager decides to add billboards only to cities that already have very high deposits. That way, he or she can later boast that cities with billboards generate more deposties, so of course the marketing campaign was a success.\nThis violates the independence assumption: we are giving the treatment to cities with high \\(Y^0\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-5",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-5",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nParallel Trends\n\nRecall that a natural extension of this assumption is the conditional independence assumption, which allows the potential outcomes to be dependent on the treatment at first, but independent once we condition on the confounders \\(X\\)\n\n\\[\nY^d \\perp D | X\n\\]\n\nIf the traditional independence assumption states that the treatment assignment canâ€™t be related to the levels of potential outcomes, the parallel trends states that the treatment assignment canâ€™t be related to the growth in potential outcomes over time. In fact, one way to write the parallel trends assumption is as follows\n\n\\[\n\\big(Y^d(t) - Y^d(t-1) \\big)  \\perp D\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-6",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-6",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nParallel Trends\n\nThis assumption says it is fine that we assign the treatment to units that have a higher or lower level of the outcome. What we canâ€™t do is assign the treatment to units based on how the outcome is growing.\nIn our billboard example, this means it is OK to place billboards only in cities with originally high deposits level. What we canâ€™t do is place billboards only in cities where the deposits are growing the most.\nRemember that DiD is imputting the counterfactual growth in the treated unit with the growth in the control unit. If growth in the treated unit under the control is different than the growth in the control unit, then we are in trouble."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-7",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-7",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nControlling What you Cannot See\n\nMethods like propensity score, linear regression and matching are very good at controlling for confounding in non-random data, but they rely on a key assumption: conditional unconfoundedness\n\n\\[\n(Y^0, Y^1) \\perp D | X\n\\]\n\nTo put it in words, they require that all the confounders are known and measured, so that we can condition on them and make the treatment as good as random."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-8",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-8",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nControlling What you Cannot See\n\nOne major issue is that sometimes we simply canâ€™t measure a confounder. For instance, take a classical labor economics problem of figuring out the impact of marriage on menâ€™s earnings. Itâ€™s a well known fact in economics that married men earn more than single men. However, it is not clear if this relationship is causal or not.\nIt could be that more educated men are both more likely to marry and more likely to have a high earnings job, which would mean that education is a confounder of the effect of marriage on earnings. For this confounder, we could measure the education of the person in the study and run a regression controlling for that.\nBut another confounder could be beauty. It could be that more handsome men are both more likely to get married and more likely to have a high paying job. Unfortunately, beauty is one of those characteristics like intelligence. Itâ€™s something we canâ€™t measure very well."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-9",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-9",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\n\nCode\ndata_panel &lt;- readr::read_csv(\"data/wage_panel.csv\", show_col_types = FALSE)\n\ndata_panel %&gt;% \n  dplyr::slice_head(n=5) %&gt;% \n  gt::gt() %&gt;% \n  gt::tab_options( table.font.size = gt::px(28) ) |&gt;\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nnr\nyear\nblack\nexper\nhisp\nhours\nmarried\neduc\nunion\nlwage\nexpersq\noccupation\n\n\n\n\n13\n1980\n0\n1\n0\n2672\n0\n14\n0\n1.198\n1\n9\n\n\n13\n1981\n0\n2\n0\n2320\n0\n14\n1\n1.853\n4\n9\n\n\n13\n1982\n0\n3\n0\n2940\n0\n14\n0\n1.344\n9\n9\n\n\n13\n1983\n0\n4\n0\n2960\n0\n14\n0\n1.433\n16\n9\n\n\n13\n1984\n0\n5\n0\n3071\n0\n14\n0\n1.568\n25\n5\n\n\n\n\n\n\n\n\nGenerally, the fixed effect model is defined as\n\n\\[\ny_{it} = \\beta X_{it} + \\gamma U_i + e_{it}\n\\]\n\nwhere \\(y_{it}\\) is the outcome of individual \\(i\\) at time \\(t\\), \\(X_{it}\\) is the vector of variables for individual \\(i\\) at time \\(t\\). \\(U_i\\) is a set of unobservables for individual \\(i\\) .\nNotice that those unobservables are unchanging through time, hence the lack of the time subscript.\nFinally, \\(e_{it}\\) is the error term. For the education example, \\(y_{it}\\) is log wages, \\(X_{it}\\) are the observable variables that change in time, like marriage and experience and \\(U_i\\) are the variables that are not observed but constant for each individual, like beauty and intelligence."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-10",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-10",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nUsing panel data with a fixed effect model is as simple as adding a dummy for the entities.\nThis is true, but in practice, we donâ€™t actually do it. Imagine a dataset where we have 1 million customers. If we add one dummy for each of them, we would end up with 1 million columns, which is probably not a good idea. Instead, we use the trick of partitioning the linear regression into 2 separate models. Weâ€™ve seen this before, but now is a good time to recap it."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-11",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-11",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nSuppose you have a linear regression model with a set of features \\(X_1\\) and another set of features \\(X_2\\).\n\n\\[\n\\hat{Y} = \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2\n\\]\n\nwhere \\(X_1\\) and \\(X_2\\) are feature matrices (one row per feature and one column per observation) and \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_2}\\) are row vectors. You can get the exact same \\(\\hat{\\beta_1}\\) parameter by doing\n\nregress the outcome \\(y\\) on the second set of features \\(\\hat{y^*} = \\hat{\\gamma_1} X_2\\)\nregress the first set of features on the second \\(\\hat{X_1} = \\hat{\\gamma_2} X_2\\)\nobtain the residuals \\(\\tilde{X}_1 = X_1 - \\hat{X_1}\\) and \\(\\tilde{y}_1 = y_1 - \\hat{y^*}\\)\nregress the residuals of the outcome on the residuals of the features \\(\\hat{y} = \\hat{\\beta_1} \\tilde{X}_1\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-12",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-12",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\nThe parameter from this last regression will be exactly the same as running the regression with all the features.\nBut how exactly does this help us? Well, we can break the estimation of the model with the entity dummies into 2. First, we use the dummies to predict the outcome and the feature. These are steps 1 and 2 above."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-13",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-13",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\nRecall how running a regression on a dummy variable is as simple as estimating the mean for that dummy? Letâ€™s run a model where we predict wages as a function of the year dummy.\n\n\n# A tibble: 8 Ã— 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.39     0.0220     63.5  0       \n2 year1981       0.119    0.0311      3.84 1.22e- 4\n3 year1982       0.178    0.0311      5.74 1.02e- 8\n4 year1983       0.226    0.0311      7.27 4.21e-13\n5 year1984       0.297    0.0311      9.56 1.94e-21\n6 year1985       0.346    0.0311     11.1  1.93e-28\n7 year1986       0.406    0.0311     13.1  2.19e-38\n8 year1987       0.473    0.0311     15.2  4.40e-51"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-14",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-14",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nNotice how this model is predicting the average income in 1980 to be 1.3935, in 1981 to be 1.5129 (1.3935+0.1194) and so on. Now, if we compute the average by year, we get the exact same result. (Remember that the base year, 1980, is the intercept. So you have to add the intercept to the parameters of the other years to get the mean lwage for the year).\n\n\n\n\n\n\n\n\n\n\navg_lwage\n\n\n\n\n1980\n1.393\n\n\n1981\n1.513\n\n\n1982\n1.572\n\n\n1983\n1.619\n\n\n1984\n1.690\n\n\n1985\n1.739\n\n\n1986\n1.800\n\n\n1987\n1.866"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-15",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-15",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nThis means that if we get the average for every person in our panel, we are essentially regressing the individual dummy on the other variables. This motivates the following estimation procedure:\n\n\nCreate time-demeaned variables by subtracting the mean for the individual: \\[\n\\begin{align*}\n\\ddot{Y}_{it} & =Y_{it}-\\bar{Y}_{i}\\\\\n\\ddot{X}_{it} & =X_{it}-\\bar{X}_{i}\n\\end{align*}\n\\]\nRegress \\(\\ddot{Y}_{it}\\) on \\(\\ddot{X}_{it}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-16",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-16",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nNotice that when we do so, the unobserved \\(U_i\\) vanishes. Since \\(U_i\\) is constant across time, we have that \\(\\bar{U_i}=U_i\\). If we have the following system of two equations\n\n\\[\n\\begin{split}\n\\begin{align}\nY_{it} & = \\beta X_{it} + \\gamma U_i + e_{it} \\\\\n\\bar{Y}_{i} & = \\beta \\bar{X}_{it} + \\gamma \\bar{U}_i + \\bar{e}_{it} \\\\\n\\end{align}\n\\end{split}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-17",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-17",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nAnd we subtract one from the other, we get\n\n\\[\n\\begin{split}\n\\begin{align}\n(Y_{it} - \\bar{Y}_{i}) & = (\\beta X_{it} - \\beta \\bar{X}_{it}) + (\\gamma U_i - \\gamma U_i) + (e_{it}-\\bar{e}_{it}) \\\\\n(Y_{it} - \\bar{Y}_{i}) & = \\beta(X_{it} - \\bar{X}_{it}) + (e_{it}-\\bar{e}_{it}) \\\\\n\\ddot{Y}_{it} & = \\beta \\ddot{X}_{it} + \\ddot{e}_{it} \\\\\n\\end{align}\n\\end{split}\n\\]\n\nwhich wipes out all unobserved that are constant across time. To be honest, not only do the unobserved variables vanish. This happens to all the variables that are constant in time. For this reason, you canâ€™t include any variables that are constant across time, as they would be a linear combination of the dummy variables and the model wouldnâ€™t run."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-18",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-18",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nTo check which variables are those, we can group our data by individual and get the sum of the standard deviations. If it is zero, it means the variable isnâ€™t changing across time for any of the individuals.\n\n\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\nyear\n1335.0\n\n\nblack\n0.0\n\n\nexper\n1335.0\n\n\nhisp\n0.0\n\n\nhours\n203098.2\n\n\nmarried\n140.4\n\n\neduc\n0.0\n\n\nunion\n106.5\n\n\nlwage\n173.9\n\n\nexpersq\n17608.2\n\n\noccupation\n739.2"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-19",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-19",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nFor our data, we need to remove ethnicity dummies, black and hisp, since they are constant for the individual. Also, we need to remove education. We will also not use occupation, since this is probably mediating the effect of marriage on wage (it could be that single men are able to take more time demanding positions). Having selected the features we will use, itâ€™s time to estimate this model.\nTo run our fixed effect model, first, letâ€™s get our mean data. We can achieve this by grouping everything by individuals and taking the mean.\n\n\n\n\n\n\n\n\n\n\nmarried\nexpersq\nunion\nhours\nlwage\n\n\n\n\n13\n0.000\n25.5\n0.125\n2808\n1.256\n\n\n17\n0.000\n61.5\n0.000\n2504\n1.638\n\n\n18\n1.000\n61.5\n0.000\n2350\n2.034\n\n\n45\n0.125\n35.5\n0.250\n2226\n1.774\n\n\n110\n0.500\n77.5\n0.125\n2108\n2.055"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-20",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-20",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nTo demean the data, we need to set the index of the original data to be the individual identifier, nr. Then, we can simply subtract one data frame from the mean data frame.\n\n\n\n\n\n\n\n\n\n\nmarried\nexpersq\nunion\nhours\nlwage\n\n\n\n\n13\n0\n-24.5\n-0.125\n-135.6\n-0.05811\n\n\n13\n0\n-21.5\n0.875\n-487.6\n0.59741\n\n\n13\n0\n-16.5\n-0.125\n132.4\n0.08881\n\n\n13\n0\n-9.5\n-0.125\n152.4\n0.17756\n\n\n13\n0\n-0.5\n-0.125\n263.4\n0.31247"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-21",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-21",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nFinally, we can run our fixed effect model on the time-demeaned data.\n\n\n\n# A tibble: 5 Ã— 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -4.71e-17 0.00507   -9.28e-15 1.00e+  0\n2 married      1.15e- 1 0.0170     6.76e+ 0 1.61e- 11\n3 expersq      3.95e- 3 0.000180   2.20e+ 1 1.92e-101\n4 union        7.84e- 2 0.0184     4.26e+ 0 2.08e-  5\n5 hours       -8.46e- 5 0.0000125 -6.74e+ 0 1.74e- 11"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-22",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-22",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nIf we believe that fixed effect eliminates the all omitted variable bias, this model is telling us that marriage increases a manâ€™s wage by 11%. This result is very significant. One detail here is that for fixed effect models, the standard errors need to be clustered. So, instead of doing all our estimation by hand (which is only nice for pedagogical reasons), we can use the library linearmodels and set the argument cluster_entity to True.\n\n\n\n     term   estimate std.error statistic   p.value\n1 expersq  0.0039509 1.861e-04    21.232 1.208e-94\n2   union  0.0784442 1.991e-02     3.940 8.287e-05\n3 married  0.1146543 1.825e-02     6.281 3.739e-10\n4   hours -0.0000846 1.842e-05    -4.593 4.512e-06\n    conf.low  conf.high   df outcome\n1  0.0035861  4.316e-03 3811   lwage\n2  0.0394117  1.175e-01 3811   lwage\n3  0.0788661  1.504e-01 3811   lwage\n4 -0.0001207 -4.849e-05 3811   lwage"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-23",
    "href": "slides/BSMM_8740_lec_08.html#panel-data-and-fixed-effects-23",
    "title": "Causality Part 2",
    "section": "Panel Data and Fixed Effects",
    "text": "Panel Data and Fixed Effects\nFixed Effects\n\nNotice how the parameter estimates are identical to the ones weâ€™ve got with time-demeaned data. The only difference is that the standard errors are a bit larger. Now, compare this to the simple OLS model that doesnâ€™t take the time structure of the data into account. For this model, we add back the variables that are constant in time.\n\n\n\n# A tibble: 8 Ã— 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.265     0.0647        4.10  4.16e-  5\n2 expersq      0.00324   0.000206     15.7   2.14e- 54\n3 union        0.183     0.0173       10.6   6.27e- 26\n4 married      0.141     0.0158        8.93  6.11e- 19\n5 hours       -0.0000532 0.0000134    -3.98  7.05e-  5\n6 black       -0.135     0.0237       -5.68  1.44e-  8\n7 hisp         0.0132    0.0210        0.632 5.28e-  1\n8 educ         0.106     0.00469      22.6   1.38e-106\n\n\n\nThis model is saying that marriage increases the manâ€™s wage by 14%. A somewhat larger effect than the one we found with the fixed effect model. This suggests some omitted variable bias due to fixed individual factors, like intelligence and beauty, not being added to the model."
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#more",
    "href": "slides/BSMM_8740_lec_08.html#more",
    "title": "Causality Part 2",
    "section": "More",
    "text": "More\n\nRead Statistical tools for causal inference\nRead Causal inference in R"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08.html#recap",
    "href": "slides/BSMM_8740_lec_08.html#recap",
    "title": "Causality Part 2",
    "section": "Recap",
    "text": "Recap\n\nWe introduced the fundamental problems of inference and the biases of some intuitive estimators,\nWe developed a basic understanding of the tools used to state and then satisfy causality assumptions,\nWe looked at an example of how econometric methods recover treatment effects.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/misc_causality.html",
    "href": "slides/misc_causality.html",
    "title": "Misc Causality",
    "section": "",
    "text": "[Applied Causal Analysis in R](https://bookdown.org/paul/applied-causal-analysis/att.html)\n\n[Intro to Causality](https://digitalcausalitylab.github.io/lecture/lecture_1_intro/Lecture_1_intro.html#/introduction-to-causality-4)\n\n[Causal inference in R](https://www.r-causal.org/)\n\n[CausInfinR-github](https://github.com/r-causal/causal-inference-in-R/blob/main/chapters/chapter-05.qmd)\n\n[Statistical tools for causal inference](https://chabefer.github.io/STCI/index.html)\n\n[Causal graphs](https://github.com/NickCH-K/causalgraphs)\n\n[Causal data sets](https://cran.r-project.org/web/packages/causaldata/causaldata.pdf)\n\n[Causality slides](https://github.com/NickCH-K/CausalitySlides/tree/main)\n\n[more slides](https://users.ssc.wisc.edu/~felwert/causality/?page_id=66)\n\n[workshop](https://github.com/r-causal/causal_inference_r_workshop)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_09b.html#recap-of-last-week",
    "title": "Monte Carlo Methods",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced the fundamental problems of inference and the biases of some intuitive estimators.\nWe also built a basic understanding of the tools used to state and then satisfy causality assumptions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#this-week",
    "href": "slides/BSMM_8740_lec_09b.html#this-week",
    "title": "Monte Carlo Methods",
    "section": "This week",
    "text": "This week\n\nWe will get explore Monte Carlo methods as a way to integrate difficult functions, and sample from difficult probability distributions.\nAlong the way we will look at Markov Chains which both underlie sampling methods and provide a way to model the generation of data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#monte-carlo-mc-methods",
    "href": "slides/BSMM_8740_lec_09b.html#monte-carlo-mc-methods",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo (MC) Methods",
    "text": "Monte Carlo (MC) Methods\nMonte Carlo methods are a class of simulation-based methods that seek to avoid complicated and/or intractable mathematical computations.\nEspecially those that arise from probability distributions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nWe can use MC methods to estimate probabilities: for a random variable \\(Z\\) with outcomes in a set \\(\\Omega\\), with some subset \\(S\\subset\\Omega\\) and event \\(E\\equiv Z\\in S\\), we can compute the probability of \\(E\\) (i.e.Â \\(\\mathbb{P}(E)\\)) with of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) as\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-1",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-1",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample\nIf \\(Z\\sim\\mathscr{N}(1,3)\\) and \\(S=Z:0\\le Z\\le 3\\) then\n\\[\n\\mathbb{P}(E) = \\mathbb{P}(0\\le Z\\le 3) = \\int_0^3\\frac{1}{\\sqrt{2\\pi3}}e^{-\\frac{(t-1)^2}{2*3}}dt\n\\] In which case it is easier to just use R and calculate:\n\n\nCode\npnorm(3, mean=1, sd=sqrt(3)) - pnorm(0, mean=1, sd=sqrt(3))\n\n\n[1] 0.594\n\n\nthan it is to compute the integral."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-2",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-2",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample continued\nSurprisingly, in case we donâ€™t know that pnorm exists, we could do this:\n\n\nMC computation\n# define the event\nevent_E_happened &lt;- function( x ) {\n  if( 0 &lt;= x & x &lt;= 3 ) {\n    return( TRUE ) # The event happened\n  } else {\n    return( FALSE ) # The event DIDN'T happen\n  }\n}\n\n# Now MC says that we should generate lots of copies of Z...\nNMC &lt;- 10000; # 10000 seems like \"a lot\".\nrnorm( NMC, mean=1, sd=sqrt(3) ) |&gt; \n  purrr::map_lgl(event_E_happened) |&gt; \n  sum()/NMC\n\n\n[1] 0.6084"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-3",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-3",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nNow\n\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]\n\nis the MC estimate of \\(\\mathbb{E}[E]\\), which is unbiased because for each \\(i\\), \\(\\mathbb{E}[1_{z_i\\in S}]=\\mathbb{E}[E]\\), and the variance is\n\n\\[\n\\mathrm{Var}\\left({\\mathbb{P}(E)}\\right)=\\frac{1}{M^2}\\mathrm{Var}\\left(\\sum_{i=1}^M 1_{z_i\\in S}\\right)=\\frac{1}{M^2}\\sum_{i=1}^M \\mathrm{Var}\\left(1_{z_i\\in S}\\right)=\\frac{1}{M} \\mathrm{Var}\\left(1_{z_i\\in S}\\right)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-4",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-4",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nThis is true for any function of the random variable \\(Z\\), and\n\\[\n\\mathbb{E}\\left[h(Z)\\right]\\approx \\hat{h}=\\frac{1}{M}\\sum_{i=1}^M h(z_i)\n\\]\nand the variance of \\(h(Z)\\) decreases as \\(1/M\\) by the same reasoning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nMonte Carlo simulation starts with random number generation, usually split into 2 stages:\n\ngeneration of independent uniform \\((0, 1)\\) random variables, and\nconversion into random variables with a particular distribution (e.g.Â Normal)\n\nVery important: never write your own generator, always use a well validated generator from a reputable source (e.g.Â R)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation-1",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation-1",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nPseudo-random generators found in computers use a deterministic (i.e.Â repeatable) algorithm to generate a sequence of (apparently) random numbers on the \\((0, 1)\\) interval.\nWhat defines a good random number generator (RNG) has a long period â€“ how long it takes before the sequence repeats itself \\(2^{32}\\) is not enough (need at least \\(2^{40}\\)). various statistical tests to measure â€œrandomnessâ€ â€“ well validated software will have gone through these checks."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation-2",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation-2",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nRecall that \\(\\mathscr{N}(0, 1)\\) Normal random variables (mean 0, variance 1) have the probability density function:\n\\[\np(x)=\\frac{1}{2\\pi}e^{-\\frac{1}{2}x^2}\\equiv\\phi(x)\n\\] and if \\(X\\sim\\mathscr{N}(0, 1)\\) then its CDF is:\n\\[\n\\mathbb{P}[X\\le x] = \\int_{-\\infty}^x\\phi(x)dx\n\\] ## Random Number Generation\nThe Box-Muller transformation method takes two independent uniform \\((0, 1)\\) random numbers \\(y_1\\), \\(y_2\\), and defines:\n\\[\n\\begin{align*}\nx_{1} & =\\sqrt{-2\\log y_{1}}\\cos(2\\pi y_{2})\\\\\nx_2& =\\sqrt{-2\\log y_{1}}\\sin(2\\pi y_{2})\n\\end{align*}\n\\] It can be proved that \\(x_1\\) and \\(x_2\\) are independent \\(\\mathscr{N}(0, 1)\\) random variables"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation-3",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation-3",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nCode\nsamples &lt;- matrix(runif(10000), ncol=2) |&gt; data.frame() |&gt; \n  dplyr::mutate(\n    normals = \n      purrr::map2(\n        X1, X2\n        ,(\\(x1,x2){\n          data.frame(\n            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)\n            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) \n          )\n        })\n      )\n  ) |&gt; \n  tidyr::unnest(normals)  \n  \n\nsamples |&gt; \n  tidyr::pivot_longer(-c(X1,X2)) |&gt; \n  ggplot(aes(x=value, color=name, fill=name)) + \n  geom_histogram(aes(y=..density..), bins = 60, position=\"identity\", alpha=0.3) + \n  labs(x=\"Value\", y=\"Density\") + theme_minimal()\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in\nggplot2 3.4.0.\nâ„¹ Please use `after_stat(density)` instead.\n\n\nCode\nsamples |&gt; \nggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#random-number-generation-4",
    "href": "slides/BSMM_8740_lec_09b.html#random-number-generation-4",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nYour computer is only capable of producing pseudorandom numbers. These are made by running a pseudorandom number generator algorithm which is deterministic, e.g.\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\nOnce the RNG seed is set, the â€œrandomâ€ numbers that R generates arenâ€™t random at all. But someone looking at these random numbers would have a very hard time distinguishing these numbers from truly random numbers. That is what â€œstatistical randomnessâ€ means!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#central-limit-theorem",
    "href": "slides/BSMM_8740_lec_09b.html#central-limit-theorem",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT says that for \\(f=\\mathbb{P}(E)\\) if \\(\\sigma^2\\equiv\\mathrm{Var}\\left(f\\right)\\) is finite then the error of the MC estimate\n\\[\ne_N(f)=\\bar{f}-\\mathbb{E}[f]\n\\]\nis approximately Normal in distribution for large \\(M\\), i.e.\n\\[\ne_N(f)\\sim\\sigma M^{1/2}Z\n\\] where \\(Z\\sim\\mathscr{N}(0,1)\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-5",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-5",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nSuppose we need to compute an expectation \\(\\mathbb{E}[g(Z)]\\) for some random variable \\(Z\\) and some function \\(g:\\mathbb{R}\\to\\mathbb{R}\\). Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) and estimate \\(\\mathbb{E}[g(Z)]\\) as \\(\\frac{1}{M}\\sum_{i=1}^Mg(z_i)\\). The law of large numbers states that this sample mean should be close to \\(\\mathbb{E}[g(Z)]\\).\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-6",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-6",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nWe can use Monte Carlo to estimate probabilities of the form \\(\\mathbb{P}\\left[E\\right]\\) by approximating expectations of the form \\(\\mathbb{E}[1_{X\\in E}]\\).\nIf \\(X\\sim\\mathscr{N}(\\mu,\\sigma)\\) and we want to compute \\(\\mathbb{E}[\\log|X|]\\), we could set up and solve the integral\n\\[\n\\mathbb{E} \\log |X|\n= \\int_{-\\infty}^\\infty \\left( \\log |t| \\right) f( t; \\mu, \\sigma) dt\n= \\int_{-\\infty}^\\infty \\frac{ \\log |t| }{ \\sqrt{2\\pi \\sigma^2} }\n                  \\exp\\left\\{ \\frac{ -(t-\\mu)^2 }{ 2\\sigma^2 } \\right\\}dt\n\\]\nAlternatively, we could just draw lots of Monte Carlo replicates \\(X_1,X_2,\\cdots,X_M\\) from a normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and look at the sample mean \\(M^{-1}\\sum_{i=1}^M\\log|x_i|\\), once again appealing to the law of large numbers to ensure that this sample mean is close to its expectation.\nMonte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-7",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-7",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis idea can be pushed still further. Suppose that we want to compute an integral\n\\[\n\\int_Dg(x)dx\n\\] where \\(D\\) is some domain of integration and \\(g(.)\\) is a function.\nLet \\(f(x)\\) be the density of some random variable with \\(f(x)&gt;0, \\forall x\\in D\\). In other words, \\(f\\) is the density of a random variable supported on \\(D\\). Then we can rewrite the integral as\n\\[\n\\int_Dg(x)dx = \\int_D\\frac{g(x)}{f(x)}f(x)dx = \\mathbb{E}[h(x)]\n\\] where \\(h(x)=g(x)/f(x)\\) and \\(X\\sim f\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-8",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-8",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nNow suppose we are given \\(h(x)\\) and we want to compute \\(\\mathbb{E}[h(X)]\\) where \\(X\\sim f,\\,x\\in D\\). So we need to sample from \\(f\\), but what if we could not do that directly?\nIf there were some other distribution \\(g(x)\\) we could sample from, such that \\(g(x)&gt;0,\\,x\\in D\\), then\n\\[\n\\begin{align*}\n\\mathbb{E}_{f}\\left[h(x)\\right] & =\\int_{D}h(x)f(x)dx\\\\\n& =\\int_{S}h(x)\\frac{f(x)}{g(x)}g(x)dx=\\mathbb{E}_{g}\\left[h(x)\\frac{f(x)}{g(x)}\\right]\\\\\n& =\\frac{1}{n}\\sum_{i=1}^{n}h(x_{i})\\frac{f(x_{i})}{g(x_{i})}\\quad x_{i}\\sim g\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-9",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-9",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis is called importance sampling (IS).\n\ndraw iid \\(x_1,x_2,\\ldots,x_n\\) from g and calculate the importance weight \\[\nw(x_i)=\\frac{f(x_{i})}{g(x_{i})}\n\\]\nestimate \\(\\mathbb{E}_f(h)\\) by \\[\n\\hat{\\mu}_h=\\frac{1}{n}\\sum_{i=1}^nw(x_i)h(x_i)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-10",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-10",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nexample\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = \\sqrt{2/\\pi}e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution)\n\n\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X) / sqrt(2 * pi)\n\nmu_h  &lt;- mean(W*X)\nvar_h &lt;- var(W*X)/n\nse_h  &lt;- sqrt(var_h)\n\ntibble::tibble(mean = mu_h,  variance = var_h, 'standard error' = se_h) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4)\n\n\n\n\n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.8030\n0.0003\n0.0178"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-11",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-11",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nunknown normalizing constant\n\nSuppose that \\(q(x)&gt;0;\\;x\\in D\\) and \\(\\int_Dq(x)dx=Z_q&lt;\\infty\\) The \\(q()\\) is an un-normalized density on \\(D\\) whereas the corresponding normalized density is \\(\\frac{1}{Z_q}q(x)\\).\nUsing IS, let \\(g(x) = \\frac{1}{Z_r}r(x);\\;Z_r=\\int r(x)dx\\), so \\(r\\) is an un-normalized density with \\(Z_r\\) possibly unknown.\n\nDraw \\(x_1,x_2,\\ldots,x_n\\) from \\(g(x)\\) and calculate importance weights \\(w(x_i)=g(x_i)/r(x_i)\\)\nEstimate \\(\\mathbb{E}_f\\left[h(X)\\right]\\) by \\[\n\\hat{\\mu_h}=\\frac{\\sum_{i=1}^nw(x_i)h(x_i)}{\\sum_{i=1}^nw(x_i)}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-12",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-12",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nunknown normalizing constant\n\nsince\n\\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}\\right]=\\int\\frac{q(X)}{r(X)}g(x)dx=\\frac{Z_{q}}{Z_{r}}\\\\\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}h(X)\\right]=\\int\\frac{q(X)}{r(X)}g(x)h(x)dx=\\frac{1}{Z_{r}}\\int g(x)h(x)dx\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-13",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-13",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrepeating the prior example, but un-normalized\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution, un-normalized)\n\n\n# un-normalized weights\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X)\n\nmu_h2  &lt;- sum(W*X)/sum(W)\nvar_h2 &lt;- var(W/mean(W))\nse_h2  &lt;- sqrt(var_h2)\n\ntibble::tibble(mean = mu_h2,  variance = var_h2, 'standard error' = se_h2) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4)\n\n\n\n\n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.7895\n0.4058\n0.6370"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-14",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-14",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling procedure\n\nAssume we have an un-normalized \\(g(x)\\), i.e.Â \\(\\pi(x)=cg(x)\\) but \\(c\\) is unknown.\nWe want to generate iid samples \\(x_1,x_2,\\ldots,x_M\\sim \\pi\\) to estimate \\(\\mathbb{E}_\\pi[h]\\)\nNow assume we have an easily sampled density \\(f\\), and known \\(K&gt;0\\), such that \\(Kf(x)\\ge g(x),\\;\\forall x\\), i.e.Â \\(Kf(x)\\ge \\pi(x)/c\\) ( or \\(cKf(x)\\ge \\pi(x)\\)).\nThen use the following procedure:\n\nsample \\(X\\sim f\\) and \\(U\\sim \\mathrm{uniform}[0,1]\\)\nif \\(U\\le\\frac{g(X)}{Kf(x)}\\), the accept X as a draw from \\(\\pi\\)\notherwise reject the sample and repeat"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-15",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-15",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nSince \\(0\\le\\frac{g(x)}{Kf(x)}\\le 1\\) we know that \\(\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\\)\nand so\n\n\\[\n\\mathbb{E}_f\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]=\\mathbb{E}_f\\left[\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^\\infty\\frac{g(X)}{Kf(X)}f(x)dx=\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-16",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-16",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nSince \\(0\\le\\frac{g(x)}{Kf(x)}\\le 1\\) we know that \\(\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\\)\nand so\n\n\\[\n\\mathbb{E}_f\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]=\\mathbb{E}_f\\left[\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^\\infty\\frac{g(X)}{Kf(X)}f(x)dx=\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-17",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-17",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nSimilarly, for any \\(y\\in\\mathbb{R}\\), we can calculate the joint probability\n\\[\n\\begin{align*}\n\\mathbb{P}\\left(X\\le y,U\\le\\frac{g(X)}{Kf(X)}\\right) & =\\mathbb{E}\\left[1_{X\\le y}1_{U\\le\\frac{g(X)}{Kf(X)}}\\right]\\\\\n& =\\mathbb{E}\\left[1_{X\\le y}\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X\\right)\\right]\\\\\n& =\\mathbb{E}\\left[1_{X\\le y}\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^{y}\\frac{g(x)}{Kf(x)}f(x)dx\\\\\n& =\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx\n\\end{align*}\n\\]\n\nand so we have the joint probability (above - \\(\\mathbb{P}(A,B)\\)), and the probability of acceptance (previous slide - \\(\\mathbb{P}(B)\\)), so the probability, conditional on acceptance (\\(\\mathbb{P}(A|B)\\)) is \\(\\frac{\\mathbb{P}(A,B)}{\\mathbb{P}(B)}\\) by Bayes rule."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-18",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-18",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\\[\n\\mathbb{P}\\left(X\\le y|U\\le\\frac{g(X)}{Kf(X)}\\right)=\\frac{\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx}{\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx}=\\int_{-\\infty}^{y}\\pi(x)dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-19",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-19",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nconsider random variable \\(X\\) with pdf/pmf \\(q(x)&gt;0;\\;x\\in D\\) which is difficult to sample from\nwe will sample from \\(q\\) using a proposal pdf/pmf \\(f\\) which we can sample from\nif we can find a constant \\(K\\) such that \\(q(x)\\le Kf(x); \\forall x\\in D\\). Alternatively \\(\\frac{q(x)}{f(x)}\\le K\\)\nthen there is a rejection method that returns \\(X\\sim q\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-20",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-20",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling method\n\ngiven a proposal pdf/pmf \\(f\\) we can sample from, and constant \\(K\\) such that \\(\\frac{q(x)}{f(x)}\\le K; \\forall x\\in D\\)\nsample \\(Y_i\\sim f\\) and \\(U_i\\sim\\mathrm{U}[0,1]\\)\nfor \\(U_i\\le\\frac{q(Y_i)}{Kf(Y_i)}\\) return \\(X_i=Y_i\\); otherwise return nothing and continue."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-21",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-21",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nWe have \\(\\mathbb{P}(X=x) = \\sum_{i=1}^n\\mathbb{P}(\\mathrm{reject }\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw }\\,Y=x\\,\\mathrm{and\\, accept})\\)\nWe also have\n\\[\n\\begin{align*}\n& \\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n= & \\mathbb{P}(\\mathrm{draw}\\,Y=x)\\mathbb{P}(\\left.\\mathrm{accept}\\,Y\\right|Y=x)\\\\\n= & f(x)\\mathbb{P}(\\left.U\\le\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n= & \\frac{q(x)}{K}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-22",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-22",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nThe probability of rejection of a draw is\n\\[\n\\begin{align*}\n\\mathbb{P}(\\mathrm{{reject}}\\,Y) & =\\sum_{x\\in D}\\mathbb{P}(\\mathrm{{draw}}\\,Y=x\\,\\mathrm{and\\,reject\\,it})\\\\\n& =\\sum_{x\\in D}f(x)\\mathbb{P}(\\left.U\\ge\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n& =\\sum_{x\\in D}f(x)(1-\\frac{q(x)}{Kf(x)})=1-\\frac{1}{K}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-23",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-23",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nand so1\n\\[\n\\begin{align*}\n\\mathbb{P}(X=x) & =\\sum_{n=1}^{\\infty}\\mathbb{P}(\\mathrm{reject}\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n& =\\sum_{n=1}^{\\infty}\\left(1-\\frac{1}{K}\\right)^{n-1}\\frac{q(x)}{K}=q(x)\n\\end{align*}\n\\]\n\nThe geometric distribution is a discrete distribution that can be interpreted as the number of failures before the first success (\\(\\mathbb{P}(X=k)=(1-p)^{k-1}p\\), with mean \\(p\\))."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-24",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-24",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for continuous scalar rv\n\nRecal that we accept the proposal \\(Y\\) whenever \\((U,Y)\\sim q_{U,Y}\\) where \\(q_{U,Y}\\left(u,y\\right)=q(y)U_{0,1}(u)\\) such that \\(U\\le q(Y)/(Kf(Y))\\)\nWe have\n\\[\n\\begin{align*}\n\\mathbb{P}\\left(X\\le x\\right) & =\\mathbb{P}\\left(\\left.Y\\le x\\right|U\\le q(Y)/Kf(Y))\\right)\\\\\n& =\\frac{\\mathbb{P}\\left(Y\\le x,U\\le q(Y)/Kf(Y))\\right)}{\\mathbb{P}\\left(U\\le q(Y)/Kf(Y))\\right)}\\\\\n& =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}\\\\\n& =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}=\\int_{-\\infty}^{x}q(y)dy\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-25",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-25",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: unknown normalizing constants\n\nIn most practical scenarios, we know \\(f(x)\\) and \\(q(x)\\) only up to some normalizing constants\n\\[\nf(x)=\\bar{f}(x)/K_f\\;\\mathrm{and}\\;q(x)=\\bar{q}(x)/K_q\n\\] We can still use rejection sampling since\n\\[\n\\frac{q(x)}{f(x)}\\le K\\;\\mathrm{iff}\\;\\frac{\\bar{q}(x)}{\\bar{f}(x)}\\le\\hat{K}\\equiv  K\\frac{K_q}{K_f}\n\\] In practice this means we can ignore the normalizing constants if we can find \\(\\hat{K}\\) to bound \\(\\frac{\\bar{q}(x)}{\\bar{f}(x)}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mc-methods-26",
    "href": "slides/BSMM_8740_lec_09b.html#mc-methods-26",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nSuppose we need to compute an expectation \\(\\mathbb{E}[g(Z)]\\) for some random variable \\(Z\\) and some function \\(g:\\mathbb{R}\\to\\mathbb{R}\\). Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) and estimate \\(\\mathbb{E}[g(Z)]\\) as \\(\\frac{1}{M}\\sum_{i=1}^Mg(z_i)\\). The law of large numbers states that this sample mean should be close to \\(\\mathbb{E}[g(Z)]\\).\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#stochastic-processes",
    "href": "slides/BSMM_8740_lec_09b.html#stochastic-processes",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nA stochastic process, which we will usually write as \\((X_n)\\), is an indexed sequence of random variables that are (usually) dependent on each other.\nEach random variable \\(X_n\\) takes a value in a state space \\(\\mathcal S\\) which is the set of possible values for the process. As with usual random variables, the state space \\(\\mathcal S\\) can be discrete or continuous. A discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, \\(\\mathcal S = \\{\\text{Heads},\\text{Tails}\\}\\) is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers \\(\\mathcal S = \\mathbb Z_+ = \\{0,1,2,\\dots\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#stochastic-processes-1",
    "href": "slides/BSMM_8740_lec_09b.html#stochastic-processes-1",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nFurther, the process has an index set that puts the random variables that make up the process in order. The index set is usually interpreted as a time variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at distinct points, often denoted by \\(n = 0,1,2,\\dots\\), while continuous time denotes a process monitored constantly over time, often denoted by \\(t \\in \\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-property",
    "href": "slides/BSMM_8740_lec_09b.html#markov-property",
    "title": "Monte Carlo Methods",
    "section": "Markov property",
    "text": "Markov property\nThink of a simple board game where we roll a dice and move that many squares forward on the board. Suppose we are currently on the square \\(X_n\\). Then what can we say about which square \\(X_{n+1}\\) we move to on our next turn?\n\n\\(X_{n+1}\\) is random, since it depends on the roll of the dice.\n\\(X_{n+1}\\) depends on where we are now \\(X_n\\), since the score of dice will be added onto the number our current square,\nGiven the square \\(X_n\\) we are now, \\(X_{n+1}\\) doesnâ€™t depend any further on which sequence of squares \\(X_0, X_1, \\dots, X_{n-1}\\) we used to get here.\n\nThe third point is called the Markov property or memoryless property. We say â€œmemorylessâ€, because we only need to remember what square weâ€™ve reached, not which squares we used to get here. The stochastic process before this moment has no bearing on the future, given where we are now. A mathematical way to say this is that â€œthe past and the future are conditionally independent given the present.â€"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider the following simple random walk on the integers \\(\\mathbb Z\\): We start at \\(0\\), then at each time step, we go up by one with probability \\(p\\) and down by one with probability \\(q = 1-p\\). When \\(p = q = \\frac12\\), weâ€™re equally as likely to go up as down, and we call this the simple symmetric random walk.\nThe simple random walk is a simple but very useful model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In many modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the â€œdrunkardâ€™s walkâ€, suggesting it could model a drunk person trying to stagger home."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-1",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-1",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nrandom walks\nrequire(ggplot2, quietly = TRUE)\nset.seed(315)\n\nrrw &lt;- function(n, p = 1/2) {\n  q &lt;- 1 - p\n  Z &lt;- sample(c(1, -1), n, replace = TRUE, prob = c(p, q))\n  X &lt;- c(0, cumsum(Z))\n  c(0, cumsum(Z))\n}\n\nn &lt;- 2000\nrw_dat &lt;- tibble::tibble(x=0:n) |&gt; \n  dplyr::mutate(\n    \"p = 2/3\" = rrw(n, 2/3)\n    , \"p = 1/3\" = rrw(n, 1/3)\n    , \"p = 1/2\" = rrw(n, 1/2)\n  )\n\np0 &lt;- rw_dat |&gt; dplyr::slice_head(n=20) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np1 &lt;- rw_dat |&gt; dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np3 &lt;- rw_dat |&gt; #dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np0+p1+p3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-2",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-2",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nWe can write this as a stochastic process \\((X_n)\\) with discrete time \\(n = \\{0,1,2,\\dots\\} = \\mathbb Z_+\\) and discrete state space \\(\\mathcal S = \\mathbb Z\\), where \\(X_0 = 0\\) and, for \\(n \\geq 0\\), we have \\[ X_{n+1} = \\begin{cases} X_n + 1 & \\text{with probability $p$,} \\\\\n                             X_n - 1 & \\text{with probability $q$.} \\end{cases} \\]\nItâ€™s clear from this definition that \\(X_{n+1}\\) (the future) depends on \\(X_n\\) (the present), but, given \\(X_n\\), does not depend on \\(X_{n-1}, \\dots, X_1, X_0\\) (the past). Thus the Markov property holds, and the simple random walk is a discrete time Markov process or Markov chain."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-3",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-3",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nSo far weâ€™ve seen a a few examples of stochastic processes in discrete time and discrete space with the Markov memoryless property. Now we will develop the theory more generally.\nTo define a so-called â€œMarkov chainâ€, we first need to say where we start from, and second what the probabilities of transitions from one state to another are.\nIn our examples of the simple random walk and gamblerâ€™s ruin, we specified the start point \\(X_0 = i\\) exactly, but we could pick the start point at random according to some distribution \\(\\lambda_i = \\mathbb P(X_0 = i)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-4",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-4",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nAfter that, we want to know the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for \\(i,j \\in \\mathcal S\\). Here, because of the Markov property, the transition probability only needs to condition on the state weâ€™re in now \\(X_n = i\\), and not on the whole history of the process.\nIn the case of the simple random walk, for example, we had initial distribution \\[ \\lambda_i = \\mathbb P(X_0 = i) = \\begin{cases} 1 & \\text{if $i = 0$} \\\\ 0 & \\text{otherwise} \\end{cases} \\] and transition probabilities \\[ \\mathbb P(X_{n+1} = j \\mid X_n = i) = \\begin{cases} p & \\text{if $j = i+1$} \\\\ q & \\text{if $j = i-1$} \\\\ 0 & \\text{otherwise.} \\end{cases} \\]\nFor the random walk (and also the gamblerâ€™s ruin), the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) donâ€™t depend on \\(n\\); in other words, the transition probabilities stay the same over time. A Markov process with this property is called time homogeneous. We will always consider time homogeneous processes from now on (unless we say otherwise)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-5",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-5",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nLetâ€™s write \\(p_{ij} = \\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for the transition probabilities, which are independent of \\(n\\). We must have \\(p_{ij} \\geq 0\\), since it is a probability, and we must also have \\(\\sum_j p_{ij} = 1\\) for all states \\(i\\), as this is the sum of the probabilities of all the places you can move to from state i.\nWhen the state space is finite (and even sometimes when itâ€™s not), itâ€™s convenient to write the transition probabilities \\((p_{ij})\\) as a matrix \\(\\mathsf P\\), called the transition matrix, whose \\((i,j)\\)th entry is \\(p_{ij}\\). Then the condition that \\(\\sum_j p_{ij} = 1\\) is the condition that each of the rows of \\(\\mathsf P\\) add up to \\(1\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-6",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-6",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nConsider a simple two-state Markov chain with state space \\(\\mathcal S = \\{0,1\\}\\) and transition matrix \\[ \\mathsf P = \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\beta & 1-\\beta \\end{pmatrix}  \\] for some \\(0 &lt; \\alpha, \\beta &lt; 1\\). Note that the rows of \\(\\mathsf P\\) add up to \\(1\\), as they must.\nWe can illustrate \\(\\mathsf P\\) by a transition diagram, where the blobs are the states and the arrows give the transition probabilities. (We donâ€™t draw the arrow if \\(p_{ij} = 0\\).) In this case, our transition diagram looks like this:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-7",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-7",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTransition diagram for the two-state Markov chain"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-8",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-8",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nWe can use this as a simple model of customer churn, for example. If the customer has closed their account (state 0) on one period, then with probability \\(\\alpha\\) we will be able to entice them to open their account again (state 1) by the next period; while if the customer has an account (state 1), then with probability \\(\\beta\\) they will have closed their account (state 0) by the next period."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-9",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-9",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIf the customer has an account on period n, whatâ€™s the probability they also have an account on period n+2?\n\\[\np_{11}(2) = \\mathbb P (X_{n+2} = 1 \\mid X_n = 1)\n\\]\nThe key to calculating this is to condition on the first step again â€“ that is, on whether the printer is working on Tuesday. We have\n\\[\n\\begin{align*}\n  p_{11}(2) &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0, X_n = 1) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1, X_n = 1) \\\\\n  &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1) \\\\\n  &= p_{10}p_{01} + p_{11}p_{11} \\\\\n  &= \\beta\\alpha + (1-\\beta)^2 .\n\\end{align*}\n\\]\nIn the second equality, we used the Markov property to mean conditional probabilities like \\(\\mathbb P(X_{n+2} = 1 \\mid X_{n+1} = k)\\) did not have to depend on \\(X_n\\).\nAnother way to think of this as we summing the probabilities of all length-2 paths from 1 to 1, which are \\(1\\to 0\\to 1\\) with probability \\(\\beta\\alpha\\) and \\(1 \\to 1 \\to 1\\) with probability \\((1-\\beta)^2\\)\n\nIn the above example, we calculated a two-step transition probability \\(p_{ij}(2) = \\mathbb P (X_{n+2} = j \\mid X_n = i)\\) by conditioning on the first step. That is, by considering all the possible intermediate steps \\(k\\), we have\n\\[\np_{ij}(2) = \\sum_{k\\in\\mathcal S} \\mathbb P (X_{n+1} = k \\mid X_n = i)\\mathbb P (X_{n+2} = j \\mid X_{n+1} = k) = \\sum_{k\\in\\mathcal S} p_{ik}p_{kj}\n\\]\nBut this is exactly the formula for multiplying the matrix \\(\\mathsf P\\) with itself! In other words, \\(p_{ij}(2) = \\sum_{k} p_{ik}p_{kj}\\) is the \\((i,j)\\)th entry of the matrix square \\(\\mathsf P^2 = \\mathsf{PP}\\). If we write \\(\\mathsf P(2)  = (p_{ij}(2))\\) for the matrix of two-step transition probabilities, we have \\(\\mathsf P(2) = \\mathsf P^2\\).\nMore generally, we see that this rule holds over multiple steps, provided we sum over all the possible paths \\(i\\to k_1 \\to k_2 \\to \\cdots \\to k_{n-1} \\to j\\) of length \\(n\\) from \\(i\\) to \\(j\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-10",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-10",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTheorem 1 Let \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). For \\(i,j \\in \\mathcal S\\), write \\[ p_{ij}(n) = \\mathbb P(X_n = j \\mid X_0 = i) \\] for the \\(n\\)-step transition probability. Then\n\\[\np_{ij}(n) = \\sum_{k_1, k_2, \\dots, k_{n-1} \\in \\mathcal S} p_{ik_1} p_{k_1k_2} \\cdots p_{k_{n-2}k_{n-1}} p_{k_{n-1}j}\n\\]\nIn particular, \\(p_{ij}(n)\\) is the \\((i,j)\\)th element of the matrix power \\(\\mathsf P^n\\), and the matrix of \\(n\\)-step transition probabilities is given by \\(\\mathsf P(n) = \\mathsf P^n\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-11",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-11",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nThe so-called Chapmanâ€“Kolmogorov equations follow immediately from this.\n\nLet \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). Then, for non-negative integers \\(n,m\\), we have \\[ p_{ij}(n+m) = \\sum_{k \\in \\mathcal S} p_{ik}(n)p_{kj}(m) , \\] or, in matrix notation, \\(\\mathsf P(n+m) = \\mathsf P(n)\\mathsf P(m)\\).\n\nIn other words, a trip of length \\(n + m\\) from \\(i\\) to \\(j\\) is a trip of length \\(n\\) from \\(i\\) to some other state \\(k\\), then a trip of length \\(m\\) from \\(k\\) back to \\(j\\), and this intermediate stop \\(k\\) can be any state, so we have to sum the probabilities.\nOf course, once we know that \\(\\mathsf P(n) = \\mathsf P^n\\) is given by the matrix power, itâ€™s clear to see that \\(\\mathsf P(n+m) = \\mathsf P^{n+m} = \\mathsf P^n \\mathsf P^m = \\mathsf P(n)\\mathsf P(m)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-12",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-12",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nIf we start from a state given by a distribution \\(\\boldsymbol \\pi = (\\pi_i)\\), then after step 1 the probability weâ€™re in state \\(j\\) is \\(\\sum_i \\pi_i p_{ij}\\). So if \\(\\pi_j = \\sum_i \\pi_i p_{ij}\\), we stay in this distribution forever. We call such a distribution a stationary distribution. We again recognise this formula as a matrix-vector multiplication, so this is \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\), where \\(\\boldsymbol \\pi\\) is a row vector.\n\nLet \\((X_n)\\) be a Markov chain on a state space \\(\\mathcal S\\) with transition matrix \\(\\mathsf P\\). Let \\(\\boldsymbol \\pi = (\\pi_i)\\) be a distribution on \\(\\mathcal S\\), in that \\(\\pi_i \\geq 0\\) for all \\(i \\in \\mathcal S\\) and \\(\\sum_{i \\in \\mathcal S} \\pi_i = 1\\). We call \\(\\boldsymbol \\pi\\) a stationary distribution if\n\\[\n\\pi_j =* \\sum_{i\\in \\mathcal S} \\pi_i p_{ij} \\quad \\text{for all $j \\in \\mathcal S$}\n\\]\nor, equivalently, if \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\).\n\nNote that weâ€™re saying the distribution \\(\\mathbb P(X_n = i)\\) stays the same; the Markov chain \\((X_n)\\) itself will keep moving. One way to think is that if we started off a thousand Markov chains, choosing each starting position to be \\(i\\) with probability \\(\\pi_i\\), then (roughly) \\(1000 \\pi_j\\) of them would be in state \\(j\\) at any time in the future â€“ but not necessarily the same ones each time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-13",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-13",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nproperties\nA Markov Chain is irreducible if you have positive probability of eventually getting from anywhere to anywhere else.\nA Markov Chain is aperiodic if there are no forced cycles, i.e.Â there do not exist disjoint non-empty subsets X1,X2,â€¦,Xd fordâ‰¥2,suchthatP(x,Xi+1)=1 forallxâˆˆXi (1â‰¤iâ‰¤dâˆ’1),andP(x,X1)=1forallxâˆˆXd. [Diagram.]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chains-14",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chains-14",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nMC with cycle"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#markov-chain-monte-carlo",
    "href": "slides/BSMM_8740_lec_09b.html#markov-chain-monte-carlo",
    "title": "Monte Carlo Methods",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\nSuppose have complicated, high-dimensional density \\(\\pi = cg\\) and we want samples \\(X_1, X_2,\\dots \\sim \\pi\\). (Then can do Monte Carlo.)\nDefine a Markovchain (dependent random process) \\(X_0, X_1,X_2\\dots\\) in such a way that for large enough \\(n\\), \\(X_n\\sim \\pi\\).\nThen we can estimate \\(\\mathbb{E}_{\\pi}(h) â‰¡ \\int h(x) \\pi(x) dx\\) by:\n\\[\n\\mathbb{E}_{\\pi}[h] \\approx \\frac{1}{M-B}\\sum_{i=B+1}^{M}\n\\]\nwhere \\(B\\) (â€œburn-inâ€) is chosen large enough so \\(X_B\\sim\\pi\\), and \\(M\\) is chosen large enough to get good Monte Carlo estimates."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mcmc-metropolis-algo",
    "href": "slides/BSMM_8740_lec_09b.html#mcmc-metropolis-algo",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\nchoose some initial value \\(X_0\\), then\ngiven \\(X_{n-1}\\), choose a proposal state \\(Y_n\\sim \\textrm{MVN}(X_{n-1},\\sigma^2\\textrm{I})\\) for some fixed \\(\\sigma^2&gt;0\\)\nlet \\(A_n=\\pi(Y_n)/\\pi(X_{n-1}=g(Y_n)/g(X_{n-1})\\) and \\(U_n\\sim\\textrm{U}[0,1]\\), then\nid \\(U_n&lt;A_n\\) set \\(X_n=Y_n\\) (â€œacceptâ€), otherwise set $X_n = X_{n-1} ) â€œrejectâ€\nrepeat for \\(n=1,2,3,\\ldots,M\\)\n\nThis version is called random-walk Metropolis"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#mcmc-metropolis-algo-1",
    "href": "slides/BSMM_8740_lec_09b.html#mcmc-metropolis-algo-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\n\na simple Metropolis algorithm in one dimension\ng = function(y) {\n    if ( (y&lt;0) || (y&gt;1) )\n    return(0)\n    else\n    return( y^3 * sin(y^4) * cos(y^5) )\n}\n\nh = function(y) { return(y^2) }\n\nM = 11000  # run length\nB = 1000  # amount of burn-in\nX = runif(1)  # overdispersed starting distribution\nsigma = 1  # proposal scaling\nxlist = rep(0,M)  # for keeping track of chain values\nhlist = rep(0,M)  # for keeping track of h function values\nnumaccept = 0;\n\nfor (i in 1:M) {\n  Y = X + sigma * rnorm(1)  # proposal value\n  U = runif(1)              # for accept/reject\n  alpha = g(Y) / g(X)       # for accept/reject\n  if (U &lt; alpha) {\n    X = Y                   # accept proposal\n    numaccept = numaccept + 1;\n  }\n    xlist[i] = X;\n    hlist[i] = h(X);\n}\n\ncat(\"ran Metropolis algorithm for\", M, \"iterations, with burn-in\", B, \"\\n\");\n\n\nran Metropolis algorithm for 11000 iterations, with burn-in 1000 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"acceptance rate =\", numaccept/M, \"\\n\");\n\n\nacceptance rate = 0.1046 \n\n\na simple Metropolis algorithm in one dimension\nu = mean(hlist[(B+1):M])\ncat(\"mean of h is about\", u, \"\\n\")\n\n\nmean of h is about 0.773 \n\n\na simple Metropolis algorithm in one dimension\nse1 =  sd(hlist[(B+1):M]) / sqrt(M-B)\ncat(\"iid standard error would be about\", se1, \"\\n\")\n\n\niid standard error would be about 0.001658 \n\n\na simple Metropolis algorithm in one dimension\nvarfact &lt;- function(xxx) { 2 * sum(acf(xxx, plot=FALSE)$acf) - 1 }\nthevarfact = varfact(hlist[(B+1):M])\nse = se1 * sqrt( thevarfact )\ncat(\"varfact = \", thevarfact, \"\\n\")\n\n\nvarfact =  21.02 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"true standard error is about\", se, \"\\n\")\n\n\ntrue standard error is about 0.007601 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"approximate 95% confidence interval is (\", u - 1.96 * se, \",\",\n                        u + 1.96 * se, \")\\n\\n\")\n\n\napproximate 95% confidence interval is ( 0.7581 , 0.7879 )\n\n\na simple Metropolis algorithm in one dimension\nplot(xlist, type='l')\n\n\n\na simple Metropolis algorithm in one dimension\n# acf(xlist)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#more",
    "href": "slides/BSMM_8740_lec_09b.html#more",
    "title": "Monte Carlo Methods",
    "section": "More",
    "text": "More\n\nRead Bayes Rules!\nRead Think Bayes\nRead Statistical Rethinking"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09b.html#recap",
    "href": "slides/BSMM_8740_lec_09b.html#recap",
    "title": "Monte Carlo Methods",
    "section": "Recap",
    "text": "Recap\n\nWeâ€™ve had the smallest possible taste of statistical programming using Bayes theorem and sampling methods, in the context of adressing the limitations of off-the-shelf implementations of statistical methods and algorithms.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/temp.html",
    "href": "slides/temp.html",
    "title": "temp",
    "section": "",
    "text": "Code\nsamples &lt;- matrix(runif(10000), ncol=2) |&gt; data.frame() |&gt; \n  dplyr::mutate(\n    normals = \n      purrr::map2(\n        X1, X2\n        ,(\\(x1,x2){\n          data.frame(\n            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)\n            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) \n          )\n        })\n      )\n  ) |&gt; \n  tidyr::unnest(normals)  \n  \n\nsamples |&gt; \n  tidyr::pivot_longer(-c(X1,X2)) |&gt; \n  ggplot(aes(x=value, color=name, fill=name)) + \n  geom_histogram(aes(y=..density..), bins = 60, position=\"identity\", alpha=0.3) + \n  labs(x=\"Value\", y=\"Density\") + theme_minimal()\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in\nggplot2 3.4.0.\nâ„¹ Please use `after_stat(density)` instead.\n\n\nCode\nsamples |&gt; \nggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs"
  },
  {
    "objectID": "slides/temp.html#random-number-generation",
    "href": "slides/temp.html#random-number-generation",
    "title": "temp",
    "section": "",
    "text": "Code\nsamples &lt;- matrix(runif(10000), ncol=2) |&gt; data.frame() |&gt; \n  dplyr::mutate(\n    normals = \n      purrr::map2(\n        X1, X2\n        ,(\\(x1,x2){\n          data.frame(\n            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)\n            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) \n          )\n        })\n      )\n  ) |&gt; \n  tidyr::unnest(normals)  \n  \n\nsamples |&gt; \n  tidyr::pivot_longer(-c(X1,X2)) |&gt; \n  ggplot(aes(x=value, color=name, fill=name)) + \n  geom_histogram(aes(y=..density..), bins = 60, position=\"identity\", alpha=0.3) + \n  labs(x=\"Value\", y=\"Density\") + theme_minimal()\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in\nggplot2 3.4.0.\nâ„¹ Please use `after_stat(density)` instead.\n\n\nCode\nsamples |&gt; \nggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs"
  },
  {
    "objectID": "slides/temp.html#random-number-generation-1",
    "href": "slides/temp.html#random-number-generation-1",
    "title": "temp",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nYour computer is only capable of producing pseudorandom numbers. These are made by running a pseudorandom number generator algorithm which is deterministic, e.g.\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\nOnce the RNG seed is set, the â€œrandomâ€ numbers that R generates arenâ€™t random at all. But someone looking at these random numbers would have a very hard time distinguishing these numbers from truly random numbers. That is what â€œstatistical randomnessâ€ means!"
  },
  {
    "objectID": "slides/temp.html#central-limit-theorem",
    "href": "slides/temp.html#central-limit-theorem",
    "title": "temp",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT says that for f=â„™(E)f=\\mathbb{P}(E) if Ïƒ2â‰¡Var(f)\\sigma^2\\equiv\\mathrm{Var}\\left(f\\right) is finite then the error of the MC estimate\neN(f)=fâ€¾âˆ’ğ”¼[f]\ne_N(f)=\\bar{f}-\\mathbb{E}[f]\n\nis approximately Normal in distribution for large MM, i.e.\neN(f)âˆ¼ÏƒM1/2Z\ne_N(f)\\sim\\sigma M^{1/2}Z\n where Zâˆ¼ğ’©(0,1)Z\\sim\\mathscr{N}(0,1)"
  },
  {
    "objectID": "slides/temp.html#mc-methods",
    "href": "slides/temp.html#mc-methods",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\nSuppose we need to compute an expectation ğ”¼[g(Z)]\\mathbb{E}[g(Z)] for some random variable ZZ and some function g:â„â†’â„g:\\mathbb{R}\\to\\mathbb{R}. Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of ZZ, say z1,z2,â€¦,zMz_1,z_2,\\ldots,z_M and estimate ğ”¼[g(Z)]\\mathbb{E}[g(Z)] as 1Mâˆ‘i=1Mg(zi)\\frac{1}{M}\\sum_{i=1}^Mg(z_i). The law of large numbers states that this sample mean should be close to ğ”¼[g(Z)]\\mathbb{E}[g(Z)].\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/temp.html#mc-methods-1",
    "href": "slides/temp.html#mc-methods-1",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nWe can use Monte Carlo to estimate probabilities of the form â„™[E]\\mathbb{P}\\left[E\\right] by approximating expectations of the form ğ”¼[1XâˆˆE]\\mathbb{E}[1_{X\\in E}].\nIf Xâˆ¼ğ’©(Î¼,Ïƒ)X\\sim\\mathscr{N}(\\mu,\\sigma) and we want to compute ğ”¼[log|X|]\\mathbb{E}[\\log|X|], we could set up and solve the integral\nğ”¼log|X|=âˆ«âˆ’âˆâˆ(log|t|)f(t;Î¼,Ïƒ)dt=âˆ«âˆ’âˆâˆlog|t|2Ï€Ïƒ2exp{âˆ’(tâˆ’Î¼)22Ïƒ2}dt\n\\mathbb{E} \\log |X|\n= \\int_{-\\infty}^\\infty \\left( \\log |t| \\right) f( t; \\mu, \\sigma) dt\n= \\int_{-\\infty}^\\infty \\frac{ \\log |t| }{ \\sqrt{2\\pi \\sigma^2} }\n                  \\exp\\left\\{ \\frac{ -(t-\\mu)^2 }{ 2\\sigma^2 } \\right\\}dt\n\nAlternatively, we could just draw lots of Monte Carlo replicates X1,X2,â‹¯,XMX_1,X_2,\\cdots,X_M from a normal with mean Î¼\\mu and variance Ïƒ2\\sigma^2, and look at the sample mean Mâˆ’1âˆ‘i=1Mlog|xi|M^{-1}\\sum_{i=1}^M\\log|x_i|, once again appealing to the law of large numbers to ensure that this sample mean is close to its expectation.\nMonte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/temp.html#mc-methods-2",
    "href": "slides/temp.html#mc-methods-2",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis idea can be pushed still further. Suppose that we want to compute an integral\nâˆ«Dg(x)dx\n\\int_Dg(x)dx\n where DD is some domain of integration and g(.)g(.) is a function.\nLet f(x)f(x) be the density of some random variable with f(x)&gt;0,âˆ€xâˆˆDf(x)&gt;0, \\forall x\\in D. In other words, ff is the density of a random variable supported on DD. Then we can rewrite the integral as\nâˆ«Dg(x)dx=âˆ«Dg(x)f(x)f(x)dx=ğ”¼[h(x)]\n\\int_Dg(x)dx = \\int_D\\frac{g(x)}{f(x)}f(x)dx = \\mathbb{E}[h(x)]\n where h(x)=g(x)/f(x)h(x)=g(x)/f(x) and Xâˆ¼fX\\sim f"
  },
  {
    "objectID": "slides/temp.html#mc-methods-3",
    "href": "slides/temp.html#mc-methods-3",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nNow suppose we are given h(x)h(x) and we want to compute ğ”¼[h(X)]\\mathbb{E}[h(X)] where Xâˆ¼f,xâˆˆDX\\sim f,\\,x\\in D. So we need to sample from ff, but what if we could not do that directly?\nIf there were some other distribution g(x)g(x) we could sample from, such that g(x)&gt;0,xâˆˆDg(x)&gt;0,\\,x\\in D, then\nğ”¼f[h(x)]=âˆ«Dh(x)f(x)dx=âˆ«Sh(x)f(x)g(x)g(x)dx=ğ”¼g[h(x)f(x)g(x)]=1nâˆ‘i=1nh(xi)f(xi)g(xi)xiâˆ¼g\n\\begin{align*}\n\\mathbb{E}_{f}\\left[h(x)\\right] & =\\int_{D}h(x)f(x)dx\\\\\n & =\\int_{S}h(x)\\frac{f(x)}{g(x)}g(x)dx=\\mathbb{E}_{g}\\left[h(x)\\frac{f(x)}{g(x)}\\right]\\\\\n & =\\frac{1}{n}\\sum_{i=1}^{n}h(x_{i})\\frac{f(x_{i})}{g(x_{i})}\\quad x_{i}\\sim g\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-4",
    "href": "slides/temp.html#mc-methods-4",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis is called importance sampling (IS).\n\ndraw iid x1,x2,â€¦,xnx_1,x_2,\\ldots,x_n from g and calculate the importance weight w(xi)=f(xi)g(xi)\nw(x_i)=\\frac{f(x_{i})}{g(x_{i})}\n\nestimate ğ”¼f(h)\\mathbb{E}_f(h) by Î¼Ì‚h=1nâˆ‘i=1nw(xi)h(xi)\n\\hat{\\mu}_h=\\frac{1}{n}\\sum_{i=1}^nw(x_i)h(x_i)"
  },
  {
    "objectID": "slides/temp.html#mc-methods-5",
    "href": "slides/temp.html#mc-methods-5",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nexample\n\nEstimate ğ”¼f(X)\\mathbb E_f(X) where f(x)=2/Ï€eâˆ’x22;xâ‰¥0f(x) = \\sqrt{2/\\pi}e^{-\\frac{x^2}{2}};\\;x\\ge 0 (this is the half-Normal distribution)\n\n\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X) / sqrt(2 * pi)\n\nmu_h  &lt;- mean(W*X)\nvar_h &lt;- var(W*X)/n\nse_h  &lt;- sqrt(var_h)\n\ntibble::tibble(mean = mu_h,  variance = var_h, 'standard error' = se_h) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4)\n\n\n\n\n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.8030\n0.0003\n0.0178"
  },
  {
    "objectID": "slides/temp.html#mc-methods-6",
    "href": "slides/temp.html#mc-methods-6",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nunknown normalizing constant\n\nSuppose that q(x)&gt;0;xâˆˆDq(x)&gt;0;\\;x\\in D and âˆ«Dq(x)dx=Zq&lt;âˆ\\int_Dq(x)dx=Z_q&lt;\\infty The q()q() is an un-normalized density on DD whereas the corresponding normalized density is 1Zqq(x)\\frac{1}{Z_q}q(x).\nUsing IS, let g(x)=1Zrr(x);Zr=âˆ«r(x)dxg(x) = \\frac{1}{Z_r}r(x);\\;Z_r=\\int r(x)dx, so rr is an un-normalized density with ZrZ_r possibly unknown.\n\nDraw x1,x2,â€¦,xnx_1,x_2,\\ldots,x_n from g(x)g(x) and calculate importance weights w(xi)=g(xi)/r(xi)w(x_i)=g(x_i)/r(x_i)\nEstimate ğ”¼f[h(X)]\\mathbb{E}_f\\left[h(X)\\right] by Î¼hÌ‚=âˆ‘i=1nw(xi)h(xi)âˆ‘i=1nw(xi)\n\\hat{\\mu_h}=\\frac{\\sum_{i=1}^nw(x_i)h(x_i)}{\\sum_{i=1}^nw(x_i)}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-7",
    "href": "slides/temp.html#mc-methods-7",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nunknown normalizing constant\n\nsince\n1nâˆ‘i=1nw(xi)â†’ğ”¼g[q(X)r(X)]=âˆ«q(X)r(X)g(x)dx=ZqZr1nâˆ‘i=1nw(xi)â†’ğ”¼g[q(X)r(X)h(X)]=âˆ«q(X)r(X)g(x)h(x)dx=1Zrâˆ«g(x)h(x)dx\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}\\right]=\\int\\frac{q(X)}{r(X)}g(x)dx=\\frac{Z_{q}}{Z_{r}}\\\\\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}h(X)\\right]=\\int\\frac{q(X)}{r(X)}g(x)h(x)dx=\\frac{1}{Z_{r}}\\int g(x)h(x)dx\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-8",
    "href": "slides/temp.html#mc-methods-8",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrepeating the prior example, but un-normalized\n\nEstimate ğ”¼f(X)\\mathbb E_f(X) where f(x)=eâˆ’x22;xâ‰¥0f(x) = e^{-\\frac{x^2}{2}};\\;x\\ge 0 (this is the half-Normal distribution, un-normalized)\n\n\n# un-normalized weights\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X)\n\nmu_h2  &lt;- sum(W*X)/sum(W)\nvar_h2 &lt;- var(W/mean(W))\nse_h2  &lt;- sqrt(var_h2)\n\ntibble::tibble(mean = mu_h2,  variance = var_h2, 'standard error' = se_h2) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4)\n\n\n\n\n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.7895\n0.4058\n0.6370"
  },
  {
    "objectID": "slides/temp.html#mc-methods-9",
    "href": "slides/temp.html#mc-methods-9",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling procedure\n\nAssume we have an un-normalized g(x)g(x), i.e.Â Ï€(x)=cg(x)\\pi(x)=cg(x) but cc is unknown.\nWe want to generate iid samples x1,x2,â€¦,xMâˆ¼Ï€x_1,x_2,\\ldots,x_M\\sim \\pi to estimate ğ”¼Ï€[h]\\mathbb{E}_\\pi[h]\nNow assume we have an easily sampled density ff, and known K&gt;0K&gt;0, such that Kf(x)â‰¥g(x),âˆ€xKf(x)\\ge g(x),\\;\\forall x, i.e.Â Kf(x)â‰¥Ï€(x)/cKf(x)\\ge \\pi(x)/c ( or cKf(x)â‰¥Ï€(x)cKf(x)\\ge \\pi(x)).\nThen use the following procedure:\n\nsample Xâˆ¼fX\\sim f and Uâˆ¼uniform[0,1]U\\sim \\mathrm{uniform}[0,1]\nif Uâ‰¤g(X)Kf(x)U\\le\\frac{g(X)}{Kf(x)}, the accept X as a draw from Ï€\\pi\notherwise reject the sample and repeat"
  },
  {
    "objectID": "slides/temp.html#mc-methods-10",
    "href": "slides/temp.html#mc-methods-10",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\n\nSince 0â‰¤g(x)Kf(x)â‰¤10\\le\\frac{g(x)}{Kf(x)}\\le 1 we know that â„™(Uâ‰¤g(X)Kf(X)|X=x)=g(x)Kf(x)\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\nand so\n\nğ”¼f[â„™(Uâ‰¤g(X)Kf(X)|X=x)]=ğ”¼f[g(X)Kf(X)]=âˆ«âˆ’âˆâˆg(X)Kf(X)f(x)dx=âˆ«âˆ’âˆâˆg(X)Kdx\n\\mathbb{E}_f\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]=\\mathbb{E}_f\\left[\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^\\infty\\frac{g(X)}{Kf(X)}f(x)dx=\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx"
  },
  {
    "objectID": "slides/temp.html#mc-methods-11",
    "href": "slides/temp.html#mc-methods-11",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\n\nSince 0â‰¤g(x)Kf(x)â‰¤10\\le\\frac{g(x)}{Kf(x)}\\le 1 we know that â„™(Uâ‰¤g(X)Kf(X)|X=x)=g(x)Kf(x)\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\nand so\n\nğ”¼f[â„™(Uâ‰¤g(X)Kf(X)|X=x)]=ğ”¼f[g(X)Kf(X)]=âˆ«âˆ’âˆâˆg(X)Kf(X)f(x)dx=âˆ«âˆ’âˆâˆg(X)Kdx\n\\mathbb{E}_f\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]=\\mathbb{E}_f\\left[\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^\\infty\\frac{g(X)}{Kf(X)}f(x)dx=\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx"
  },
  {
    "objectID": "slides/temp.html#mc-methods-12",
    "href": "slides/temp.html#mc-methods-12",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\nSimilarly, for any yâˆˆâ„y\\in\\mathbb{R}, we can calculate the joint probability\nâ„™(Xâ‰¤y,Uâ‰¤g(X)Kf(X))=ğ”¼[1Xâ‰¤y1Uâ‰¤g(X)Kf(X)]=ğ”¼[1Xâ‰¤yâ„™(Uâ‰¤g(X)Kf(X)|X)]=ğ”¼[1Xâ‰¤yg(X)Kf(X)]=âˆ«âˆ’âˆyg(x)Kf(x)f(x)dx=âˆ«âˆ’âˆyg(x)Kdx\n\\begin{align*}\n\\mathbb{P}\\left(X\\le y,U\\le\\frac{g(X)}{Kf(X)}\\right) & =\\mathbb{E}\\left[1_{X\\le y}1_{U\\le\\frac{g(X)}{Kf(X)}}\\right]\\\\\n & =\\mathbb{E}\\left[1_{X\\le y}\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X\\right)\\right]\\\\\n & =\\mathbb{E}\\left[1_{X\\le y}\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^{y}\\frac{g(x)}{Kf(x)}f(x)dx\\\\\n & =\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx\n\\end{align*}\n\n\nand so we have the joint probability (above - â„™(A,B)\\mathbb{P}(A,B)), and the probability of acceptance (previous slide - â„™(B)\\mathbb{P}(B)), so the probability, conditional on acceptance (â„™(A|B)\\mathbb{P}(A|B)) is â„™(A,B)â„™(B)\\frac{\\mathbb{P}(A,B)}{\\mathbb{P}(B)} by Bayes rule."
  },
  {
    "objectID": "slides/temp.html#mc-methods-13",
    "href": "slides/temp.html#mc-methods-13",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\nâ„™(Xâ‰¤y|Uâ‰¤g(X)Kf(X))=âˆ«âˆ’âˆyg(x)Kdxâˆ«âˆ’âˆâˆg(X)Kdx=âˆ«âˆ’âˆyÏ€(x)dx\n\\mathbb{P}\\left(X\\le y|U\\le\\frac{g(X)}{Kf(X)}\\right)=\\frac{\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx}{\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx}=\\int_{-\\infty}^{y}\\pi(x)dx"
  },
  {
    "objectID": "slides/temp.html#mc-methods-14",
    "href": "slides/temp.html#mc-methods-14",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling\n\nconsider random variable XX with pdf/pmf q(x)&gt;0;xâˆˆDq(x)&gt;0;\\;x\\in D which is difficult to sample from\nwe will sample from qq using a proposal pdf/pmf ff which we can sample from\nif we can find a constant KK such that q(x)â‰¤Kf(x);âˆ€xâˆˆDq(x)\\le Kf(x); \\forall x\\in D. Alternatively q(x)f(x)â‰¤K\\frac{q(x)}{f(x)}\\le K\nthen there is a rejection method that returns Xâˆ¼qX\\sim q"
  },
  {
    "objectID": "slides/temp.html#mc-methods-15",
    "href": "slides/temp.html#mc-methods-15",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling method\n\ngiven a proposal pdf/pmf ff we can sample from, and constant KK such that q(x)f(x)â‰¤K;âˆ€xâˆˆD\\frac{q(x)}{f(x)}\\le K; \\forall x\\in D\nsample Yiâˆ¼fY_i\\sim f and Uiâˆ¼U[0,1]U_i\\sim\\mathrm{U}[0,1]\nfor Uiâ‰¤q(Yi)Kf(Yi)U_i\\le\\frac{q(Y_i)}{Kf(Y_i)} return Xi=YiX_i=Y_i; otherwise return nothing and continue."
  },
  {
    "objectID": "slides/temp.html#mc-methods-16",
    "href": "slides/temp.html#mc-methods-16",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: proof for discrete rv\n\nWe have â„™(X=x)=âˆ‘i=1nâ„™(rejectY)nâˆ’1â„™(drawY=xandaccept)\\mathbb{P}(X=x) = \\sum_{i=1}^n\\mathbb{P}(\\mathrm{reject }\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw }\\,Y=x\\,\\mathrm{and\\, accept})\nWe also have\nâ„™(drawY=xandaccept)=â„™(drawY=x)â„™(acceptY|Y=x)=f(x)â„™(Uâ‰¤q(Y)Kf(Y)|Y=x)=q(x)K\n\\begin{align*}\n & \\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n= & \\mathbb{P}(\\mathrm{draw}\\,Y=x)\\mathbb{P}(\\left.\\mathrm{accept}\\,Y\\right|Y=x)\\\\\n= & f(x)\\mathbb{P}(\\left.U\\le\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n= & \\frac{q(x)}{K}\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-17",
    "href": "slides/temp.html#mc-methods-17",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: proof for discrete rv\n\nThe probability of rejection of a draw is\nâ„™(rejectY)=âˆ‘xâˆˆDâ„™(drawY=xandrejectit)=âˆ‘xâˆˆDf(x)â„™(Uâ‰¥q(Y)Kf(Y)|Y=x)=âˆ‘xâˆˆDf(x)(1âˆ’q(x)Kf(x))=1âˆ’1K\n\\begin{align*}\n\\mathbb{P}(\\mathrm{{reject}}\\,Y) & =\\sum_{x\\in D}\\mathbb{P}(\\mathrm{{draw}}\\,Y=x\\,\\mathrm{and\\,reject\\,it})\\\\\n & =\\sum_{x\\in D}f(x)\\mathbb{P}(\\left.U\\ge\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n & =\\sum_{x\\in D}f(x)(1-\\frac{q(x)}{Kf(x)})=1-\\frac{1}{K}\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-18",
    "href": "slides/temp.html#mc-methods-18",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: proof for discrete rv\n\nand so1\nâ„™(X=x)=âˆ‘n=1âˆâ„™(rejectY)nâˆ’1â„™(drawY=xandaccept)=âˆ‘n=1âˆ(1âˆ’1K)nâˆ’1q(x)K=q(x)\n\\begin{align*}\n\\mathbb{P}(X=x) & =\\sum_{n=1}^{\\infty}\\mathbb{P}(\\mathrm{reject}\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n & =\\sum_{n=1}^{\\infty}\\left(1-\\frac{1}{K}\\right)^{n-1}\\frac{q(x)}{K}=q(x)\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-19",
    "href": "slides/temp.html#mc-methods-19",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: proof for continuous scalar rv\n\nRecal that we accept the proposal YY whenever (U,Y)âˆ¼qU,Y(U,Y)\\sim q_{U,Y} where qU,Y(u,y)=q(y)U0,1(u)q_{U,Y}\\left(u,y\\right)=q(y)U_{0,1}(u) such that Uâ‰¤q(Y)/(Kf(Y))U\\le q(Y)/(Kf(Y))\nWe have\nâ„™(Xâ‰¤x)=â„™(Yâ‰¤x|Uâ‰¤q(Y)/Kf(Y)))=â„™(Yâ‰¤x,Uâ‰¤q(Y)/Kf(Y)))â„™(Uâ‰¤q(Y)/Kf(Y)))=âˆ«âˆ’âˆxâˆ«0q(y)/Kf(y)fU,Y(u,y)dudyâˆ«âˆ’âˆâˆâˆ«0q(y)/Kf(y)fU,Y(u,y)dudy=âˆ«âˆ’âˆxâˆ«0q(y)/Kf(y)f(y)dudyâˆ«âˆ’âˆâˆâˆ«0q(y)/Kf(y)f(y)dudy=âˆ«âˆ’âˆxq(y)dy\n\\begin{align*}\n\\mathbb{P}\\left(X\\le x\\right) & =\\mathbb{P}\\left(\\left.Y\\le x\\right|U\\le q(Y)/Kf(Y))\\right)\\\\\n & =\\frac{\\mathbb{P}\\left(Y\\le x,U\\le q(Y)/Kf(Y))\\right)}{\\mathbb{P}\\left(U\\le q(Y)/Kf(Y))\\right)}\\\\\n & =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}\\\\\n & =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}=\\int_{-\\infty}^{x}q(y)dy\n\\end{align*}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-20",
    "href": "slides/temp.html#mc-methods-20",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nrejection sampling: unknown normalizing constants\n\nIn most practical scenarios, we know f(x)f(x) and q(x)q(x) only up to some normalizing constants\nf(x)=fâ€¾(x)/Kfandq(x)=qâ€¾(x)/Kq\nf(x)=\\bar{f}(x)/K_f\\;\\mathrm{and}\\;q(x)=\\bar{q}(x)/K_q\n We can still use rejection sampling since\nq(x)f(x)â‰¤Kiffqâ€¾(x)fâ€¾(x)â‰¤KÌ‚â‰¡KKqKf\n\\frac{q(x)}{f(x)}\\le K\\;\\mathrm{iff}\\;\\frac{\\bar{q}(x)}{\\bar{f}(x)}\\le\\hat{K}\\equiv  K\\frac{K_q}{K_f}\n In practice this means we can ignore the normalizing constants if we can find KÌ‚\\hat{K} to bound qâ€¾(x)fâ€¾(x)\\frac{\\bar{q}(x)}{\\bar{f}(x)}"
  },
  {
    "objectID": "slides/temp.html#mc-methods-21",
    "href": "slides/temp.html#mc-methods-21",
    "title": "temp",
    "section": "MC Methods",
    "text": "MC Methods\n\nSuppose we need to compute an expectation ğ”¼[g(Z)]\\mathbb{E}[g(Z)] for some random variable ZZ and some function g:â„â†’â„g:\\mathbb{R}\\to\\mathbb{R}. Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of ZZ, say z1,z2,â€¦,zMz_1,z_2,\\ldots,z_M and estimate ğ”¼[g(Z)]\\mathbb{E}[g(Z)] as 1Mâˆ‘i=1Mg(zi)\\frac{1}{M}\\sum_{i=1}^Mg(z_i). The law of large numbers states that this sample mean should be close to ğ”¼[g(Z)]\\mathbb{E}[g(Z)].\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/temp.html#stochastic-processes",
    "href": "slides/temp.html#stochastic-processes",
    "title": "temp",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nA stochastic process, which we will usually write as (Xn)(X_n), is an indexed sequence of random variables that are (usually) dependent on each other.\nEach random variable XnX_n takes a value in a state space ğ’®\\mathcal S which is the set of possible values for the process. As with usual random variables, the state space ğ’®\\mathcal S can be discrete or continuous. A discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, ğ’®={Heads,Tails}\\mathcal S = \\{\\text{Heads},\\text{Tails}\\} is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers ğ’®=â„¤+={0,1,2,â€¦}\\mathcal S = \\mathbb Z_+ = \\{0,1,2,\\dots\\}."
  },
  {
    "objectID": "slides/temp.html#stochastic-processes-1",
    "href": "slides/temp.html#stochastic-processes-1",
    "title": "temp",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nFurther, the process has an index set that puts the random variables that make up the process in order. The index set is usually interpreted as a time variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at distinct points, often denoted by n=0,1,2,â€¦n = 0,1,2,\\dots, while continuous time denotes a process monitored constantly over time, often denoted by tâˆˆâ„+={xâˆˆâ„:xâ‰¥0}t \\in \\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}."
  },
  {
    "objectID": "slides/temp.html#markov-property",
    "href": "slides/temp.html#markov-property",
    "title": "temp",
    "section": "Markov property",
    "text": "Markov property\nThink of a simple board game where we roll a dice and move that many squares forward on the board. Suppose we are currently on the square XnX_n. Then what can we say about which square Xn+1X_{n+1} we move to on our next turn?\n\nXn+1X_{n+1} is random, since it depends on the roll of the dice.\nXn+1X_{n+1} depends on where we are now XnX_n, since the score of dice will be added onto the number our current square,\nGiven the square XnX_n we are now, Xn+1X_{n+1} doesnâ€™t depend any further on which sequence of squares X0,X1,â€¦,Xnâˆ’1X_0, X_1, \\dots, X_{n-1} we used to get here.\n\nThe third point is called the Markov property or memoryless property. We say â€œmemorylessâ€, because we only need to remember what square weâ€™ve reached, not which squares we used to get here. The stochastic process before this moment has no bearing on the future, given where we are now. A mathematical way to say this is that â€œthe past and the future are conditionally independent given the present.â€"
  },
  {
    "objectID": "slides/temp.html#markov-chains",
    "href": "slides/temp.html#markov-chains",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider the following simple random walk on the integers â„¤\\mathbb Z: We start at 00, then at each time step, we go up by one with probability pp and down by one with probability q=1âˆ’pq = 1-p. When p=q=12p = q = \\frac12, weâ€™re equally as likely to go up as down, and we call this the simple symmetric random walk.\nThe simple random walk is a simple but very useful model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In many modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the â€œdrunkardâ€™s walkâ€, suggesting it could model a drunk person trying to stagger home."
  },
  {
    "objectID": "slides/temp.html#markov-chains-1",
    "href": "slides/temp.html#markov-chains-1",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nrandom walks\nrequire(ggplot2, quietly = TRUE)\nset.seed(315)\n\nrrw &lt;- function(n, p = 1/2) {\n  q &lt;- 1 - p\n  Z &lt;- sample(c(1, -1), n, replace = TRUE, prob = c(p, q))\n  X &lt;- c(0, cumsum(Z))\n  c(0, cumsum(Z))\n}\n\nn &lt;- 2000\nrw_dat &lt;- tibble::tibble(x=0:n) |&gt; \n  dplyr::mutate(\n    \"p = 2/3\" = rrw(n, 2/3)\n    , \"p = 1/3\" = rrw(n, 1/3)\n    , \"p = 1/2\" = rrw(n, 1/2)\n  )\n\np0 &lt;- rw_dat |&gt; dplyr::slice_head(n=20) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np1 &lt;- rw_dat |&gt; dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np3 &lt;- rw_dat |&gt; #dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np0+p1+p3"
  },
  {
    "objectID": "slides/temp.html#markov-chains-2",
    "href": "slides/temp.html#markov-chains-2",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nWe can write this as a stochastic process (Xn)(X_n) with discrete time n={0,1,2,â€¦}=â„¤+n = \\{0,1,2,\\dots\\} = \\mathbb Z_+ and discrete state space ğ’®=â„¤\\mathcal S = \\mathbb Z, where X0=0X_0 = 0 and, for nâ‰¥0n \\geq 0, we have Xn+1={Xn+1with probability p,Xnâˆ’1with probability q. X_{n+1} = \\begin{cases} X_n + 1 & \\text{with probability $p$,} \\\\\n                             X_n - 1 & \\text{with probability $q$.} \\end{cases} \nItâ€™s clear from this definition that Xn+1X_{n+1} (the future) depends on XnX_n (the present), but, given XnX_n, does not depend on Xnâˆ’1,â€¦,X1,X0X_{n-1}, \\dots, X_1, X_0 (the past). Thus the Markov property holds, and the simple random walk is a discrete time Markov process or Markov chain."
  },
  {
    "objectID": "slides/temp.html#markov-chains-3",
    "href": "slides/temp.html#markov-chains-3",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nSo far weâ€™ve seen a a few examples of stochastic processes in discrete time and discrete space with the Markov memoryless property. Now we will develop the theory more generally.\nTo define a so-called â€œMarkov chainâ€, we first need to say where we start from, and second what the probabilities of transitions from one state to another are.\nIn our examples of the simple random walk and gamblerâ€™s ruin, we specified the start point X0=iX_0 = i exactly, but we could pick the start point at random according to some distribution Î»i=â„™(X0=i)\\lambda_i = \\mathbb P(X_0 = i)."
  },
  {
    "objectID": "slides/temp.html#markov-chains-4",
    "href": "slides/temp.html#markov-chains-4",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nAfter that, we want to know the transition probabilities â„™(Xn+1=jâˆ£Xn=i)\\mathbb P(X_{n+1} = j \\mid X_n = i) for i,jâˆˆğ’®i,j \\in \\mathcal S. Here, because of the Markov property, the transition probability only needs to condition on the state weâ€™re in now Xn=iX_n = i, and not on the whole history of the process.\nIn the case of the simple random walk, for example, we had initial distribution Î»i=â„™(X0=i)={1if i=00otherwise \\lambda_i = \\mathbb P(X_0 = i) = \\begin{cases} 1 & \\text{if $i = 0$} \\\\ 0 & \\text{otherwise} \\end{cases}  and transition probabilities â„™(Xn+1=jâˆ£Xn=i)={pif j=i+1qif j=iâˆ’10otherwise. \\mathbb P(X_{n+1} = j \\mid X_n = i) = \\begin{cases} p & \\text{if $j = i+1$} \\\\ q & \\text{if $j = i-1$} \\\\ 0 & \\text{otherwise.} \\end{cases} \nFor the random walk (and also the gamblerâ€™s ruin), the transition probabilities â„™(Xn+1=jâˆ£Xn=i)\\mathbb P(X_{n+1} = j \\mid X_n = i) donâ€™t depend on nn; in other words, the transition probabilities stay the same over time. A Markov process with this property is called time homogeneous. We will always consider time homogeneous processes from now on (unless we say otherwise)."
  },
  {
    "objectID": "slides/temp.html#markov-chains-5",
    "href": "slides/temp.html#markov-chains-5",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nLetâ€™s write pij=â„™(Xn+1=jâˆ£Xn=i)p_{ij} = \\mathbb P(X_{n+1} = j \\mid X_n = i) for the transition probabilities, which are independent of nn. We must have pijâ‰¥0p_{ij} \\geq 0, since it is a probability, and we must also have âˆ‘jpij=1\\sum_j p_{ij} = 1 for all states ii, as this is the sum of the probabilities of all the places you can move to from state i.\nWhen the state space is finite (and even sometimes when itâ€™s not), itâ€™s convenient to write the transition probabilities (pij)(p_{ij}) as a matrix ğ–¯\\mathsf P, called the transition matrix, whose (i,j)(i,j)th entry is pijp_{ij}. Then the condition that âˆ‘jpij=1\\sum_j p_{ij} = 1 is the condition that each of the rows of ğ–¯\\mathsf P add up to 11."
  },
  {
    "objectID": "slides/temp.html#markov-chains-6",
    "href": "slides/temp.html#markov-chains-6",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nConsider a simple two-state Markov chain with state space ğ’®={0,1}\\mathcal S = \\{0,1\\} and transition matrix ğ–¯=(p00p01p10p11)=(1âˆ’Î±Î±Î²1âˆ’Î²) \\mathsf P = \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\beta & 1-\\beta \\end{pmatrix}   for some 0&lt;Î±,Î²&lt;10 &lt; \\alpha, \\beta &lt; 1. Note that the rows of ğ–¯\\mathsf P add up to 11, as they must.\nWe can illustrate ğ–¯\\mathsf P by a transition diagram, where the blobs are the states and the arrows give the transition probabilities. (We donâ€™t draw the arrow if pij=0p_{ij} = 0.) In this case, our transition diagram looks like this:"
  },
  {
    "objectID": "slides/temp.html#markov-chains-7",
    "href": "slides/temp.html#markov-chains-7",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\n\n\n\nTransition diagram for the two-state Markov chain"
  },
  {
    "objectID": "slides/temp.html#markov-chains-8",
    "href": "slides/temp.html#markov-chains-8",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nWe can use this as a simple model of customer churn, for example. If the customer has closed their account (state 0) on one period, then with probability Î±\\alpha we will be able to entice them to open their account again (state 1) by the next period; while if the customer has an account (state 1), then with probability Î²\\beta they will have closed their account (state 0) by the next period."
  },
  {
    "objectID": "slides/temp.html#markov-chains-9",
    "href": "slides/temp.html#markov-chains-9",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIf the customer has an account on period n, whatâ€™s the probability they also have an account on period n+2?\np11(2)=â„™(Xn+2=1âˆ£Xn=1)\np_{11}(2) = \\mathbb P (X_{n+2} = 1 \\mid X_n = 1)\n\nThe key to calculating this is to condition on the first step again â€“ that is, on whether the printer is working on Tuesday. We have\np11(2)=â„™(Xn+1=0âˆ£Xn=1)â„™(Xn+2=1âˆ£Xn+1=0,Xn=1)+â„™(Xn+1=1âˆ£Xn=1)â„™(Xn+2=1âˆ£Xn+1=1,Xn=1)=â„™(Xn+1=0âˆ£Xn=1)â„™(Xn+2=1âˆ£Xn+1=0)+â„™(Xn+1=1âˆ£Xn=1)â„™(Xn+2=1âˆ£Xn+1=1)=p10p01+p11p11=Î²Î±+(1âˆ’Î²)2.\n\\begin{align*}\n  p_{11}(2) &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0, X_n = 1) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1, X_n = 1) \\\\\n  &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1) \\\\\n  &= p_{10}p_{01} + p_{11}p_{11} \\\\\n  &= \\beta\\alpha + (1-\\beta)^2 .\n\\end{align*} \n\nIn the second equality, we used the Markov property to mean conditional probabilities like â„™(Xn+2=1âˆ£Xn+1=k)\\mathbb P(X_{n+2} = 1 \\mid X_{n+1} = k) did not have to depend on XnX_n.\nAnother way to think of this as we summing the probabilities of all length-2 paths from 1 to 1, which are 1â†’0â†’11\\to 0\\to 1 with probability Î²Î±\\beta\\alpha and 1â†’1â†’11 \\to 1 \\to 1 with probability (1âˆ’Î²)2(1-\\beta)^2\n\nIn the above example, we calculated a two-step transition probability pij(2)=â„™(Xn+2=jâˆ£Xn=i)p_{ij}(2) = \\mathbb P (X_{n+2} = j \\mid X_n = i) by conditioning on the first step. That is, by considering all the possible intermediate steps kk, we have\npij(2)=âˆ‘kâˆˆğ’®â„™(Xn+1=kâˆ£Xn=i)â„™(Xn+2=jâˆ£Xn+1=k)=âˆ‘kâˆˆğ’®pikpkj \np_{ij}(2) = \\sum_{k\\in\\mathcal S} \\mathbb P (X_{n+1} = k \\mid X_n = i)\\mathbb P (X_{n+2} = j \\mid X_{n+1} = k) = \\sum_{k\\in\\mathcal S} p_{ik}p_{kj}\n\nBut this is exactly the formula for multiplying the matrix ğ–¯\\mathsf P with itself! In other words, pij(2)=âˆ‘kpikpkjp_{ij}(2) = \\sum_{k} p_{ik}p_{kj} is the (i,j)(i,j)th entry of the matrix square ğ–¯2=ğ–¯ğ–¯\\mathsf P^2 = \\mathsf{PP}. If we write ğ–¯(2)=(pij(2))\\mathsf P(2)  = (p_{ij}(2)) for the matrix of two-step transition probabilities, we have ğ–¯(2)=ğ–¯2\\mathsf P(2) = \\mathsf P^2.\nMore generally, we see that this rule holds over multiple steps, provided we sum over all the possible paths iâ†’k1â†’k2â†’â‹¯â†’knâˆ’1â†’ji\\to k_1 \\to k_2 \\to \\cdots \\to k_{n-1} \\to j of length nn from ii to jj."
  },
  {
    "objectID": "slides/temp.html#markov-chains-10",
    "href": "slides/temp.html#markov-chains-10",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTheorem 1 Let (Xn)(X_n) be a Markov chain with state space ğ’®\\mathcal S and transition matrix ğ–¯=(pij)\\mathsf P = (p_{ij}). For i,jâˆˆğ’®i,j \\in \\mathcal S, write pij(n)=â„™(Xn=jâˆ£X0=i) p_{ij}(n) = \\mathbb P(X_n = j \\mid X_0 = i)  for the nn-step transition probability. Then\npij(n)=âˆ‘k1,k2,â€¦,knâˆ’1âˆˆğ’®pik1pk1k2â‹¯pknâˆ’2knâˆ’1pknâˆ’1j \np_{ij}(n) = \\sum_{k_1, k_2, \\dots, k_{n-1} \\in \\mathcal S} p_{ik_1} p_{k_1k_2} \\cdots p_{k_{n-2}k_{n-1}} p_{k_{n-1}j}\n\nIn particular, pij(n)p_{ij}(n) is the (i,j)(i,j)th element of the matrix power ğ–¯n\\mathsf P^n, and the matrix of nn-step transition probabilities is given by ğ–¯(n)=ğ–¯n\\mathsf P(n) = \\mathsf P^n."
  },
  {
    "objectID": "slides/temp.html#markov-chains-11",
    "href": "slides/temp.html#markov-chains-11",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nThe so-called Chapmanâ€“Kolmogorov equations follow immediately from this.\n\nLet (Xn)(X_n) be a Markov chain with state space ğ’®\\mathcal S and transition matrix ğ–¯=(pij)\\mathsf P = (p_{ij}). Then, for non-negative integers n,mn,m, we have pij(n+m)=âˆ‘kâˆˆğ’®pik(n)pkj(m), p_{ij}(n+m) = \\sum_{k \\in \\mathcal S} p_{ik}(n)p_{kj}(m) ,  or, in matrix notation, ğ–¯(n+m)=ğ–¯(n)ğ–¯(m)\\mathsf P(n+m) = \\mathsf P(n)\\mathsf P(m).\n\nIn other words, a trip of length n+mn + m from ii to jj is a trip of length nn from ii to some other state kk, then a trip of length mm from kk back to jj, and this intermediate stop kk can be any state, so we have to sum the probabilities.\nOf course, once we know that ğ–¯(n)=ğ–¯n\\mathsf P(n) = \\mathsf P^n is given by the matrix power, itâ€™s clear to see that ğ–¯(n+m)=ğ–¯n+m=ğ–¯nğ–¯m=ğ–¯(n)ğ–¯(m)\\mathsf P(n+m) = \\mathsf P^{n+m} = \\mathsf P^n \\mathsf P^m = \\mathsf P(n)\\mathsf P(m)."
  },
  {
    "objectID": "slides/temp.html#markov-chains-12",
    "href": "slides/temp.html#markov-chains-12",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\nIf we start from a state given by a distribution ğ›‘=(Ï€i)\\boldsymbol \\pi = (\\pi_i), then after step 1 the probability weâ€™re in state jj is âˆ‘iÏ€ipij\\sum_i \\pi_i p_{ij}. So if Ï€j=âˆ‘iÏ€ipij\\pi_j = \\sum_i \\pi_i p_{ij}, we stay in this distribution forever. We call such a distribution a stationary distribution. We again recognise this formula as a matrix-vector multiplication, so this is ğ›‘=ğ›‘ğ–¯\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P, where ğ›‘\\boldsymbol \\pi is a row vector.\n\nLet (Xn)(X_n) be a Markov chain on a state space ğ’®\\mathcal S with transition matrix ğ–¯\\mathsf P. Let ğ›‘=(Ï€i)\\boldsymbol \\pi = (\\pi_i) be a distribution on ğ’®\\mathcal S, in that Ï€iâ‰¥0\\pi_i \\geq 0 for all iâˆˆğ’®i \\in \\mathcal S and âˆ‘iâˆˆğ’®Ï€i=1\\sum_{i \\in \\mathcal S} \\pi_i = 1. We call ğ›‘\\boldsymbol \\pi a stationary distribution if\nÏ€j=*âˆ‘iâˆˆğ’®Ï€ipijfor all jâˆˆğ’®\n\\pi_j =* \\sum_{i\\in \\mathcal S} \\pi_i p_{ij} \\quad \\text{for all $j \\in \\mathcal S$} \n\nor, equivalently, if ğ›‘=ğ›‘ğ–¯\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P.\n\nNote that weâ€™re saying the distribution â„™(Xn=i)\\mathbb P(X_n = i) stays the same; the Markov chain (Xn)(X_n) itself will keep moving. One way to think is that if we started off a thousand Markov chains, choosing each starting position to be ii with probability Ï€i\\pi_i, then (roughly) 1000Ï€j1000 \\pi_j of them would be in state jj at any time in the future â€“ but not necessarily the same ones each time."
  },
  {
    "objectID": "slides/temp.html#markov-chains-13",
    "href": "slides/temp.html#markov-chains-13",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nproperties\nA Markov Chain is irreducible if you have positive probability of eventually getting from anywhere to anywhere else.\nA Markov Chain is aperiodic if there are no forced cycles, i.e.Â there do not exist disjoint non-empty subsets X1,X2,â€¦,Xd fordâ‰¥2,suchthatP(x,Xi+1)=1 forallxâˆˆXi (1â‰¤iâ‰¤dâˆ’1),andP(x,X1)=1forallxâˆˆXd. [Diagram.]"
  },
  {
    "objectID": "slides/temp.html#markov-chains-14",
    "href": "slides/temp.html#markov-chains-14",
    "title": "temp",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\n\n\n\nMC with cycle"
  },
  {
    "objectID": "slides/temp.html#markov-chain-monte-carlo",
    "href": "slides/temp.html#markov-chain-monte-carlo",
    "title": "temp",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\nSuppose have complicated, high-dimensional density Ï€=cg\\pi = cg and we want samples X1,X2,â€¦âˆ¼Ï€X_1, X_2,\\dots \\sim \\pi. (Then can do Monte Carlo.)\nDefine a Markovchain (dependent random process) X0,X1,X2â€¦X_0, X_1,X_2\\dots in such a way that for large enough nn, Xnâˆ¼Ï€X_n\\sim \\pi.\nThen we can estimate ğ”¼Ï€(h)â‰¡âˆ«h(x)Ï€(x)dx\\mathbb{E}_{\\pi}(h) â‰¡ \\int h(x) \\pi(x) dx by:\nğ”¼Ï€[h]â‰ˆ1Mâˆ’Bâˆ‘i=B+1M\n\\mathbb{E}_{\\pi}[h] \\approx \\frac{1}{M-B}\\sum_{i=B+1}^{M}\n\nwhere BB (â€œburn-inâ€) is chosen large enough so XBâˆ¼Ï€X_B\\sim\\pi, and MM is chosen large enough to get good Monte Carlo estimates."
  },
  {
    "objectID": "slides/temp.html#mcmc-metropolis-algo",
    "href": "slides/temp.html#mcmc-metropolis-algo",
    "title": "temp",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\nchoose some initial value X0X_0, then\ngiven Xnâˆ’1X_{n-1}, choose a proposal state Ynâˆ¼MVN(Xnâˆ’1,Ïƒ2I)Y_n\\sim \\textrm{MVN}(X_{n-1},\\sigma^2\\textrm{I}) for some fixed Ïƒ2&gt;0\\sigma^2&gt;0\nlet An=Ï€(Yn)/Ï€(Xnâˆ’1=g(Yn)/g(Xnâˆ’1)A_n=\\pi(Y_n)/\\pi(X_{n-1}=g(Y_n)/g(X_{n-1}) and Unâˆ¼U[0,1]U_n\\sim\\textrm{U}[0,1], then\nid Un&lt;AnU_n&lt;A_n set Xn=YnX_n=Y_n (â€œacceptâ€), otherwise set $X_n = X_{n-1} ) â€œrejectâ€\nrepeat for n=1,2,3,â€¦,Mn=1,2,3,\\ldots,M\n\nThis version is called random-walk Metropolis"
  },
  {
    "objectID": "slides/temp.html#mcmc-metropolis-algo-1",
    "href": "slides/temp.html#mcmc-metropolis-algo-1",
    "title": "temp",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\n\na simple Metropolis algorithm in one dimension\ng = function(y) {\n    if ( (y&lt;0) || (y&gt;1) )\n    return(0)\n    else\n    return( y^3 * sin(y^4) * cos(y^5) )\n}\n\nh = function(y) { return(y^2) }\n\nM = 11000  # run length\nB = 1000  # amount of burn-in\nX = runif(1)  # overdispersed starting distribution\nsigma = 1  # proposal scaling\nxlist = rep(0,M)  # for keeping track of chain values\nhlist = rep(0,M)  # for keeping track of h function values\nnumaccept = 0;\n\nfor (i in 1:M) {\n  Y = X + sigma * rnorm(1)  # proposal value\n  U = runif(1)              # for accept/reject\n  alpha = g(Y) / g(X)       # for accept/reject\n  if (U &lt; alpha) {\n    X = Y                   # accept proposal\n    numaccept = numaccept + 1;\n  }\n    xlist[i] = X;\n    hlist[i] = h(X);\n}\n\ncat(\"ran Metropolis algorithm for\", M, \"iterations, with burn-in\", B, \"\\n\");\n\n\nran Metropolis algorithm for 11000 iterations, with burn-in 1000 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"acceptance rate =\", numaccept/M, \"\\n\");\n\n\nacceptance rate = 0.106 \n\n\na simple Metropolis algorithm in one dimension\nu = mean(hlist[(B+1):M])\ncat(\"mean of h is about\", u, \"\\n\")\n\n\nmean of h is about 0.7643 \n\n\na simple Metropolis algorithm in one dimension\nse1 =  sd(hlist[(B+1):M]) / sqrt(M-B)\ncat(\"iid standard error would be about\", se1, \"\\n\")\n\n\niid standard error would be about 0.00171 \n\n\na simple Metropolis algorithm in one dimension\nvarfact &lt;- function(xxx) { 2 * sum(acf(xxx, plot=FALSE)$acf) - 1 }\nthevarfact = varfact(hlist[(B+1):M])\nse = se1 * sqrt( thevarfact )\ncat(\"varfact = \", thevarfact, \"\\n\")\n\n\nvarfact =  20.52 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"true standard error is about\", se, \"\\n\")\n\n\ntrue standard error is about 0.007744 \n\n\na simple Metropolis algorithm in one dimension\ncat(\"approximate 95% confidence interval is (\", u - 1.96 * se, \",\",\n                        u + 1.96 * se, \")\\n\\n\")\n\n\napproximate 95% confidence interval is ( 0.7492 , 0.7795 )\n\n\na simple Metropolis algorithm in one dimension\nplot(xlist, type='l')\n\n\n\n\n\n\n\n\n\na simple Metropolis algorithm in one dimension\n# acf(xlist)"
  },
  {
    "objectID": "slides/temp.html#footnotes",
    "href": "slides/temp.html#footnotes",
    "title": "temp",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe geometric distribution is a discrete distribution that can be interpreted as the number of failures before the first success (â„™(X=k)=(1âˆ’p)kâˆ’1p\\mathbb{P}(X=k)=(1-p)^{k-1}p, with mean pp).â†©ï¸"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_09_eh.html#recap-of-last-week",
    "title": "Monte Carlo Methods",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced the fundamental problems of inference and the biases of some intuitive estimators.\nWe also built a basic understanding of the tools used to state and then satisfy causality assumptions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#this-week",
    "href": "slides/BSMM_8740_lec_09_eh.html#this-week",
    "title": "Monte Carlo Methods",
    "section": "This week",
    "text": "This week\n\nWe will get explore Monte Carlo methods as a way to integrate difficult functions, and sample from difficult probability distributions.\nAlong the way we will look at Markov Chains which both underlie sampling methods and provide a way to model the generation of data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#monte-carlo-mc-methods",
    "href": "slides/BSMM_8740_lec_09_eh.html#monte-carlo-mc-methods",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo (MC) Methods",
    "text": "Monte Carlo (MC) Methods\nMonte Carlo methods are a class of simulation-based methods that seek to avoid complicated and/or intractable mathematical computations.\nEspecially those that arise from probability distributions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nWe can use MC methods to estimate probabilities: for a random variable \\(Z\\) with outcomes in a set \\(\\Omega\\), with some subset \\(S\\subset\\Omega\\) and event \\(E\\equiv Z\\in S\\), we can compute the probability of \\(E\\) (i.e.Â \\(\\mathbb{P}(E)\\)) with of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) as\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-1",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-1",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample\nIf \\(Z\\sim\\mathscr{N}(1,3)\\) and \\(S=Z:0\\le Z\\le 3\\) then\n\\[\n\\mathbb{P}(E) = \\mathbb{P}(0\\le Z\\le 3) = \\int_0^3\\frac{1}{\\sqrt{2\\pi3}}e^{-\\frac{(t-1)^2}{2*3}}dt\n\\] In which case it is easier to just use R and calculate:\n\n\nCode\npnorm(3, mean=1, sd=sqrt(3)) - pnorm(0, mean=1, sd=sqrt(3))\n\n\n[1] 0.594\n\n\nthan it is to compute the integral."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-2",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-2",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample continued\nSurprisingly, in case we donâ€™t know that pnorm exists, we could do this:\n\n\nMC computation\n# define the event\nevent_E_happened &lt;- function( x ) {\n  if( 0 &lt;= x & x &lt;= 3 ) {\n    return( TRUE ) # The event happened\n  } else {\n    return( FALSE ) # The event DIDN'T happen\n  }\n}\n\n# generate lots of copies of Z...\nNMC &lt;- 10000; # 10000 seems like \"a lot\".\nrnorm( NMC, mean=1, sd=sqrt(3) ) |&gt; \n  purrr::map_lgl(event_E_happened) |&gt; \n  sum()/NMC\n\n\n[1] 0.5968"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-3",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-3",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nNow\n\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]\n\nis the MC estimate of \\(\\mathbb{E}[E]\\), which is unbiased because for each \\(i\\), \\(\\mathbb{E}[1_{z_i\\in S}]=\\mathbb{E}[E]\\), and the variance is\n\n\\[\n\\mathrm{Var}\\left({\\mathbb{P}(E)}\\right)=\\frac{1}{M^2}\\mathrm{Var}\\left(\\sum_{i=1}^M 1_{z_i\\in S}\\right)=\\frac{1}{M^2}\\sum_{i=1}^M \\mathrm{Var}\\left(1_{z_i\\in S}\\right)=\\frac{1}{M} \\mathrm{Var}\\left(1_{z_i\\in S}\\right)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-4",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-4",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nThis is true for any function of the random variable \\(Z\\), and\n\\[\n\\mathbb{E}\\left[h(Z)\\right]\\approx \\hat{h}=\\frac{1}{M}\\sum_{i=1}^M h(z_i)\n\\]\nand the variance of \\(h(Z)\\) decreases as \\(1/M\\) by the same reasoning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#random-number-generation",
    "href": "slides/BSMM_8740_lec_09_eh.html#random-number-generation",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nMonte Carlo simulation starts with random number generation, usually split into 2 stages:\n\ngeneration of independent uniform \\((0, 1)\\) random variables, and\nconversion into random variables with a particular distribution (e.g.Â Normal)\n\nVery important: never write your own generator, always use a well validated generator from a reputable source (e.g.Â R)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#random-number-generation-1",
    "href": "slides/BSMM_8740_lec_09_eh.html#random-number-generation-1",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nPseudo-random generators found in computers use a deterministic (i.e.Â repeatable) algorithm to generate a sequence of (apparently) random numbers on the \\((0, 1)\\) interval.\nWhat defines a good random number generator (RNG) has a long period â€“ how long it takes before the sequence repeats itself \\(2^{32}\\) is not enough (need at least \\(2^{40}\\)). various statistical tests to measure â€œrandomnessâ€ â€“ well validated software will have gone through these checks."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#random-number-generation-2",
    "href": "slides/BSMM_8740_lec_09_eh.html#random-number-generation-2",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nRecall that \\(\\mathscr{N}(0, 1)\\) Normal random variables (mean 0, variance 1) have the probability density function:\n\\[\np(x)=\\frac{1}{2\\pi}e^{-\\frac{1}{2}x^2}\\equiv\\phi(x)\n\\] and if \\(X\\sim\\mathscr{N}(0, 1)\\) then its CDF is:\n\\[\n\\mathbb{P}[X\\le x] = \\int_{-\\infty}^x\\phi(x)dx\n\\] ## Random Number Generation\nThe Box-Muller transformation method takes two independent uniform \\((0, 1)\\) random numbers \\(y_1\\), \\(y_2\\), and defines:\n\\[\n\\begin{align*}\nx_{1} & =\\sqrt{-2\\log y_{1}}\\cos(2\\pi y_{2})\\\\\nx_2& =\\sqrt{-2\\log y_{1}}\\sin(2\\pi y_{2})\n\\end{align*}\n\\] It can be proved that \\(x_1\\) and \\(x_2\\) are independent \\(\\mathscr{N}(0, 1)\\) random variables"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#random-number-generation-3",
    "href": "slides/BSMM_8740_lec_09_eh.html#random-number-generation-3",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nCode\nsamples &lt;- matrix(runif(10000), ncol=2) |&gt; data.frame() |&gt; \n  dplyr::mutate(\n    normals = \n      purrr::map2(\n        X1, X2\n        ,(\\(x1,x2){\n          data.frame(\n            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)\n            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) \n          )\n        })\n      )\n  ) |&gt; \n  tidyr::unnest(normals)  \n  \n\nsamples |&gt; \n  tidyr::pivot_longer(-c(X1,X2)) |&gt; \n  ggplot(aes(x=value, color=name, fill=name)) + \n  geom_histogram(\n    aes(y=..density..), bins = 60, position=\"identity\", alpha=0.3\n  ) + \n  labs(x=\"Value\", y=\"Density\") + theme_minimal()\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in\nggplot2 3.4.0.\nâ„¹ Please use `after_stat(density)` instead.\n\n\nCode\nsamples |&gt; \nggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#random-number-generation-4",
    "href": "slides/BSMM_8740_lec_09_eh.html#random-number-generation-4",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nYour computer is only capable of producing pseudorandom numbers. These are made by running a pseudorandom number generator algorithm which is deterministic, e.g.\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\nOnce the RNG seed is set, the â€œrandomâ€ numbers that R generates arenâ€™t random at all. But someone looking at these random numbers would have a very hard time distinguishing these numbers from truly random numbers. That is what â€œstatistical randomnessâ€ means!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#central-limit-theorem",
    "href": "slides/BSMM_8740_lec_09_eh.html#central-limit-theorem",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT says that for \\(f=\\mathbb{P}(E)\\) if \\(\\sigma^2\\equiv\\mathrm{Var}\\left(f\\right)\\) is finite then the error of the MC estimate\n\\[\ne_N(f)=\\bar{f}-\\mathbb{E}[f]\n\\]\nis approximately Normal in distribution for large \\(M\\), i.e.\n\\[\ne_N(f)\\sim\\sigma M^{1/2}Z\n\\] where \\(Z\\sim\\mathscr{N}(0,1)\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-5",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-5",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nSuppose we need to compute an expectation \\(\\mathbb{E}[g(Z)]\\) for some random variable \\(Z\\) and some function \\(g:\\mathbb{R}\\to\\mathbb{R}\\). Monte Carlo methods avoid doing any integration or summation and instead just generate lots of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) and estimate \\(\\mathbb{E}[g(Z)]\\) as \\(\\frac{1}{M}\\sum_{i=1}^Mg(z_i)\\). The law of large numbers states that this sample mean should be close to \\(\\mathbb{E}[g(Z)]\\).\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-6",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-6",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nWe can use Monte Carlo to estimate probabilities of the form \\(\\mathbb{P}\\left[E\\right]\\) by approximating expectations of the form \\(\\mathbb{E}[1_{X\\in E}]\\).\nIf \\(X\\sim\\mathscr{N}(\\mu,\\sigma)\\) and we want to compute \\(\\mathbb{E}[\\log|X|]\\), we could set up and solve the integral\n\\[\n\\mathbb{E} \\log |X|\n= \\int_{-\\infty}^\\infty \\left( \\log |t| \\right) f( t; \\mu, \\sigma) dt\n= \\int_{-\\infty}^\\infty \\frac{ \\log |t| }{ \\sqrt{2\\pi \\sigma^2} }\n                  \\exp\\left\\{ \\frac{ -(t-\\mu)^2 }{ 2\\sigma^2 } \\right\\}dt\n\\]\nAlternatively, we could just draw lots of Monte Carlo replicates \\(X_1,X_2,\\cdots,X_M\\) from a normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and look at the sample mean \\(M^{-1}\\sum_{i=1}^M\\log|x_i|\\), once again appealing to the law of large numbers to ensure that this sample mean is close to its expectation.\nMonte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-start-here",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-start-here",
    "title": "Monte Carlo Methods",
    "section": "MC Methods start here",
    "text": "MC Methods start here\n\nThis idea can be pushed still further. Suppose that we want to compute an integral\n\\[\n\\int_Dh(x)dx\n\\] where \\(D\\) is some domain of integration and \\(h(.)\\) is a function.\nLet \\(g(x)\\) be the density of some random variable with \\(g(x)&gt;0, \\forall x\\in D\\). In other words, \\(g\\) is the density of a random variable supported on \\(D\\). Then we can rewrite the integral as\n\\[\n\\int_Dh(x)dx = \\int_D\\frac{h(x)}{g(x)}g(x)dx = \\mathbb{E}_g[w(x)]\n\\] where \\(w(x)=h(x)/g(x)\\) and \\(X\\sim g\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-7",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-7",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nSuppose we are given \\(h(x)\\) and we want to compute \\(\\mathbb{E}[h(X)]\\) where \\(X\\sim g,\\,x\\in D\\). So we need to sample from \\(g\\), but what if we could not do that directly?\nIf there were some other distribution \\(f(x)\\) we could sample from, such that \\(f(x)&gt;0,\\,x\\in D\\), then\n\\[\n\\begin{align*}\n\\mathbb{E}_{g}\\left[h(x)\\right] & =\\int_{D}h(x)g(x)dx\\\\\n& =\\int_{S}h(x)\\frac{g(x)}{f(x)}f(x)dx=\\mathbb{E}_f\\left[h(x)\\frac{g(x)}{f(x)}\\right]\\\\\n& =\\frac{1}{n}\\sum_{i=1}^{n}h(x_{i})\\frac{g(x_{i})}{f(x_{i})}\\quad x_{i}\\sim f\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-8",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-8",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis is called importance sampling (IS).\n\ndraw iid \\(x_1,x_2,\\ldots,x_n\\) from \\(f\\) and calculate the importance weight\n\n\\[\nw(x_i)=\\frac{g(x_{i})}{f(x_{i})}\n\\] 2. estimate \\(\\mathbb{E}_g(h)\\) by\n\\[\n\\hat{\\mu}_h=\\frac{1}{n}\\sum_{i=1}^nw(x_i)h(x_i)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-9",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-9",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nexample\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = \\sqrt{2/\\pi}e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution)\n\n\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X) / sqrt(2 * pi)\n\nmu_h  &lt;- mean(W*X)\nvar_h &lt;- var(W*X)/n\nse_h  &lt;- sqrt(var_h)\n\ntibble::tibble(mean = mu_h,  variance = var_h, 'standard error' = se_h) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4) |&gt; \n  gt::tab_options( table.font.size = gt::px(20) ) |&gt;  \n  #gt::tab_header(title = )\n  gt::as_raw_html()\n\n\n  \n  \n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.8030\n0.0003\n0.0178"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-10",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-10",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nunknown normalizing constant\n\nSuppose that \\(g(x)&gt;0;\\;x\\in D\\) and \\(\\int_Dg(x)dx=c&lt;\\infty\\). Here \\(g()\\) is an un-normalized density on \\(D\\) whereas the corresponding normalized density is \\(\\frac{1}{c}q(x)\\).\nNow we want to compute \\(I=\\int h(x)\\pi(x)dx = I=\\int h(x)cg(x)dx = \\frac{\\int h(x)g(x)dx}{\\int g(x)dx}\\), but \\(c=\\frac{1}{\\int g(x)dx}\\) might be hard to compute\nUsing IS, let \\(f(x) = \\frac{1}{Z_r}r(x);\\;Z_r=\\int r(x)dx\\), so \\(r\\) is an un-normalized density with \\(Z_r\\) possibly unknown.\n\nDraw \\(x_1,x_2,\\ldots,x_n\\) from \\(f(x)\\) and calculate importance weights \\(w(x_i)=g(x_i)/f(x_i)\\)\n\\(\\int h(x)g(x)dx\\approx\\frac{1}{M}\\sum_{i=1}^Mh(x_i)g(x_i)/f(x_i)\\)\n\n\\(\\int g(x)dx\\approx\\frac{1}{M}\\sum_{i=1}^Mg(x_i)/f(x_i)\\)\n\nEstimate \\(\\mathbb{E}_f\\left[h(X)g(X)/f(X)\\right]\\) by \\[\n\\hat{\\mu_h}=\\frac{\\sum_{i=1}^nw(x_i)h(x_i)}{\\sum_{i=1}^nw(x_i)}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-11",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-11",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nunknown normalizing constant\n\nsince\n\\[\n\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}\\right]=\\int\\frac{q(X)}{r(X)}g(x)dx=\\frac{Z_{q}}{Z_{r}}\\\\\n\\frac{1}{n}\\sum_{i=1}^{n}w(x_{i}) & \\rightarrow\\mathbb{E}_{g}\\left[\\frac{q(X)}{r(X)}h(X)\\right]\\\\\n& =\\int\\frac{q(X)}{r(X)}g(x)h(x)dx\\\\\n& =\\frac{1}{Z_{r}}\\int g(x)h(x)dx\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-12",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-12",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrepeating the prior example, but un-normalized\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution, un-normalized)\n\n\n\ncalculation of some kind\n# un-normalized weights\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X)\n\nmu_h2  &lt;- sum(W*X)/sum(W)\nvar_h2 &lt;- var(W/mean(W))\nse_h2  &lt;- sqrt(var_h2)\n\ntibble::tibble(mean = mu_h2,  variance = var_h2, 'standard error' = se_h2) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4) |&gt; \n  gt::tab_options( table.font.size = gt::px(20) ) |&gt;  \n  #gt::tab_header(title = )\n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.7895\n0.4058\n0.6370"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-13",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-13",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling procedure\n\n\nAssume we have an un-normalized \\(g(x)\\), i.e.Â \\(\\pi(x)=cg(x)\\) but \\(c\\) is unknown.\nWe want to generate iid samples \\(x_1,x_2,\\ldots,x_M\\sim \\pi\\) to estimate \\(\\mathbb{E}_\\pi[h]\\)\nNow assume we have an easily sampled density \\(f\\), and known \\(K&gt;0\\), such that \\(Kf(x)\\ge g(x),\\;\\forall x\\), i.e.Â \\(Kf(x)\\ge \\pi(x)/c\\) ( or \\(cKf(x)\\ge \\pi(x)\\)).\n\nThen use the following procedure:\n\nsample \\(X\\sim f\\) and \\(U\\sim \\mathrm{uniform}[0,1]\\)\nif \\(U\\le\\frac{g(X)}{Kf(x)}\\), the accept X as a draw from \\(\\pi\\)\notherwise reject the sample and repeat"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-14",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-14",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nSince \\(0\\le\\frac{g(x)}{Kf(x)}\\le 1\\) we know that \\(\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\\)\nand so\n\n\\[\n\\mathbb{E}_f\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]=\\mathbb{E}_f\\left[\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^\\infty\\frac{g(X)}{Kf(X)}f(x)dx=\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-15",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-15",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nSince \\(0\\le\\frac{g(x)}{Kf(x)}\\le 1\\) we know that \\(\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\\)\nand so\n\n\\[\n\\begin{align*}\n\\mathbb{E}_{f}\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right] & =\\mathbb{E}_{f}\\left[\\frac{g(X)}{Kf(X)}\\right]\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{g(X)}{Kf(X)}f(x)dx\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{g(X)}{K}dx\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-16",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-16",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nSimilarly, for any \\(y\\in\\mathbb{R}\\), we can calculate the joint probability\n\\[\n\\begin{align*}\n\\mathbb{P}\\left(X\\le y,U\\le\\frac{g(X)}{Kf(X)}\\right) & =\\mathbb{E}\\left[1_{X\\le y}1_{U\\le\\frac{g(X)}{Kf(X)}}\\right]\\\\\n& =\\mathbb{E}\\left[1_{X\\le y}\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X\\right)\\right]\\\\\n& =\\mathbb{E}\\left[1_{X\\le y}\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^{y}\\frac{g(x)}{Kf(x)}f(x)dx\\\\\n& =\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx\n\\end{align*}\n\\]\n\nand so we have the joint probability (above - \\(\\mathbb{P}(A,B)\\)), and the probability of acceptance (previous slide - \\(\\mathbb{P}(B)\\)), so the probability, conditional on acceptance (\\(\\mathbb{P}(A|B)\\)) is \\(\\frac{\\mathbb{P}(A,B)}{\\mathbb{P}(B)}\\) by Bayes rule."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-17",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-17",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\\[\n\\mathbb{P}\\left(X\\le y|U\\le\\frac{g(X)}{Kf(X)}\\right)=\\frac{\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx}{\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx}=\\int_{-\\infty}^{y}\\pi(x)dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-18",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-18",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nconsider a random variable \\(X\\) with pdf/pmf \\(q(x)&gt;0;\\;x\\in D\\), which is difficult to sample from\nwe will sample from \\(q\\) using a proposal pdf/pmf \\(f\\) which we can sample from\nif we can find a constant \\(K\\) such that \\(q(x)\\le Kf(x); \\forall x\\in D\\). Alternatively \\(\\frac{q(x)}{f(x)}\\le K\\)\nthen there is a rejection method that returns \\(X\\sim q\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-19",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-19",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling method\n\ngiven a proposal pdf/pmf \\(f\\) we can sample from, and constant \\(K\\) such that \\(\\frac{q(x)}{f(x)}\\le K; \\forall x\\in D\\)\nsample \\(Y_i\\sim f\\) and \\(U_i\\sim\\mathrm{U}[0,1]\\)\nfor \\(U_i\\le\\frac{q(Y_i)}{Kf(Y_i)}\\) return \\(X_i=Y_i\\); otherwise return nothing and continue."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-20",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-20",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nWe have \\(\\mathbb{P}(X=x) = \\sum_{i=1}^n\\mathbb{P}(\\mathrm{reject }\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw }\\,Y=x\\,\\mathrm{and\\, accept})\\)\nWe also have\n\\[\n\\begin{align*}\n& \\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n= & \\mathbb{P}(\\mathrm{draw}\\,Y=x)\\mathbb{P}(\\left.\\mathrm{accept}\\,Y\\right|Y=x)\\\\\n= & f(x)\\mathbb{P}(\\left.U\\le\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n= & \\frac{q(x)}{K}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-21",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-21",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nThe probability of rejection of a draw is\n\\[\n\\begin{align*}\n\\mathbb{P}(\\mathrm{{reject}}\\,Y) & =\\sum_{x\\in D}\\mathbb{P}(\\mathrm{{draw}}\\,Y=x\\,\\mathrm{and\\,reject\\,it})\\\\\n& =\\sum_{x\\in D}f(x)\\mathbb{P}(\\left.U\\ge\\frac{q(Y)}{Kf(Y)}\\right|Y=x)\\\\\n& =\\sum_{x\\in D}f(x)(1-\\frac{q(x)}{Kf(x)})=1-\\frac{1}{K}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-22",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-22",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nand so1\n\\[\n\\begin{align*}\n\\mathbb{P}(X=x) & =\\sum_{n=1}^{\\infty}\\mathbb{P}(\\mathrm{reject}\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n& =\\sum_{n=1}^{\\infty}\\left(1-\\frac{1}{K}\\right)^{n-1}\\frac{q(x)}{K}=q(x)\n\\end{align*}\n\\]\n\nThe geometric distribution is a discrete pmf that can be interpreted as the number of failures before the first success (\\(\\mathbb{P}(X=k)=(1-p)^{k-1}p\\), with mean \\(p\\))."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-23",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-23",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for continuous scalar rv\n\nRecal that we accept the proposal \\(Y\\) whenever \\((U,Y)\\sim q_{U,Y}\\) where \\(q_{U,Y}\\left(u,y\\right)=q(y)U_{0,1}(u)\\) such that \\(U\\le q(Y)/(Kf(Y))\\)\nWe have\n\\[\n\\begin{align*}\n\\mathbb{P}\\left(X\\le x\\right) & =\\mathbb{P}\\left(\\left.Y\\le x\\right|U\\le q(Y)/Kf(Y))\\right)\\\\\n& =\\frac{\\mathbb{P}\\left(Y\\le x,U\\le q(Y)/Kf(Y))\\right)}{\\mathbb{P}\\left(U\\le q(Y)/Kf(Y))\\right)}\\\\\n& =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f_{U,Y}\\left(u,y\\right)dudy}\\\\\n& =\\frac{\\int_{-\\infty}^{x}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}{\\int_{-\\infty}^{\\infty}\\int_{0}^{q(y)/Kf(y)}f\\left(y\\right)dudy}=\\int_{-\\infty}^{x}q(y)dy\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-24",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-24",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: unknown normalizing constants\n\nIn most practical scenarios, we know \\(f(x)\\) and \\(q(x)\\) only up to some normalizing constants\n\\[\nf(x)=\\bar{f}(x)/K_f\\;\\mathrm{and}\\;q(x)=\\bar{q}(x)/K_q\n\\] We can still use rejection sampling since\n\\[\n\\frac{q(x)}{f(x)}\\le K\\;\\mathrm{iff}\\;\\frac{\\bar{q}(x)}{\\bar{f}(x)}\\le\\hat{K}\\equiv  K\\frac{K_q}{K_f}\n\\] In practice this means we can ignore the normalizing constants if we can find \\(\\hat{K}\\) to bound \\(\\frac{\\bar{q}(x)}{\\bar{f}(x)}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mc-methods-25",
    "href": "slides/BSMM_8740_lec_09_eh.html#mc-methods-25",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nSuppose we need to compute an expectation \\(\\mathbb{E}[g(Z)]\\) for some random variable \\(Z\\) and some function \\(g:\\mathbb{R}\\to\\mathbb{R}\\).\nMonte Carlo methods avoid doing any integration or summation over the pdf/pmf and instead just generate lots of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) and estimate \\(\\mathbb{E}[g(Z)]\\) as \\(\\frac{1}{M}\\sum_{i=1}^Mg(z_i)\\). The law of large numbers states that this sample mean should be close to \\(\\mathbb{E}[g(Z)]\\).\nSaid another way, Monte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#stochastic-processes",
    "href": "slides/BSMM_8740_lec_09_eh.html#stochastic-processes",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\n\nA stochastic process, which we will usually write as \\((X_n)\\), is an indexed sequence of random variables that are (usually) dependent on each other.\nEach random variable \\(X_n\\) takes a value in a state space \\(\\mathcal S\\) which is the set of possible values for the process. As with usual random variables, the state space \\(\\mathcal S\\) can be discrete or continuous. A discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, \\(\\mathcal S = \\{\\text{Heads},\\text{Tails}\\}\\) is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers \\(\\mathcal S = \\mathbb Z_+ = \\{0,1,2,\\dots\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#stochastic-processes-1",
    "href": "slides/BSMM_8740_lec_09_eh.html#stochastic-processes-1",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nFurther, the process has an index set that puts the random variables that make up the process in order.\nThe index set is usually interpreted as a time variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at distinct points, often denoted by \\(n = 0,1,2,\\dots\\), while continuous time denotes a process monitored constantly over time, often denoted by \\(t \\in \\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-property",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-property",
    "title": "Monte Carlo Methods",
    "section": "Markov property",
    "text": "Markov property\n\nThink of a simple board game where we roll a dice and move that many squares forward on the board. Suppose we are currently on the square \\(X_n\\). Then what can we say about which square \\(X_{n+1}\\) we move to on our next turn?\n\n\\(X_{n+1}\\) is random, since it depends on the roll of the dice.\n\\(X_{n+1}\\) depends on where we are now \\(X_n\\), since the score of dice will be added onto the number our current square,\nGiven the square \\(X_n\\) we are now, \\(X_{n+1}\\) doesnâ€™t depend any further on which sequence of squares \\(X_0, X_1, \\dots, X_{n-1}\\) we used to get here.\n\nThe third point is called the Markov property or memoryless property. We say â€œmemorylessâ€, because we only need to remember what square weâ€™ve reached, not which squares we used to get here. The stochastic process before this moment has no bearing on the future, given where we are now. A mathematical way to say this is that â€œthe past and the future are conditionally independent given the present.â€"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider the following simple random walk on the integers \\(\\mathbb Z\\):\nWe start at \\(0\\), then at each time step, we go up by one with probability \\(p\\) and down by one with probability \\(q = 1-p\\). When \\(p = q = \\frac12\\), weâ€™re equally as likely to go up as down, and we call this the simple symmetric random walk.\nThe simple random walk is a simple but very useful model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In many modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the â€œdrunkardâ€™s walkâ€, suggesting it could model a drunk person trying to stagger home."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-1",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-1",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nrandom walks\nrequire(ggplot2, quietly = TRUE)\nset.seed(315)\n\nrrw &lt;- function(n, p = 1/2) {\n  q &lt;- 1 - p\n  Z &lt;- sample(c(1, -1), n, replace = TRUE, prob = c(p, q))\n  X &lt;- c(0, cumsum(Z))\n  c(0, cumsum(Z))\n}\n\nn &lt;- 2000\nrw_dat &lt;- tibble::tibble(x=0:n) |&gt; \n  dplyr::mutate(\n    \"p = 2/3\" = rrw(n, 2/3)\n    , \"p = 1/3\" = rrw(n, 1/3)\n    , \"p = 1/2\" = rrw(n, 1/2)\n  )\n\np0 &lt;- rw_dat |&gt; dplyr::slice_head(n=20) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np1 &lt;- rw_dat |&gt; dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np3 &lt;- rw_dat |&gt; #dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np0+p1+p3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-2",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-2",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWe can write this as a stochastic process \\((X_n)\\) with discrete time \\(n = \\{0,1,2,\\dots\\} = \\mathbb Z_+\\) and discrete state space \\(\\mathcal S = \\mathbb Z\\), where \\(X_0 = 0\\) and, for \\(n \\geq 0\\), we have \\[ X_{n+1} = \\begin{cases} X_n + 1 & \\text{with probability $p$,} \\\\\n                             X_n - 1 & \\text{with probability $q$.} \\end{cases} \\]\nItâ€™s clear from this definition that \\(X_{n+1}\\) (the future) depends on \\(X_n\\) (the present), but, given \\(X_n\\), does not depend on \\(X_{n-1}, \\dots, X_1, X_0\\) (the past). Thus the Markov property holds, and the simple random walk is a discrete time Markov process or Markov chain."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-3",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-3",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nSo far weâ€™ve seen a a few examples of stochastic processes in discrete time and discrete space with the Markov memoryless property. Now we will develop the theory more generally.\nTo define a so-called â€œMarkov chainâ€, we first need to say where we start from, and second what the probabilities of transitions from one state to another are.\nIn our examples of the simple random walk and gamblerâ€™s ruin, we specified the start point \\(X_0 = i\\) exactly, but we could pick the start point at random according to some distribution \\(\\lambda_i = \\mathbb P(X_0 = i)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-4",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-4",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nAfter that, we want to know the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for \\(i,j \\in \\mathcal S\\). Here, because of the Markov property, the transition probability only needs to condition on the state weâ€™re in now \\(X_n = i\\), and not on the whole history of the process.\nIn the case of the simple random walk, for example, we had initial distribution \\[ \\lambda_i = \\mathbb P(X_0 = i) = \\begin{cases} 1 & \\text{if $i = 0$} \\\\ 0 & \\text{otherwise} \\end{cases} \\] and transition probabilities \\[ \\mathbb P(X_{n+1} = j \\mid X_n = i) = \\begin{cases} p & \\text{if $j = i+1$} \\\\ q & \\text{if $j = i-1$} \\\\ 0 & \\text{otherwise.} \\end{cases} \\]\nFor the random walk (and also the gamblerâ€™s ruin), the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) donâ€™t depend on \\(n\\); in other words, the transition probabilities stay the same over time. A Markov process with this property is called time homogeneous. We will always consider time homogeneous processes from now on (unless we say otherwise)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-5",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-5",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWrite \\(p_{ij} = \\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for the transition probabilities, which are independent of \\(n\\). We must have \\(p_{ij} \\geq 0\\), since it is a probability, and we must also have \\(\\sum_j p_{ij} = 1\\) for all states \\(i\\), as this is the sum of the probabilities of all the places you can move to from state i.\nWhen the state space is finite (and even sometimes when itâ€™s not), itâ€™s convenient to write the transition probabilities \\((p_{ij})\\) as a matrix \\(\\mathsf P\\), called the transition matrix, whose \\((i,j)\\)th entry is \\(p_{ij}\\). Then the condition that \\(\\sum_j p_{ij} = 1\\) is the condition that each of the rows of \\(\\mathsf P\\) add up to \\(1\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-6",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-6",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider a simple two-state Markov chain with state space \\(\\mathcal S = \\{0,1\\}\\) and transition matrix \\[ \\mathsf P = \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\beta & 1-\\beta \\end{pmatrix}  \\] for some \\(0 &lt; \\alpha, \\beta &lt; 1\\). Note that the rows of \\(\\mathsf P\\) add up to \\(1\\), as they must.\nWe can illustrate \\(\\mathsf P\\) by a transition diagram, where the blobs are the states and the arrows give the transition probabilities. (We donâ€™t draw the arrow if \\(p_{ij} = 0\\).) In this case, our transition diagram looks like this:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-7",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-7",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTransition diagram for the two-state Markov chain"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-8",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-8",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWe can use this as a simple model of customer churn, for example. If the customer has closed their account (state 0) on one period, then with probability \\(\\alpha\\) we will be able to entice them to open their account again (state 1) by the next period; while if the customer has an account (state 1), then with probability \\(\\beta\\) they will have closed their account (state 0) by the next period."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-9",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-9",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\nIf the customer has an account on period n, whatâ€™s the probability they also have an account on period n+2?\n\\[\np_{11}(2) = \\mathbb P (X_{n+2} = 1 \\mid X_n = 1)\n\\]\nThe key to calculating this is to condition on the first step again â€“ that is, on whether the printer is working on Tuesday. We have\n\\[\n\\begin{align*}\n  p_{11}(2) &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0, X_n = 1) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1, X_n = 1) \\\\\n  &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1) \\\\\n  &= p_{10}p_{01} + p_{11}p_{11} \\\\\n  &= \\beta\\alpha + (1-\\beta)^2 .\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-10",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-10",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIn the second equality, we used the Markov property to mean conditional probabilities like \\(\\mathbb P(X_{n+2} = 1 \\mid X_{n+1} = k)\\) did not have to depend on \\(X_n\\).\nAnother way to think of this as we summing the probabilities of all length-2 paths from 1 to 1, which are \\(1\\to 0\\to 1\\) with probability \\(\\beta\\alpha\\) and \\(1 \\to 1 \\to 1\\) with probability \\((1-\\beta)^2\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-11",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-11",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIn the above example, we calculated a two-step transition probability \\(p_{ij}(2) = \\mathbb P (X_{n+2} = j \\mid X_n = i)\\) by conditioning on the first step. That is, by considering all the possible intermediate steps \\(k\\), we have\n\\[\np_{ij}(2) = \\sum_{k\\in\\mathcal S} \\mathbb P (X_{n+1} = k \\mid X_n = i)\\mathbb P (X_{n+2} = j \\mid X_{n+1} = k) = \\sum_{k\\in\\mathcal S} p_{ik}p_{kj}\n\\]\nBut this is exactly the formula for multiplying the matrix \\(\\mathsf P\\) with itself! In other words, \\(p_{ij}(2) = \\sum_{k} p_{ik}p_{kj}\\) is the \\((i,j)\\)th entry of the matrix square \\(\\mathsf P^2 = \\mathsf{PP}\\). If we write \\(\\mathsf P(2)  = (p_{ij}(2))\\) for the matrix of two-step transition probabilities, we have \\(\\mathsf P(2) = \\mathsf P^2\\).\nMore generally, we see that this rule holds over multiple steps, provided we sum over all the possible paths \\(i\\to k_1 \\to k_2 \\to \\cdots \\to k_{n-1} \\to j\\) of length \\(n\\) from \\(i\\) to \\(j\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-12",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-12",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\nTheorem 1 Let \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). For \\(i,j \\in \\mathcal S\\), write \\[ p_{ij}(n) = \\mathbb P(X_n = j \\mid X_0 = i) \\] for the \\(n\\)-step transition probability. Then\n\\[\np_{ij}(n) = \\sum_{k_1, k_2, \\dots, k_{n-1} \\in \\mathcal S} p_{ik_1} p_{k_1k_2} \\cdots p_{k_{n-2}k_{n-1}} p_{k_{n-1}j}\n\\]\nIn particular, \\(p_{ij}(n)\\) is the \\((i,j)\\)th element of the matrix power \\(\\mathsf P^n\\), and the matrix of \\(n\\)-step transition probabilities is given by \\(\\mathsf P(n) = \\mathsf P^n\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-13",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-13",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nThe so-called Chapmanâ€“Kolmogorov equations follow immediately from this.\n\nLet \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). Then, for non-negative integers \\(n,m\\), we have \\[ p_{ij}(n+m) = \\sum_{k \\in \\mathcal S} p_{ik}(n)p_{kj}(m) , \\] or, in matrix notation, \\(\\mathsf P(n+m) = \\mathsf P(n)\\mathsf P(m)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-14",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-14",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIn other words, a trip of length \\(n + m\\) from \\(i\\) to \\(j\\) is a trip of length \\(n\\) from \\(i\\) to some other state \\(k\\), then a trip of length \\(m\\) from \\(k\\) back to \\(j\\), and this intermediate stop \\(k\\) can be any state, so we have to sum the probabilities.\nOf course, once we know that \\(\\mathsf P(n) = \\mathsf P^n\\) is given by the matrix power, itâ€™s clear to see that \\(\\mathsf P(n+m) = \\mathsf P^{n+m} = \\mathsf P^n \\mathsf P^m = \\mathsf P(n)\\mathsf P(m)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-15",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-15",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIf we start from a state given by a distribution \\(\\boldsymbol \\pi = (\\pi_i)\\), then after step 1 the probability weâ€™re in state \\(j\\) is \\(\\sum_i \\pi_i p_{ij}\\). So if \\(\\pi_j = \\sum_i \\pi_i p_{ij}\\), we stay in this distribution forever. We call such a distribution a stationary distribution. We again recognise this formula as a matrix-vector multiplication, so this is \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\), where \\(\\boldsymbol \\pi\\) is a row vector.\n\nLet \\((X_n)\\) be a Markov chain on a state space \\(\\mathcal S\\) with transition matrix \\(\\mathsf P\\). Let \\(\\boldsymbol \\pi = (\\pi_i)\\) be a distribution on \\(\\mathcal S\\), in that \\(\\pi_i \\geq 0\\) for all \\(i \\in \\mathcal S\\) and \\(\\sum_{i \\in \\mathcal S} \\pi_i = 1\\). We call \\(\\boldsymbol \\pi\\) a stationary distribution if\n\\[\n\\pi_j = \\sum_{i\\in \\mathcal S} \\pi_i p_{ij} \\quad \\text{for all $j \\in \\mathcal S$}\n\\]\nor, equivalently, if \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\).\n\nNote that weâ€™re saying the distribution \\(\\mathbb P(X_n = i)\\) stays the same; the Markov chain \\((X_n)\\) itself will keep moving. One way to think is that if we started off a thousand Markov chains, choosing each starting position to be \\(i\\) with probability \\(\\pi_i\\), then (roughly) \\(1000 \\pi_j\\) of them would be in state \\(j\\) at any time in the future â€“ but not necessarily the same ones each time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-16",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-16",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nproperties\n\nA Markov Chain is irreducible if you have positive probability of eventually getting from anywhere to anywhere else.\nA Markov Chain is aperiodic if there are no forced cycles, i.e.Â there do not exist disjoint non-empty subsets \\(X_1,X_2,...,X_d\\) for \\(dâ‰¥2\\),such that \\(P(x,Xi+1)=1\\) for all \\(x\\in Xi (1â‰¤iâ‰¤dâˆ’1)\\),and \\(P(x,X1)=1\\) for all \\(x\\in Xd\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chains-17",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chains-17",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nMC with cycle"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#markov-chain-monte-carlo",
    "href": "slides/BSMM_8740_lec_09_eh.html#markov-chain-monte-carlo",
    "title": "Monte Carlo Methods",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\nSuppose have complicated, high-dimensional density \\(\\pi = cg\\) and we want samples \\(X_1, X_2,\\dots \\sim \\pi\\). (Then we can do Monte Carlo.)\nDefine a Markov chain (dependent random process) \\(X_0, X_1,X_2\\dots\\) in such a way that for large enough \\(n\\), \\(X_n\\sim \\pi\\).\nThen we can estimate \\(\\mathbb{E}_{\\pi}(h) â‰¡ \\int h(x) \\pi(x) dx\\) by:\n\\[\n\\mathbb{E}_{\\pi}[h] \\approx \\frac{1}{M-B}\\sum_{i=B+1}^{M}\n\\]\nwhere \\(B\\) (â€œburn-inâ€) is chosen large enough so \\(X_B\\sim\\pi\\), and \\(M\\) is chosen large enough to get good Monte Carlo estimates."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-algo",
    "href": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-algo",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\n\nchoose some initial value \\(X_0\\), then\ngiven \\(X_{n-1}\\), choose a proposal state \\(Y_n\\sim \\textrm{MVN}(X_{n-1},\\sigma^2\\textrm{I})\\), for some fixed \\(\\sigma^2&gt;0\\)\nlet \\(A_n=\\pi(Y_n)/\\pi(X_{n-1}=g(Y_n)/g(X_{n-1})\\) and \\(U_n\\sim\\textrm{U}[0,1]\\), then\nid \\(U_n&lt;A_n\\) set \\(X_n=Y_n\\) (â€œacceptâ€), otherwise set $X_n = X_{n-1} ) â€œrejectâ€\nrepeat for \\(n=1,2,3,\\ldots,M\\)\n\nThis version is called random-walk Metropolis"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-algo-1",
    "href": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-algo-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\na simple Metropolis algorithm in one dimension\ng = function(y) {\n    if ( (y&lt;0) || (y&gt;1) )\n    return(0)\n    else\n    return( y^3 * sin(y^4) * cos(y^5) )\n}\n\nh = function(y) { return(y^2) }\n\nM = 11000  # run length\nB = 1000  # amount of burn-in\nX = runif(1)  # overdispersed starting distribution\nsigma = 1  # proposal scaling\nxlist = rep(0,M)  # for keeping track of chain values\nhlist = rep(0,M)  # for keeping track of h function values\nnumaccept = 0;\n\nfor (i in 1:M) {\n  Y = X + sigma * rnorm(1)  # proposal value\n  U = runif(1)              # for accept/reject\n  alpha = g(Y) / g(X)       # for accept/reject\n  if (U &lt; alpha) {\n    X = Y                   # accept proposal\n    numaccept = numaccept + 1;\n  }\n    xlist[i] = X;\n    hlist[i] = h(X);\n}\n\nu = mean(hlist[(B+1):M])\nse1 =  sd(hlist[(B+1):M]) / sqrt(M-B)\nvarfact &lt;- function(xxx) { 2 * sum(acf(xxx, plot=FALSE)$acf) - 1 }\nthevarfact = varfact(hlist[(B+1):M])\nse = se1 * sqrt( thevarfact )\n\ntibble::tibble(\n  measure = c(\"iterations\",\"burn-in\",\"mean of h\",\"iid se\",\"varfact\",\"true se\",\"95% CI lb\",\"95% CI ub\")\n  , value = c(M,B,u,se1,thevarfact,se,u-1.96*se,u+1.96*se)\n) |&gt; \ngt::gt(\"measure\") |&gt; \ngt::fmt_number() |&gt; \ngtExtras::gt_theme_espn()\n#cat(\"ran Metropolis algorithm for\", M, \"iterations, with burn-in\", B, \"\\n\");\n#cat(\"acceptance rate =\", numaccept/M, \"\\n\");\n#u = mean(hlist[(B+1):M])\n#cat(\"mean of h is about\", u, \"\\n\")\n\n#se1 =  sd(hlist[(B+1):M]) / sqrt(M-B)\n#cat(\"iid standard error would be about\", se1, \"\\n\")\n\n#varfact &lt;- function(xxx) { 2 * sum(acf(xxx, plot=FALSE)$acf) - 1 }\n#thevarfact = varfact(hlist[(B+1):M])\n#se = se1 * sqrt( thevarfact )\n#cat(\"varfact = \", thevarfact, \"\\n\")\n#cat(\"true standard error is about\", se, \"\\n\")\n#cat(\"approximate 95% confidence interval is (\", u - 1.96 * se, \",\",\n#                       u + 1.96 * se, \")\\n\\n\")\n# acf(xlist)\n#plot(xlist, type='l')\n\ntibble::tibble(x=1:length(xlist), y=xlist) |&gt; \n  ggplot(aes(x=x,y=y)) + geom_line()\n\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\niterations\n11,000.00\n\n\nburn-in\n1,000.00\n\n\nmean of h\n0.77\n\n\niid se\n0.00\n\n\nvarfact\n21.02\n\n\ntrue se\n0.01\n\n\n95% CI lb\n0.76\n\n\n95% CI ub\n0.79"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-hastings-algo",
    "href": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-hastings-algo",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nMetropolis algorithm requires that the state transition probabilities are symmetric, so the proposal distribution is symmetric.\nif \\(q(x,y) \\gg q(y,x)\\) the the Metropolis chain would spend too much time at \\(y\\) and not enought at \\(x\\), so it accepts fewer moves \\(x\\rightarrow y\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-hastings-algo-1",
    "href": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-hastings-algo-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nReplace\n\\[\nA_n = \\frac{\\pi(Y_n)}{\\pi(X_{n-1})}\\;\\mathrm{with}\\; A_n = \\frac{\\pi(Y_n)q(Y_n,X_{n-1})}{\\pi(X_{n-1})q(X_{n-1},Y_n)}\n\\]\nand the algorithm is still valid even is \\(q\\) is not symmetric.\nWe do require that \\(q(x,y)&gt;0\\;\\textrm{iff}\\; q(y,x)&gt;0\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-hastings-algo-2",
    "href": "slides/BSMM_8740_lec_09_eh.html#mcmc-metropolis-hastings-algo-2",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nWhy is this valid?\nFor metropolis, th key was that the Markov chain was reversible, i.e.Â \\(\\pi(x)\\mathbb{P}(x,y)=\\pi(y)\\mathbb{P}(y,x)\\)\nIf instead \\(A_n = \\frac{\\pi(Y_n)q(Y_n,X_{n-1})}{\\pi(X_{n-1})q(X_{n-1},Y_n)}\\), with acceptance probability \\(\\alpha(x,y)=\\min\\left[1,\\frac{\\pi(Y_n)q(Y_n,X_{n-1})}{\\pi(X_{n-1})q(X_{n-1},Y_n)}\\right]\\), then\n\\[\n\\begin{align*}\nq(x,y)\\alpha(x,y)\\pi(x) & =q(x,y)\\min\\left[1,\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}\\right]\\pi(x)\\\\\n& =\\min\\left[\\pi(x)q(x,y),\\pi(y)q(y,x)\\right]\n\\end{align*}\\]\nand \\(\\pi(x)\\mathbb{P}(x,y)\\) is still symmetric, even if \\(q\\) wasnâ€™t"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#need",
    "href": "slides/BSMM_8740_lec_09_eh.html#need",
    "title": "Monte Carlo Methods",
    "section": "Need",
    "text": "Need\nBalance equation"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#more",
    "href": "slides/BSMM_8740_lec_09_eh.html#more",
    "title": "Monte Carlo Methods",
    "section": "More",
    "text": "More\n\nRead Bayes Rules!\nRead Think Bayes\nRead Statistical Rethinking"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_eh.html#recap",
    "href": "slides/BSMM_8740_lec_09_eh.html#recap",
    "title": "Monte Carlo Methods",
    "section": "Recap",
    "text": "Recap\n\nWeâ€™ve had the smallest possible taste of statistical programming using Bayes theorem and sampling methods, in the context of adressing the limitations of off-the-shelf implementations of statistical methods and algorithms.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/R/da_data_repo/australia-weather-forecasts/raw/GET-DATA.html",
    "href": "slides/R/da_data_repo/australia-weather-forecasts/raw/GET-DATA.html",
    "title": "BSMM-8740 - Fall 2024",
    "section": "",
    "text": "To get data for Australian weather forecast"
  },
  {
    "objectID": "slides/R/da_data_repo/australia-weather-forecasts/raw/GET-DATA.html#download-data",
    "href": "slides/R/da_data_repo/australia-weather-forecasts/raw/GET-DATA.html#download-data",
    "title": "BSMM-8740 - Fall 2024",
    "section": "Download data",
    "text": "Download data\nGo to Data source website Download and extract bometa20150501-20160430.zip\nfrom HERE"
  },
  {
    "objectID": "slides/R/da_data_repo/australia-weather-forecasts/raw/GET-DATA.html#set-up-folders",
    "href": "slides/R/da_data_repo/australia-weather-forecasts/raw/GET-DATA.html#set-up-folders",
    "title": "BSMM-8740 - Fall 2024",
    "section": "Set up folders",
    "text": "Set up folders\nUnzip to create bometa20150501-20160430/BoM_ETA_20150501-20160430 folder\nThere will be subdirectories are fcst and obs and spatial folders containing forecast and observation csvs.\nItâ€™s gonna be a very large folder, like 10GB when unzipped."
  },
  {
    "objectID": "slides/R/da_data_repo/australia-weather-forecasts/raw/GET-DATA.html#data-prep",
    "href": "slides/R/da_data_repo/australia-weather-forecasts/raw/GET-DATA.html#data-prep",
    "title": "BSMM-8740 - Fall 2024",
    "section": "Data prep",
    "text": "Data prep\nRun the australia_get_data.R to combine all the small csv files into a larger one. The code will save a simple and small csv into the /clean folder."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_09_old.html#recap-of-last-week",
    "title": "A few words on Bayesian Analysis",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced the fundamental problems of inference and the biases of some intuitive estimators.\nWe also built a basic understanding of the tools used to state and then satisfy causality assumptions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#this-week",
    "href": "slides/BSMM_8740_lec_09_old.html#this-week",
    "title": "A few words on Bayesian Analysis",
    "section": "This week",
    "text": "This week\n\nWe will get a taste of Bayesian regression applied to elasticity analysis."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation",
    "href": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation",
    "title": "A few words on Bayesian Analysis",
    "section": "Elasticity estimation",
    "text": "Elasticity estimation\nSince elasticity is defined as the percentage change in volume (\\(\\Delta V/V\\)) for a given percentage change in price (\\(\\Delta p/p\\)), then with elasticity parameter \\(\\beta\\) we write:\n\\[\n\\begin{align*}\n\\frac{\\Delta V}{V} & = \\beta\\times\\frac{\\Delta p}{p} \\\\\n\\frac{\\partial V}{V} & = \\beta\\times\\frac{\\partial p}{p} \\\\\n\\partial\\log(V) & = \\beta\\times\\partial\\log(p)\n\\end{align*}\n\\tag{1}\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation-1",
    "href": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation-1",
    "title": "A few words on Bayesian Analysis",
    "section": "Elasticity estimation",
    "text": "Elasticity estimation\nThis equation is the justification for the log-log regression model of elasticity, and this model has solution \\(V = Kp^\\beta\\), where \\(K\\) is a constant.\nAs written, the value of \\(K\\) is either the volume when \\(p=1\\) which may or may not be useful, or it is the volume when \\(\\beta=0\\), which is uninteresting."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation-2",
    "href": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation-2",
    "title": "A few words on Bayesian Analysis",
    "section": "Elasticity estimation",
    "text": "Elasticity estimation\nTo make the interpretation of the constant \\(K\\) more useful, the model can be written as\n\\[\n\\partial\\log(V) = \\beta\\times\\partial\\log(p/p_{\\text{baseline}});\\qquad V = K\\left(\\frac{p}{p_{\\text{baseline}}}\\right)^{\\beta}\n\\]\nin which case the constant is interpreted as the volume when the price equals the baseline price; the elasticity parameter \\(\\beta\\) is unchanged."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation-3",
    "href": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation-3",
    "title": "A few words on Bayesian Analysis",
    "section": "Elasticity estimation",
    "text": "Elasticity estimation\nIf \\(V = Kp^\\beta\\) then \\(\\log(V) = \\log(K) + \\beta\\log(p)\\), and \\(\\partial\\log(V)/\\partial\\log(p) = \\beta\\) as in the last line of equation (EquationÂ 1).\nThe equation \\(\\log(V) = \\log(K) + \\beta\\log(p)\\) defines a linear relation between the log term and is sometimes estimated as a linear regression on the log terms."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation-4",
    "href": "slides/BSMM_8740_lec_09_old.html#elasticity-estimation-4",
    "title": "A few words on Bayesian Analysis",
    "section": "Elasticity estimation",
    "text": "Elasticity estimation\nIn this version of the problem there are only two parameters, the constant \\(\\log(K)\\) (aka the intercept in the log-log plot of volume vs price plot) and the elasticity \\(\\beta\\), the slope of the log-log plot.\nAs in all linear regressions the variance of the error term is assumed constant and its mean is assumed zero."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#maximum-likelihood-mle",
    "href": "slides/BSMM_8740_lec_09_old.html#maximum-likelihood-mle",
    "title": "A few words on Bayesian Analysis",
    "section": "Maximum likelihood (MLE)",
    "text": "Maximum likelihood (MLE)\nFor a linear regression \\(y\\sim \\mathcal{N}(\\beta x,\\sigma^2)\\) the likelihood of any one observation \\(y_i\\) is\n\\[\n\\pi\\left(\\left.y_{i}\\right|x_{i},\\beta,\\sigma^{2}\\right)=\\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(y_{i}-\\beta x_{i})^{2}}{2\\sigma^{2}}}\n\\] and the log-likelihood of \\(N\\) observations is\n\\[\n\\log\\prod_{i=1}^{N}\\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right) = \\sum_{i=1}^{N}\\log \\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#maximum-likelihood-mle-1",
    "href": "slides/BSMM_8740_lec_09_old.html#maximum-likelihood-mle-1",
    "title": "A few words on Bayesian Analysis",
    "section": "Maximum likelihood (MLE)",
    "text": "Maximum likelihood (MLE)\nThe maximum likelihood estimate of \\(\\beta\\) is\n\\[\n\\hat{\\theta}_{\\text{MLE}}=\\arg\\max_{\\theta} -\\sum_{i=1}^{N}\\log \\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)\n\\]\n\\[\n\\log\\prod_{i=1}^{N}\\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right) = \\sum_{i=1}^{N}\\log \\pi\\left(\\left.y_{i}\\right|x_{i},\\theta\\right)\n\\]\nthis is equivalent to minimizing the sum of the squared errors, and is also called the a priori estimate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#bayesian-model",
    "href": "slides/BSMM_8740_lec_09_old.html#bayesian-model",
    "title": "A few words on Bayesian Analysis",
    "section": "Bayesian model",
    "text": "Bayesian model\nThe Bayesian model for our elasticity problem is (to within a scaling constant)\n\\[\n\\begin{align*}\n\\pi\\left(\\left.\\theta\\right|V,P\\right)\\sim\\pi\\left(\\left.V\\right|P,\\theta\\right)\\times\\pi\\left(\\theta\\right)\n\\end{align*}\n\\tag{2}\\]\nwhere the parameters are \\(\\theta=\\{\\beta,K\\}\\).\n\nIn words: the joint probability of the parameters given the observed volume data is equal to (to within a scaling constant) the probability of the observed volume data given the parameters, times the prior probabilities of the parameters. In practice we refer to the probabilities as likelihoods, and use log-likelihoods in equation (EquationÂ 2) to avoid numerical problems arising from the product of small probabilities."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#maximum-a-posteriori-mape",
    "href": "slides/BSMM_8740_lec_09_old.html#maximum-a-posteriori-mape",
    "title": "A few words on Bayesian Analysis",
    "section": "Maximum a posteriori (MAPE)",
    "text": "Maximum a posteriori (MAPE)\nThe maximum a posteriori estimate of the parameters is\n\\[\n\\begin{align*}\n\\hat{\\theta}_{\\text{MAP}} & =\\arg\\max_{\\theta}\\log\\prod_{i=1}^{N}\\pi\\left(\\left. \\theta \\right|v_{i},p_{i}\\right)\\\\\n& =\\arg\\max_{\\theta}\\sum_{i=1}^{N} \\left(\\log \\pi\\left(\\left.v_{i}\\right|p_{i},\\theta\\right)+\\log\\pi\\left(\\theta\\right)\\right)\\\\\n& =\\arg\\min_{\\theta}-\\sum_{i=1}^{N} \\left(\\log \\pi\\left(\\left.v_{i}\\right|p_{i},\\theta\\right)+\\log\\pi\\left(\\theta\\right)\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#likelihood-function",
    "href": "slides/BSMM_8740_lec_09_old.html#likelihood-function",
    "title": "A few words on Bayesian Analysis",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe key choice we need to make in the Bayesian model is the form of the likelihood function for the observed volumes given the parameters. This is a statistical model describing how the observed volume data is generated given the parameters.\nSince the volume data is units sold per unit time (i.e.Â integers), we have several options for the likelihood function (e.g.Â Poisson, Negative Binomial, Binomial, mixture models of various sorts), but the Poisson model is the simplest."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#likelihood-function-1",
    "href": "slides/BSMM_8740_lec_09_old.html#likelihood-function-1",
    "title": "A few words on Bayesian Analysis",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe Poisson model of the data has a single, positive, real-valued rate parameter \\(\\lambda\\) which represents the units sold per unit time (a rate), so we can choose:\n\\[\n\\begin{align*}\n\\lambda = \\exp^{\\log(K) + \\beta\\log(p)}\\Rightarrow \\log(\\lambda) = \\log(K) + \\beta\\log(p)\n\\end{align*}\n\\] which gives us the log-log relationship of the model, with the crucial difference that we have additionally chosen a model for the data-generating process: a Poisson process with parameter \\(\\lambda\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#likelihood-function-2",
    "href": "slides/BSMM_8740_lec_09_old.html#likelihood-function-2",
    "title": "A few words on Bayesian Analysis",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nNote that a Poisson process is quite different than the Gaussian process, so we canâ€™t use a OLS model.\nWe need a glm model instead, e.g the regression should be modeled as\nglm(volume ~ log(price), family = 'poisson')"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#economics",
    "href": "slides/BSMM_8740_lec_09_old.html#economics",
    "title": "A few words on Bayesian Analysis",
    "section": "Economics",
    "text": "Economics\nOne challenge with standard regression models is that they donâ€™t admit assumptions outside the likelihoods.\nIn the case of elasticity models though, we have economic reasons to expect the estimated coefficient of price to be negative, i.e.Â that the demand curve slopes downwards.\nSo, how to incorporate this or any other assumptions about the data generating process (think DAGs again) when off-the-shelf packages arenâ€™t flexible enough?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#stan",
    "href": "slides/BSMM_8740_lec_09_old.html#stan",
    "title": "A few words on Bayesian Analysis",
    "section": "Stan",
    "text": "Stan\nOne popular option for developing flexible statistical models is the Stan language.\nStan is a high-level probabilistic programming language used for statistical modeling and Bayesian inference. Itâ€™s designed to make it easier for researchers, data scientists, and statisticians to specify and estimate complex statistical models.\nR has several interfaces to Stan, including RStan, CmdStanR, and brms."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#stan-1",
    "href": "slides/BSMM_8740_lec_09_old.html#stan-1",
    "title": "A few words on Bayesian Analysis",
    "section": "Stan",
    "text": "Stan\nIn Stan, you declare your model using a domain-specific language. You specify the relationships between variables and define the likelihood and prior distributions.\nStan then samples from the posterior distributions of the model parameters."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#stan-2",
    "href": "slides/BSMM_8740_lec_09_old.html#stan-2",
    "title": "A few words on Bayesian Analysis",
    "section": "Stan",
    "text": "Stan\n\n\nPoisson elasticity model implemented in the Stan language\ndata {\n  /* Dimensions */\n  int&lt;lower=1&gt; N; // rows\n\n  /* log price vector (integer) */\n  array[N] real P;\n  \n  /* demand vector (integer) */\n  array[N] int&lt;lower=0&gt; Y;\n\n  /* hyperparameters*/\n  real&lt;lower=0&gt; s;       // scale parameter for intercept prior\n  real&lt;lower=0&gt; e_scale; // scale parameter for elasticity prior\n}\n\nparameters {\n  real &lt;upper=0&gt; elasticity;      // elasticities variable &lt; 0 \n  real intercept;                 // intercepts variable\n}\n\ntransformed parameters {\n  array[N] real log_lambda;       // log volume for likelihoods\n  \n  for (i in 1:N){\n    log_lambda[i] = intercept + elasticity * P[i];\n  }\n  \n}\n\nmodel {\n  /* Priors on the parameters */\n  target += normal_lpdf(intercept  | 0, s);\n  target += cauchy_lpdf(elasticity | 0, e_scale);\n\n  /* Conditional log-likelihoods for each observed volume */\n  for (i in 1 : N) {\n    target += poisson_lpmf(Y[i] | exp(log_lambda[i]) );\n  }\n}\n\ngenerated quantities {\n  array[N] int&lt;lower=-1&gt; y_new;  // estimate volumes\n  vector[N] log_lik;             // compute log-likelihood for this model\n  for (i in 1 : N) {\n      y_new[i]   = poisson_rng( exp(log_lambda[i]) );\n      log_lik[i] = poisson_lpmf(Y[i] | exp(log_lambda[i]) );\n  }\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#stan-3",
    "href": "slides/BSMM_8740_lec_09_old.html#stan-3",
    "title": "A few words on Bayesian Analysis",
    "section": "Stan",
    "text": "Stan\n\nThe Stan programme produces samples from the posterior distributions of the parameters (below). These can be used to produce posterior predictive samples for the volumes given the prices, for comparison with the observed data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#more",
    "href": "slides/BSMM_8740_lec_09_old.html#more",
    "title": "A few words on Bayesian Analysis",
    "section": "More",
    "text": "More\n\nRead Bayes Rules!\nRead Think Bayes\nRead Statistical Rethinking"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09_old.html#recap",
    "href": "slides/BSMM_8740_lec_09_old.html#recap",
    "title": "A few words on Bayesian Analysis",
    "section": "Recap",
    "text": "Recap\n\nWeâ€™ve had the smallest possible taste of statistical programming using Bayes theorem and sampling methods, in the context of adressing the limitations of off-the-shelf implementations of statistical methods and algorithms.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#recap-of-last-lecture",
    "href": "slides/BSMM_8740_lec_03.html#recap-of-last-lecture",
    "title": "Regression methods",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\nLast time we worked with the recipes package to develop workflows for pre-processing our data.\nToday we look at regression methods we might apply to understand relationships between measurements in our data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nIn the simple linear regression (SLR) model, we have \\(N\\) observations of a single outcome variable \\(Y\\) along with \\(D\\) predictor (aka co-variate) variables \\(\\mathbf{x}\\) where the likelihood1 of observing \\(Y=y\\) is conditional on the predictor values \\(x\\) and parameters \\(\\theta=\\{\\beta,\\sigma^2\\}\\):\n\\[\n\\pi\\left(Y=y|\\mathbf{x,\\theta}\\right)=\\mathscr{N}\\left(\\left.y\\right|\\mu(\\mathbf{x};\\beta),\\sigma^{2}\\right)\n\\]\nIn SLR models we assume a Normal (Gaussian) probability model for the data generation process. For non-Normal data generation processes we use generalized linear models, which weâ€™ll discuss later."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-1",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-1",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nIn the SLR model, \\(\\mathscr{N}\\left(\\left.y\\right|\\mu(\\mathbf{x};\\beta),\\sigma^{2}\\right)\\) is a Normal probability density with mean \\(\\mu(\\mathbf{x};\\beta)\\) and variance \\(\\sigma^2\\), where \\(\\sigma^2\\) is a constant and the mean is a function of the predictors \\(\\mathbf{x}\\) and a vector of parameters \\(\\beta\\).\nThe mean function \\(\\mu(\\mathbf{x};\\beta)\\) is often assumed to be continuous, i.e.Â a small change in the predictors implies a small change in the outcome. In addition, it is often convenient to decompose the mean function into a sum of simpler functions, e.g.Â polynomial functions (like straight lines, parabolas, and more)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#taylor-series",
    "href": "slides/BSMM_8740_lec_03.html#taylor-series",
    "title": "Regression methods",
    "section": "Taylor Series",
    "text": "Taylor Series\nThe decomposition of a function \\(f\\) of a single variable \\(x\\) into a sum of simpler polynomial functions is called a Taylor series and is defined as follows:\n\n\\[\nf(x;x_0)=\\sum_{n=0}\\beta_n(x-x_0)^n=\\beta_0+\\beta_1(x-x_0)+\\beta_2(x-x_0)^2+\\beta_3(x-x_0)^3+\\ldots\n\\]\n\nwhere \\(\\frac{d^nf(x)}{dx^n}|_{x=x_0} \\equiv f^{(n)}(x_0) = n!\\beta_n\\;\\rightarrow \\beta_n=\\frac{1}{n!}f^{(n)}(x_0)\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#taylor-series-1",
    "href": "slides/BSMM_8740_lec_03.html#taylor-series-1",
    "title": "Regression methods",
    "section": "Taylor Series",
    "text": "Taylor Series\n\n\\[\nf(x;x_0)=\\sum_n\\beta_n(x-x_0)^n=\\beta_0+\\beta_1(x-x_0)+\\beta_2(x-x_0)^2+\\beta_3(x-x_0)^3+\\ldots\n\\]\n\nWe can use the following constructive proof to find the coefficients in the series:\n\n\n\\(0^{th}\\) derivative at \\(x=x_0\\): \\(f(x_0) = \\beta_0\\)\n\\(1^{st}\\) derivative at \\(x=x_0\\): \\(f'(x_0) = \\beta_1\\)\n\\(2^{nd}\\) derivative at \\(x=x_0\\): \\(f''(x_0) = 2\\beta_2\\)\n\\(3^{rd}\\) derivative at \\(x=x_0\\): \\(f'''(x_0) = 6\\beta_3\\)\n\\(n^{th}\\) derivative at \\(x=x_0\\): \\(f^{(n)}(x_0) = n!\\beta_n\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-2",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-2",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nSimilarly, when the number of variables is \\(D=2\\) the Taylor series is (writing \\(f_{x}\\equiv\\frac{\\partial f}{\\partial x}\\), \\(f_{y}\\equiv\\frac{\\partial f}{\\partial y}\\), \\(f_{x,y}\\equiv\\frac{\\partial^2 f}{\\partial x,\\partial x}\\) and so on):\n\n\\[\n\\begin{align*}\nf(x,y;x_0,y_0) & =f(x_{0},y_{0})+f_{x}(x_{0},y_{0})(x-x_{0})+f_{y}(x_{0},y_{0})(y-y_{0})\\\\\n& = + \\frac{1}{2}f_{x,x}(x_{0},y_{0})(x-x_{0})^{2}+\\frac{1}{2}f_{y,y}(x_{0},y_{0})(y-y_{0})^{2}\\\\\n& = + \\frac{1}{2}f_{x,y}(x_{0},y_{0})(x-x_{0})(y-y_{0})+\\ldots\n\\end{align*}\n\\]\nwhich decomposes a function of two variables into a sum of simpler functions, e.g.Â polynomial functions (like 2-D straight lines, parabolas, and more)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-3",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-3",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nFor one co-variate, if the mean function is smooth (i.e.Â not changing rapidly with the co-variate) then \\(\\mu^{(n)}\\) will be decreasing in \\(n\\), and furthermore \\(\\beta_n\\) decreases as \\(1/n!\\), so SLR models in one co-variate typically use only the first two or at most three \\(\\beta\\) coefficients.\nThus the likelihood of observing \\(Y=y\\) is conditional on the predictor values \\(x\\) and parameters \\(\\theta=\\{\\beta_0,\\beta_1,\\sigma^2\\}\\):\n\\(\\theta=\\{\\beta_{0},\\mathbf{\\beta},\\sigma^{2}\\}\\) are the parameters of the model, where \\(\\beta_0\\) is a constant and \\(\\beta_1\\) is the co-variate weight or regression coefficient."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-4",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-4",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nFor multiple co-variates, If the mean function is smooth (i.e.Â not changing rapidly with the co-variates) then under similar (reasonable) assumptions on the differentials, it is common to see SLR models using only the first order coefficients, i.e.Â a constant and one coefficient for each co-variate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-5",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-5",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\n\n\n\n\n\n\nNote\n\n\nIt is common to add the unit constant to the covariate vector \\(x=(1,x_1,x_2,\\ldots)\\) so that the coefficient vector is \\(\\mathbf{\\beta}=(\\beta_0,\\beta_{x_1},\\beta_{x_2},\\ldots)\\), and the model can be expressed1 as a vector equation: \\(y=\\mathbf{\\beta}\\cdot \\mathbf{x}\\).\n\n\n\nsometime written as \\(\\beta'\\mathbf{x}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-6",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-6",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nRecall that the one dimensional Normal/Gaussian probability density (aka likelihood) is:\n\\[\n\\mathscr{N}\\left(x;\\mu,\\sigma^2\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\frac{1}{2}\\frac{x-\\mu}{\\sigma^2}}\n\\]\nThe likelihood of multiple observations is the product of the likelihoods for each observation."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-7",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-7",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nTo fit the 1D linear regression model given \\(N\\) data samples, we minimize the negative log-likelihood (NLL) on the training set.\n\n\\[\n\\begin{align*}\n\\text{NLL}\\left(\\beta,\\sigma^{2}\\right) & =-\\sum_{n=1}^{N}\\log\\left[\\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{\\frac{1}{2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(y_{n}-\\beta'x_{n}\\right)^{2}\\right)\\right]\\\\\n& =\\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{N}\\left(y_{n}-\\hat{y}_{n}\\right)^{2}-\\frac{N}{2}\\log\\left(2\\pi\\sigma^{2}\\right)\n\\end{align*}\n\\]\n\nwhere the predicted response is \\(\\hat{y}\\equiv\\beta'x_{n}\\). This is also the maximum likelihood estimation (MLE) method."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-8",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-8",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nMinimizing the NLL by minimizing the residual sum of squares (RSS) is the same as minimizing\n\nthe mean squared error \\(\\text{MSE}\\left(\\beta\\right) = \\frac{1}{N}\\text{RSS}\\left(\\beta\\right)\\)\nthe root mean squared error \\(\\text{RMSE}\\left(\\beta\\right) = \\sqrt{\\text{MSE}\\left(\\beta\\right)}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#aside-empirical-risk-minimization",
    "href": "slides/BSMM_8740_lec_03.html#aside-empirical-risk-minimization",
    "title": "Regression methods",
    "section": "Aside: empirical risk minimization",
    "text": "Aside: empirical risk minimization\nThe MLE can be generalized by replacing the NLL (\\(\\ell\\left(y_{n},\\theta;x_{n}\\right)=-\\log\\pi\\left(y_n|x_n,\\theta\\right)\\)) with any other loss function to get\n\\[\n\\mathscr{L}\\left(\\theta\\right)=\\frac{1}{N}\\sum_{n=1}^{N}\\ell\\left(y_{n},\\theta;x_{n}\\right)\n\\]\nThis is known as the empirical risk minimization (ERM) - the expected loss taken with respect to the empirical distribution."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-9",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-9",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nFocusing on just the coefficients \\(\\beta\\), the minimum NLL is (up to a constant) the minimum of the residual sum of squares (RSS)1 with coefficient estimates \\(\\hat\\beta\\) :\n\\[\n\\begin{align*}\\text{RSS}\\left(\\beta\\right) & =\\frac{1}{2}\\sum_{n=1}^{N}\\left(y_{n}-\\beta'x_{n}\\right)^{2}=\\frac{1}{2}\\left\\Vert y_{n}-\\beta'x_{n}\\right\\Vert ^{2}\\\\\n& =\\frac{1}{2}\\left(y_{n}-\\beta'x_{n}\\right)'\\left(y_{n}-\\beta'x_{n}\\right)\\\\\n\\\\\n\\end{align*}\n\\]\ni.e.Â minimizing the squared prediction error, aka ordinary least squares."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-10",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-10",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nNote that, given the assumption that the data generation process is Normal/Gaussian, we can write our regression equation in terms of individual observations as\n\\[\ny_i=\\beta_0+\\beta_1 x_i + u_i\n\\]\nwhere error term \\(u_i\\) is a sample from \\(\\mathscr{N}\\left(0,\\sigma^{2}\\right)\\) which in turn implies \\(\\mathbb{E}\\left[u\\right]=0;\\;\\mathbb{E}\\left[\\left.u\\right|x\\right]=0\\)\nThe independence of the covariates and the errors/residuals is a testable assumption."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-11",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-11",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nIt follows independence of the covariates and the errors that\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[y-\\beta_{0}-\\beta_{1}x\\right] & =0\\\\\n\\mathbb{E}\\left[x\\left(y-\\beta_{0}-\\beta_{1}x\\right)\\right] & =0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-12",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-12",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nWriting these equations in terms of our samples (where \\(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}\\) are our coefficient estimates)\n\\[\n\\begin{align*}\n\\frac{1}{N}\\sum_{i-1}^{N}y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i} & =0\\\\\n\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i}\\right) & =0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-13",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-13",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nFrom the first equation\n\\[\n\\begin{align*}\n\\bar{y}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}\\bar{x} & =0\\\\\n\\bar{y}-\\hat{\\beta}_{1}\\bar{x} & =\\hat{\\beta}_{0}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-14",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-14",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nSubstituting the expression for \\(\\hat{\\beta}_{0}\\) in the independence equation\n\n\\[\n\\begin{align*}\n\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(y_{i}-\\left(\\bar{y}-\\hat{\\beta}_{1}\\bar{x}\\right)-\\hat{\\beta}_{1}x_{i}\\right) & =0\\\\\n\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(y_{i}-\\bar{y}\\right) & =\\hat{\\beta}_{1}\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(\\bar{x}-x_{i}\\right)\\\\\n\\frac{1}{N}\\sum_{i-1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right) & =\\hat{\\beta}_{1}\\frac{1}{N}\\sum_{i-1}^{N}\\left(\\bar{x}-x_{i}\\right)^2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-15",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-15",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nSo as long as \\(\\sum_{i-1}^{N}\\left(\\bar{x}-x_{i}\\right)^2\\ne 0\\)\n\\[\n\\begin{align*}\n\\hat{\\beta}_{1} & =\\frac{\\sum_{i-1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum_{i-1}^{N}\\left(\\bar{x}_{i}-x_{i}\\right)^2}\\\\\n& =\\frac{\\text{sample covariance}(x_{i}y_{i})}{\\text{sample variance}(x_{i})}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-models-16",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-models-16",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nSimilarly, in the vector equation, the minimum of the RSS is solved by (assuming \\(N&gt;D\\)):\n\\[\n\\hat{\\mathbf{\\beta}}_{OLS}=\\left(X'X\\right)^{-1}\\left(X'Y\\right) = \\frac{\\text{cov}(X,Y)}{\\text{var}(X)}\n\\]\nThere are algorithmic issues with computing \\(\\left(X'X\\right)^{-1}\\) though, so we could instead start with \\(X\\beta=y\\) and write \\(\\hat{\\mathbf{\\beta}}_{OLS}=X^{-1}y\\) ."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\nComputing the inverse of \\(X'X\\) directly, while theoretically possible, can be numerically unstable.\nIn R, the \\(QR\\) decomposition is used to solve for \\(\\beta\\). Let \\(X=QR\\) where \\(Q'Q=I\\) and write:\n\\[\n\\begin{align*}\n(QR)\\beta & = y\\\\\nQ'QR\\beta & = Q'y\\\\\n\\beta & = R^{-1}(Q'y)\n\\end{align*}\n\\]\nSince \\(R\\) is upper triangular, the last equation can be solved by back-substitution."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-1",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-1",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\n\n&gt; A &lt;- matrix(c(1,2,5, 2,4,6, 3, 3, 3), nrow=3)\n&gt; QR &lt;- qr(A)\n\n\nQRA\n\n\n\n&gt; Q &lt;- qr.Q(QR); Q\n\n           [,1]       [,2]          [,3]\n[1,] -0.1825742 -0.4082483 -8.944272e-01\n[2,] -0.3651484 -0.8164966  4.472136e-01\n[3,] -0.9128709  0.4082483  2.593051e-16\n\n\n\n&gt; Q %*% t(Q) |&gt; round()\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n\n\n\n&gt; R &lt;- qr.R(QR); R\n\n          [,1]      [,2]      [,3]\n[1,] -5.477226 -7.302967 -4.381780\n[2,]  0.000000 -1.632993 -2.449490\n[3,]  0.000000  0.000000 -1.341641\n\n\n\n\n\n&gt; Q %*% R\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    2    4    3\n[3,]    5    6    3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-2",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-2",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\n\nCode\n&gt; # A linear system of equations y = Ax\n&gt; cat(\"matrix A\\n\")\n&gt; A &lt;- matrix(c(3, 2, -1, 2, -2, .5, -1, 4, -1), nrow=3); A\n&gt; cat(\"vector x\\n\")\n&gt; x &lt;- c(1, -2, -2); x\n&gt; cat(\"vector y\\n\")\n&gt; y &lt;- A %*% x ; y\n\n\n\n\nmatrix A\n\n\n     [,1] [,2] [,3]\n[1,]    3  2.0   -1\n[2,]    2 -2.0    4\n[3,]   -1  0.5   -1\n\n\n\n\nvector x\n\n\n[1]  1 -2 -2\n\n\n\n\nvector y\n\n\n     [,1]\n[1,]    1\n[2,]   -2\n[3,]    0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-3",
    "href": "slides/BSMM_8740_lec_03.html#linear-regression-algorithms-3",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\n\n&gt; # Compute the QR decomposition of A\n&gt; QR &lt;- qr(A)\n&gt; Q &lt;- qr.Q(QR)\n&gt; R &lt;- qr.R(QR)\n&gt; \n&gt; # Compute b=Q'y\n&gt; b &lt;- crossprod(Q, y); b\n\n           [,1]\n[1,]  0.2672612\n[2,]  2.1472519\n[3,] -0.5638092\n\n&gt; # Solve the upper triangular system Rx=b\n&gt; backsolve(R, b)\n\n     [,1]\n[1,]    1\n[2,]   -2\n[3,]   -2"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#collinearity",
    "href": "slides/BSMM_8740_lec_03.html#collinearity",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\n\nOne of the important assumptions of the classical linear regression models is that there is no exact collinearity among the regressors.\nWhile high correlation between regressors is a necessary indicator of the collinearity problem, a direct linear relationship beween regressors is sufficient."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#collinearity-1",
    "href": "slides/BSMM_8740_lec_03.html#collinearity-1",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\n\nData collection methods, constraints on the fitted regression model, model specification error, an overdefined model, may be some potential sources of multicollinearity.\nIn other cases it is an artifact caused by creating new predictors from other predictors."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#collinearity-2",
    "href": "slides/BSMM_8740_lec_03.html#collinearity-2",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\nThe problem of collinearity has potentially serious effect on the regression estimates such as:\n\nimplausible coefficient signs,\nimpossible inversion of matrix \\(X'X\\) as it becomes near or exactly singular,\nlarge magnitude of coefficients in absolute value,\nlarge variance or standard errors with wider confidence intervals."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#collinearity-3",
    "href": "slides/BSMM_8740_lec_03.html#collinearity-3",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\nMitigating Collinearity:\n\nRemove Highly Correlated Variables: If two variables are highly correlated, consider removing one of them.\nCombine Variables: Create a new variable that combines the collinear variables\nPrincipal Component Analysis (PCA): Use PCA to transform the correlated variables into a smaller set of uncorrelated variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-vs-variance",
    "href": "slides/BSMM_8740_lec_03.html#bias-vs-variance",
    "title": "Regression methods",
    "section": "Bias vs Variance",
    "text": "Bias vs Variance\nWe introduced truncated Taylor series approximations to motivate using simplified models of the mean function when using regression.\nBut bias is the error introduced by approximating a real-world problem, which may be complex, by a simplified model.\nSo to reduce bias, why not include more Taylor series terms, or more covariates in a first-order model?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-vs-variance-1",
    "href": "slides/BSMM_8740_lec_03.html#bias-vs-variance-1",
    "title": "Regression methods",
    "section": "Bias vs Variance",
    "text": "Bias vs Variance\nNote that for random variables in general and Gaussian random variables in particular\n\nthe mean of the sum of random variables is the sum of the means of the random variables.\nthe variance of the sum of random variables is the sum of the variances of the random variables.\n\nSo adding more terms or more covariates may reduce bias by improving the mean estimate, but will certainly increase the variance of the estimate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-b-vs-variance-v-tradeoffs",
    "href": "slides/BSMM_8740_lec_03.html#bias-b-vs-variance-v-tradeoffs",
    "title": "Regression methods",
    "section": "Bias (B) vs Variance (V) tradeoffs",
    "text": "Bias (B) vs Variance (V) tradeoffs\n\n\\(\\downarrow\\) B \\(\\uparrow\\) V\\(\\uparrow\\) B \\(\\downarrow\\) V\\(-\\) B \\(-\\) V\n\n\nLow Bias and High Variance\n\nA model with low bias fits the training data very closely, capturing all the details and fluctuations.\nThis leads to overfitting, where the model performs well on the training data but poorly on new data because it has learned the noise in the training data as if it were a signal.\n\n\n\nHigh Bias and Low Variance\n\nA model with high bias makes oversimplified assumptions about the data, ignoring relevant complexities.\nThis leads to underfitting, where the model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and new data.\n\n\n\nBalancing Bias and Variance\n\nThe goal is to find a sweet spot where the model is complex enough to capture the underlying patterns (low bias) but simple enough not to capture the noise (low variance).\nAchieving this balance ensures the model generalizes well to new data, providing good performance overall."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-b-vs-variance-v-examples",
    "href": "slides/BSMM_8740_lec_03.html#bias-b-vs-variance-v-examples",
    "title": "Regression methods",
    "section": "Bias (B) vs Variance (V) examples",
    "text": "Bias (B) vs Variance (V) examples\n\nUnderfittingOverfittingBalanced\n\n\nUnderfitting (High Bias, Low Variance)\n\nSuppose youâ€™re predicting house prices using just the size of the house (one variable) in a linear regression model.\nIf the true relationship is complex (e.g., non-linear, involving multiple factors), this simple model will have high bias and underfit the data, missing important patterns.\n\n\n\nOverfitting (Low Bias, High Variance)\n\nNow, imagine you use a very complex model, like a high-degree polynomial regression, that uses many variables and interactions.\nThis model fits the training data very well but captures noise as well. When applied to new data, its performance drops because it has learned patterns that donâ€™t generalize (high variance).\n\n\n\nBalanced Model\n\nA balanced model might use a moderate number of relevant variables and a reasonable complexity (like a linear regression with interaction terms or a low-degree polynomial).\nThis model captures the essential patterns without fitting the noise, resulting in good generalization to new data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#bias-vs-variance-2",
    "href": "slides/BSMM_8740_lec_03.html#bias-vs-variance-2",
    "title": "Regression methods",
    "section": "Bias vs Variance",
    "text": "Bias vs Variance\nThe following regression models techniques with the higher variance that follows from a large number of covariates by adding a bit of bias. The variance is reduced by penalizing covariate coefficients, shrinking then towards zero.\nThe resulting simpler models may not fully capture the patterns in the data, thus underfitting the data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression",
    "title": "Regression methods",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nRidge regression is an example of a penalized regression model; in this case the magnitude of the weights are penalized by adding the \\(\\ell_2\\) norm of the weights to the loss function. In particular, the ridge regression weights are:\n\\[\n\\hat{\\beta}_{\\text{ridge}}=\\arg\\!\\min\\text{RSS}\\left(\\beta\\right)+\\lambda\\left\\Vert \\beta\\right\\Vert _{2}^{2}\n\\]\nwhere \\(\\lambda\\) is the strength of the penalty term.\nThe Ridge objective function is\n\\[\n\\mathscr{L}\\left(\\beta,\\lambda\\right)=\\text{NLL}+\\lambda\\left\\Vert \\beta\\right\\Vert_2^2\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-1",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-1",
    "title": "Regression methods",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nThe solution is:\n\\[\n\\begin{align*}\n\\hat{\\mathbf{\\beta}}_{ridge} & =\\left(X'X-\\lambda I_{D}\\right)^{-1}\\left(X'Y\\right)\\\\\n& =\\left(\\sum_{n}x_{n}x'_{n}+\\lambda I_{D}\\right)^{-1}\\left(\\sum_{n}y_{n}x_{n}\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-2",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-2",
    "title": "Regression methods",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nAs for un-penalized linear regression, using matrix inversion to solve for \\(\\hat{\\mathbf{\\beta}}_{ridge}\\) can be a bad idea. The QR transformation can be used here, however, ridge regression is often used when \\(D&gt;N\\), in which case the SVD transformation is faster."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-example",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-example",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n&gt; #define response variable\n&gt; y &lt;- mtcars %&gt;% dplyr::pull(hp)\n&gt; \n&gt; #define matrix of predictor variables\n&gt; x &lt;- mtcars %&gt;% dplyr::select(mpg, wt, drat, qsec) %&gt;% data.matrix()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-example-1",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-example-1",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n&gt; # fit ridge regression model\n&gt; model &lt;- glmnet::glmnet(x, y, alpha = 0)\n&gt; \n&gt; # get coefficients when lambda = 7.6\n&gt; coef(model, s = 7.6)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 477.91365858\nmpg          -3.29697140\nwt           20.31745927\ndrat         -0.09524492\nqsec        -18.48934710"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-example-2",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-example-2",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n\nglmnet example\n&gt; # perform k-fold cross-validation to find optimal lambda value\n&gt; cv_model &lt;- glmnet::cv.glmnet(x, y, alpha = 0)\n&gt; \n&gt; # find optimal lambda value that minimizes test MSE\n&gt; best_lambda &lt;- cv_model$lambda.min\n&gt; \n&gt; # produce plot of test MSE by lambda value\n&gt; cv_model %&gt;% broom::tidy() %&gt;% \n+ ggplot(aes(x=lambda, y = estimate)) +\n+   geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#00ABFD\", alpha=0.5) +\n+   geom_point() +\n+   geom_vline(xintercept=best_lambda) +\n+   labs(title='Ridge Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}\"\n+          )\n+   ) +\n+   ggplot2::scale_x_log10()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#ridge-regression-example-3",
    "href": "slides/BSMM_8740_lec_03.html#ridge-regression-example-3",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n\nglmnet coefficients\n&gt; model$beta %&gt;% \n+   as.matrix() %&gt;% \n+   t() %&gt;% \n+   tibble::as_tibble() %&gt;% \n+   tibble::add_column(lambda = model$lambda, .before = 1) %&gt;% \n+   tidyr::pivot_longer(-lambda, names_to = 'parameter') %&gt;% \n+   ggplot(aes(x=lambda, y=value, color=parameter)) +\n+   geom_line() + geom_point() +\n+   xlim(0,2000) +\n+   labs(title='Ridge Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"Parameters as a function of lambda\"\n+          )\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#lasso-regression",
    "href": "slides/BSMM_8740_lec_03.html#lasso-regression",
    "title": "Regression methods",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nLasso regression is another example of a penalized regression model; in this case both the magnitude of the weights and the number of parameters are penalized by using the \\(\\ell_1\\) norm of the weights to the loss function of the lasso regression. In particular, the lasso regression weights are:\n\\[\n\\hat{\\beta}_{\\text{lasso}}=\\arg\\!\\min\\text{RSS}\\left(\\beta\\right)+\\lambda\\left\\Vert \\beta\\right\\Vert _{1}\n\\]\nThe Lasso objective function is\n\\[\n\\mathscr{L}\\left(\\beta,\\lambda\\right)=\\text{NLL}+\\lambda\\left\\Vert \\beta\\right\\Vert _{1}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#lasso-regression-example",
    "href": "slides/BSMM_8740_lec_03.html#lasso-regression-example",
    "title": "Regression methods",
    "section": "Lasso Regression Example",
    "text": "Lasso Regression Example\n\n\nlasso model\n&gt; # define response variable\n&gt; y &lt;- mtcars %&gt;% dplyr::pull(hp)\n&gt; \n&gt; # define matrix of predictor variables\n&gt; x &lt;- mtcars %&gt;% dplyr::select(mpg, wt, drat, qsec) %&gt;% data.matrix()\n&gt; \n&gt; # fit ridge regression model\n&gt; model &lt;- glmnet::glmnet(x, y, alpha = 1)\n&gt; \n&gt; # get coefficients when lambda = 3.53\n&gt; coef(model, s = 3.53)\n\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) 480.761125\nmpg          -3.036337\nwt           20.222451\ndrat          .       \nqsec        -18.944318"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#lasso-regression-example-1",
    "href": "slides/BSMM_8740_lec_03.html#lasso-regression-example-1",
    "title": "Regression methods",
    "section": "Lasso Regression Example",
    "text": "Lasso Regression Example\n\n\nlasso example\n&gt; #perform k-fold cross-validation to find optimal lambda value\n&gt; cv_model &lt;- glmnet::cv.glmnet(x, y, alpha = 1)\n&gt; \n&gt; #find optimal lambda value that minimizes test MSE\n&gt; best_lambda &lt;- cv_model$lambda.min\n&gt; \n&gt; #produce plot of test MSE by lambda value\n&gt; cv_model %&gt;% broom::tidy() %&gt;% \n+ ggplot(aes(x=lambda, y = estimate)) +\n+   geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#00ABFD\", alpha=0.5) +\n+   geom_point() +\n+   geom_vline(xintercept=best_lambda) +\n+   labs(title='Lasso Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}\"\n+          )\n+   ) +\n+   xlim(0,exp(4)) + ggplot2::scale_x_log10()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#lasso-regression-example-2",
    "href": "slides/BSMM_8740_lec_03.html#lasso-regression-example-2",
    "title": "Regression methods",
    "section": "Lasso Regression Example",
    "text": "Lasso Regression Example\n\n\nlasso coefficients\n&gt; model %&gt;%\n+   broom::tidy() %&gt;%\n+   tidyr::pivot_wider(names_from=term, values_from=estimate) %&gt;%\n+   dplyr::select(-c(step,dev.ratio, `(Intercept)`)) %&gt;%\n+   dplyr::mutate_all(dplyr::coalesce, 0) %&gt;% \n+   tidyr::pivot_longer(-lambda, names_to = 'parameter') %&gt;% \n+   ggplot(aes(x=lambda, y=value, color=parameter)) +\n+   geom_line() + geom_point() +\n+   xlim(0,70) +\n+   labs(title='Ridge Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"Parameters as a function of lambda\"\n+          )\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#elastic-net-regression",
    "href": "slides/BSMM_8740_lec_03.html#elastic-net-regression",
    "title": "Regression methods",
    "section": "Elastic Net Regression",
    "text": "Elastic Net Regression\nElastic Net regression is a hybrid of ridge and lasso regression.\nThe elastic net objective function is\n\\[\n\\mathscr{L}\\left(\\beta,\\lambda,\\alpha\\right)=\\text{NLL}+\\lambda\\left(\\left(1-\\alpha\\right)\\left\\Vert \\beta\\right\\Vert _{2}^{2}+\\alpha\\left\\Vert \\beta\\right\\Vert _{1}\\right)\n\\]\nso that \\(\\alpha=0\\) is ridge regression and \\(\\alpha=1\\) is lasso regression and \\(\\alpha\\in\\left(0,1\\right)\\) is the general elastic net."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#elastic-net-regression-example",
    "href": "slides/BSMM_8740_lec_03.html#elastic-net-regression-example",
    "title": "Regression methods",
    "section": "Elastic Net Regression Example",
    "text": "Elastic Net Regression Example\n\n\nelastic net example\n&gt; # set length of data and seed for reproducability\n&gt; n &lt;- 50\n&gt; set.seed(2467)\n&gt; # create the dataset\n&gt; dat &lt;- tibble::tibble(\n+   a = sample(1:20, n, replace = T)/10\n+   , b = sample(1:10, n, replace = T)/10\n+   , c = sort(sample(1:10, n, replace = T))\n+ ) %&gt;% \n+   dplyr::mutate(\n+     z = (a*b)/2 + c + sample(-10:10, n, replace = T)/10\n+     , .before = 1\n+   )\n&gt; # cross validate to get the best alpha\n&gt; alpha_dat &lt;- tibble::tibble( alpha = seq(0.01, 0.99, 0.01) ) %&gt;% \n+   dplyr::mutate(\n+     mse =\n+       purrr::map_dbl(\n+         alpha\n+         , (\\(a){\n+           cvg &lt;- \n+            glmnet::cv.glmnet(\n+              x = dat %&gt;% dplyr::select(-z) %&gt;% as.matrix() \n+              , y = dat$z \n+              , family = \"gaussian\"\n+              , gamma = a\n+           )\n+           min(cvg$cvm)\n+         })\n+       )\n+   ) \n&gt; \n&gt; best_alpha &lt;- alpha_dat %&gt;% \n+   dplyr::filter(mse == min(mse)) %&gt;% \n+   dplyr::pull(alpha)\n&gt; \n&gt; cat(\"best alpha:\", best_alpha)\n\n\nbest alpha: 0.64\n\n\n\n\nelastic net example, part 2\n&gt; elastic_cv &lt;- \n+   glmnet::cv.glmnet(\n+     x = dat %&gt;% dplyr::select(-z) %&gt;% as.matrix() \n+     , y = dat$z \n+     , family = \"gaussian\"\n+     , gamma = best_alpha)\n&gt; \n&gt; best_lambda &lt;- elastic_cv$lambda.min\n&gt; cat(\"best lambda:\", best_lambda)\n\n\nbest lambda: 0.01015384\n\n\nelastic net example, part 2\n&gt; elastic_mod &lt;- glmnet::glmnet(\n+   x = dat %&gt;% dplyr::select(-z) %&gt;% as.matrix() \n+   , y = dat$z \n+   , family = \"gaussian\"\n+   , gamma = best_alpha, lambda = best_lambda)\n&gt; \n&gt; elastic_mod %&gt;% broom::tidy()\n\n\n# A tibble: 4 Ã— 5\n  term         step estimate lambda dev.ratio\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1   -0.467 0.0102     0.963\n2 a               1    0.221 0.0102     0.963\n3 b               1    0.560 0.0102     0.963\n4 c               1    1.03  0.0102     0.963"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#elastic-net-regression-example-1",
    "href": "slides/BSMM_8740_lec_03.html#elastic-net-regression-example-1",
    "title": "Regression methods",
    "section": "Elastic Net Regression Example",
    "text": "Elastic Net Regression Example\n\n\nelastic net example, part 3\n&gt; pred &lt;- predict(elastic_mod, dat %&gt;% dplyr::select(-z) %&gt;% as.matrix())\n&gt; \n&gt; rmse &lt;- sqrt(mean( (pred - dat$z)^2 ))\n&gt; R2 &lt;- 1 - (sum((dat$z - pred )^2)/sum((dat$z - mean(y))^2))\n&gt; mse &lt;- mean((dat$z - pred)^2)\n&gt; \n&gt; cat(\" RMSE:\", rmse, \"\\n\", \"R-squared:\", R2, \"\\n\", \"MSE:\", mse)\n\n\n RMSE: 0.5817823 \n R-squared: 0.9999828 \n MSE: 0.3384707\n\n\n\n\nelastic net example, part 4\n&gt; dat %&gt;% \n+   tibble::as_tibble() %&gt;% \n+   tibble::add_column(pred = pred[,1]) %&gt;% \n+   tibble::rowid_to_column(\"ID\") %&gt;% \n+   ggplot(aes(x=ID, y=z)) +\n+   geom_point() +\n+   geom_line(aes(y=pred),color='red')"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA generalized linear model (GLM) is a flexible generalization of ordinary linear regression.\nOrdinary linear regression predicts the expected value of the outcome variable, a random variable, as a linear combination of a set of observed values (predictors). In a generalized linear model (GLM), each outcome \\(Y\\) is assumed to be generated from a particular distribution in an exponential family, The mean, \\(\\mu\\), of the distribution depends on the independent variables, \\(X\\), through:\n\\[\n\\mathbb{E}\\left[\\left.Y\\right|X\\right]=\\mu=\\text{g}^{-1}\\left(X\\beta\\right)\n\\] where \\(g\\) is called the link function."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models-1",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models-1",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nFor example, if \\(Y\\) is Poisson distributed, then\n\\[\n\\mathbb{P}\\left[\\left.Y=y\\right|X,\\lambda\\right]=\\frac{\\lambda^{y}}{y!}e^{-\\lambda}=e^{y\\log\\lambda-\\lambda-\\log y!}\n\\]\nWhere \\(\\lambda\\) is both the mean and the variance. In the glm the link function is \\(\\log\\) and\n\\[\n\\log\\mathbb{E}\\left[\\left.Y\\right|X\\right] = \\beta X=\\log\\lambda\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models-2",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models-2",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nKey Components of GLMs\n\nRandomSystemicLink\n\n\nRandom Component:\n\nSpecifies the probability distribution of the data generation process of the response variable (\\(Y\\)). Examples include Normal, Binomial, Poisson, etc.\n\n\n\nSystematic Component:\n\nSpecifies the linear predictor (\\(\\eta = X\\beta)\\), where (\\(X\\)) is the matrix of predictors and (\\(\\beta\\)) is the vector of coefficients.\n\n\n\nLink Function:\n\nConnects the mean of the response variable (\\(\\mathbb{E}(Y)\\)) to the linear predictor (\\(\\eta\\)). It transforms the expected value of the response variable to the linear predictor scale."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models-3",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models-3",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nCommon Types of GLMs\n\nLinearLogisticPoissonGamma\n\n\n\nLinear Regression (Binomial Distribution)\n\nResponse Variable: Continuous\nLink Function: Identity (\\((g(\\mu) = \\mu\\)))\nExample: Predicting house prices based on square footage, number of bedrooms, etc.\nFormula: \\((Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\epsilon)\\), where \\((Y\\)) is normally distributed.\n\n\n\n\n\nLogistic Regression (binomial Distribution)\n\nResponse Variable: Binary (0 or 1)\nLink Function: Logit (\\((g(\\mu) = \\log(\\frac{\\mu}{1-\\mu})\\)))\nExample: Predicting whether a customer will buy a product (yes/no) based on age, income, etc.\nFormula: \\((\\log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots\\)), where (\\(p\\)) is the probability of the event occurring.\n\n\n\n\n\nPoisson Regression (Poisson Distribution)\n\nResponse Variable: Count data (non-negative integers)\nLink Function: Log (\\((g(\\mu) = \\log(\\mu)\\)))\nExample: Predicting the number of insurance claims in a year based on driver age, vehicle type, etc.\nFormula: \\((\\log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots)\\), where \\((\\lambda)\\) is the expected count.\n\n\n\n\n\nGamma Regression (Gamma Distribution)\n\nResponse Variable: Continuous and positive\nLink Function: Inverse (\\((g(\\mu) = \\frac{1}{\\mu}\\)))\nExample: Predicting the time until failure of a machine based on temperature, pressure, etc.\nFormula: \\((\\frac{1}{\\mu} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots)\\), where (\\(\\mu\\)) is the mean of the response variable."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#generalized-linear-models-4",
    "href": "slides/BSMM_8740_lec_03.html#generalized-linear-models-4",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nExamples of GLMs\n\nLogisticPoissonGamma\n\n\n\nLogistic Regression Example:\n\nScenario: A marketing team wants to predict whether a customer will buy a product.\nVariables: Customer age, income, and previous purchase history.\nModel: \\((\\log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 \\text{Age} + \\beta_2 \\text{Income} + \\beta_3 \\text{History}\\))\nInterpretation: The coefficients (_1, _2, _3) indicate how each predictor affects the log odds of making a purchase.\n\n\n\n\n\nPoisson Regression Example:\n\nScenario: An insurance company wants to predict the number of claims a policyholder will file.\nVariables: Age of the policyholder, type of vehicle, and driving experience.\nModel: \\((\\log(\\lambda) = \\beta_0 + \\beta_1 \\text{Age} + \\beta_2 \\text{VehicleType} + \\beta_3 \\text{Experience}\\))\nInterpretation: The coefficients \\((\\beta_1, \\beta_2, \\beta_3)\\) indicate how each predictor affects the expected number of claims.\n\n\n\n\n\nGamma Regression Example:\n\nScenario: A manufacturing company wants to predict the lifetime of a machine part.\nVariables: Operating temperature, pressure, and usage frequency.\nModel: \\((\\frac{1}{\\mu} = \\beta_0 + \\beta_1 \\text{Temperature} + \\beta_2 \\text{Pressure} + \\beta_3 \\text{Frequency})\\)\nInterpretation: The coefficients \\((\\beta_1, \\beta_2, \\beta_3)\\) indicate how each predictor affects the inverse of the expected lifetime."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\n\n\nCode\n&gt; dat &lt;- MASS::Boston\n\n\nThere are many methodologies for constructing regression trees but one of the oldest is known as the classification and regression tree (CART) approach.\nBasic regression trees partition a data set into smaller subgroups and then fit a simple constant for each observation in the subgroup. The partitioning is achieved by successive binary partitions (akaÂ recursive partitioning) based on the different predictors."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-1",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nAs a simple example, consider a continuous response variable \\(y\\) with two covariates \\(x_1,x_2\\) and the support of \\(x_1,x_2\\) partitioned into three regions. Then we write the tree regression model for \\(y\\) as:\n\\[\n\\hat{y} = \\hat{f}(x_1,x_2)=\\sum_{i=1}^{3}c_1\\times I_{(x_1,x_2)\\in R_i}\n\\]\nTree algorithms differ in how they grow the regression tree, i.e.Â partition the space of the covariates."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-2",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nAll partitioning of variables is done in a top-down, greedy fashion. This just means that a partition performed earlier in the tree will not change based on later partitions. In general the partitions are made to minimize following objective function (support initially partitioned into 2 regions, i.e.Â a binary tree):\n\\[\n\\text{SSE}=\\left\\{ \\sum_{i\\in R_{1}}\\left(y_{i}-c_{i}\\right)^{2}+\\sum_{i\\in R_{2}}\\left(y_{i}-c_{i}\\right)^{2}\\right\\}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-3",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nHaving found the best split, we repeat the splitting process on each of the two regions.\nThis process is continued until some stopping criterion is reached. What typically results is a very deep, complex tree that may produce good predictions on the training set, but is likely to overfit the data, particularly at the lower nodes.\nBy pruning these lower level nodes, we can introduce a little bit of bias in our model that help to stabilize predictions and will tend to generalize better to new, unseen data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-4",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-4",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nAs with penalized linear regression, we can use a complexity parameter \\(\\alpha\\) to penalize the number of terminal nodes of the tree (\\(T\\)), like the lasso \\(L_1\\) norm penalty, and find the smallest tree with lowest penalized error, i.e.Â the minimizing the following objective function:\n\\[\n\\text{SSE}+\\alpha\\left|T\\right|\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-5",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-5",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\n\n\nStrengths\n\nThey are very interpretable.\nMaking predictions is fast; just lookup constants in the tree.\nVariables importance is easy; those variables that most reduce the SSE.\nTree models give a non-linear response; better if the true regression surface is not smooth.\nThere are fast, reliable algorithms to learn these trees.\n\n\nWeaknesses\n\nSingle regression trees have high variance, resulting in unstable predictions (an alternative subsample of training data can significantly change the terminal nodes).\nDue to the high variance single regression trees have poor predictive accuracy."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\nAs mentioned, single tree models suffer from high variance. Although pruning the tree helps reduce this variance, there are alternative methods that actually exploite the variability of single trees in a way that can significantly improve performance over and above that of single trees. Bootstrap aggregating (bagging) is one such approach.\nBagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-1",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\nBagging combines and averages multiple tree models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, improving predictive performance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-2",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\nBagging follows three steps:\n\nCreate \\(m\\) bootstrap samples from the training data. Bootstrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.\nFor each bootstrap sample train a single, unpruned regression tree.\nAverage individual predictions from each tree to create an overall average predicted value."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-trees-bagging-3",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\n\nFig: The bagging process."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\nBagging trees introduces a random component into the tree building process that reduces the variance of a single treeâ€™s prediction and improves predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree.\nSo trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships. They are correlated."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-1",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\nTree correlation prevents bagging from optimally reducing the variance of the predictive values. Reducing variance further can be achieved by injecting more randomness into the tree-growing process. Random forests achieve this in two ways:\n\n\nBootstrap: similar to bagging - each tree is grown from a bootstrap resampled data set, which somewhat decorrelates them.\nSplit-variable randomization: each time a split is made, the search for the split variable is limited to a random subset of \\(m\\) of the \\(p\\) variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-2",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\nFor regression trees, typical default values used in split-value randomization are \\(m=\\frac{p}{3}\\) but this should be considered a tuning parameter.\nWhen \\(m=p\\), the randomization amounts to using only step 1 and is the same as bagging."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-a-random-forest-3",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\n\n\nStrengths\n\nTypically have very good performance\nRemarkably good â€œout-of-the boxâ€ - very little tuning required\nBuilt-in validation set - donâ€™t need to sacrifice data for extra validation\nNo pre-processing required\nRobust to outliers\n\n\nWeaknesses\n\nCan become slow on large data sets\nAlthough accurate, often cannot compete with advanced boosting algorithms\nLess interpretable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nGradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-1",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nWhereas random forests build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful â€œcommitteeâ€ that are often hard to beat with other algorithms."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-2",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nThe main idea of boosting is to add new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.\n\nSequential ensemble approach."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-3",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nBoosting is a framework that iteratively improves any weak learning model. Many gradient boosting applications allow you to â€œplug inâ€ various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-4",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-4",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nA weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. With regards to decision trees, shallow trees represent a weak learner. Commonly, trees with only 1-6 splits are used."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-5",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-5",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nCombining many weak models (versus strong ones) has a few benefits:\n\n\nSpeed: Constructing weak models is computationally cheap.\nAccuracy improvement: Weak models allow the algorithm to learn slowly; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.\nAvoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-6",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-gradient-boosting-6",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nHere is the algorithm for boosted regression trees with features \\(x\\) and response \\(y\\):\n\n\nFit a decision tree to the data: \\(F_1(x)=y\\),\nWe then fit the next decision tree to the residuals of the previous: \\(h_1(x)=yâˆ’F_1(x)\\)\nAdd this new tree to our algorithm: \\(F_2(x)=F_1(x)+h_1(x)\\),\nFit the next decision tree to the residuals of \\(F_2: h_2(x)=yâˆ’F_2(x)\\),\nAdd this new tree to our algorithm: \\(F_3(x)=F_2(x)+h_1(x)\\),\nContinue this process until some mechanism (i.e.Â cross validation) tells us to stop."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#xgboost-example",
    "href": "slides/BSMM_8740_lec_03.html#xgboost-example",
    "title": "Regression methods",
    "section": "XGBoost Example",
    "text": "XGBoost Example\nXGBoost is short for eXtreme Gradient Boosting package.\nWhile the XGBoost model often achieves higher accuracy than a single decision tree, it sacrifices the intrinsic interpretability of decision trees. For example, following the path that a decision tree takes to make its decision is trivial and self-explained, but following the paths of hundreds or thousands of trees is much harder.\nWe will work with XGBoost in todayâ€™s lab."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#kernel-regression",
    "href": "slides/BSMM_8740_lec_03.html#kernel-regression",
    "title": "Regression methods",
    "section": "Kernel Regression",
    "text": "Kernel Regression\nKernel Regression is a non-parametric technique in machine learning used to estimate the relationship between a dependent variable and one or more independent variables.\nUnlike linear regression, Kernel Regression does not assume a specific form for the relationship between the variables. Instead, it uses a weighted average of nearby observed data points to make predictions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#kernel-regression-1",
    "href": "slides/BSMM_8740_lec_03.html#kernel-regression-1",
    "title": "Regression methods",
    "section": "Kernel Regression",
    "text": "Kernel Regression\n\n\nSelect a Kernel Function:\n\nChoose a kernel function that will determine how weights are assigned to nearby data points. The Gaussian kernel is a common choice, where weights decrease with distance according to a normal distribution.\n\nChoose a Bandwidth:\n\nDecide on the bandwidth parameter that will control the spread of the kernel function. This affects the smoothness of the regression curve.\n\nCompute Weights:\n\nFor each point where you want to estimate the dependent variable, compute the weights for all observed data points using the kernel function.\n\nCalculate Weighted Average:\n\nUse the weights to compute a weighted average of the dependent variable values, giving more influence to points closer to the point of interest."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#kernel-regression-example",
    "href": "slides/BSMM_8740_lec_03.html#kernel-regression-example",
    "title": "Regression methods",
    "section": "Kernel Regression: example",
    "text": "Kernel Regression: example\n\n\nCode\n&gt; #Kernel regression\n&gt; # from https://towardsdatascience.com/kernel-regression-made-easy-to-understand-86caf2d2b844\n&gt; Kdata &lt;- \n+   tibble::tibble(\n+     Area = c(11,22,33,44,50,56,67,70,78,89,90,100)\n+     , RiverFlow = c(2337,2750,2301,2500,1700,2100,1100,1750,1000,1642, 2000,1932)\n+   )\n&gt; \n&gt; #function to calculate Gaussian kernel\n&gt; gausinKernel &lt;- function(x,b){exp(-0.5 *(x/b)^2)/(sqrt(2*pi))}\n&gt; #plotting function\n&gt; plt_fit &lt;- function(bandwidth = 10, support = seq(5,110,1)){\n+   tibble::tibble(x_hat = support) |&gt; \n+   dplyr::mutate(\n+     y_hat =\n+       purrr::map_dbl(\n+         x_hat\n+         , (\n+         \\(x){\n+           K &lt;- gausinKernel(Kdata$Area-x, bandwidth)\n+           sum( Kdata$RiverFlow * K/sum(K) )\n+         })\n+       )\n+   ) |&gt; \n+   ggplot(aes(x=x_hat, y=y_hat)) + \n+   geom_line(color=\"blue\") +\n+   geom_point(data = Kdata, aes(x=Area, y=RiverFlow), size=4, color=\"red\") +\n+   labs(title = \"Kernel regression\", subtitle = stringr::str_glue(\"bandwith = {bandwidth}; data = red | fit = blue\") ) +\n+   theme_minimal()\n+ }\n\n\n\nB = 5B=10B=15"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#kernel-regression-2",
    "href": "slides/BSMM_8740_lec_03.html#kernel-regression-2",
    "title": "Regression methods",
    "section": "Kernel Regression:",
    "text": "Kernel Regression:\n\nAdvantages of Kernel Regression\n\nFlexibility: Can capture complex, non-linear relationships between variables.\nNo Assumptions: Does not require the assumption of a specific functional form for the relationship.\n\nDisadvantages of Kernel Regression\n\nComputationally Intensive: Can be slow, especially with large datasets, since it requires calculating weights for all data points for each estimate.\nChoice of Parameters: The results can be sensitive to the choice of kernel function and bandwidth, requiring careful tuning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\nRegression with neural nets involves using artificial neural networks (ANNs) to predict a continuous output variable based on one or more input variables. Neural nets are powerful, flexible models that can capture complex relationships and patterns in the data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns-components",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns-components",
    "title": "Regression methods",
    "section": "Regression with ANNs: Components",
    "text": "Regression with ANNs: Components\n\n\nNeurons:\n\nThe building blocks of neural networks. Each neuron takes an input, processes it, and passes the output to the next layer.\n\nLayers:\n\nInput Layer: Receives the input data.\nHidden Layers: Intermediate layers that process the input data through neurons. There can be one or more hidden layers.\nOutput Layer: Produces the final prediction.\n\nWeights and Biases:\n\nEach connection between neurons has a weight, which adjusts the strength of the signal.\nEach neuron has a bias, which adjusts the output along with the weighted sum of inputs."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns-components-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns-components-1",
    "title": "Regression methods",
    "section": "Regression with ANNs: Components",
    "text": "Regression with ANNs: Components\n\n\nActivation Functions:\n\nFunctions applied to the output of each neuron in hidden layers to introduce non-linearity. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n\nLoss Function:\n\nMeasures the difference between the predicted output and the actual output. For regression tasks, common loss functions include Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n\nOptimization Algorithm:\n\nAdjusts the weights and biases to minimize the loss function. The most common optimization algorithm is Gradient Descent and its variants like Adam."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns-algorithm",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns-algorithm",
    "title": "Regression methods",
    "section": "Regression with ANNs: Algorithm",
    "text": "Regression with ANNs: Algorithm\n\n\nForward Propagation:\n\nInput data is passed through the network, layer by layer, with each neuron applying its weights, bias, and activation function, until the output layer produces the prediction.\n\nLoss Calculation:\n\nThe loss function calculates the error between the predicted output and the actual target value.\n\nBackward Propagation:\n\nThe network uses the error to adjust the weights and biases. This involves calculating the gradient of the loss function with respect to each weight and bias (using the chain rule), and then updating the weights and biases to reduce the error.\n\nIterative Training:\n\nThe process of forward propagation, loss calculation, and backward propagation is repeated for many iterations (epochs) until the loss converges to a minimum value."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns",
    "title": "Regression methods",
    "section": "Regression with ANNs:",
    "text": "Regression with ANNs:\n\nAdvantagesDisadvantages\n\n\nAdvantages of ANNs for Regression\n\nFlexibility: Can model complex, non-linear relationships between inputs and outputs.\nHigh Performance: Can achieve high accuracy with sufficient data and proper tuning.\nFeature Learning: Automatically learns relevant features from raw input data.\n\n\n\nDisadvantages of ANNs for Regression\n\nComputationally Intensive: Requires significant computational resources.\nData Hungry: Needs a large amount of training data to perform well.\nComplexity: Requires careful tuning of hyperparameters (e.g., number of layers, neurons, learning rate) and can be prone to overfitting if not properly regularized."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-1",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-1",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\nArchitecture of an ANN\n\nSingle layer NN architecture"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-2",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-2",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\n\nCommon Activation Functions"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-anns-example",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-anns-example",
    "title": "Regression methods",
    "section": "Regression with ANNs: example",
    "text": "Regression with ANNs: example\n\n\nCode\n&gt; set.seed(500)\n&gt;   \n&gt; # Boston dataset from MASS\n&gt; data &lt;- MASS::Boston\n&gt; \n&gt; # Normalize the data\n&gt; maxs &lt;- data %&gt;% dplyr::summarise_all(max) %&gt;% as.matrix() %&gt;% as.vector()\n&gt; mins &lt;- data %&gt;% dplyr::summarise_all(min) %&gt;% as.matrix() %&gt;% as.vector()\n&gt; data_scaled &lt;- data %&gt;% \n+   scale(center = mins, scale = maxs - mins) %&gt;% \n+   tibble::as_tibble()\n&gt;   \n&gt; # Split the data into training and testing set\n&gt; data_split &lt;- data_scaled %&gt;% rsample::initial_split(prop = .75)\n&gt; # extracting training data and test data as two seperate dataframes\n&gt; data_train &lt;- rsample::training(data_split)\n&gt; data_test  &lt;- rsample::testing(data_split)\n&gt; \n&gt; nn &lt;- data_train %&gt;% \n+   neuralnet::neuralnet(\n+     medv ~ .\n+     , data = .\n+     , hidden = c(5, 3)\n+     , linear.output = TRUE\n+   )\n&gt;   \n&gt; # Predict on test data\n&gt; pr.nn &lt;- neuralnet::compute( nn, data_test %&gt;% dplyr::select(-medv) )\n&gt;   \n&gt; # Compute mean squared error\n&gt; pr.nn_ &lt;- \n+   pr.nn$net.result * \n+   (max(data$medv) - min(data$medv)) +\n+   min(data$medv)\n&gt; test.r &lt;- \n+   data_test$medv * \n+   (max(data$medv) - min(data$medv)) + \n+   min(data$medv)\n&gt; MSE.nn &lt;- sum((test.r - pr.nn_)^2) / nrow(data_test)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-3",
    "href": "slides/BSMM_8740_lec_03.html#regression-with-neural-nets-3",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\n\nNNRegression"
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#recap",
    "href": "slides/BSMM_8740_lec_03.html#recap",
    "title": "Regression methods",
    "section": "Recap",
    "text": "Recap\n\nToday we worked though a parametric and non-parametric regression methods that are useful for predicting a value given a set of covariates.\nNext week we will look in detail at the tidymodels package which will give a way to develop a workflow for fitting and comparing our models across different feature sets."
  },
  {
    "objectID": "slides/BSMM_8740_lec_03.html#section",
    "href": "slides/BSMM_8740_lec_03.html#section",
    "title": "Regression methods",
    "section": "",
    "text": "bsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/recipes.html#r-model-formulas",
    "href": "slides/recipes.html#r-model-formulas",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "R Model Formulas",
    "text": "R Model Formulas\nA simple formula in a linear model to predict house prices:\n\n&gt; mod1 &lt;- stats::lm(\n+   log(price) ~ type + sqft\n+   , data = Sacramento\n+   , subset = beds &gt; 2\n+   )\n\n\nThe purpose of this code chunk:\n\nsubset some of the data points (subset)\ncreate a design matrix for 2 predictor variable (but 3 model terms)\nlog transform the outcome variable\nfit a linear regression model\n\n\nThe first two steps create the design matrix."
  },
  {
    "objectID": "slides/recipes.html#summary-model-formula-method",
    "href": "slides/recipes.html#summary-model-formula-method",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Summary: Model Formula Method",
    "text": "Summary: Model Formula Method\n\nModel formulas are very expressive in that they can represent model terms easily\nThe formula/terms framework does some elegant functional programming\nFunctions can be embedded inline to do fairly complex things (on single variables) and these can be applied to new data sets."
  },
  {
    "objectID": "slides/recipes.html#summary-model-formula-method-1",
    "href": "slides/recipes.html#summary-model-formula-method-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Summary: Model Formula Method",
    "text": "Summary: Model Formula Method\nHowever, there are significant limitations to what this framework can do and, in some cases, it can be very inefficient.\nThis is mostly due of being written well before large scale modeling and machine learning were commonplace."
  },
  {
    "objectID": "slides/recipes.html#limitations-of-the-current-system",
    "href": "slides/recipes.html#limitations-of-the-current-system",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Limitations of the Current System",
    "text": "Limitations of the Current System\n\nFormulas are not very extensible especially with nested or sequential operations (e.g.Â y ~ scale(center(knn_impute(x)))).\nWhen used in modeling functions, you cannot recycle the previous computations.\nFor wide data sets, the formula method can be very inefficient and consume a significant proportion of the total execution time."
  },
  {
    "objectID": "slides/recipes.html#limitations-of-the-current-system-1",
    "href": "slides/recipes.html#limitations-of-the-current-system-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Limitations of the Current System",
    "text": "Limitations of the Current System\n\nMultivariate outcomes are kludgy by requiring cbind\nFormulas have a limited set of roles (next two slides)\n\nA more in-depth discussion of these issues can be found in this blog post."
  },
  {
    "objectID": "slides/recipes.html#variable-roles",
    "href": "slides/recipes.html#variable-roles",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Variable Roles",
    "text": "Variable Roles\nFormulas have been re-implemented in different packages for a variety of different reasons:\n\n&gt; # ?lme4::lmer\n&gt; # Subjects need to be in the data but are not part of the model\n&gt; lme4::lmer(Reaction ~ Days + (Days | Subject), data = lme4::sleepstudy)\n&gt; \n&gt; # BradleyTerry2\n&gt; # We want to make the outcomes to be a function of a \n&gt; # competitor-specific function of reach \n&gt; BradleyTerry2::BTm(outcome = 1, player1 = winner, player2 = loser,\n+     formula = ~ SVL[..] + (1|..), \n+     data = BradleyTerry2::flatlizards)\n&gt; \n&gt; # modeltools::ModelEnvFormula (using the modeltools package for formulas)\n&gt; # mob\n&gt; data(PimaIndiansDiabetes, package = 'mlbench')\n&gt; modeltools::ModelEnvFormula(diabetes ~ glucose | pregnant + mass +  age,\n+     data = PimaIndiansDiabetes)"
  },
  {
    "objectID": "slides/recipes.html#variable-roles-1",
    "href": "slides/recipes.html#variable-roles-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Variable Roles",
    "text": "Variable Roles\n\nA general list of possible variables roles could be\n\noutcomes\npredictors\nstratification\nmodel performance data (e.g.Â loan amount to compute expected loss)\nconditioning or faceting variables (e.g.Â lattice or ggplot2)\nrandom effects or hierarchical model ID variables\ncase weights (*)\noffsets (*)\nerror terms (limited to Error in the aov function)(*)\n\n(*) Can be handled in formulas but are hard-coded into functions."
  },
  {
    "objectID": "slides/recipes.html#recipes",
    "href": "slides/recipes.html#recipes",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Recipes",
    "text": "Recipes\nWe can approach the design matrix and preprocessing steps by first specifying a sequence of steps.\n\nprice is an outcome\ntype and sqft are predictors\nlog transform price\nconvert type to dummy variables"
  },
  {
    "objectID": "slides/recipes.html#recipes-1",
    "href": "slides/recipes.html#recipes-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Recipes",
    "text": "Recipes\nA recipe is a specification of intent.\nOne issue with the formula method is that it couples the specification for your predictors along with the implementation.\nRecipes, as youâ€™ll see, separates the planning from the doing.\nWebsite: https://topepo.github.io/recipes"
  },
  {
    "objectID": "slides/recipes.html#recipes-2",
    "href": "slides/recipes.html#recipes-2",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Recipes",
    "text": "Recipes\nA recipe can be trained then applied to any data.\n\n&gt; ## Create an initial recipe with only predictors and outcome\n&gt; rec &lt;- recipes::recipe(price ~ type + sqft, data = Sacramento)\n&gt; \n&gt; rec &lt;- rec %&gt;% \n+   recipes::step_log(price) %&gt;%\n+   recipes::step_dummy(type)\n&gt; \n&gt; rec_trained &lt;- recipes::prep(rec, training = Sacramento, retain = TRUE)\n&gt; \n&gt; design_mat  &lt;- recipes::bake(rec_trained, new_data = Sacramento)"
  },
  {
    "objectID": "slides/recipes.html#selecting-variables",
    "href": "slides/recipes.html#selecting-variables",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Selecting Variables",
    "text": "Selecting Variables\nIn the last slide, we used dplyr-like syntax for selecting variables such as step_dummy(type).\nIn some cases, the names of the predictors may not be known at the time when you construct a recipe (or model formula). For example:\n\ndummy variable columns\nPCA feature extraction when you keep components that capture \\(X\\)% of the variability.\ndiscretized predictors with dynamic bins"
  },
  {
    "objectID": "slides/recipes.html#selecting-variables-1",
    "href": "slides/recipes.html#selecting-variables-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Selecting Variables",
    "text": "Selecting Variables\ndplyr selectors can also be used on variables names, such as\n\n&gt; rec %&gt;% \n+   recipes::step_spatialsign(\n+     matches(\"^PC[1-9]\")\n+     , all_numeric()\n+     , -all_outcomes()\n+   )\n\nVariables can be selected by name, role, data type, or any combination of these."
  },
  {
    "objectID": "slides/recipes.html#extending",
    "href": "slides/recipes.html#extending",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Extending",
    "text": "Extending\nNeed to add more preprocessing or other operations?\n\n&gt; standardized &lt;- rec_trained %&gt;%\n+   recipes::step_center(recipes::all_numeric()) %&gt;%\n+   recipes::step_scale(recipes::all_numeric()) %&gt;%\n+   recipes::step_pca(recipes::all_numeric())\n&gt;           \n&gt; ## Only estimate the new parts:\n&gt; standardized &lt;- recipes::prep(standardized)\n\nIf an initial step is computationally expensive, you donâ€™t have to redo those operations to add more."
  },
  {
    "objectID": "slides/recipes.html#available-steps",
    "href": "slides/recipes.html#available-steps",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Available Steps",
    "text": "Available Steps\n\nBasic: logs, roots, polynomials, logits, hyperbolics\nEncodings: dummy variables, â€œotherâ€ factor level collapsing, discretization\nDate Features: Encodings for day/doy/month etc, holiday indicators\nFilters: correlation, near-zero variables, linear dependencies\nImputation: K-nearest neighbors, bagged trees, mean/mode imputation,"
  },
  {
    "objectID": "slides/recipes.html#available-steps-1",
    "href": "slides/recipes.html#available-steps-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Available Steps",
    "text": "Available Steps\n\nNormalization/Transformations: center, scale, range, Box-Cox, Yeo-Johnson\nDimension Reduction: PCA, kernel PCA, ICA, Isomap, data depth features, class distances\nOthers: spline basis functions, interactions, spatial sign\n\nMore on the way (i.e.Â autoencoders, more imputation methods, etc.)\nOne of the package vignettes shows how to write your own step functions."
  },
  {
    "objectID": "slides/recipes.html#extending-1",
    "href": "slides/recipes.html#extending-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Extending",
    "text": "Extending\nRecipes can also be created with different roles manually\n\n&gt; rec &lt;- \n+   recipes::recipe(x  = Sacramento) %&gt;%\n+   recipes::update_role(price, new_role = \"outcome\") %&gt;%\n+   recipes::update_role(type, sqft, new_role = \"predictor\") %&gt;%\n+   recipes::update_role(zip, new_role = \"strata\")\n\nAlso, the sequential nature of steps means that they donâ€™t have to be R operations and could call other compute engines (e.g.Â Weka, scikit-learn, Tensorflow, etc. )"
  },
  {
    "objectID": "slides/recipes.html#extending-2",
    "href": "slides/recipes.html#extending-2",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Extending",
    "text": "Extending\nWe can create wrappers to work with recipes too:\n\n&gt; lin_reg.recipe &lt;- function(rec, data, ...) {\n+   trained &lt;- recipes::prep(rec, training = data)\n+   lm.fit(\n+     x = recipes::bake(trained, newdata = data, all_predictors())\n+     , y = recipes::bake(trained, newdata = data, all_outcomes())\n+     , ...\n+   )\n+ }"
  },
  {
    "objectID": "slides/recipes.html#an-example",
    "href": "slides/recipes.html#an-example",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "An Example",
    "text": "An Example\nKuhn and Johnson (2013) analyze a data set where thousands of cells are determined to be well-segmented (WS) or poorly segmented (PS) based on 58 image features. We would like to make predictions of the segmentation quality based on these features."
  },
  {
    "objectID": "slides/recipes.html#an-example-1",
    "href": "slides/recipes.html#an-example-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "An Example",
    "text": "An Example\n\n&gt; data(segmentationData, package = \"caret\")\n&gt; \n&gt; seg_train &lt;- segmentationData %&gt;% \n+   dplyr::filter(Case == \"Train\") %&gt;% \n+   dplyr::select(-Case, -Cell)\n&gt; \n&gt; seg_test  &lt;- segmentationData %&gt;% \n+   dplyr::filter(Case == \"Test\")  %&gt;% \n+   dplyr::select(-Case, -Cell)"
  },
  {
    "objectID": "slides/recipes.html#a-simple-recipe",
    "href": "slides/recipes.html#a-simple-recipe",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "A Simple Recipe",
    "text": "A Simple Recipe\n\n&gt; rec &lt;- recipes::recipe(Class  ~ ., data = seg_train)\n&gt; \n&gt; basic &lt;- rec %&gt;%\n+   # Correct some predictors for skewness\n+   recipes::step_YeoJohnson(recipes::all_predictors()) %&gt;%\n+   # Standardize the values\n+   recipes::step_center(recipes::all_predictors()) %&gt;%\n+   recipes::step_scale(recipes::all_predictors())\n&gt; \n&gt; # Estimate the transformation and standardization parameters \n&gt; basic &lt;- \n+   recipes::prep(\n+     basic\n+     , training = seg_train\n+     , verbose = FALSE\n+     , retain = TRUE\n+   )"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis",
    "href": "slides/recipes.html#principal-component-analysis",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca &lt;- basic %&gt;% \n+   recipes::step_pca(\n+     recipes::all_predictors()\n+     , threshold = .9\n+   )\n&gt; \n&gt; summary(pca)\n\n# A tibble: 59 Ã— 4\n   variable                type      role      source  \n   &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 AngleCh1                &lt;chr [2]&gt; predictor original\n 2 AreaCh1                 &lt;chr [2]&gt; predictor original\n 3 AvgIntenCh1             &lt;chr [2]&gt; predictor original\n 4 AvgIntenCh2             &lt;chr [2]&gt; predictor original\n 5 AvgIntenCh3             &lt;chr [2]&gt; predictor original\n 6 AvgIntenCh4             &lt;chr [2]&gt; predictor original\n 7 ConvexHullAreaRatioCh1  &lt;chr [2]&gt; predictor original\n 8 ConvexHullPerimRatioCh1 &lt;chr [2]&gt; predictor original\n 9 DiffIntenDensityCh1     &lt;chr [2]&gt; predictor original\n10 DiffIntenDensityCh3     &lt;chr [2]&gt; predictor original\n# â„¹ 49 more rows"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis-1",
    "href": "slides/recipes.html#principal-component-analysis-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca %&lt;&gt;% recipes::prep() \n&gt; \n&gt; pca %&gt;% summary()\n\n# A tibble: 16 Ã— 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Class    &lt;chr [3]&gt; outcome   original\n 2 PC01     &lt;chr [2]&gt; predictor derived \n 3 PC02     &lt;chr [2]&gt; predictor derived \n 4 PC03     &lt;chr [2]&gt; predictor derived \n 5 PC04     &lt;chr [2]&gt; predictor derived \n 6 PC05     &lt;chr [2]&gt; predictor derived \n 7 PC06     &lt;chr [2]&gt; predictor derived \n 8 PC07     &lt;chr [2]&gt; predictor derived \n 9 PC08     &lt;chr [2]&gt; predictor derived \n10 PC09     &lt;chr [2]&gt; predictor derived \n11 PC10     &lt;chr [2]&gt; predictor derived \n12 PC11     &lt;chr [2]&gt; predictor derived \n13 PC12     &lt;chr [2]&gt; predictor derived \n14 PC13     &lt;chr [2]&gt; predictor derived \n15 PC14     &lt;chr [2]&gt; predictor derived \n16 PC15     &lt;chr [2]&gt; predictor derived"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis-2",
    "href": "slides/recipes.html#principal-component-analysis-2",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca &lt;-\n+   recipes::bake(\n+     pca\n+     , new_data = seg_test\n+     , everything()\n+   )\n&gt; pca[1:4, 1:8]\n\n# A tibble: 4 Ã— 8\n  Class  PC01  PC02   PC03   PC04  PC05   PC06   PC07\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 PS     4.86 -5.85 -0.891 -4.13  1.84  -2.29  -3.88 \n2 PS     3.28 -1.51  0.353 -2.24  0.441 -0.911  0.800\n3 WS    -7.03 -1.77 -2.42  -0.652 3.22  -0.212  0.118\n4 WS    -6.96 -2.08 -2.89  -1.79  3.20  -0.845 -0.204"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis-3",
    "href": "slides/recipes.html#principal-component-analysis-3",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n\nPCA1\n&gt; ggplot(pca, aes(x = PC01, y = PC02, color = Class)) + \n+   geom_point(alpha = .4)"
  },
  {
    "objectID": "slides/recipes.html#principal-component-analysis-4",
    "href": "slides/recipes.html#principal-component-analysis-4",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n\nPCA2\n&gt; rngs &lt;- extendrange(c(pca$PC01, pca$PC02))\n&gt; ggplot(pca, aes(x = PC01, y = PC02, color = Class)) + \n+   geom_point(alpha = .4) + \n+   xlim(rngs) + ylim(rngs) + \n+   theme(legend.position = \"top\")"
  },
  {
    "objectID": "slides/recipes.html#kernel-principal-component-analysis",
    "href": "slides/recipes.html#kernel-principal-component-analysis",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Kernel Principal Component Analysis",
    "text": "Kernel Principal Component Analysis\n\n&gt; kern_pca &lt;- basic %&gt;% \n+   recipes::step_kpca(\n+     recipes::all_predictors()\n+     , num_comp = 2\n+     , options = \n+       list(\n+         kernel = \"rbfdot\"\n+         , kpar = list(sigma = 0.05)\n+       )\n+   )\n&gt; \n&gt; kern_pca &lt;- recipes::prep(kern_pca)\n&gt; \n&gt; kern_pca &lt;- recipes::bake(kern_pca, new_data = seg_test, everything())"
  },
  {
    "objectID": "slides/recipes.html#kernel-principal-component-analysis-1",
    "href": "slides/recipes.html#kernel-principal-component-analysis-1",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Kernel Principal Component Analysis",
    "text": "Kernel Principal Component Analysis\n\n\nKernel PCA\n&gt; rngs &lt;- extendrange(c(kern_pca$kPC1, kern_pca$kPC2))\n&gt; ggplot(kern_pca, aes(x = kPC1, y = kPC2, color = Class)) + \n+   geom_point(alpha = .4) + \n+   xlim(rngs) + ylim(rngs) + \n+   theme(legend.position = \"top\")"
  },
  {
    "objectID": "slides/recipes.html#distance-to-each-class-centroid",
    "href": "slides/recipes.html#distance-to-each-class-centroid",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Distance to Each Class Centroid",
    "text": "Distance to Each Class Centroid\n\n\nKernel PCA - distance to each centroid\n&gt; dist_to_classes &lt;- basic %&gt;% \n+   recipes::step_classdist(recipes::all_predictors(), class = \"Class\") %&gt;%\n+   # Take log of the new distance features\n+   recipes::step_log(starts_with(\"classdist\"))\n&gt; \n&gt; dist_to_classes &lt;- recipes::prep(dist_to_classes, verbose = FALSE)\n&gt; \n&gt; # All variables are retained plus an additional one for each class\n&gt; dist_to_classes &lt;- recipes::bake(dist_to_classes, new_data = seg_test, matches(\"[Cc]lass\"))\n&gt; dist_to_classes\n\n\n# A tibble: 1,010 Ã— 3\n   Class classdist_PS classdist_WS\n   &lt;fct&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 PS            1.53         1.74\n 2 PS            1.35         1.46\n 3 WS            1.71         1.53\n 4 WS            1.75         1.61\n 5 PS            1.47         1.65\n 6 WS            1.48         1.47\n 7 WS            1.49         1.55\n 8 WS            1.55         1.40\n 9 PS            1.54         1.71\n10 PS            1.55         1.57\n# â„¹ 1,000 more rows"
  },
  {
    "objectID": "slides/recipes.html#distance-to-each-class",
    "href": "slides/recipes.html#distance-to-each-class",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Distance to Each Class",
    "text": "Distance to Each Class\n\n\nKernel PCA - distance to each class\n&gt; rngs &lt;- extendrange(c(dist_to_classes$classdist_PS, dist_to_classes$classdist_WS))\n&gt; ggplot(dist_to_classes, aes(x = classdist_PS, y = classdist_WS, color = Class)) + \n+   geom_point(alpha = .4) + \n+   xlim(rngs) + ylim(rngs) + \n+   theme(legend.position = \"top\") + \n+   xlab(\"Distance to PS Centroid (log scale)\") + \n+   ylab(\"Distance to WS Centroid (log scale)\")"
  },
  {
    "objectID": "slides/recipes.html#next-steps",
    "href": "slides/recipes.html#next-steps",
    "title": "Creating and Preprocessing a Design Matrix with Recipes",
    "section": "Next Steps",
    "text": "Next Steps\n\nGet it on CRAN once tidyselect is on CRAN\nAdd more steps\ncaret methods for recipes (instead of using preProcess):\n\nmodel1 &lt;- train(recipe, data = data, method, ...)\nas an alternative to\nmodel2 &lt;- train(x, y, method, preProcess, ...) # or\nmodel3 &lt;- train(y ~ x1 + x2, data = data, method, preProcess, ...)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_07.html#recap-of-last-week",
    "title": "Causality",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced the modeltime and timetk R packages.\nWe showed how these integrate into the tidymodels workflows for predicting timeseries."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#this-week",
    "href": "slides/BSMM_8740_lec_07.html#this-week",
    "title": "Causality",
    "section": "This week",
    "text": "This week\n\nKnowledge of the fundamental problems of inference and the biases of some intuitive estimators,\nA basic understanding of the tools used to state and then satisfy causality assumptions,\nUnderstanding of how econometric methods recover treatment effects,\nAbility to use these methods and estimate their precision using R."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#the-fundamental-problems-of-inference",
    "href": "slides/BSMM_8740_lec_07.html#the-fundamental-problems-of-inference",
    "title": "Causality",
    "section": "The fundamental problems of inference",
    "text": "The fundamental problems of inference\nWhen trying to estimate the effect of a treatment / intervention on an outcome, we face two very difficult problems: the Fundamental Problem of Causal Inference (FPCI) and the Fundamental Problem of Statistical Inference (FPSI).\nIn this lecture weâ€™ll assume all interventions are binary, e.g.Â we offer a customer a discount or we donâ€™t; we re-brand a branch office or we donâ€™t."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#rubin-causal-model-rcm1",
    "href": "slides/BSMM_8740_lec_07.html#rubin-causal-model-rcm1",
    "title": "Causality",
    "section": "Rubin Causal Model (RCM1)",
    "text": "Rubin Causal Model (RCM1)\nRCM is made of three distinct building blocks:\n\na treatment allocation rule, that decides which unit receives the treatment / intervention;\npotential outcomes, that measure how each unit reacts to the treatment;\nthe switching equation that relates potential outcomes to observed outcomes through the allocation rule.\n\nThe Rubin Causal Model (RCM), also known as the potential outcomes framework, was developed by Donald Rubin in the 1970s and gained prominence in the 1980s."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#treatment-allocation",
    "href": "slides/BSMM_8740_lec_07.html#treatment-allocation",
    "title": "Causality",
    "section": "Treatment allocation",
    "text": "Treatment allocation\nTreatment allocation is represented by the variable \\(D_i\\), where \\(D_i=1\\) if unit \\(i\\) receives the treatment and \\(D_i=0\\) if unit \\(i\\) does not receive the treatment.\nThe treatment allocation rule is critical:\n\nit switches the treatment on or off for each unit, it is going to be at the source of the FPCI.\nthe specific properties of the treatment allocation rule are going to matter for the feasibility and bias of various effect estimation methods."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#potential-outcomes",
    "href": "slides/BSMM_8740_lec_07.html#potential-outcomes",
    "title": "Causality",
    "section": "Potential outcomes",
    "text": "Potential outcomes\nEach unit can be treated or untreated, and so there are two potential outcomes:\n\n\\(Y_i^1\\): the outcome unit \\(i\\) will have if treated\n\\(Y_i^0\\): the outcome unit \\(i\\) will have if not treated"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#switching-equation",
    "href": "slides/BSMM_8740_lec_07.html#switching-equation",
    "title": "Causality",
    "section": "Switching equation",
    "text": "Switching equation\nThe switching equation. It links the observed outcome to the potential outcomes through the allocation rule:\n\\[\n\\begin{align*}\nY_{i} & =\\begin{cases}\nY_{i}^{1} & \\text{if}\\,D_{i}=1\\\\\nY_{i}^{0} & \\text{if}\\,D_{i}=0\n\\end{cases}\\\\\n& =Y_{i}^{1}D_{i}+Y_{i}^{0}\\left(1-D_{i}\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#switching-equation-1",
    "href": "slides/BSMM_8740_lec_07.html#switching-equation-1",
    "title": "Causality",
    "section": "Switching equation",
    "text": "Switching equation\nWhat the switching equation means is that, for each unit \\(i\\) we get to observe only one of the two potential outcomes. we can never see both potential outcomes for the same unit at the same time.\nFor each of the units, one of the two potential outcomes is unobserved - it is counterfactual. It can be conceived by an effort of reason: it is the consequence of what would have happened had some action not been taken."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#treatment-effects",
    "href": "slides/BSMM_8740_lec_07.html#treatment-effects",
    "title": "Causality",
    "section": "Treatment effects",
    "text": "Treatment effects\nNow we can defined the causal effect:\nThe unit level treatment effect is defined as \\(\\Delta_i^Y=Y_i^1-Y_i^0\\) (which cannot be observed).\nNote that the treatment effects can be heterogeneous (they can differ by unit)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#treatment-effects-1",
    "href": "slides/BSMM_8740_lec_07.html#treatment-effects-1",
    "title": "Causality",
    "section": "Treatment effects",
    "text": "Treatment effects\nAverage treatment effect on the treated (TT1) gives us summary stats on the sample or the population.\n\\[\n\\Delta_{TT_{s}}^{Y}=\\frac{1}{\\sum_{i=1}^{N}D_{i}}\\sum_{i=1}^{N}\\left(Y_{i}^{1}-Y_{i}^{0}\\right)D_{i}\n\\] with the corresponding expected value in the population\n\\[\n\\Delta_{TT}^{Y}=\\mathbb{E}\\left[\\left.Y_{i}^{1}-Y_{i}^{0}\\right|D_{i}=1\\right]\n\\]\nYouâ€™ll sometimes see this written as ATT"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#fundamental-problem-of-causal-inference",
    "href": "slides/BSMM_8740_lec_07.html#fundamental-problem-of-causal-inference",
    "title": "Causality",
    "section": "Fundamental problem of causal inference",
    "text": "Fundamental problem of causal inference\nIt is impossible to observe TT, either in the population or in the sample.\n\\[\n\\begin{align*}\n\\Delta_{TT_{s}}^{Y} & =\\frac{1}{\\sum_{i=1}^{N}D_{i}}\\sum_{i=1}^{N}\\left(Y_{i}^{1}-Y_{i}^{0}\\right)D_{i}\\\\\n& =\\frac{1}{\\sum_{i=1}^{N}D_{i}}\\sum_{i=1}^{N}Y_{i}D_{i}-\\frac{1}{\\sum_{i=1}^{N}D_{i}}\\sum_{i=1}^{N}Y_{i}^{0}D_{i}\n\\end{align*}\n\\] but \\(Y_{i}^{0}D_{i}\\) is unobserved when \\(D_i=1\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#intuitive-casual-estimators",
    "href": "slides/BSMM_8740_lec_07.html#intuitive-casual-estimators",
    "title": "Causality",
    "section": "Intuitive casual estimators",
    "text": "Intuitive casual estimators\nIntuitive comparisons are often made in order to estimate causal effects, e.g.Â the with/without comparison (WW).\nWW compares the average outcomes of the treated units with those of the untreated units.\nIntuitive comparisons try to proxy for the expected counterfactual outcome in the treated group by using an observed quantity."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#intuitive-casual-estimators-1",
    "href": "slides/BSMM_8740_lec_07.html#intuitive-casual-estimators-1",
    "title": "Causality",
    "section": "Intuitive casual estimators",
    "text": "Intuitive casual estimators\nUnfortunately, these proxies are generally poor and provide biased estimates of TT.\nThe reason that these proxies are poor is that the treatment is not the only factor that differentiates the treated group from the groups used to form the proxy. The intuitive comparisons are biased because factors other than the treatment are correlated to its allocation."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#withwithout-comparison",
    "href": "slides/BSMM_8740_lec_07.html#withwithout-comparison",
    "title": "Causality",
    "section": "With/Without comparison",
    "text": "With/Without comparison\n\\[\n\\Delta_{WW}^{Y}=\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=1\\right] - \\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=0\\right]\n\\]\nNote that this is not the same as the treatment effect on the treated (TT):\n\\[\n\\Delta_{TT}^{Y}=\\mathbb{E}\\left[\\left.Y_{i}^{1}-Y_{i}^{0}\\right|D_{i}=1\\right]\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#withwithout-comparison-1",
    "href": "slides/BSMM_8740_lec_07.html#withwithout-comparison-1",
    "title": "Causality",
    "section": "With/Without comparison",
    "text": "With/Without comparison\nThe difference \\(\\Delta_{WW}^{Y}-\\Delta_{TT}^{Y}\\) is called the selection bias \\(\\Delta_{SB}^{Y}\\):\n\\[\n\\begin{align*}\n\\Delta_{SB}^{Y} & =\\Delta_{WW}^{Y}-\\Delta_{TT}^{Y}\\\\\n& =\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=0\\right]-\\mathbb{E}\\left[\\left.Y_{i}^{1}-Y_{i}^{0}\\right|D_{i}=1\\right]\\\\\n& =\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=0\\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#withwithout-comparison-2",
    "href": "slides/BSMM_8740_lec_07.html#withwithout-comparison-2",
    "title": "Causality",
    "section": "With/Without comparison",
    "text": "With/Without comparison\nUnder what conditions would we have\n\\[\n\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=0\\right]\\ne0\n\\]\nWith non-zero selection bias, selection into the treatment biases the effect of the treatment on outcomes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#withwithout-comparison-3",
    "href": "slides/BSMM_8740_lec_07.html#withwithout-comparison-3",
    "title": "Causality",
    "section": "With/Without comparison",
    "text": "With/Without comparison\n\n\nConfounding factors are the factors that generate differences between treated and untreated individuals even in the absence of the treatment.\nSuppose we wanted to test giving discounts to our smaller-value customers to induce them to buy more. The results of the test could be biased if small-value customer would just buy less anyway.\nThe mere fact of being selected for receiving the discount means that these customers have a host of characteristics that would differentiate them from the unselected customers."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#identification-assumption",
    "href": "slides/BSMM_8740_lec_07.html#identification-assumption",
    "title": "Causality",
    "section": "Identification assumption",
    "text": "Identification assumption\nThere is no selection bias if\n\\[\n\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}^{0}\\right|D_{i}=0\\right]=0\n\\]\ni.e.Â the expected counterfactual outcome of the treated is equal to the expected potential outcome of the untreated in the absence of the treatment.\nThis is called the identification assumption."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#identification-assumption-1",
    "href": "slides/BSMM_8740_lec_07.html#identification-assumption-1",
    "title": "Causality",
    "section": "Identification assumption",
    "text": "Identification assumption\nFor the identification assumption to hold, it has to be that all the determinants of \\(D_i\\) are actually unrelated to \\(Y_i^0\\).\nOne way to enforce this assumption is to randomize treatment assignment.\nOne way to test for the validity of the identification assumption is to compare the values of observed covariates in the treated and untreated group, because differences in covariates an lead to confounding."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#identification-assumption-2",
    "href": "slides/BSMM_8740_lec_07.html#identification-assumption-2",
    "title": "Causality",
    "section": "Identification assumption",
    "text": "Identification assumption\nIf the identification assumption holds, and the treatment assignment is (effectively) independent of the outcome, then\n\\[\n\\begin{align*}\n\\Delta_{WW}^{Y} & =\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=1\\right]-\\mathbb{E}\\left[\\left.Y_{i}\\right|D_{i}=0\\right]\\\\\n& =\\mathbb{E}\\left[Y_{i}\\right]-\\mathbb{E}\\left[Y_{i}\\right]\\\\\n& =\\Delta_{ATE}^{Y}\n\\end{align*}\n\\] where \\(\\Delta_{ATE}^{Y}\\) is the average treatment effect in the population."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#fundamental-problems",
    "href": "slides/BSMM_8740_lec_07.html#fundamental-problems",
    "title": "Causality",
    "section": "Fundamental problems",
    "text": "Fundamental problems\n\nThe FPCI states that our causal parameter of interest (\\(\\Delta_{TT}^{Y}\\) of the effect of treatment on the treated) is fundamentally unobservable, even when the sample size is infinite.\nThe FPSI states that, even if we have an estimator \\(E\\) that identifies \\(\\Delta_{TT}^{Y}\\) in the population, we cannot observe \\(E\\) because we only have access to a finite sample of the population."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#causal-inference-workflow",
    "href": "slides/BSMM_8740_lec_07.html#causal-inference-workflow",
    "title": "Causality",
    "section": "Causal Inference Workflow",
    "text": "Causal Inference Workflow\n\nSpecify a causal question\nDraw our assumptions using a causal diagram\nModel our assumptions\nDiagnose our models\nEstimate the causal effect\nConduct sensitivity analysis on the effect estimate\n\n\n\n\n\n\n\nImportant\n\n\nThe following material is from Causal Inference in R by Barrett, Dâ€™Agostino McGowan & Gerke under a MIT + file license."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#directed-acyclic-graphs-dags",
    "href": "slides/BSMM_8740_lec_07.html#directed-acyclic-graphs-dags",
    "title": "Causality",
    "section": "Directed acyclic graphs (DAGs)",
    "text": "Directed acyclic graphs (DAGs)\n\n\nFigureÂ 1: A causal directed acyclic graph (DAG). DAGs depict causal relationships. In this DAG, the assumption is that x causes y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags-1",
    "href": "slides/BSMM_8740_lec_07.html#dags-1",
    "title": "Causality",
    "section": "DAGs",
    "text": "DAGs\n\n\nFigureÂ 2: Three types of causal relationships: forks, chains, and colliders. The direction of the arrows and the relationships of interest dictate which type of path a series of variables represents. Forks represent a mutual cause, chains represent direct causes, and colliders represent a mutual descendant."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags-2",
    "href": "slides/BSMM_8740_lec_07.html#dags-2",
    "title": "Causality",
    "section": "DAGs",
    "text": "DAGs\n\nForks represent a common cause of two variables. Here, weâ€™re saying that q causes both x and y, the traditional definition of a confounder. Theyâ€™re called forks because the arrows from x to y are in different directions.\nChains, on the other hand, represent a series of arrows going in the same direction. Here, q is called a mediator: it is along the causal path from x to y. In this diagram, the only path from x to y is mediated through q.\nFinally, a collider is a path where two arrowheads meet at a variable. Because causality always goes forward in time, this naturally means that the collider variable is caused by two other variables. Here, weâ€™re saying that x and y both cause q."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags-3",
    "href": "slides/BSMM_8740_lec_07.html#dags-3",
    "title": "Causality",
    "section": "DAGs",
    "text": "DAGs\n\nThese three types of paths have different implications for the statistical relationship between x and y. If we only look at the correlation between the two variables under these assumptions:\n\nIn the fork, x and y will be associated, despite there being no arrow from x to y.\nIn the chain, x and y are related only through q.\nIn the collider, x and y will not be related.\n\nPaths that transmit association are called open paths. Paths that do not transmit association are called closed paths. Forks and chains are open, while colliders are closed."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---forks",
    "href": "slides/BSMM_8740_lec_07.html#dags---forks",
    "title": "Causality",
    "section": "DAGs - forks",
    "text": "DAGs - forks\n\nSo, should we adjust for q? That depends on the nature of the path.\nForks are confounding paths. Because q causes both x and y, x and y will have a spurious association. They both contain information from q, their mutual cause. That mutual causal relationship makes x and y associated statistically.\nAdjusting for q will block the bias from confounding and give us the true relationship between x and y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---mediators",
    "href": "slides/BSMM_8740_lec_07.html#dags---mediators",
    "title": "Causality",
    "section": "DAGs - mediators",
    "text": "DAGs - mediators\n\nFor chains, whether or not we adjust for mediators depends on the research question.\nHere, adjusting for q would result in a null estimate of the effect of x on y. Because the only effect of x on y is via q, no other effect remains.\nThe effect of x on y mediated by q is called the indirect effect, while the effect of x on y directly is called the direct effect. If weâ€™re only interested in the direct effect, controlling for q might be what we want. If we want to know about both effects, we shouldnâ€™t try to adjust for q."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---colliders",
    "href": "slides/BSMM_8740_lec_07.html#dags---colliders",
    "title": "Causality",
    "section": "DAGs - colliders",
    "text": "DAGs - colliders\n\nColliders are different. In the collider DAG of FigureÂ 2, x and y are not associated, but both cause q.\nAdjusting for q has the opposite effect than with confounding: it opens a biasing pathway. Sometimes, people draw the path opened up by conditioning on a collider connecting x and y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---colliders-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---colliders-1",
    "title": "Causality",
    "section": "DAGs - colliders",
    "text": "DAGs - colliders\n\nHow can this be? Since x and y happen before q, q canâ€™t impact them.\nLetâ€™s turn the DAG on its side and consider FigureÂ 3 on the next slide. If we break down the two time points, at time point 1, q hasnâ€™t happened yet, and x and y are unrelated. At time point 2, q happens due to x and y. But causality only goes forward in time.\nq happening later canâ€™t change the fact that x and y happened independently in the past."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---colliders-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---colliders-2",
    "title": "Causality",
    "section": "DAGs - colliders",
    "text": "DAGs - colliders\n\n\nFigureÂ 3: A collider relationship over two points in time. At time point one, there is no relationship between x and y. Both cause q by time point two, but this does not change what already happened at time point one."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---colliders-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---colliders-3",
    "title": "Causality",
    "section": "DAGs - colliders",
    "text": "DAGs - colliders\n\nCausality only goes forward. Association, however, is time-agnostic. Itâ€™s just an observation about the numerical relationships between variables. When we control for the future, we risk introducing bias. It takes time to develop an intuition for this.\nConsider a case where x and y are the only causes of q, and all three variables are binary. When either x or y equals 1, then q happens. If we know q = 1 and x = 0 then logically it must be that y = 1. Thus, knowing about q gives us information about y via x.\nThis example is extreme, but it shows how this type of bias, sometimes called collider-stratification bias or selection bias, occurs: conditioning on q provides statistical information about x and y and distorts their relationship."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags-4",
    "href": "slides/BSMM_8740_lec_07.html#dags-4",
    "title": "Causality",
    "section": "DAGs",
    "text": "DAGs\n\nCorrectly identifying the causal structure between the exposure and outcome thus helps us\n\ncommunicate the assumptions weâ€™re making about the relationships between variables and\nidentify sources of bias.\n\nImportantly, in doing 2), we are also often able to identify ways to prevent bias based on the assumptions in 1). In the simple case of the three DAGs (FigureÂ 2), we know whether or not to control for q depending on the nature of the causal structure. The set or sets of variables we need to adjust for is called the adjustment set. DAGs can help us identify adjustment sets even in complex settings."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---exchangeability",
    "href": "slides/BSMM_8740_lec_07.html#dags---exchangeability",
    "title": "Causality",
    "section": "DAGs - exchangeability",
    "text": "DAGs - exchangeability\n\nWe commonly refer to exchangeability as the assumption of no confounding. Actually, this isnâ€™t quite right. Itâ€™s the assumption of no open, non-causal paths. Many times, these are confounding pathways. However, conditioning on a collider can also open paths. Even though these arenâ€™t confounders, doing so creates non-exchangeability between the two groups: they are different in a way that matters to the exposure and outcome.\nOpen, non-causal paths are also called backdoor paths. Weâ€™ll use this terminology often because it captures the idea well: these are any open paths biasing the effect weâ€™re interested in estimating."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---exchangeability-vs-identification",
    "href": "slides/BSMM_8740_lec_07.html#dags---exchangeability-vs-identification",
    "title": "Causality",
    "section": "DAGs - exchangeability vs identification",
    "text": "DAGs - exchangeability vs identification\n\n\nExchangeability is one of the assumptions that can help in achieving identification. If you can assume exchangeability, then you can often identify the causal effect using the observed data.\nExchangeability is more about the structure of the data and the treatment assignment mechanism, while identification is a broader concept that encompasses the entire process of linking causal models to statistical models and data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\nTo create a DAG object, weâ€™ll use ggdag::dagify() which returns a dagitty object that works with both the dagitty and ggdag packages.\nThe dagify() function takes formulas, separated by commas, that specify causes and effects, with the left element of the formula defining the effect and the right all of the factors that cause it. This is just like the type of formula we specify for most regression models in R.\nggdag::dagify( \n  effect1 ~ cause1 + cause2 + cause3\n  , effect2 ~ cause1 + cause4\n  , ...\n)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization-1",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\nWhat are all of the factors that cause graduate students to listen to a podcast the morning before an exam? What are all of the factors that could cause a graduate student to do well on a test? Letâ€™s posit some here.\n\nggdag::dagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared\n)\n\ndag {\nexam\nhumor\nmood\npodcast\nprepared\nhumor -&gt; podcast\nmood -&gt; exam\nmood -&gt; podcast\nprepared -&gt; exam\nprepared -&gt; podcast\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization-2",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\nIn the code, we assume that:\n\na graduate studentâ€™s mood, sense of humor, and how prepared they feel for the exam could influence whether they listened to a podcast the morning of the test, and\ntheir mood and how prepared they are also influence their exam score.\n\nNotice we do not see podcast in the exam equation; this means that we assume that there is no causal relationship between podcast and the exam score."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization-3",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\nSome other useful arguments for dagify():\n\nexposure and outcome: Telling ggdag the variables that are the exposure and outcome of your research question is required for many of the most valuable queries we can make of DAGs.\nlatent: This argument lets us tell ggdag that some variables in the DAG are unmeasured. latent helps identify valid adjustment sets with the data we actually have.\ncoords: Coordinates for the variables. You can choose between algorithmic or manual layouts, as discussed below. Weâ€™ll use time_ordered_coords() here.\nlabels: A character vector of labels for the variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---visualization-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---visualization-4",
    "title": "Causality",
    "section": "DAGs - visualization",
    "text": "DAGs - visualization\n\n\n\nCode\npodcast_dag &lt;- ggdag::dagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared,\n  coords = ggdag::time_ordered_coords(\n    list(\n      c(\"prepared\", \"humor\", \"mood\"), # time point 1\n      \"podcast\",                      # time point 2\n      \"exam\"                          # time point 3\n    )\n  ),\n  exposure = \"podcast\",\n  outcome = \"exam\",\n  labels = c(\n    podcast = \"podcast\",\n    exam = \"exam score\",\n    mood = \"mood\",\n    humor = \"humor\",\n    prepared = \"prepared\"\n  )\n)\nggdag::ggdag(podcast_dag, use_labels = \"label\", text = FALSE) +\n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigureÂ 4: Proposed DAG to answer the question: Does listening to a comedy podcast the morning before an exam improve graduate studentsâ€™ test scores?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\nWeâ€™ve specified the DAG for this question and told ggdag what the exposure and outcome of interest are. According to the DAG, there is no direct causal relationship between listening to a podcast and exam scores.\nAre there any other open paths? ggdag_paths() takes a DAG and visualizes the open paths.\nIn FigureÂ 5 (next slide), we see two open paths:\n\npodcast &lt;- mood -&gt; exam and\npodcast &lt;- prepared -&gt; exam.\n\nThese are both forksâ€”confounding pathways. Since there is no causal relationship between listening to a podcast and exam scores, the only open paths are backdoor paths, these two confounding pathways."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-1",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\n\n\nCode\npodcast_dag |&gt; \n  # show the whole dag as a light gray shadow rather than just the paths\n  ggdag::ggdag_paths(shadow = TRUE, text = FALSE, use_labels = \"label\") +\n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigureÂ 5: ggdag_paths() visualizes open paths in a DAG. There are two open paths in podcast_dag: the fork from mood and the fork from prepared."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-2",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\n\nDAGs are not pure data frames, but you can retrieve either the dataframe or dagitty object to work with them directly using pull_dag_data() or pull_dag(). pull_dag() can be useful when you want to work with dagitty functions:\nggdag::tidy_dagitty()\n\n\nCode\npodcast_dag_tidy &lt;- podcast_dag |&gt; \n  ggdag::tidy_dagitty()\n\npodcast_dag_tidy\n\n\n# A DAG with 5 nodes and 5 edges\n#\n# Exposure: podcast\n# Outcome: exam\n#\n# A tibble: 7 Ã— 9\n  name         x     y direction to       xend  yend\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n1 exam         3     0 &lt;NA&gt;      &lt;NA&gt;       NA    NA\n2 humor        1     0 -&gt;        podcast     2     0\n3 mood         1     1 -&gt;        exam        3     0\n4 mood         1     1 -&gt;        podcast     2     0\n5 podcast      2     0 &lt;NA&gt;      &lt;NA&gt;       NA    NA\n6 prepared     1    -1 -&gt;        exam        3     0\n7 prepared     1    -1 -&gt;        podcast     2     0\n# â„¹ 2 more variables: circular &lt;lgl&gt;, label &lt;chr&gt;\n\n\n\nggdag::dag_paths()\n\n\nCode\npodcast_dag_tidy |&gt; \n  ggdag::dag_paths() |&gt; \n  filter(set == 2, path == \"open path\")\n\n\n# A DAG with 3 nodes and 2 edges\n#\n# Exposure: podcast\n# Outcome: exam\n#\n# A tibble: 4 Ã— 11\n  set   name        x     y direction to     xend  yend\n  &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 2     exam        3     0 &lt;NA&gt;      &lt;NA&gt;     NA    NA\n2 2     podcast     2     0 &lt;NA&gt;      &lt;NA&gt;     NA    NA\n3 2     preparâ€¦     1    -1 -&gt;        exam      3     0\n4 2     preparâ€¦     1    -1 -&gt;        podcâ€¦     2     0\n# â„¹ 3 more variables: circular &lt;lgl&gt;, label &lt;chr&gt;,\n#   path &lt;chr&gt;\n\n\nggdag::pull_dag()\n\n\nCode\npodcast_dag_tidy |&gt; \n  ggdag::pull_dag() |&gt; \n  dagitty::paths()\n\n\n$paths\n[1] \"podcast &lt;- mood -&gt; exam\"    \n[2] \"podcast &lt;- prepared -&gt; exam\"\n\n$open\n[1] TRUE TRUE"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-3",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\n\nggdag::ggdag_adjustment_set() visualizes any valid adjustment sets implied by the DAG. Figure 5.11 shows adjusted variables as squares. Any arrows coming out of adjusted variables are removed from the DAG because the path is longer open at that variable.\n\nggdag::ggdag_adjustment_set(\n  podcast_dag, \n  text = FALSE, \n  use_labels = \"label\"\n) + ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\n\nFigureÂ 6: A visualization of the minimal adjustment set for the podcast-exam DAG. If this DAG is correct, two variables are required to block the backdoor paths: mood and prepared."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-4",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\nFigureÂ 6 shows the minimal adjustment set. By default, ggdag returns the set(s) that can close all backdoor paths with the fewest number of variables possible.\nIn this DAG, thatâ€™s just one set: mood and prepared. This set makes sense because there are two backdoor paths, and the only other variables on them besides the exposure and outcome are these two variables.\nSo, at minimum, we must account for both to get a valid estimate.\nMinimal adjustment sets are only one type of valid adjustment set. Sometimes, other combinations of variables can get us an unbiased effect estimate. Two other options available in ggdag are full adjustment sets and canonical adjustment sets. Full adjustment sets are every combination of variables that result in a valid set."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-5",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-5",
    "title": "Causality",
    "section": "DAGs - open paths",
    "text": "DAGs - open paths\n\n\nCode\nggdag::ggdag_adjustment_set(\n  podcast_dag, \n  text = FALSE, \n  use_labels = \"label\",\n  # get full adjustment sets\n  type = \"all\"\n) + ggdag::theme_dag()\n\n\n\n\nFigureÂ 7: All valid adjustment sets for podcast_dag."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-example",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-example",
    "title": "Causality",
    "section": "DAGs - open paths: example",
    "text": "DAGs - open paths: example\n\n\nCode\nset.seed(8740)\nsim_data &lt;- podcast_dag |&gt;\n  ggdag::simulate_data()\nsim_data\n\n\n# A tibble: 500 Ã— 5\n     exam  humor    mood podcast prepared\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.687  0.529  0.0451  0.0784  -0.477 \n 2  0.743  1.23  -0.235   2.28    -1.34  \n 3  0.906 -0.615 -0.778  -1.36    -0.839 \n 4  0.487  0.756 -0.187  -1.12     0.461 \n 5 -2.39   1.00  -0.0725 -1.02     1.81  \n 6  0.519 -1.42  -0.0496 -1.08    -0.429 \n 7 -0.178  0.321 -0.560   0.594    0.193 \n 8  0.424  0.196 -0.435  -0.396    0.618 \n 9  0.972  1.98  -2.06    0.990    1.40  \n10 -0.595 -1.66   0.215  -2.20    -0.0223\n# â„¹ 490 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---open-paths-example-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---open-paths-example-1",
    "title": "Causality",
    "section": "DAGs - open paths: example",
    "text": "DAGs - open paths: example\n\n\nSince we have simulated this data, we know that this is a case where standard methods will succeed and, therefore, can estimate the causal effect using a basic linear regression model.\nFigureÂ 8 (Estimates tab) shows a forest plot of the simulated data based on our DAG. Notice the model that only included the exposure resulted in a spurious effect (an estimate of -0.1 when we know the truth is 0).\nBy contrast, the model that adjusted for the two variables as suggested by ggdag_adjustment_set() is not spurious (much closer to 0).\n\n\n\nCode\n## Model that does not close backdoor paths\nunadjusted_model &lt;- lm(exam ~ podcast, sim_data) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"podcast\") |&gt;\n  dplyr::mutate(formula = \"podcast\")\n\n## Model that closes backdoor paths\nadjusted_model &lt;- lm(exam ~ podcast + mood + prepared, sim_data) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"podcast\") |&gt;\n  dplyr::mutate(formula = \"podcast + mood + prepared\")\n\ndplyr::bind_rows(\n  unadjusted_model,\n  adjusted_model\n) |&gt;\n  ggplot(aes(x = estimate, y = formula, xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, linewidth = 1, color = \"grey80\") +\n  geom_pointrange(fatten = 3, size = 1) +\n  theme_minimal(18) +\n  labs(\n    y = NULL,\n    caption = \"correct effect size: 0\"\n  )\n\n\n\n\n\n\n\n\nFigureÂ 8: Forest plot of simulated data based on the DAG described in FigureÂ 4."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding",
    "href": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding",
    "title": "Causality",
    "section": "DAGs - more advanced confounding",
    "text": "DAGs - more advanced confounding\n\nIn podcast_dag, mood and prepared were direct confounders: an arrow was going directly from them to podcast and exam.\nOften, backdoor paths are more complex. For example, letâ€™s add two new variables: alertness and skills_course. alertness represents the feeling of alertness from a good mood, thus an arrow from mood to alertness. skills_course represents whether the student took a College Skills Course and learned time management techniques.\nNow, skills_course is what frees up the time to listen to a podcast as well as being prepared for the exam. mood and prepared are no longer direct confounders: they are two variables along a more complex backdoor path. Additionally, weâ€™ve added an arrow going from humor to mood."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-1",
    "title": "Causality",
    "section": "DAGs - more advanced confounding",
    "text": "DAGs - more advanced confounding\n\nLetâ€™s take a look at FigureÂ 9.\n\n\nCode\npodcast_dag2 &lt;- ggdag::dagify(\n    podcast ~ mood + humor + skills_course,\n    alertness ~ mood,\n    mood ~ humor,\n    prepared ~ skills_course,\n    exam ~ alertness + prepared,\n    coords = ggdag::time_ordered_coords(),\n    exposure = \"podcast\",\n    outcome = \"exam\",\n    labels = c(\n        podcast = \"podcast\",\n        exam = \"exam score\",\n        mood = \"mood\",\n        alertness = \"alertness\",\n        skills_course = \"college\\nskills course\",\n        humor = \"humor\",\n        prepared = \"prepared\"\n    )\n)\n\nggdag::ggdag(podcast_dag2, use_labels = \"label\", text = FALSE)  + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigureÂ 9: An expanded version of podcast_dag that includes two additional variables: skills_course, representing a College Skills Course, and alertness."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-2",
    "title": "Causality",
    "section": "DAGs - more advanced confounding",
    "text": "DAGs - more advanced confounding\n\nNow there are three backdoor paths we need to close: podcast &lt;- humor -&gt; mood -&gt; alertness -&gt; exam, podcast &lt;- mood -&gt; alertness -&gt; exam, and podcast &lt;- skills_course -&gt; prepared -&gt; exam.\n\ncodepaths\n\n\n\nggdag::ggdag_paths(podcast_dag2, use_labels = \"label\", text = FALSE, shadow = TRUE) + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 10: Three open paths in podcast_dag2. Since there is no effect of podcast on exam, all three are backdoor paths that must be closed to get the correct effect."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-3",
    "title": "Causality",
    "section": "DAGs - more advanced confounding",
    "text": "DAGs - more advanced confounding\n\nThere are four minimal adjustment sets to close all three paths (and eighteen full adjustment sets!). The minimal adjustment sets are alertness + prepared, alertness + skills_course, mood + prepared, mood + skills_course.\nWe can now block the open paths in several ways - mood and prepared still work, but weâ€™ve got other options now.\nNotably, prepared and alertness could happen at the same time or even after podcast. skills_course and mood still happen before both podcast and exam, so the idea is still the same: the confounding pathway starts before the exposure and outcome.\nThe next slide shows the result of executing:\nggdag::ggdag_adjustment_set(podcast_dag2, use_labels = \"label\", text = FALSE)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-4",
    "title": "Causality",
    "section": "DAGs - more advanced confounding",
    "text": "DAGs - more advanced confounding\n\n\n\n\n\n\n\n\n\nFigureÂ 11: Valid minimal adjustment sets that will close the backdoor paths in FigureÂ 10."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-5",
    "href": "slides/BSMM_8740_lec_07.html#dags---more-advanced-confounding-5",
    "title": "Causality",
    "section": "DAGs - more advanced confounding",
    "text": "DAGs - more advanced confounding\n\nDeciding between these adjustment sets is a matter of judgment: if all data are perfectly measured, the DAG is correct, and weâ€™ve modeled them correctly, then it doesnâ€™t matter which we use. Each adjustment set will result in an unbiased estimate.\nAll three of those assumptions are usually untrue to some degree. Letâ€™s consider the path via skills_course and prepared.\nIt may be that we are better able to assess whether or not someone took the College Skills Course than how prepared for the exam they are. In that case, an adjustment set with skills_course is a better option.\nBut perhaps we better understand the relationship between preparedness and exam results. If we have it measured, controlling for that might be better. We could get the best of both worlds by including both variables: between the better measurement of skills_course and the better modeling of prepared, we might have a better chance of minimizing confounding from this path."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\nSelection bias is another name for the type of bias that is induced by adjusting for a collider. Itâ€™s called â€œselection biasâ€ because a common form of collider-induced bias is a variable inherently stratified upon by the design of the studyâ€”selection into the study.\nLetâ€™s consider a case based on the original podcast_dag but with one additional variable: whether or not the student showed up to the exam.\nNow, there is an indirect effect of podcast on exam: listening to a podcast influences whether or not the students attend the exam. The true result of exam is missing for those who didnâ€™t show up; by studying the group of people who did show up, we are inherently stratifying on this variable."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias-1",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\n\n\nCode\npodcast_dag3 &lt;- ggdag::dagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared + showed_up,\n  showed_up ~ podcast + mood + prepared,\n  coords = ggdag::time_ordered_coords(\n    list(\n      # time point 1\n      c(\"prepared\", \"humor\", \"mood\"), \n      # time point 2\n      \"podcast\",  \n      \"showed_up\",  \n      # time point 3\n      \"exam\"\n    )\n  ),\n  exposure = \"podcast\",\n  outcome = \"exam\",\n  labels = c(\n    podcast = \"podcast\",\n    exam = \"exam score\",\n    mood = \"mood\",\n    humor = \"humor\",\n    prepared = \"prepared\",\n    showed_up = \"showed up\"\n  )\n)\nggdag::ggdag(podcast_dag3, use_labels = \"label\", text = FALSE) + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigureÂ 12: Another variant of podcast_dag, this time including the inherent stratification on those who appear for the exam. There is still no direct effect of podcast on exam, but there is an indirect effect via showed_up."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias-2",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\nThe problem is that showed_up is both a collider and a mediator: stratifying on it induces a relationship between many of the variables in the DAG but blocks the indirect effect of podcast on exam.\nLuckily, the adjustment sets can handle the first problem; because showed_up happens before exam, weâ€™re less at risk of collider bias between the exposure and outcome.\nUnfortunately, we cannot calculate the total effect of podcast on exam because part of the effect is missing: the indirect effect is closed at showed_up."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias-3",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\n\n\nCode\npodcast_dag3 |&gt; \n  ggdag::adjust_for(\"showed_up\") |&gt; \n  ggdag::ggdag_adjustment_set(text = FALSE, use_labels = \"label\") + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigureÂ 13: The adjustment set for podcast_dag3 given that the data are inherently conditioned on showing up to the exam. In this case, there is no way to recover an unbiased estimate of the total effect of podcast on exam."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---selection-bias-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---selection-bias-4",
    "title": "Causality",
    "section": "DAGs - selection bias",
    "text": "DAGs - selection bias\n\n\nSometimes, you can still estimate effects in this situation by changing the estimate you wish to calculate. We canâ€™t calculate the total effect because we are missing the indirect effect, but we can still calculate the direct effect of podcast on exam.\n\npodcast_dag3 |&gt; \n  ggdag::adjust_for(\"showed_up\") |&gt; \n  ggdag::ggdag_adjustment_set(\n    effect = \"direct\"\n    , text = FALSE\n    , use_labels = \"label\"\n  )\n\n\n\n\n\n\n\n\n\n\nFigureÂ 14: The adjustment set for podcast_dag3 when targeting a different effect. There is one minimal adjustment set that we can use to estimate the direct effect of podcast on exam."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias",
    "href": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias",
    "title": "Causality",
    "section": "DAGs - M-Bias and Butterfly Bias",
    "text": "DAGs - M-Bias and Butterfly Bias\n\n\nA particular case of selection bias that youâ€™ll often see is M-bias. Itâ€™s called M-bias because it looks like an M when arranged top to bottom.\n\n\n\n\n\n\nTip\n\n\nggdag has several quick-DAGs for demonstrating basic causal structures, including confounder_triangle(), collider_triangle(), m_bias(), and butterfly_bias().\n\n\n\n\ndagitty::paths( ggdag::m_bias() )\n\n$paths\n[1] \"x &lt;- a -&gt; m &lt;- b -&gt; y\"\n\n$open\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 15: A DAG representing M-Bias, a situation where a collider predates the exposure and outcome."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-1",
    "title": "Causality",
    "section": "DAGs - M-Bias and Butterfly Bias",
    "text": "DAGs - M-Bias and Butterfly Bias\n\nLetâ€™s focus on the mood path of the podcast-exam DAG.\nWhat if we were wrong about mood, and the actual relationship was M-shaped? Letâ€™s say that, rather than causing podcast and exam, mood was itself caused by two mutual causes of podcast and exam, u1 and u2.\nWe donâ€™t know what u1 and u2 are, and we donâ€™t have them measured. As above, there are no open paths in this subset of the DAG."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-2",
    "title": "Causality",
    "section": "DAGs - M-Bias and Butterfly Bias",
    "text": "DAGs - M-Bias and Butterfly Bias\n\n\n\nCode\npodcast_dag4 &lt;- ggdag::dagify(\n  podcast ~ u1,\n  exam ~ u2,\n  mood ~ u1 + u2,\n  coords = time_ordered_coords(list(\n    c(\"u1\", \"u2\"),\n    \"mood\",\n    \"podcast\", \n    \"exam\"\n  )),\n  exposure = \"podcast\",\n  outcome = \"exam\",\n  labels = c(\n    podcast = \"podcast\",\n    exam = \"exam score\",\n    mood = \"mood\",\n    u1 = \"unmeasured\",\n    u2 = \"unmeasured\"\n  ),\n  # we don't have them measured\n  latent = c(\"u1\", \"u2\")\n)\n\nggdag::ggdag(podcast_dag4, use_labels = \"label\", text = FALSE)  + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigureÂ 16: A reconfiguration of FigureÂ 4 where mood is a collider on an M-shaped path."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---m-bias-and-butterfly-bias-3",
    "title": "Causality",
    "section": "DAGs - M-Bias and Butterfly Bias",
    "text": "DAGs - M-Bias and Butterfly Bias\n\n\nThe problem arises when we think our original DAG is the right DAG: mood is in the adjustment set, so we control for it. But this induces bias!\nIt opens up a path between u1 and u2, thus creating a path from podcast to exam.\nIf we had either u1 or u2 measured, we could adjust for them to close this path, but we donâ€™t. There is no way to close this open path.\n\n\n\n\n\n\n\n\n\nFigureÂ 17: The adjustment set where mood is a collider. If we control for mood and donâ€™t know about or have the unmeasured causes of mood, we have no means of closing the backdoor path opened by adjusting for a collider."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome",
    "href": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome",
    "title": "Causality",
    "section": "DAGs - One of exposure / outcome",
    "text": "DAGs - One of exposure / outcome\nLetâ€™s consider one other type of causal structure thatâ€™s important: causes of the exposure and not the outcome, and their opposites, causes of the outcome and not the exposure.\nLetâ€™s add a variable, grader_mood, to the original DAG."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-1",
    "title": "Causality",
    "section": "DAGs - One of exposure / outcome",
    "text": "DAGs - One of exposure / outcome\n\n\n\nCode\npodcast_dag5 &lt;- ggdag::dagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared + grader_mood,\n  coords = ggdag::time_ordered_coords(\n    list(\n      # time point 1\n      c(\"prepared\", \"humor\", \"mood\"), \n      # time point 2\n      c(\"podcast\", \"grader_mood\"),  \n      # time point 3\n      \"exam\"\n    )\n  ),\n  exposure = \"podcast\",\n  outcome = \"exam\",\n  labels = c(\n    podcast = \"podcast\",\n    exam = \"exam score\",\n    mood = \"student\\nmood\",\n    humor = \"humor\",\n    prepared = \"prepared\",\n    grader_mood = \"grader\\nmood\"\n  )\n)\nggdag::ggdag(podcast_dag5, use_labels = \"label\", text = FALSE)  + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\nFigureÂ 18: A DAG containing a cause of the exposure that is not the cause of the outcome (humor) and a cause of the outcome that is not a cause of the exposure (grader_mood)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-2",
    "title": "Causality",
    "section": "DAGs - One of exposure / outcome",
    "text": "DAGs - One of exposure / outcome\n\nStarting with humor:\nVariables that cause the exposure but not the outcome are also called instrumental variables (IVs). IVs are an unusual circumstance where, under certain conditions, controlling for them can make other types of bias worse.\nWhatâ€™s unique about this is that IVs can also be used to conduct an entirely different approach to estimating an unbiased effect of the exposure on the outcome. IVs are commonly used this way in econometrics and are increasingly popular in other areas.\nIn short, IV analysis allows us to estimate the causal effect using a different set of assumptions than the approaches weâ€™ve talked about thus far.\nIf youâ€™re unsure if the variable is an IV or not, you should probably add it to your model: itâ€™s more likely to be a confounder than an IV, and, it turns out, the bias from adding an IV is usually small in practice. So, like adjusting for a potential M-structure variable, the risk of bias is worse from confounding."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---one-of-exposure-outcome-3",
    "title": "Causality",
    "section": "DAGs - One of exposure / outcome",
    "text": "DAGs - One of exposure / outcome\n\nAbout variables that are a cause of the outcome but are not the cause of the exposure:\nWeâ€™ll call them precision variables because weâ€™re concerned about the relationship to the research question at hand, not to another research question where they are exposures.\nLike IVs, precision variables do not occur along paths from the exposure to the outcome. Thus, including them is not necessary.\nUnlike IVs, including precision variables is beneficial. Including other causes of the outcomes helps a statistical model capture some of its variation.\nThis doesnâ€™t impact the point estimate of the effect, but it does reduce the variance, resulting in smaller standard errors and narrower confidence intervals. Thus, itâ€™s recommended they be included when possible."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\n\nIn principle, using DAGs is easy: specify the causal relationships you think exist and then query the DAG for information like valid adjustment sets.\nIn practice, assembling DAGs takes considerable time and thought. Next to defining the research question itself, itâ€™s one of the most challenging steps in making causal inferences.\nVery little guidance exists on best practices in assembling DAGs. The next few slides contain some approaches to consider:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-1",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-1",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nIterate early and often\n\nMake the DAG before you conduct the study, ideally before you even collect the data.\nIf youâ€™re already working with your data, build your DAG before doing data analysis. Declaring your assumptions ahead of time can help clarify what you need to do, reduce the risk of overfitting (e.g., determining confounders incorrectly from the data), and give you time to get feedback on your DAG.\nThis last benefit is significant: you should ideally democratize your DAG. Share it early and often with others who are experts on the data, domain, and models.\nIf you have more than one candidate DAG, check their adjustment sets. If two DAGs have overlapping adjustment sets, focus on those sets; then, you can move forward in a way that satisfies the plausible assumptions you have."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-2",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-2",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nConsider your question\n\nSome questions can be challenging to answer with certain data, while others are more approachable. You should consider precisely what it is you want to estimate.\nAnother important detail about how your DAG relates to your question is the population and time. Many causal structures are not static over time and space.\nThe same is true for confounders. Even if something can cause the exposure and outcome, if the prevalence of that thing is zero in the population youâ€™re analyzing, itâ€™s irrelevant to the causal question.\nThe reverse is also true: something might be unique to the target population."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-3",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-3",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nOrder nodes by time\n\nIt is recommended to order your variables by time, either left-to-right or up-to-down. There are two reasons for this.\nFirst, time ordering is an integral part of your assumptions. After all, something happening before another thing is a requirement for it to be a cause. Thinking this through carefully will clarify your DAG and the variables you need to address.\nSecond, after a certain level of complexity, itâ€™s easier to read a DAG when arranged by time because you have to think less about that dimension; itâ€™s inherent to the layout.\nThe time ordering algorithm in ggdag automates much of this for you, although itâ€™s sometimes helpful to give it more information about the order."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-4",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-4",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nOrder nodes by time - feedback loops\n\n\nWe might think about two things that mutually cause each other as happening in a circle, like global warming and A/C use (A/C use increases global warming, which makes it hotter, which increases A/C use, and so on).\nItâ€™s tempting to visualize that relationship like this:\n\n\nCode\nggdag::dagify(\n  ac_use ~ global_temp,\n  global_temp ~ ac_use,\n  labels = \n    c(ac_use = \"A/C use\", global_temp = \"Global\\ntemperature\")\n) |&gt; \n  ggdag::ggdag(\n    layout = \"circle\", edge_type = \"arc\", text = FALSE, use_labels = \"label\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 19: A DAG representing the reciprocal relationship between A/C use and global temperature because of global warming. Feedback loops are useful mental shorthands to describe variables that impact each other over time compactly, but they are not true causal diagrams."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-5",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-5",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nOrder nodes by time - feedback loops\n\n\nFrom a DAG perspective, this is a problem because of the A part of DAG: itâ€™s cyclic!\nFeedback loops are a shorthand for what really happens, which is that the two variables mutually affect each other over time. Causality only goes forward in time, so it doesnâ€™t make sense to go back and forth like in FigureÂ 19.\nThe real DAG looks something like this:\n\n\nCode\nggdag::dagify(\n  global_temp_2000 ~ ac_use_1990 + global_temp_1990,\n  ac_use_2000 ~ ac_use_1990 + global_temp_1990,\n  global_temp_2010 ~ ac_use_2000 + global_temp_2000,\n  ac_use_2010 ~ ac_use_2000 + global_temp_2000,\n  global_temp_2020 ~ ac_use_2010 + global_temp_2010,\n  ac_use_2020 ~ ac_use_2010 + global_temp_2010,\n  coords = ggdag::time_ordered_coords(),\n  labels = c(\n    ac_use_1990 = \"A/C use\\n(1990)\", \n    global_temp_1990 = \"Global\\ntemperature\\n(1990)\",\n    ac_use_2000 = \"A/C use\\n(2000)\", \n    global_temp_2000 = \"Global\\ntemperature\\n(2000)\",\n    ac_use_2010 = \"A/C use\\n(2010)\", \n    global_temp_2010 = \"Global\\ntemperature\\n(2010)\",\n    ac_use_2020 = \"A/C use\\n(2020)\", \n    global_temp_2020 = \"Global\\ntemperature\\n(2020)\"\n  )\n) |&gt; \n  ggdag::ggdag(text = FALSE, use_labels = \"label\") + \n  ggdag::theme_dag()\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 20: A DAG showing the relationship between A/C use and global temperature over time. The true causal relationship in a feedback loop goes forward."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#dags---construction-6",
    "href": "slides/BSMM_8740_lec_07.html#dags---construction-6",
    "title": "Causality",
    "section": "DAGs - Construction",
    "text": "DAGs - Construction\nUse robustness checks\n\nFinally, check your DAG for robustness. It is unlikely the correctness of your DAG can be verified, but you can use the implications in your DAG to check the support for it. Some robustness checks:\n\nDAG-data consistency. There are many implications of your DAG. Because blocking a path removes statistical dependencies from that path, you can check those assumptions in several places in your DAG (see dagitty::impliedConditionalIndependencies).\nAlternate adjustment sets. Adjustment sets should give roughly the same answer because, outside of random and measurement errors, they are all sets that block backdoor paths. If more than one adjustment set seems reasonable, you can use that as a sensitivity analysis by checking multiple models.\n\nThe checks should be complementary to your initial DAG, not a way of replacing it. If you use more than one adjustment set during your analysis, you should report the results from all of them to avoid overfitting your results to your data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-steps",
    "href": "slides/BSMM_8740_lec_07.html#example-steps",
    "title": "Causality",
    "section": "Example: steps",
    "text": "Example: steps\nIn the remainder of todayâ€™s slides, weâ€™ll analyze simulated data using a few key steps\n\nSpecify a causal question\nDraw our assumptions using a causal diagram\nModel our assumptions\nDiagnose our models\nEstimate the causal effect\nConduct sensitivity analysis on the effect estimate"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-causal-question",
    "href": "slides/BSMM_8740_lec_07.html#example-causal-question",
    "title": "Causality",
    "section": "Example: causal question",
    "text": "Example: causal question\n\n\nresearchers are interested in whether using mosquito nets decreases an individualâ€™s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. The data is not experimentalâ€”researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-causal-question-1",
    "href": "slides/BSMM_8740_lec_07.html#example-causal-question-1",
    "title": "Causality",
    "section": "Example: causal question",
    "text": "Example: causal question\n\n\nid - an ID variable\nnet and net_num - a binary variable indicating if the participant used a net (1) or didnâ€™t use a net (0)\nmalaria_risk- risk of malaria scale ranging from 0-100\nincome - weekly income, measured in dollars\nhealth - a health score scale ranging from 0â€“100\nhousehold - number of people living in the household\neligible - a binary variable indicating if the household is eligible for the free net program.\ntemperature - the average temperature at night, in Celsius\nresistance - Insecticide resistance of local mosquitoes. A scale of 0â€“100, with higher values indicating higher resistance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-causal-question-2",
    "href": "slides/BSMM_8740_lec_07.html#example-causal-question-2",
    "title": "Causality",
    "section": "Example: causal question",
    "text": "Example: causal question\n\nThe distribution of malaria risk appears to be quite different by net usage.\n\n\n\n\n\n\n\n\nFigureÂ 21: A density plot of malaria risk for those who did and did not use nets. The average risk of malaria is lower for those who use nets."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-causal-question-3",
    "href": "slides/BSMM_8740_lec_07.html#example-causal-question-3",
    "title": "Causality",
    "section": "Example: causal question",
    "text": "Example: causal question\n\nIn FigureÂ 21, the density of those who used nets is to the left of those who did not use nets. The mean difference in malaria risk is about 16.4, suggesting net use might be protective against malaria.\n\n\nCode\ncausalworkshop::net_data |&gt;\n  dplyr::group_by(net) |&gt;\n  dplyr::summarize(malaria_risk = mean(malaria_risk))\n\n\n# A tibble: 2 Ã— 2\n  net   malaria_risk\n  &lt;lgl&gt;        &lt;dbl&gt;\n1 FALSE         43.9\n2 TRUE          27.5\n\n\nAnd thatâ€™s what we see with simple linear regression, as well, as we would expect.\n\n\nCode\ncausalworkshop::net_data |&gt;\n  lm(malaria_risk ~ net, data = _) |&gt;\n  broom::tidy()\n\n\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     43.9     0.377     116.  0       \n2 netTRUE        -16.4     0.741     -22.1 1.10e-95"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag",
    "href": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag",
    "title": "Causality",
    "section": "Example: assumptions using a DAG",
    "text": "Example: assumptions using a DAG\n\nThe problem that we face is that other factors may be responsible for the effect weâ€™re seeing.\nIn this example, weâ€™ll focus on confounding: a common cause of net usage and malaria will bias the effect we see unless we account for it somehow.\nTo determine which variables we need to account weâ€™ll use use a causal diagram or DAG, to visualize the assumptions that weâ€™re making about the causal relationships between the exposure, outcome, and other variables we think might be related.\nThe proposed DAG for this question is one the next slide."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag-1",
    "href": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag-1",
    "title": "Causality",
    "section": "Example: assumptions using a DAG",
    "text": "Example: assumptions using a DAG\n\n\n\n\n\n\n\n\n\nFigureÂ 22: A proposed causal diagram of the effect of bed net use on malaria. This directed acyclic graph (DAG) states our assumption that bed net use causes a reduction in malaria risk. It also says that we assume: malaria risk is impacted by net usage, income, health, temperature, and insecticide resistance; net usage is impacted by income, health, temperature, eligibility for the free net program, and the number of people in a household; eligibility for the free net programs is impacted by income and the number of people in a household; and health is impacted by income."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag-2",
    "href": "slides/BSMM_8740_lec_07.html#example-assumptions-using-a-dag-2",
    "title": "Causality",
    "section": "Example: assumptions using a DAG",
    "text": "Example: assumptions using a DAG\n\n\n\n\n\n\n\n\n\nFigureÂ 23: In the proposed DAG, there are eight open pathways that contribute to the causal effect seen in the naive regression: the true effect (in green) of net usage on malaria risk and seven other confounding pathways (in orange). The naive estimate is wrong because it is a composite of all these effects."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions",
    "href": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions",
    "title": "Causality",
    "section": "Example: Model our assumptions",
    "text": "Example: Model our assumptions\n\nFor this DAG, we need to control for three variables: health, income, and temperature. These three variables are a minimal adjustment set, the minimum set (or sets) of variables you need to block all confounding pathways.\nWe could estimate the causal effect using Regression Adjustment: including the minimal set of covariates in our regression.\nInstead weâ€™ll use a technique called Inverse Probability Weighting (IPW) to control for these variables:\nWeâ€™ll use logistic regression to predict the probability of treatmentâ€”the propensity score. Then, weâ€™ll calculate inverse probability weights to apply to the linear regression model we fit above.\nThe propensity score model includes the exposureâ€”net useâ€”as the dependent variable and the minimal adjustment set as the independent variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-1",
    "href": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-1",
    "title": "Causality",
    "section": "Example: Model our assumptions",
    "text": "Example: Model our assumptions\n\nThe propensity score model is a logistic regression model with the formula net ~ income + health + temperature, which predicts the probability of bed net usage based on the confounders income, health, and temperature.\n\n\nCode\npropensity_model &lt;- glm(\n  net ~ income + health + temperature,\n  data = causalworkshop::net_data,\n  family = binomial()\n)\n\n# the first six propensity scores\nhead(predict(propensity_model, type = \"response\"))\n\n\n     1      2      3      4      5      6 \n0.2464 0.2178 0.3230 0.2307 0.2789 0.3060 \n\n\nWe can use propensity scores to control for confounding in various ways. In this example, weâ€™ll focus on weighting.\nIn particular, weâ€™ll compute the inverse probability weight for the average treatment effect (ATE). The ATE represents a particular causal question: what if everyone in the study used bed nets vs.Â what if no one in the study used bed nets?\nTo calculate the ATE, weâ€™ll use the broom and propensity packages. broomâ€™s augment() function extracts prediction-related information from the model and joins it to the data. propensityâ€™s wt_ate() function calculates the inverse probability weight given the propensity score and exposure."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-2",
    "href": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-2",
    "title": "Causality",
    "section": "Example: Model our assumptions",
    "text": "Example: Model our assumptions\n\nFor inverse probability weighting, the ATE weight is the probability of receiving the treatment you actually received. In other words, if you used a bed net, the ATE weight is the probability that you used a net, and if you did not use a net, it is the probability that you did not use a net.\n\n\nCode\nnet_data_wts &lt;- propensity_model |&gt;\n  broom::augment(newdata = causalworkshop::net_data, type.predict = \"response\") |&gt;\n  # .fitted is the value predicted by the model\n  # for a given observation\n  dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\nnet_data_wts |&gt;\n  dplyr::select(net, .fitted, wts) |&gt;\n  dplyr::slice_head(n=16)\n\n\n# A tibble: 16 Ã— 3\n   net   .fitted   wts\n   &lt;lgl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 FALSE   0.246  1.33\n 2 FALSE   0.218  1.28\n 3 FALSE   0.323  1.48\n 4 FALSE   0.231  1.30\n 5 FALSE   0.279  1.39\n 6 FALSE   0.306  1.44\n 7 FALSE   0.332  1.50\n 8 FALSE   0.168  1.20\n 9 FALSE   0.222  1.29\n10 FALSE   0.255  1.34\n11 FALSE   0.215  1.27\n12 FALSE   0.220  1.28\n13 FALSE   0.195  1.24\n14 FALSE   0.167  1.20\n15 FALSE   0.200  1.25\n16 TRUE    0.413  2.42"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-3",
    "href": "slides/BSMM_8740_lec_07.html#example-model-our-assumptions-3",
    "title": "Causality",
    "section": "Example: Model our assumptions",
    "text": "Example: Model our assumptions\n\nwts represents the amount each observation will be up-weighted or down-weighted in the outcome model we will fit.\nFor instance, the 16th household used a bed net and had a predicted probability of 0.41. Thatâ€™s a pretty low probability considering they did, in fact, use a net, so their weight is higher at 2.42. In other words, this household will be up-weighted compared to the naive linear model we fit above.\nThe first household did not use a bed net; theyâ€™re predicted probability of net use was 0.25 (or put differently, a predicted probability of not using a net of 0.75). Thatâ€™s more in line with their observed value of net, but thereâ€™s still some predicted probability of using a net, so their weight is 1.28."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models",
    "href": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models",
    "title": "Causality",
    "section": "Example: Diagnose our models",
    "text": "Example: Diagnose our models\n\n\nThe goal of propensity score weighting is to weight the population of observations such that the distribution of confounders is balanced between the exposure groups.\nIn principle, weâ€™re removing the arrows between the confounders and exposure in the DAG, so that the confounding paths no longer distort our estimates.\n\n\n\nCode\nlibrary(halfmoon)\nggplot(net_data_wts, aes(.fitted)) +\n  halfmoon::geom_mirror_histogram(\n    aes(fill = net),\n    bins = 50\n  ) +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\")\n\n\n\n\n\n\n\n\nFigureÂ 24: A mirrored histogram of the propensity scores of those who used nets (top, blue) versus those who who did not use nets (bottom, orange). The range of propensity scores is similar between groups, with those who used nets slightly to the left of those who didnâ€™t, but the shapes of the distribution are different."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-1",
    "href": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-1",
    "title": "Causality",
    "section": "Example: Diagnose our models",
    "text": "Example: Diagnose our models\n\n\nThe weighted propensity score creates a pseudo-population where the distributions are much more similar:\n\n\nCode\nggplot(net_data_wts, aes(.fitted)) +\n  halfmoon::geom_mirror_histogram(\n    aes(group = net),\n    bins = 50\n  ) +\n  halfmoon::geom_mirror_histogram(\n    aes(fill = net, weight = wts),\n    bins = 50,\n    alpha = .5\n  ) +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\")\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 25: A mirrored histogram of the propensity scores of those who used nets (top, blue) versus those who who did not use nets (bottom, orange). The shaded region represents the unweighted distribution, and the colored region represents the weighted distributions. The ATE weights up-weight the groups to be similar in range and shape of the distribution of propensity scores."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-2",
    "href": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-2",
    "title": "Causality",
    "section": "Example: Diagnose our models",
    "text": "Example: Diagnose our models\n\n\nWe might also want to know how well-balanced the groups are by each confounder. One way to do this is to calculate the standardized mean differences (SMDs) for each confounder with and without weights. Weâ€™ll calculate the SMDs with tidy_smd() then plot them with geom_love() (see SMD).\n\n\nCode\nplot_df &lt;- tidysmd::tidy_smd(\n  net_data_wts,\n  c(income, health, temperature),\n  .group = net,\n  .wts = wts\n)\n\nplot_df %&gt;% ggplot(\n  aes(x = abs(smd), y = variable,\n    group = method, color = method\n  )\n) + tidysmd::geom_love()\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 26: A love plot representing the standardized mean differences (SMD) between exposure groups of three confounders: temperature, income, and health. Before weighting, there are considerable differences in the groups. After weighting, the confounders are much more balanced between groups."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-3",
    "href": "slides/BSMM_8740_lec_07.html#example-diagnose-our-models-3",
    "title": "Causality",
    "section": "Example: Diagnose our models",
    "text": "Example: Diagnose our models\n\n\nBefore we apply the weights to the outcome model, letâ€™s check their overall distribution for extreme weights.\nExtreme weights can destabilize the estimate and variance in the outcome model, so we want to be aware of it.\n\n\nCode\nnet_data_wts |&gt;\n  ggplot(aes(wts)) +\n  geom_density(\n    fill = \"#CC79A7\"\n    , color = NA\n    , alpha = 0.8\n  )\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 27: A density plot of the average treatment effect (ATE) weights. The plot is skewed, with higher values towards 8. This may indicate a problem with the model, but the weights arenâ€™t so extreme to destabilize the variance of the estimate."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nWeâ€™re now ready to use the ATE weights to (attempt to) account for confounding in the naive linear regression model. Fitting such a model is pleasantly simple in this case: we fit the same model as before but with weights = wts, which will incorporate the inverse probability weights.\n\n\nCode\nnet_data_wts |&gt;\n  lm(malaria_risk ~ net, data = _, weights = wts) |&gt;\n  broom::tidy(conf.int = TRUE)\n\n\n# A tibble: 2 Ã— 7\n  term   estimate std.error statistic  p.value conf.low\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 (Inteâ€¦     42.7     0.442      96.7 0            41.9\n2 netTRâ€¦    -12.5     0.624     -20.1 5.50e-81    -13.8\n# â„¹ 1 more variable: conf.high &lt;dbl&gt;\n\n\n\n\nCode\nestimates &lt;- net_data_wts |&gt;\n  lm(malaria_risk ~ net, data = _, weights = wts) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  dplyr::filter(term == \"netTRUE\") |&gt;\n  dplyr::select(estimate, starts_with(\"conf\")) |&gt;\n  dplyr::mutate( \n    dplyr::across( everything(), \\(x)round(x, digits = 1) ) \n  )\nestimates\n\n\n# A tibble: 1 Ã— 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    -12.5    -13.8     -11.3\n\n\nThe estimate for the average treatment effect is -12.5 (95% CI -13.8, -11.3)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-1",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-1",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nUnfortunately, the confidence intervals donâ€™t account for the dependence within the weights! Generally, confidence intervals for propensity score weighted models will be too narrow unless we correct for this dependence.\nThe nominal coverage of the confidence intervals will thus be wrong (they arenâ€™t 95% CIs because their coverage is much lower than 95%) and may lead to misinterpretation.\nFor this example, weâ€™ll use the bootstrap, a flexible tool that calculates distributions of parameters using re-sampling. Weâ€™ll use the rsample package from tidymodels to work with bootstrap samples."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-2",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-2",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nBecause the the inverse probability weights are not fixed values (they depend on the data), we need to account for this by bootstrapping the entire modeling process.\nFor every bootstrap sample, we need to fit the propensity score model, calculate the inverse probability weights, then fit the weighted outcome model, using the following function:\n\n\nCode\nfit_ipw &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  .df &lt;- rsample::analysis(split)\n\n  # fit propensity score model\n  propensity_model &lt;- glm(\n    net ~ income + health + temperature,\n    data = .df,\n    family = binomial()\n  )\n\n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\n  # fit correctly bootstrapped ipw model\n  lm(malaria_risk ~ net, data = .df, weights = wts) |&gt;\n    broom::tidy()\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-3",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-3",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nWe generate a distribution of estimates as follows:\n\n\nbootstrap - ipw - estimate\n# create bootstrap samples\nbootstrapped_net_data &lt;- rsample::bootstraps(\n  causalworkshop::net_data,\n  times = 1000,\n  # required to calculate CIs later\n  apparent = TRUE\n)\n\n# create ipw and fit each bootstrap sample\nipw_results &lt;- bootstrapped_net_data |&gt;\n  dplyr::mutate(\n    boot_fits = purrr::map(splits, fit_ipw))\n\n\n\n\ndistribution\nipw_results |&gt;\n  mutate(\n    estimate = map_dbl(\n      boot_fits,\n      # pull the `estimate` for `netTRUE` for each fit\n      \\(.fit) .fit |&gt;\n        filter(term == \"netTRUE\") |&gt;\n        pull(estimate)\n    )\n  ) |&gt;\n  ggplot(aes(estimate)) +\n  geom_histogram(fill = \"#D55E00FF\", color = \"white\", alpha = 0.8)\n\n\n\n\n\n\n\n\nFigureÂ 28: â€œA histogram of 1,000 bootstrapped estimates of the effect of net use on malaria risk. The spread of these estimates accounts for the dependency and uncertainty in the use of IPW weights.â€"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-4",
    "href": "slides/BSMM_8740_lec_07.html#example-estimate-the-causal-effect-4",
    "title": "Causality",
    "section": "Example: Estimate the causal effect",
    "text": "Example: Estimate the causal effect\n\nFigureÂ 28 gives a sense of the variation in estimate, but letâ€™s calculate 95% confidence intervals from the bootstrapped distribution using rsampleâ€™s int_t() :\n\n\nCode\nboot_estimate &lt;- ipw_results |&gt;\n  # calculate T-statistic-based CIs\n  rsample::int_t(boot_fits) |&gt;\n  dplyr::filter(term == \"netTRUE\")\n\nboot_estimate\n\n\n# A tibble: 1 Ã— 6\n  term    .lower .estimate .upper .alpha .method  \n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n1 netTRUE  -13.4     -12.5  -11.6   0.05 student-t\n\n\nNow we have a confounder-adjusted estimate with correct standard errors. The estimate of the effect of all households using bed nets versus no households using bed nets on malaria risk is -12.5 (95% CI -13.4, -11.6).\nBed nets do indeed seem to reduce malaria risk in this study."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis",
    "href": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis",
    "title": "Causality",
    "section": "Example: Conduct sensitivity analysis",
    "text": "Example: Conduct sensitivity analysis\n\nWhen conducting a causal analysis, itâ€™s a good idea to use sensitivity analyses to test your assumptions. There are many potential sources of bias in any study and many sensitivity analyses to go along with them; weâ€™ll focus on the assumption of no confounding.\nWhen we have less information about unmeasured confounders, we can use tipping point analysis to ask how much confounding it would take to tip my estimate to the null. In other words, what would the strength of the unmeasured confounder have to be to explain our results away? The tipr package is a toolkit for conducting sensitivity analyses."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis-1",
    "href": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis-1",
    "title": "Causality",
    "section": "Example: Conduct sensitivity analysis",
    "text": "Example: Conduct sensitivity analysis\n\nAssume an unknown, normally-distributed confounder.\nThe tip_coef() function takes an estimate (e.g.Â the upper or lower bound of the coefficient) and further requires either the - the scaled differences in means of the confounder between exposure groups or - the effect of the confounder on the outcome.\nFor the estimate, weâ€™ll use conf.high, which is closer to 0 (the null), and ask: how much would the confounder have to affect malaria risk to have an unbiased upper confidence interval of 0?\nWeâ€™ll use tipr to calculate this answer for 5 scenarios, where the mean difference in the confounder between exposure groups is 1, 2, 3, 4, or 5."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis-2",
    "href": "slides/BSMM_8740_lec_07.html#example-conduct-sensitivity-analysis-2",
    "title": "Causality",
    "section": "Example: Conduct sensitivity analysis",
    "text": "Example: Conduct sensitivity analysis\n\n\n\nCode\ntipping_points &lt;- tipr::tip_coef(boot_estimate$.upper, exposure_confounder_effect = 1:5)\n\ntipping_points |&gt;\n  ggplot(aes(confounder_outcome_effect, exposure_confounder_effect)) +\n  geom_line(color = \"#009E73\", linewidth = 1.1) +\n  geom_point(fill = \"#009E73\", color = \"white\", size = 2.5, shape = 21) +\n  labs(\n    x = \"Confounder-Outcome Effect\",\n    y = \"Scaled mean differences in\\n confounder between exposure groups\"\n  )\n\n\n\n\n\n\n\n\nFigureÂ 29: A tipping point analysis under several confounding scenarios where the unmeasured confounder is a normally-distributed continuous variable. The line represents the strength of confounding necessary to tip the upper confidence interval of the causal effect estimate to 0. The x-axis represents the coefficient of the confounder-outcome relationship adjusted for the exposure and the set of measured confounders. The y-axis represents the scaled mean difference of the confounder between exposure groups."
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#more",
    "href": "slides/BSMM_8740_lec_07.html#more",
    "title": "Causality",
    "section": "More",
    "text": "More\n\nRead Statistical tools for causal inference\nRead Causal inference in R"
  },
  {
    "objectID": "slides/BSMM_8740_lec_07.html#recap",
    "href": "slides/BSMM_8740_lec_07.html#recap",
    "title": "Causality",
    "section": "Recap",
    "text": "Recap\n\nWe introduced the fundamental problems of inference and the biases of some intuitive estimators,\nWe developed a basic understanding of the tools used to state and then satisfy causality assumptions,\nWe looked at an example of how econometric methods recover treatment effects.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_04.html#recap-of-last-week",
    "title": "The Tidymodels Framework",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we looked at several regression models that are useful for predictions.\nHowever, R packages that implement these models have different conventions on how they accept data and specify the model.\nToday we look at the tidymodels package which will give us a workflow to describe, fit and compare models, using the same approach across methods."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\n\nTidymodels is a collection of R packages that provides a unified and consistent framework for modeling and machine learning tasks.\nIt is built on top of the tidyverse, making it easy to integrate with other tidyverse packages.\nTidymodels promotes best practices, repeatability, and clear documentation in your data analysis and modeling workflow."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-2",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-2",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nKey Components of Tidymodels\n\nModel Building: the parsnip package provides various modeling engines for different algorithms like lm(), glm(), randomForest(), xgboost(), etc.\nPreprocessing: Easy and flexible data preprocessing using the recipes package, allowing for seamless data transformation and feature engineering."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-3",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-3",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nKey Components of Tidymodels\n\nResampling: The rsample package supplies efficient methods for handling data splitting, cross-validation, bootstrapping, and more.\nMetrics: the yardstick package gives a wide range of evaluation metrics to assess model performance and choose the best model.\nTuning: the tune package facilitates hyperparameter tuning for the tidymodels packages."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-4",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-4",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nKey Components of Tidymodels\n\n\nWorkflows: functions in the workflows package can bundle together your pre-processing, modeling, and post-processing requests. The advantages are:\n\nYou donâ€™t have to keep track of separate objects in your workspace.\nThe recipe prepping and model fitting can be executed using a single call to fit().\nIf you have custom tuning parameter settings, these can be defined using a simpler interface when combined with tune.\n\nWorkflowsets: The workflowsets package facilitates multiple workwflows - applying different types of models and preprocessing methods on a given data set."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-5",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-5",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nIn base R, the predict function returns results in a format that depends on the models.\nBy contrast, parsnip and workflows conforms to the following rules:\n\nThe results are always a tibble.\nThe column names of the tibble are always predictable.\nThere are always as many rows in the result tibble as there are in the input data set, and in the same order."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-6",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-6",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-parsnip",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-parsnip",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: parsnip",
    "text": "Tidymodels: parsnip\nWeâ€™ve seen how the form of the arguments to linear models in R can be very different1.\nParsnip is one of the tidymodels packages that provides a standardized interface across models.\nWe look at how to fit and predict with parsnip in the next few slides, given data that has been preprocessed.\ne.g.Â stats::lm takes a formula, while glmnet::glmnet takes separate outcome and co-variate matrices"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nFor example we call stats::lm, specifying the model using a formula. By contrast glmnet::glmnet specify the model with separate outcome and co-variate matrices\nBy contrast, the tidymodels permits using both models with a uniform model specification."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-1",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-1",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\n\nSpecify theÂ typeÂ of model based on its algorithmÂ (e.g., linear regression, random forest, KNN, etc).\nSpecify theÂ engineÂ for fitting the model.Â Most often this reflects the software package and function that should be used, like lm orÂ glmnet.\nWhen required, declare theÂ modeÂ of the model.Â The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-2",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-2",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nWith parsnip specifications are built without referencing the data:\n\nlmglmnet\n\n\n\n&gt; # basic linear model\n&gt; parsnip::linear_reg() %&gt;% \n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n\n&gt; # basic penalized linear model\n&gt; parsnip::linear_reg() %&gt;% \n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::set_engine(\"glmnet\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-3",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-3",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nThe translate function can be used to see how the parsnip spec is converted to the correct syntax for the underlying package / functions.\n\nlmglmnet\n\n\n\n\n&gt; parsnip::linear_reg() %&gt;% \n+   parsnip::set_engine(\"lm\") %&gt;% \n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\n\n\n\n\n\n&gt; parsnip::linear_reg(penalty = 1) %&gt;% \n+   parsnip::set_engine(\"glmnet\") %&gt;% \n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    family = \"gaussian\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-4",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-4",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nWe can specify the model with either a formula or outcome and model matrix:\n\n&gt; # prep data\n&gt; data_split &lt;- rsample::initial_split(modeldata::ames, strata = \"Sale_Price\")\n&gt; ames_train &lt;- rsample::training(data_split)\n&gt; ames_test  &lt;- rsample::testing(data_split)\n&gt; # spec model\n&gt; lm_model &lt;- parsnip::linear_reg() %&gt;%\n+   parsnip::set_mode(\"regression\") %&gt;%\n+   parsnip::set_engine(\"lm\")\n&gt; # fit model\n&gt; lm_form_fit &lt;- lm_model %&gt;% \n+   # Recall that Sale_Price has been pre-logged\n+    parsnip::fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n&gt; # fit model with data in (x,y) form\n&gt; lm_xy_fit &lt;- \n+   lm_model %&gt;% parsnip::fit_xy(\n+     x = ames_train %&gt;% dplyr::select(Longitude, Latitude),\n+     y = ames_train %&gt;% dplyr::pull(Sale_Price)\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-5",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-5",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nModel results can be extracted from the fit object\n\n&gt; lm_form_fit %&gt;% parsnip::extract_fit_engine()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n -124610640      -765904      1262537  \n\n&gt; lm_form_fit %&gt;% parsnip::extract_fit_engine() %&gt;% stats::vcov()\n\n              (Intercept)    Longitude      Latitude\n(Intercept)  4.712249e+13 359273381968 -320668568897\nLongitude    3.592734e+11   3762633052    -164863649\nLatitude    -3.206686e+11   -164863649    7261431741"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-6",
    "href": "slides/BSMM_8740_lec_04.html#fitting-with-parsnip-6",
    "title": "The Tidymodels Framework",
    "section": "Fitting with parsnip",
    "text": "Fitting with parsnip\nA list of all parsnip-type models can be found here."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflows",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflows",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflows",
    "text": "Tidymodels: workflows\nThe model workflow collects all the steps of the analysis, including any pre-processing steps, the specification of the model, the model fit itself, as well as potential post-processing activities.\nSimilar collections of steps are sometimes called pipelines."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-example",
    "href": "slides/BSMM_8740_lec_04.html#workflows-example",
    "title": "The Tidymodels Framework",
    "section": "Workflows example",
    "text": "Workflows example\nWorkflows always require a parsnip model object\n\n&gt; # create test/train splits\n&gt; ames &lt;- modeldata::ames %&gt;% dplyr::mutate( Sale_Price = log10(Sale_Price) )\n&gt; \n&gt; set.seed(502)\n&gt; ames_split &lt;- \n+   rsample::initial_split(\n+     ames, prop = 0.80, strata = \"Sale_Price\"\n+   )\n&gt; ames_train &lt;- rsample::training(ames_split)\n&gt; ames_test  &lt;- rsample::testing(ames_split)\n&gt; \n&gt; # Create a linear regression model\n&gt; lm_model &lt;- parsnip::linear_reg() %&gt;% \n+   parsnip::set_engine(\"lm\") \n&gt; \n&gt; # Create a workflow: adding a parsnip model\n&gt; lm_wflow &lt;- \n+   workflows::workflow() %&gt;% \n+   workflows::add_model(lm_model)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-example-1",
    "href": "slides/BSMM_8740_lec_04.html#workflows-example-1",
    "title": "The Tidymodels Framework",
    "section": "Workflows example",
    "text": "Workflows example\nIf our model is very simple, a standard R formula can be used as a preprocessor:\n\n\nCode\n&gt; # preprocessing not specified; a formula is sufficient\n&gt; lm_wflow %&lt;&gt;% \n+   workflows::add_formula(Sale_Price ~ Longitude + Latitude)\n&gt; # fit the model ( can be written as fit(lm_wflow, ames_train) )\n&gt; lm_fit &lt;- lm_wflow %&gt;% parsnip::fit(ames_train)\n&gt; # tidy up the fitted coefficients\n&gt; lm_fit %&gt;%\n+   # pull the parsnip object\n+   workflows::extract_fit_parsnip() %&gt;% \n+   # tidy up the fit results\n+   broom::tidy() %&gt;% \n+   # show the first n rows\n+   dplyr::slice_head(n=3)\n\n\n# A tibble: 3 Ã— 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -303.      14.4       -21.0 3.64e-90\n2 Longitude      -2.07     0.129     -16.1 1.40e-55\n3 Latitude        2.71     0.180      15.0 9.29e-49"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-prediction",
    "href": "slides/BSMM_8740_lec_04.html#workflows-prediction",
    "title": "The Tidymodels Framework",
    "section": "Workflows prediction",
    "text": "Workflows prediction\nWhen using predict(workflow, new_data), no model or preprocessor parameters like those from recipes are re-estimated using the values in new_data.\nTake centering and scaling using step_normalize() as an example.\nUsing this step, the means and standard deviations from the appropriate columns are determined from the training set; new samples at prediction time are standardized using these values from training when predict() is invoked."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-prediction-1",
    "href": "slides/BSMM_8740_lec_04.html#workflows-prediction-1",
    "title": "The Tidymodels Framework",
    "section": "Workflows prediction",
    "text": "Workflows prediction\nThe fitted workflow can be used to predict outcomes given a new dataset of co-variates. Alternatively, the parsnip::augment function can be used to augment the new data with the prediction and other information about the prediction.\n\npredictaugment\n\n\n\n&gt; # predict on the fitted workflow\n&gt; lm_fit %&gt;% stats::predict(ames_test %&gt;% dplyr::slice(1:3))\n\n# A tibble: 3 Ã— 1\n  .pred\n  &lt;dbl&gt;\n1  5.22\n2  5.21\n3  5.28\n\n\n\n\n\n&gt; lm_fit |&gt; \n+   parsnip::augment(new_data = ames_test %&gt;% dplyr::slice(1:3)) |&gt; \n+   dplyr::select(starts_with(\".\"))\n\n# A tibble: 3 Ã— 2\n  .pred   .resid\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  5.22 -0.202  \n2  5.21  0.174  \n3  5.28 -0.00629"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-updating",
    "href": "slides/BSMM_8740_lec_04.html#workflows-updating",
    "title": "The Tidymodels Framework",
    "section": "Workflows updating",
    "text": "Workflows updating\nThe model and data pre-processor can be removed or updated:\n\n&gt; # remove the formula and use add_variables instead\n&gt; lm_wflow %&lt;&gt;% \n+   workflows::remove_formula() %&gt;% \n+   workflows::add_variables(\n+     outcome = Sale_Price, predictors = c(Longitude, Latitude)\n+   )\n\nPredictors can be selected using tidyselect selectors, e.g.Â everything(), ends_with(â€œtudeâ€), etc."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-use-of-formulas",
    "href": "slides/BSMM_8740_lec_04.html#workflows-use-of-formulas",
    "title": "The Tidymodels Framework",
    "section": "Workflows use of formulas",
    "text": "Workflows use of formulas\nWeâ€™ve noted that R formulas can specify a good deal of preprocessing, including inline transformations and creating dummy variables, interactions and other column expansions. But some R packages extend the formula in ways that base R functions cannot parse or execute.\nWhen add_formula is executed, since preprocessing is model dependent, workflows attempts to emulate what the underlying model would do whenever possible."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-use-of-formulas-1",
    "href": "slides/BSMM_8740_lec_04.html#workflows-use-of-formulas-1",
    "title": "The Tidymodels Framework",
    "section": "Workflows use of formulas",
    "text": "Workflows use of formulas\nIf a random forest model is fit using the ranger or randomForest packages, the workflow knows predictor columns that are factors should be left as is (not converted to dummy vaiables).\nBy contrast, a boosted tree created with the xgboost package requires the user to create dummy variables from factor predictors (since xgboost::xgb.train() will not). A workflow using xgboost will create the indicator columns for this engine. Also note that a different engine for boosted trees, C5.0, does not require dummy variables so none are made by the workflow."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-special-formulas",
    "href": "slides/BSMM_8740_lec_04.html#workflows-special-formulas",
    "title": "The Tidymodels Framework",
    "section": "Workflows: special formulas",
    "text": "Workflows: special formulas\nSome packages have specilized formula specification, i.e.Â the lme4 package allows random effects per\nlme4::lmer(distance ~ Sex + (age | Subject), data = Orthodont)\nThe effect of this is that each subject will have an estimated intercept and slope parameter for age. Standard R methods canâ€™t properly process this formula."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#workflows-special-formulas-1",
    "href": "slides/BSMM_8740_lec_04.html#workflows-special-formulas-1",
    "title": "The Tidymodels Framework",
    "section": "Workflows: special formulas",
    "text": "Workflows: special formulas\nIn this case The add_variables() specification provides the bare column names, and then the actual formula given to the model is set within add_model():\n\n&gt; multilevel_spec &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(\"lmer\")\n&gt; \n&gt; multilevel_workflow &lt;- \n+   workflows::workflow() %&gt;% \n+   # Pass the data along as-is: \n+   workflows::add_variables(outcome = distance, predictors = c(Sex, age, Subject)) %&gt;% \n+   workflows::add_model(multilevel_spec, \n+             # This formula is given to the model\n+             formula = distance ~ Sex + (age | Subject))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflowsets",
    "text": "Tidymodels: workflowsets\nCreating multiple workflows at once\nThe workflowset package creates combinations of workflow components.\nA list of preprocessors (e.g., formulas, dplyr selectors, or feature engineering recipe objects) can be combined (i.e.Â a crossproduct) with a list of model specifications, resulting in a set of workflows."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflowsets",
    "text": "Tidymodels: workflowsets\nCreate a set of preprocessors by formula\n\n&gt; # set up a list of formulas\n&gt; location &lt;- list(\n+   longitude = Sale_Price ~ Longitude,\n+   latitude = Sale_Price ~ Latitude,\n+   coords = Sale_Price ~ Longitude + Latitude,\n+   neighborhood = Sale_Price ~ Neighborhood\n+ )\n&gt; \n&gt; # create a workflowset\n&gt; location_models &lt;- \n+   workflowsets::workflow_set(\n+     preproc = location, models = list(lm = lm_model)\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-2",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-2",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflowsets",
    "text": "Tidymodels: workflowsets\nA workflow set is a data structure\n\n&gt; # view\n&gt; location_models\n\n# A workflow set/tibble: 4 Ã— 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 longitude_lm    &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 latitude_lm     &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 coords_lm       &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 neighborhood_lm &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nYou can extract the elements of the workflowset using tidy::unnest, dplyr::filter, etc. Alternatively the are a number of workflowsets::extract_X functions that will do the job, e.g.\nworkflowsets::extract_workflow(location_models, id = \"coords_lm\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-3",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-workflowsets-3",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: workflowsets",
    "text": "Tidymodels: workflowsets\nCreate model fits\n\n&gt; # create a new column (fit) by mapping fit \n&gt; # against the data in the info column\n&gt; location_models %&lt;&gt;%\n+    dplyr::mutate(\n+      fit = purrr::map(\n+        info\n+        , ~ parsnip::fit(.x$workflow[[1]], ames_train)\n+       )\n+    )\n&gt; \n&gt; # view\n&gt; location_models$fit[[1]] |&gt; broom::tidy()\n\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -184.      12.6       -14.6 1.97e-46\n2 Longitude      -2.02     0.135     -15.0 7.01e-49"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-evaluation",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-evaluation",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: evaluation",
    "text": "Tidymodels: evaluation\n\nOnce weâ€™ve settled on a final model there is a convenience function calledÂ last_fit()Â that will fit the model to the entire training set and evaluate it with the testing set. Notice that last_fit() takes a data split as an input, not a dataframe.\n\n&gt; # pull \n&gt; final_lm_res &lt;- tune::last_fit(lm_wflow, ames_split)\n\n\nmetricspredictions\n\n\n\n&gt; final_lm_res %&gt;% workflowsets::collect_metrics() \n\n# A tibble: 2 Ã— 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.164 Preprocessor1_Model1\n2 rsq     standard       0.189 Preprocessor1_Model1\n\n\n\n\n\n&gt; workflowsets::collect_predictions(final_lm_res) %&gt;% dplyr::slice(1:3)\n\n# A tibble: 3 Ã— 5\n  .pred id                .row Sale_Price .config             \n  &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n1  5.22 train/test split     2       5.02 Preprocessor1_Model1\n2  5.21 train/test split     4       5.39 Preprocessor1_Model1\n3  5.28 train/test split     5       5.28 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nPerformance metrics and inference\nAn inferential model is used primarily to understand relationships, and typically emphasizes the choice (and validity) of probabilistic distributions and other generative qualities that define the model.\nFor a model used primarily for prediction, by contrast, predictive strength is of primary importance and other concerns about underlying statistical qualities may be less important."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nThe point of this analysis is to demonstrate the idea that optimization of statistical characteristics of the model does not imply that the model fits the data well.\nEven for purely inferential models, some measure of fidelity to the data should accompany the inferential results. With empirical validation, the users of the analyses can calibrate their expectations of the results."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-2",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-2",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\n\n\nCode\n&gt; ames &lt;- dplyr::mutate(modeldata::ames, Sale_Price = log10(Sale_Price))\n&gt; \n&gt; set.seed(502)\n&gt; ames_split &lt;- rsample::initial_split(ames, prop = 0.80, strata = Sale_Price)\n&gt; ames_train &lt;- rsample::training(ames_split)\n&gt; ames_test  &lt;- rsample::testing(ames_split)\n&gt; \n&gt; ames_rec &lt;- \n+   recipes::recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n+            Latitude + Longitude, data = ames_train) %&gt;%\n+   recipes::step_log(Gr_Liv_Area, base = 10) %&gt;% \n+   recipes::step_other(Neighborhood, threshold = 0.01) %&gt;% \n+   recipes::step_dummy(\n+     recipes::all_nominal_predictors()\n+   ) %&gt;% \n+   recipes::step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n+   recipes::step_ns(Latitude, Longitude, deg_free = 20)\n&gt;   \n&gt; lm_model &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(\"lm\")\n&gt; \n&gt; lm_wflow &lt;- \n+   workflows::workflow() %&gt;% \n+   workflows::add_model(lm_model) %&gt;% \n+   workflows::add_recipe(ames_rec)\n&gt; \n&gt; lm_fit &lt;- parsnip::fit(lm_wflow, ames_train)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-3",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-3",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics\n\nCode\n&gt; # fit with new data\n&gt; ames_test_res &lt;- \n+   stats::predict(\n+     lm_fit\n+     , new_data = ames_test %&gt;% dplyr::select(-Sale_Price)\n+   )\n&gt; ames_test_res\n&gt; # compare predictions with corresponding data\n&gt; ames_test_res &lt;- \n+   dplyr::bind_cols(\n+     ames_test_res, ames_test %&gt;% dplyr::select(Sale_Price))\n&gt; ames_test_res\n&gt; # there is a standard output format for yardstick functions\n&gt; yardstick::rmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n\n\n\n\n# A tibble: 588 Ã— 1\n   .pred\n   &lt;dbl&gt;\n 1  5.22\n 2  5.21\n 3  5.28\n 4  5.27\n 5  5.28\n 6  5.28\n 7  5.26\n 8  5.26\n 9  5.26\n10  5.24\n# â„¹ 578 more rows\n\n\n# A tibble: 588 Ã— 2\n   .pred Sale_Price\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1  5.22       5.02\n 2  5.21       5.39\n 3  5.28       5.28\n 4  5.27       5.28\n 5  5.28       5.28\n 6  5.28       5.26\n 7  5.26       5.73\n 8  5.26       5.60\n 9  5.26       5.32\n10  5.24       4.98\n# â„¹ 578 more rows\n\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.164"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-4",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-4",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multiple metrics at once\n\n&gt; # NOTE: yardstick::metric_set returns a function\n&gt; ames_metrics &lt;- \n+   yardstick::metric_set(\n+     yardstick::rmse\n+     , yardstick::rsq\n+     , yardstick::mae\n+   )\n&gt; \n&gt; ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.164\n2 rsq     standard       0.189\n3 mae     standard       0.124"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-5",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-5",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nClassification metrics: binary class targets\n\nconfusion mataccuracymccF-metric\n\n\n\n&gt; # compute the confusion matrix: \n&gt; yardstick::conf_mat(\n+   modeldata::two_class_example, truth = truth, estimate = predicted\n+ )\n\n          Truth\nPrediction Class1 Class2\n    Class1    227     50\n    Class2     31    192\n\n\n\n\n\n&gt; # compute the accuracy:\n&gt; yardstick::accuracy(\n+   modeldata::two_class_example, truth, predicted\n+ )\n\n# A tibble: 1 Ã— 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838\n\n\n\n\n\n&gt; # Matthews correlation coefficient:\n&gt; yardstick::mcc(\n+   modeldata::two_class_example, truth, predicted\n+ )\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mcc     binary         0.677\n\n\n\n\n\n&gt; # F1 metric: (The measure \"F\" is a combination of precision and recall )  \n&gt; yardstick::f_meas(modeldata::two_class_example, truth, predicted)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  binary         0.849"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-6",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-6",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nClassification metrics: binary class targets\n\nCode\n&gt; # Combining three classification metrics together\n&gt; classification_metrics &lt;- \n+   yardstick::metric_set(\n+     yardstick::accuracy\n+     , yardstick::mcc\n+     , yardstick::f_meas\n+   )\n&gt; \n&gt; classification_metrics(\n+   modeldata::two_class_example\n+   , truth = truth\n+   , estimate = predicted\n+ )\n\n\n\n\n# A tibble: 3 Ã— 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838\n2 mcc      binary         0.677\n3 f_meas   binary         0.849"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-7",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-7",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nClassification metrics: class probabilities\n\n\nCode\n&gt; two_class_curve &lt;- \n+   yardstick::roc_curve(\n+     modeldata::two_class_example\n+     , truth\n+     , Class1\n+   )\n&gt; \n&gt; parsnip::autoplot(two_class_curve) +\n+   labs(\n+     title = stringr::str_glue(\"roc_auc = {round(yardstick::roc_auc(modeldata::two_class_example, truth, Class1)[1,3],4)}\") \n+     , subtitle = \"There are other functions that use probability estimates, including gain_curve, lift_curve, and pr_curve.\"\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-8",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-8",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\n\nThe functions for metrics that use the discrete class predictions are identical to their binary counterparts.\nMetrics designed to handle outcomes with only two classes are extended for outcomes with more than two classes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-9",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-9",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\nTake sensitivity for example:\n\n\nMacro-averaging computes a set of one-versus-all metrics using the standard two-class statistics. These are averaged.\nMacro-weighted averaging does the same but the average is weighted by the number of samples in each class.\nMicro-averaging computes the contribution for each class, aggregates them, then computes a single metric from the aggregates."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-10",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-10",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\n\n&gt; modeldata::hpc_cv\n\n     obs pred           VF            F            M            L Resample\n1     VF   VF 9.136340e-01 7.786694e-02 8.479147e-03 1.991225e-05   Fold01\n2     VF   VF 9.380672e-01 5.710623e-02 4.816447e-03 1.011557e-05   Fold01\n3     VF   VF 9.473710e-01 4.946767e-02 3.156287e-03 4.999849e-06   Fold01\n4     VF   VF 9.289077e-01 6.528949e-02 5.787179e-03 1.564496e-05   Fold01\n5     VF   VF 9.418764e-01 5.430830e-02 3.808013e-03 7.294581e-06   Fold01\n6     VF   VF 9.510978e-01 4.618223e-02 2.716177e-03 3.841455e-06   Fold01\n7     VF   VF 9.141380e-01 7.815186e-02 7.674706e-03 3.540712e-05   Fold01\n8     VF   VF 9.183508e-01 7.437284e-02 7.260583e-03 1.572869e-05   Fold01\n9     VF   VF 8.426395e-01 1.276057e-01 2.956273e-02 1.921091e-04   Fold01\n10    VF   VF 9.201676e-01 7.278837e-02 7.029299e-03 1.470748e-05   Fold01\n11    VF    F 3.761772e-01 5.456339e-01 7.679576e-02 1.393113e-03   Fold01\n12    VF    F 3.885041e-01 5.088634e-01 1.014177e-01 1.214754e-03   Fold01\n13    VF   VF 4.947838e-01 4.567374e-01 4.809350e-02 3.853588e-04   Fold01\n14    VF   VF 8.805588e-01 9.875896e-02 2.062370e-02 5.849500e-05   Fold01\n15    VF   VF 9.143142e-01 7.116937e-02 1.449491e-02 2.150462e-05   Fold01\n16    VF   VF 8.299298e-01 1.315213e-01 3.845993e-02 8.899835e-05   Fold01\n17    VF   VF 9.172870e-01 7.200110e-02 1.068621e-02 2.567282e-05   Fold01\n18    VF   VF 9.133611e-01 7.247206e-02 1.412105e-02 4.578425e-05   Fold01\n19    VF   VF 9.388037e-01 5.393117e-02 7.258208e-03 6.908569e-06   Fold01\n20    VF   VF 9.842818e-01 1.496544e-02 7.517025e-04 1.017422e-06   Fold01\n21    VF   VF 9.837444e-01 1.555593e-02 6.986590e-04 1.004350e-06   Fold01\n22    VF   VF 9.839162e-01 1.530611e-02 7.766103e-04 1.089092e-06   Fold01\n23    VF   VF 9.868004e-01 1.262830e-02 5.707369e-04 5.733360e-07   Fold01\n24    VF   VF 9.774908e-01 2.096118e-02 1.545566e-03 2.429809e-06   Fold01\n25    VF   VF 9.882188e-01 1.136615e-02 4.146959e-04 3.498133e-07   Fold01\n26    VF   VF 9.886027e-01 1.092521e-02 4.718386e-04 2.975579e-07   Fold01\n27    VF   VF 9.752615e-01 2.307828e-02 1.657259e-03 2.961357e-06   Fold01\n28    VF   VF 9.824464e-01 1.657506e-02 9.769593e-04 1.559296e-06   Fold01\n29    VF   VF 9.884003e-01 1.115648e-02 4.428435e-04 3.642457e-07   Fold01\n30    VF   VF 9.324751e-01 5.554159e-02 1.127280e-02 7.104905e-04   Fold01\n31    VF   VF 9.788454e-01 1.986417e-02 1.287635e-03 2.808330e-06   Fold01\n32    VF   VF 9.854944e-01 1.386496e-02 6.398649e-04 7.660727e-07   Fold01\n33    VF   VF 9.868995e-01 1.237899e-02 7.209161e-04 5.714906e-07   Fold01\n34    VF   VF 9.830121e-01 1.625257e-02 7.339278e-04 1.400940e-06   Fold01\n35    VF   VF 9.866763e-01 1.255464e-02 7.684679e-04 6.264748e-07   Fold01\n36    VF   VF 6.362303e-01 3.202032e-01 4.324050e-02 3.260450e-04   Fold01\n37    VF   VF 5.988413e-01 3.412932e-01 5.919116e-02 6.742845e-04   Fold01\n38    VF   VF 6.696001e-01 2.988741e-01 3.129559e-02 2.301224e-04   Fold01\n39    VF   VF 5.583367e-01 3.786000e-01 6.252555e-02 5.377030e-04   Fold01\n40    VF   VF 6.572574e-01 3.050531e-01 3.741023e-02 2.792341e-04   Fold01\n41    VF   VF 7.610149e-01 2.024816e-01 3.644132e-02 6.214494e-05   Fold01\n42    VF   VF 8.316829e-01 1.550445e-01 1.325248e-02 2.011195e-05   Fold01\n43    VF   VF 8.032623e-01 1.756865e-01 2.102688e-02 2.424176e-05   Fold01\n44    VF   VF 8.634132e-01 1.266075e-01 9.969700e-03 9.634219e-06   Fold01\n45    VF    F 8.783413e-02 5.539494e-01 3.493844e-01 8.832123e-03   Fold01\n46    VF    F 1.612212e-01 5.930725e-01 2.441203e-01 1.585951e-03   Fold01\n47    VF    F 8.451150e-02 5.770391e-01 3.307289e-01 7.720507e-03   Fold01\n48    VF   VF 9.875622e-01 1.213416e-02 3.034030e-04 2.158980e-07   Fold01\n49    VF   VF 9.769174e-01 2.217658e-02 9.050344e-04 1.032298e-06   Fold01\n50    VF   VF 9.801830e-01 1.915377e-02 6.624589e-04 7.661263e-07   Fold01\n51    VF   VF 9.575459e-01 4.054433e-02 1.904713e-03 5.046420e-06   Fold01\n52    VF   VF 9.872374e-01 1.242886e-02 3.334950e-04 2.386789e-07   Fold01\n53    VF   VF 9.596211e-01 3.815929e-02 2.216034e-03 3.592323e-06   Fold01\n54    VF   VF 9.267618e-01 6.894315e-02 4.283164e-03 1.189249e-05   Fold01\n55    VF   VF 9.264446e-01 6.921441e-02 4.329014e-03 1.200612e-05   Fold01\n56    VF   VF 9.263946e-01 6.933977e-02 4.253789e-03 1.180854e-05   Fold01\n57    VF   VF 9.887127e-01 1.102153e-02 2.655891e-04 1.851411e-07   Fold01\n58    VF   VF 9.707098e-01 2.785877e-02 1.429527e-03 1.951670e-06   Fold01\n59    VF   VF 9.355224e-01 6.101654e-02 3.453754e-03 7.340268e-06   Fold01\n60    VF   VF 8.982292e-01 9.129933e-02 1.044626e-02 2.516838e-05   Fold01\n61    VF   VF 9.908595e-01 8.875058e-03 2.652782e-04 1.450390e-07   Fold01\n62    VF   VF 9.914449e-01 8.367227e-03 1.878131e-04 7.722762e-08   Fold01\n63    VF   VF 8.711986e-01 1.026056e-01 2.552014e-02 6.756286e-04   Fold01\n64    VF   VF 9.758033e-01 2.327581e-02 9.199892e-04 8.851335e-07   Fold01\n65    VF   VF 5.337455e-01 2.703335e-01 1.807857e-01 1.513533e-02   Fold01\n66    VF   VF 5.977373e-01 2.730663e-01 1.204233e-01 8.773181e-03   Fold01\n67    VF    F 3.124458e-01 3.388463e-01 3.068530e-01 4.185484e-02   Fold01\n68    VF   VF 9.123433e-01 8.108853e-02 6.544284e-03 2.387595e-05   Fold01\n69    VF   VF 9.159462e-01 7.509021e-02 8.055620e-03 9.079857e-04   Fold01\n70    VF   VF 9.141032e-01 7.661989e-02 8.321049e-03 9.558380e-04   Fold01\n71    VF   VF 9.714111e-01 2.684255e-02 1.744117e-03 2.260981e-06   Fold01\n72    VF   VF 9.816820e-01 1.748758e-02 8.295249e-04 8.487934e-07   Fold01\n73    VF   VF 9.816855e-01 1.750611e-02 8.075625e-04 8.407729e-07   Fold01\n74    VF   VF 9.523548e-01 4.447296e-02 3.164154e-03 8.068876e-06   Fold01\n75    VF   VF 9.721119e-01 2.666614e-02 1.218474e-03 3.508354e-06   Fold01\n76    VF   VF 9.745002e-01 2.449543e-02 1.002777e-03 1.591891e-06   Fold01\n77    VF   VF 7.986738e-01 1.774858e-01 2.365189e-02 1.885431e-04   Fold01\n78    VF   VF 9.885608e-01 1.120334e-02 2.356735e-04 1.522772e-07   Fold01\n79    VF   VF 9.889737e-01 1.078012e-02 2.460542e-04 1.529213e-07   Fold01\n80    VF   VF 9.713789e-01 2.745702e-02 1.160417e-03 3.632274e-06   Fold01\n81    VF   VF 9.753267e-01 2.362954e-02 1.041529e-03 2.191413e-06   Fold01\n82    VF   VF 9.703061e-01 2.831243e-02 1.376846e-03 4.613122e-06   Fold01\n83    VF   VF 9.917060e-01 8.057310e-03 2.365507e-04 1.091971e-07   Fold01\n84    VF   VF 9.424027e-01 5.468923e-02 2.901550e-03 6.549774e-06   Fold01\n85    VF   VF 8.921387e-01 1.034190e-01 4.433228e-03 9.138659e-06   Fold01\n86    VF   VF 9.110022e-01 8.566141e-02 3.331579e-03 4.802896e-06   Fold01\n87    VF   VF 8.538234e-01 1.371248e-01 9.031568e-03 2.019706e-05   Fold01\n88    VF   VF 9.144959e-01 8.145326e-02 4.047646e-03 3.239909e-06   Fold01\n89    VF   VF 8.994292e-01 9.702336e-02 3.541015e-03 6.409939e-06   Fold01\n90    VF   VF 6.351969e-01 3.581087e-01 6.668649e-03 2.581245e-05   Fold01\n91    VF   VF 8.717157e-01 1.227939e-01 5.471967e-03 1.840272e-05   Fold01\n92    VF   VF 8.863817e-01 1.091790e-01 4.429300e-03 9.980779e-06   Fold01\n93    VF   VF 8.859842e-01 1.095496e-01 4.456089e-03 1.010652e-05   Fold01\n94    VF   VF 9.150898e-01 8.239322e-02 2.513611e-03 3.372488e-06   Fold01\n95    VF   VF 9.030168e-01 9.319102e-02 3.786256e-03 5.949777e-06   Fold01\n96    VF   VF 8.862154e-01 1.078361e-01 5.935736e-03 1.275886e-05   Fold01\n97    VF   VF 8.468075e-01 1.450910e-01 8.070122e-03 3.139134e-05   Fold01\n98    VF   VF 9.028579e-01 9.387851e-02 3.258046e-03 5.581293e-06   Fold01\n99    VF   VF 8.494848e-01 1.406018e-01 9.889040e-03 2.435532e-05   Fold01\n100   VF   VF 8.980186e-01 9.765205e-02 4.321778e-03 7.525800e-06   Fold01\n101   VF   VF 6.215213e-01 3.707613e-01 7.685053e-03 3.234477e-05   Fold01\n102   VF   VF 8.845148e-01 1.082052e-01 7.268204e-03 1.182380e-05   Fold01\n103   VF   VF 8.340881e-01 1.562953e-01 9.589948e-03 2.659680e-05   Fold01\n104   VF   VF 6.175360e-01 3.749005e-01 7.530447e-03 3.308563e-05   Fold01\n105   VF   VF 8.400409e-01 1.494389e-01 1.049174e-02 2.843310e-05   Fold01\n106   VF   VF 8.324296e-01 1.564529e-01 1.108633e-02 3.120914e-05   Fold01\n107   VF   VF 8.790226e-01 1.144739e-01 6.487872e-03 1.564006e-05   Fold01\n108   VF   VF 8.429517e-01 1.466936e-01 1.032700e-02 2.778209e-05   Fold01\n109   VF   VF 6.114596e-01 3.805150e-01 7.988716e-03 3.665608e-05   Fold01\n110   VF   VF 8.808949e-01 1.132671e-01 5.823290e-03 1.468807e-05   Fold01\n111   VF   VF 8.993866e-01 9.677692e-02 3.829533e-03 6.986108e-06   Fold01\n112   VF   VF 8.394917e-01 1.491751e-01 1.130168e-02 3.156773e-05   Fold01\n113   VF   VF 5.306079e-01 3.738703e-01 8.239941e-02 1.312247e-02   Fold01\n114   VF   VF 8.875119e-01 1.080252e-01 4.450524e-03 1.238820e-05   Fold01\n115   VF   VF 8.847947e-01 1.097347e-01 5.458649e-03 1.196155e-05   Fold01\n116   VF   VF 6.001405e-01 3.897366e-01 1.007204e-02 5.088708e-05   Fold01\n117   VF   VF 5.917984e-01 3.975205e-01 1.062426e-02 5.685470e-05   Fold01\n118   VF   VF 8.965044e-01 9.936515e-02 4.122732e-03 7.722178e-06   Fold01\n119   VF   VF 8.679495e-01 1.239747e-01 8.051185e-03 2.467113e-05   Fold01\n120   VF   VF 5.817390e-01 4.068520e-01 1.134381e-02 6.512504e-05   Fold01\n121   VF   VF 5.737648e-01 4.142328e-01 1.193005e-02 7.238104e-05   Fold01\n122   VF   VF 8.404990e-01 1.496568e-01 9.792677e-03 5.155171e-05   Fold01\n123   VF   VF 5.676901e-01 4.198411e-01 1.239048e-02 7.828540e-05   Fold01\n124   VF   VF 5.680875e-01 4.194708e-01 1.236376e-02 7.796300e-05   Fold01\n125   VF   VF 8.981045e-01 9.741964e-02 4.467601e-03 8.236929e-06   Fold01\n126   VF   VF 8.845382e-01 1.106182e-01 4.829816e-03 1.377258e-05   Fold01\n127   VF   VF 7.962954e-01 1.865537e-01 1.707704e-02 7.380697e-05   Fold01\n128   VF   VF 9.150219e-01 8.186970e-02 3.104184e-03 4.199818e-06   Fold01\n129   VF   VF 8.609217e-01 1.309869e-01 8.074670e-03 1.676395e-05   Fold01\n130   VF   VF 9.115107e-01 8.528893e-02 3.195881e-03 4.510884e-06   Fold01\n131   VF   VF 9.102747e-01 8.641357e-02 3.306981e-03 4.710561e-06   Fold01\n132   VF   VF 9.002321e-01 9.466994e-02 5.091202e-03 6.807249e-06   Fold01\n133   VF   VF 9.004551e-01 9.446465e-02 5.073455e-03 6.745023e-06   Fold01\n134   VF   VF 8.967222e-01 9.756846e-02 5.701562e-03 7.811409e-06   Fold01\n135   VF   VF 8.591927e-01 1.327091e-01 8.080800e-03 1.743518e-05   Fold01\n136   VF   VF 9.122513e-01 8.446578e-02 3.278240e-03 4.701450e-06   Fold01\n137   VF   VF 9.213432e-01 7.709308e-02 1.546999e-03 1.675097e-05   Fold01\n138   VF   VF 8.493043e-01 1.407255e-01 9.947932e-03 2.229406e-05   Fold01\n139   VF   VF 9.586919e-01 4.027547e-02 1.031713e-03 9.598498e-07   Fold01\n140   VF   VF 8.955489e-01 9.334821e-02 1.109101e-02 1.190543e-05   Fold01\n141   VF   VF 9.662497e-01 3.233910e-02 1.409680e-03 1.510032e-06   Fold01\n142   VF   VF 9.798559e-01 1.954576e-02 5.980660e-04 2.505611e-07   Fold01\n143   VF   VF 9.287899e-01 6.621300e-02 4.991993e-03 5.120976e-06   Fold01\n144   VF   VF 9.661810e-01 3.237346e-02 1.444697e-03 8.581622e-07   Fold01\n145   VF   VF 9.786443e-01 2.054377e-02 8.110262e-04 8.725714e-07   Fold01\n146   VF    F 1.187766e-01 7.771076e-01 1.037824e-01 3.333928e-04   Fold01\n147   VF    F 1.096314e-01 7.856875e-01 1.043047e-01 3.764069e-04   Fold01\n148   VF    F 1.304592e-01 7.820385e-01 8.725116e-02 2.510808e-04   Fold01\n149   VF    F 1.185873e-01 7.503935e-01 1.306802e-01 3.390273e-04   Fold01\n150   VF   VF 8.049013e-01 1.833819e-01 1.170656e-02 1.028389e-05   Fold01\n151   VF   VF 6.350021e-01 3.112810e-01 5.360323e-02 1.135919e-04   Fold01\n152   VF   VF 6.318738e-01 3.133561e-01 5.465184e-02 1.182631e-04   Fold01\n153   VF   VF 7.389381e-01 2.424602e-01 1.856696e-02 3.474295e-05   Fold01\n154   VF   VF 7.316738e-01 2.492195e-01 1.906803e-02 3.867170e-05   Fold01\n155   VF   VF 7.275003e-01 2.508938e-01 2.156650e-02 3.939398e-05   Fold01\n156   VF   VF 7.379694e-01 2.399291e-01 2.208295e-02 1.851327e-05   Fold01\n157   VF   VF 7.454390e-01 2.322463e-01 2.228534e-02 2.939182e-05   Fold01\n158   VF   VF 7.222103e-01 2.557280e-01 2.202501e-02 3.667674e-05   Fold01\n159   VF   VF 7.419233e-01 2.399265e-01 1.812666e-02 2.352907e-05   Fold01\n160   VF   VF 7.084196e-01 2.661835e-01 2.533445e-02 6.253189e-05   Fold01\n161   VF   VF 7.726722e-01 2.090576e-01 1.825533e-02 1.487307e-05   Fold01\n162   VF   VF 7.411961e-01 2.391737e-01 1.960385e-02 2.638083e-05   Fold01\n163   VF   VF 7.124028e-01 2.565346e-01 3.102221e-02 4.036243e-05   Fold01\n164   VF   VF 7.192077e-01 2.579253e-01 2.282319e-02 4.377537e-05   Fold01\n165   VF   VF 7.418596e-01 2.318153e-01 2.629117e-02 3.394051e-05   Fold01\n166   VF   VF 6.079875e-01 3.464456e-01 4.545964e-02 1.072807e-04   Fold01\n167   VF   VF 7.392631e-01 2.395576e-01 2.114679e-02 3.245415e-05   Fold01\n168   VF   VF 7.704830e-01 2.063473e-01 2.315137e-02 1.834767e-05   Fold01\n169   VF   VF 7.681594e-01 2.166717e-01 1.515070e-02 1.821612e-05   Fold01\n170   VF   VF 6.903768e-01 2.677582e-01 4.179722e-02 6.776893e-05   Fold01\n171   VF   VF 7.123066e-01 2.633203e-01 2.432734e-02 4.574183e-05   Fold01\n172   VF   VF 5.790094e-01 3.656105e-01 5.521148e-02 1.686258e-04   Fold01\n173   VF   VF 6.836925e-01 2.833857e-01 3.283596e-02 8.583775e-05   Fold01\n174   VF   VF 6.977413e-01 2.763481e-01 2.587546e-02 3.515187e-05   Fold01\n175   VF   VF 7.131733e-01 2.590263e-01 2.776507e-02 3.537681e-05   Fold01\n176   VF   VF 7.268013e-01 2.508101e-01 2.236202e-02 2.659466e-05   Fold01\n177   VF    F 9.643483e-02 7.878652e-01 1.130202e-01 2.679799e-03   Fold01\n178    F    F 3.085202e-01 5.743957e-01 1.152658e-01 1.818341e-03   Fold01\n179    F    L 3.629908e-03 8.967472e-02 2.747283e-01 6.319671e-01   Fold01\n180    F   VF 8.162113e-01 1.384098e-01 4.487402e-02 5.048867e-04   Fold01\n181    F   VF 8.054500e-01 1.443777e-01 4.980389e-02 3.683336e-04   Fold01\n182    F    F 2.425761e-01 4.550394e-01 2.182175e-01 8.416698e-02   Fold01\n183    F    F 1.799199e-01 4.485544e-01 1.949528e-01 1.765729e-01   Fold01\n184    F   VF 7.184922e-01 2.430724e-01 3.828628e-02 1.490647e-04   Fold01\n185    F   VF 7.870749e-01 1.899762e-01 2.291767e-02 3.117119e-05   Fold01\n186    F   VF 7.924986e-01 1.864006e-01 2.107759e-02 2.318856e-05   Fold01\n187    F   VF 8.838839e-01 1.090490e-01 7.062073e-03 4.941168e-06   Fold01\n188    F    F 1.316681e-01 6.202153e-01 2.452239e-01 2.892700e-03   Fold01\n189    F    F 1.265108e-01 6.096967e-01 2.608381e-01 2.954476e-03   Fold01\n190    F    M 1.456121e-02 3.262319e-01 5.954977e-01 6.370922e-02   Fold01\n191    F    F 1.590519e-01 6.312172e-01 2.078950e-01 1.835944e-03   Fold01\n192    F    F 1.092097e-01 5.990014e-01 2.881367e-01 3.652271e-03   Fold01\n193    F    M 3.138967e-03 1.425370e-01 5.875186e-01 2.668054e-01   Fold01\n194    F    F 9.425470e-02 5.827994e-01 3.182536e-01 4.692287e-03   Fold01\n195    F    F 1.854731e-01 6.403700e-01 1.729089e-01 1.248068e-03   Fold01\n196    F    F 1.051158e-01 6.099695e-01 2.817733e-01 3.141422e-03   Fold01\n197    F    F 1.514262e-01 6.238219e-01 2.225621e-01 2.189807e-03   Fold01\n198    F    F 1.515560e-01 6.264246e-01 2.198439e-01 2.175476e-03   Fold01\n199    F    F 1.066972e-01 6.181348e-01 2.707183e-01 4.449714e-03   Fold01\n200    F    F 8.135952e-02 6.016845e-01 3.097005e-01 7.255453e-03   Fold01\n201    F    F 6.759671e-02 5.726734e-01 3.469888e-01 1.274106e-02   Fold01\n202    F    F 1.744948e-01 6.318562e-01 1.921652e-01 1.483871e-03   Fold01\n203    F    M 4.319906e-02 4.659033e-01 4.751903e-01 1.570737e-02   Fold01\n204    F   VF 5.835503e-01 2.489801e-01 1.272985e-01 4.017110e-02   Fold01\n205    F   VF 8.712551e-01 1.207056e-01 8.016429e-03 2.277681e-05   Fold01\n206    F   VF 8.754459e-01 1.174705e-01 7.066784e-03 1.679573e-05   Fold01\n207    F   VF 8.832072e-01 1.109875e-01 5.791971e-03 1.335235e-05   Fold01\n208    F   VF 8.674563e-01 1.248704e-01 7.650685e-03 2.262082e-05   Fold01\n209    F   VF 8.182991e-01 1.681319e-01 1.352264e-02 4.635748e-05   Fold01\n210    F   VF 8.746294e-01 1.167866e-01 8.565726e-03 1.822287e-05   Fold01\n211    F   VF 8.317975e-01 1.577410e-01 1.040307e-02 5.841863e-05   Fold01\n212    F   VF 5.595845e-01 4.291561e-01 1.118578e-02 7.356257e-05   Fold01\n213    F   VF 5.531884e-01 4.350879e-01 1.164363e-02 8.003413e-05   Fold01\n214    F   VF 7.879990e-01 1.939855e-01 1.790277e-02 1.128042e-04   Fold01\n215    F   VF 8.673445e-01 1.266219e-01 6.013753e-03 1.984644e-05   Fold01\n216    F   VF 8.910100e-01 9.462423e-02 1.431747e-02 4.831503e-05   Fold01\n217    F   VF 8.264881e-01 1.394620e-01 3.380100e-02 2.489328e-04   Fold01\n218    F    F 1.222172e-01 5.260073e-01 3.462782e-01 5.497361e-03   Fold01\n219    F   VF 4.609566e-01 3.313034e-01 1.988226e-01 8.917323e-03   Fold01\n220    F   VF 8.762492e-01 1.116796e-01 1.204600e-02 2.521629e-05   Fold01\n221    F    F 1.374266e-01 3.954473e-01 3.753199e-01 9.180629e-02   Fold01\n222    F   VF 4.450757e-01 2.892204e-01 2.310014e-01 3.470251e-02   Fold01\n223    F    F 9.929779e-02 7.870347e-01 1.131600e-01 5.074847e-04   Fold01\n224    F    F 7.948567e-02 7.737098e-01 1.458932e-01 9.113317e-04   Fold01\n225    F    F 9.076328e-02 7.391177e-01 1.695726e-01 5.464904e-04   Fold01\n226    F    F 8.762716e-02 7.817623e-01 1.299781e-01 6.324745e-04   Fold01\n227    F    F 1.221888e-01 7.542483e-01 1.233339e-01 2.289787e-04   Fold01\n228    F    F 1.092334e-01 7.787982e-01 1.115848e-01 3.836295e-04   Fold01\n229    F    F 1.016567e-01 7.833472e-01 1.145654e-01 4.306604e-04   Fold01\n230    F    F 6.481820e-02 7.547046e-01 1.795216e-01 9.556250e-04   Fold01\n231    F    F 4.099488e-02 7.390468e-01 2.114576e-01 8.500737e-03   Fold01\n232    F    F 7.790251e-02 7.128219e-01 2.086333e-01 6.422048e-04   Fold01\n233    F    F 6.050997e-02 7.566477e-01 1.817530e-01 1.089333e-03   Fold01\n234    F    F 1.061616e-01 7.776771e-01 1.157066e-01 4.546342e-04   Fold01\n235    F    F 1.049795e-01 7.788066e-01 1.157522e-01 4.618084e-04   Fold01\n236    F    F 8.347306e-02 7.797370e-01 1.361209e-01 6.690213e-04   Fold01\n237    F    F 1.109299e-01 7.482205e-01 1.404773e-01 3.722136e-04   Fold01\n238    F    F 8.548400e-02 7.173201e-01 1.963948e-01 8.010348e-04   Fold01\n239    F    F 9.210791e-02 8.235494e-01 7.913478e-02 5.207895e-03   Fold01\n240    F    F 6.025764e-02 7.622342e-01 1.754322e-01 2.076004e-03   Fold01\n241    F    F 8.993485e-02 7.874067e-01 1.218853e-01 7.730759e-04   Fold01\n242    F    F 1.224717e-01 8.203716e-01 5.499967e-02 2.156998e-03   Fold01\n243    F    F 1.077506e-01 7.769704e-01 1.149425e-01 3.365003e-04   Fold01\n244    F    F 1.139636e-01 7.919489e-01 9.378241e-02 3.051207e-04   Fold01\n245    F    F 1.198684e-01 7.805807e-01 9.924446e-02 3.064149e-04   Fold01\n246    F   VF 7.392725e-01 2.423529e-01 1.833872e-02 3.594056e-05   Fold01\n247    F   VF 8.329427e-01 1.588675e-01 8.185231e-03 4.556007e-06   Fold01\n248    F   VF 7.410474e-01 2.304667e-01 2.845997e-02 2.585193e-05   Fold01\n249    F    F 4.301165e-01 4.701842e-01 9.325885e-02 6.440513e-03   Fold01\n250    F    F 4.323225e-01 4.691488e-01 9.224870e-02 6.280041e-03   Fold01\n251    F    F 4.302306e-01 4.701662e-01 9.318210e-02 6.421094e-03   Fold01\n252    F   VF 6.814849e-01 2.844926e-01 3.394474e-02 7.779644e-05   Fold01\n253    F    F 3.830151e-01 4.541171e-01 1.534624e-01 9.405432e-03   Fold01\n254    F    F 3.034164e-01 4.998574e-01 1.795911e-01 1.713518e-02   Fold01\n255    F   VF 6.972304e-01 2.702410e-01 3.247484e-02 5.377830e-05   Fold01\n256    F   VF 6.471921e-01 3.147512e-01 3.791467e-02 1.420286e-04   Fold01\n257    F   VF 6.353334e-01 3.256375e-01 3.887164e-02 1.575544e-04   Fold01\n258    F   VF 5.979454e-01 3.548225e-01 4.702182e-02 2.102580e-04   Fold01\n259    F   VF 6.665615e-01 3.029924e-01 3.040042e-02 4.567723e-05   Fold01\n260    F   VF 5.943269e-01 3.481596e-01 5.732233e-02 1.910942e-04   Fold01\n261    F    F 7.759463e-02 7.482968e-01 1.692793e-01 4.829242e-03   Fold01\n262    F    F 7.869524e-02 7.888136e-01 1.275408e-01 4.950419e-03   Fold01\n263    F    F 6.928254e-02 7.320389e-01 1.921349e-01 6.543636e-03   Fold01\n264    F    F 6.977566e-02 7.347491e-01 1.889911e-01 6.484129e-03   Fold01\n265    F    F 6.978968e-02 7.325013e-01 1.911785e-01 6.530551e-03   Fold01\n266    F    F 1.165881e-02 4.198061e-01 1.863739e-01 3.821611e-01   Fold01\n267    F    F 1.179715e-02 4.224329e-01 1.866518e-01 3.791182e-01   Fold01\n268    F    F 6.879429e-02 7.335038e-01 1.910055e-01 6.696364e-03   Fold01\n269    F    F 3.922435e-02 7.348875e-01 1.862365e-01 3.965173e-02   Fold01\n270    F    F 6.336183e-02 7.717532e-01 1.563192e-01 8.565765e-03   Fold01\n271    F    F 6.358290e-02 7.799444e-01 1.483085e-01 8.164159e-03   Fold01\n272    F    F 7.616875e-02 7.848653e-01 1.335304e-01 5.435470e-03   Fold01\n273    F    F 6.758068e-02 7.285866e-01 1.968608e-01 6.971862e-03   Fold01\n274    F    F 6.678626e-02 7.300235e-01 1.960342e-01 7.156119e-03   Fold01\n275    F    F 9.341467e-02 7.597267e-01 1.435434e-01 3.315189e-03   Fold01\n276    F    F 6.065368e-02 7.718283e-01 1.581569e-01 9.361049e-03   Fold01\n277    F    F 6.003814e-02 7.657586e-01 1.644387e-01 9.764570e-03   Fold01\n278    F    F 2.474163e-02 6.413660e-01 2.278819e-01 1.060104e-01   Fold01\n279    F    F 1.585978e-02 5.379014e-01 3.480197e-01 9.821916e-02   Fold01\n280    F    F 7.277590e-02 7.835962e-01 1.375826e-01 6.045321e-03   Fold01\n281    F    F 4.199578e-02 7.310529e-01 2.122579e-01 1.469349e-02   Fold01\n282    F    F 4.763636e-02 7.584089e-01 1.751642e-01 1.879050e-02   Fold01\n283    F    F 7.644313e-02 7.928165e-01 1.241020e-01 6.638411e-03   Fold01\n284    F    F 9.255571e-02 7.518003e-01 1.520139e-01 3.630120e-03   Fold01\n285    F    F 3.599022e-02 7.116566e-01 2.165581e-01 3.579506e-02   Fold01\n286    M    F 3.656146e-01 5.590954e-01 7.413402e-02 1.156016e-03   Fold01\n287    M   VF 5.046640e-01 2.059424e-01 2.784793e-01 1.091432e-02   Fold01\n288    M    M 2.557471e-01 1.845101e-01 4.788544e-01 8.088831e-02   Fold01\n289    M    M 9.554212e-02 1.441559e-01 5.819675e-01 1.783344e-01   Fold01\n290    M    M 1.048358e-01 1.434331e-01 5.806475e-01 1.710836e-01   Fold01\n291    M    F 3.411136e-01 5.263217e-01 1.303257e-01 2.239069e-03   Fold01\n292    M   VF 4.395510e-01 4.173478e-01 1.409447e-01 2.156456e-03   Fold01\n293    M    F 1.903181e-01 6.431888e-01 1.656219e-01 8.712275e-04   Fold01\n294    M    L 2.324416e-04 2.496363e-02 3.128318e-01 6.619722e-01   Fold01\n295    M    L 1.035633e-04 1.678893e-02 2.191687e-01 7.639388e-01   Fold01\n296    M   VF 7.472499e-01 2.260186e-01 2.644872e-02 2.828239e-04   Fold01\n297    M    L 2.836057e-03 1.726291e-02 2.014179e-02 9.597593e-01   Fold01\n298    M    F 7.382566e-02 7.554666e-01 1.700694e-01 6.383945e-04   Fold01\n299    M    M 1.087517e-02 4.594597e-01 4.745927e-01 5.507241e-02   Fold01\n300    M    F 5.319387e-02 7.528510e-01 1.927023e-01 1.252863e-03   Fold01\n301    M    F 9.919670e-03 5.064792e-01 3.540276e-01 1.295734e-01   Fold01\n302    M    F 1.360221e-02 5.446233e-01 2.978724e-01 1.439022e-01   Fold01\n303    M    F 8.337477e-02 7.314479e-01 1.843469e-01 8.304285e-04   Fold01\n304    M    F 8.005903e-02 7.716926e-01 1.474005e-01 8.478554e-04   Fold01\n305    M    F 5.844718e-02 7.613169e-01 1.780749e-01 2.160968e-03   Fold01\n306    M    F 7.676709e-02 7.697531e-01 1.525668e-01 9.129818e-04   Fold01\n307    M    F 7.079268e-02 7.623972e-01 1.657485e-01 1.061655e-03   Fold01\n308    M    F 4.934766e-02 7.448912e-01 2.045572e-01 1.204006e-03   Fold01\n309    M   VF 6.832505e-01 2.598014e-01 5.690574e-02 4.234545e-05   Fold01\n310    M    F 3.200191e-01 5.064819e-01 1.548760e-01 1.862297e-02   Fold01\n311    M    F 2.111733e-01 4.941866e-01 2.828528e-01 1.178732e-02   Fold01\n312    M    L 1.570983e-03 1.718572e-02 3.021684e-02 9.510265e-01   Fold01\n313    M   VF 7.153636e-01 2.528645e-01 3.173166e-02 4.028389e-05   Fold01\n314    M    M 2.347342e-02 2.255326e-01 5.576506e-01 1.933433e-01   Fold01\n315    M   VF 5.519460e-01 3.857331e-01 6.210817e-02 2.127387e-04   Fold01\n316    M   VF 4.699252e-01 4.253436e-01 1.043000e-01 4.313003e-04   Fold01\n317    M   VF 5.838790e-01 3.439192e-01 7.196280e-02 2.390813e-04   Fold01\n318    M    F 4.874664e-02 7.658500e-01 1.759823e-01 9.421111e-03   Fold01\n319    M    F 4.825635e-02 7.513312e-01 1.902829e-01 1.012964e-02   Fold01\n320    M    F 1.923061e-02 5.966262e-01 2.890027e-01 9.514040e-02   Fold01\n321    M    F 3.699025e-02 7.021783e-01 2.143871e-01 4.644432e-02   Fold01\n322    M    F 2.126716e-02 6.014982e-01 2.717149e-01 1.055197e-01   Fold01\n323    M    F 3.681704e-02 7.030506e-01 2.124787e-01 4.765367e-02   Fold01\n324    M    F 3.926112e-02 7.267427e-01 2.178923e-01 1.610396e-02   Fold01\n325    M    F 3.369507e-02 7.037111e-01 2.344293e-01 2.816456e-02   Fold01\n326    M    F 2.868885e-02 6.424514e-01 2.988210e-01 3.003871e-02   Fold01\n327    L    L 7.846397e-04 4.045227e-02 1.782932e-01 7.804699e-01   Fold01\n328    L    L 1.285633e-03 5.211588e-02 2.229619e-01 7.236366e-01   Fold01\n329    L    M 7.234951e-03 5.150661e-02 5.271950e-01 4.140634e-01   Fold01\n330    L    L 5.999027e-03 4.912893e-02 3.981381e-01 5.467339e-01   Fold01\n331    L    M 1.970209e-02 8.066688e-02 5.678960e-01 3.317350e-01   Fold01\n332    L    M 4.961505e-03 4.301445e-02 4.958910e-01 4.561330e-01   Fold01\n333    L    L 3.648615e-03 3.803177e-02 3.633666e-01 5.949531e-01   Fold01\n334    L    F 7.910197e-02 5.809876e-01 3.348174e-01 5.093085e-03   Fold01\n335    L    L 1.976828e-04 2.281506e-02 3.065967e-01 6.703905e-01   Fold01\n336    L    F 5.946487e-02 5.421973e-01 3.897932e-01 8.544651e-03   Fold01\n337    L   VF 6.122185e-01 3.218876e-01 6.483275e-02 1.061143e-03   Fold01\n338    L    L 8.204597e-16 2.131698e-11 3.001437e-07 9.999997e-01   Fold01\n339    L    F 2.997362e-01 4.165304e-01 2.601360e-01 2.359742e-02   Fold01\n340    L    F 2.619184e-02 6.451174e-01 3.224154e-01 6.275384e-03   Fold01\n341    L    F 5.340559e-02 7.462599e-01 1.975554e-01 2.779066e-03   Fold01\n342    L    L 3.207882e-03 2.488212e-01 3.371387e-01 4.108322e-01   Fold01\n343    L    L 1.379048e-06 4.330500e-04 1.394348e-03 9.981712e-01   Fold01\n344    L    L 1.381118e-06 4.332419e-04 1.395443e-03 9.981699e-01   Fold01\n345    L    F 2.629506e-02 6.768979e-01 2.404450e-01 5.636200e-02   Fold01\n346    L    F 2.604416e-02 6.754036e-01 2.413222e-01 5.723005e-02   Fold01\n347    L    L 6.960036e-08 5.436411e-05 3.210546e-04 9.996245e-01   Fold01\n348   VF   VF 9.412219e-01 5.435180e-02 4.413963e-03 1.229696e-05   Fold02\n349   VF   VF 9.482549e-01 4.826368e-02 3.473475e-03 7.924652e-06   Fold02\n350   VF   VF 9.581118e-01 3.952440e-02 2.360657e-03 3.096142e-06   Fold02\n351   VF   VF 9.095277e-01 7.808625e-02 1.230020e-02 8.587749e-05   Fold02\n352   VF   VF 8.914812e-01 8.601302e-02 2.239651e-02 1.092957e-04   Fold02\n353   VF   VF 9.109269e-01 7.371181e-02 1.531922e-02 4.204513e-05   Fold02\n354   VF   VF 9.604986e-01 3.598692e-02 3.510844e-03 3.661911e-06   Fold02\n355   VF   VF 9.543906e-01 3.883720e-02 6.767580e-03 4.584495e-06   Fold02\n356   VF   VF 9.828308e-01 1.634729e-02 8.207968e-04 1.091500e-06   Fold02\n357   VF   VF 9.858069e-01 1.358387e-02 6.083039e-04 9.110665e-07   Fold02\n358   VF   VF 9.850278e-01 1.403103e-02 9.402040e-04 9.969921e-07   Fold02\n359   VF   VF 9.845470e-01 1.448080e-02 9.713373e-04 8.669615e-07   Fold02\n360   VF   VF 9.885874e-01 1.100112e-02 4.111571e-04 3.696151e-07   Fold02\n361   VF   VF 9.885936e-01 1.098762e-02 4.184856e-04 2.750856e-07   Fold02\n362   VF   VF 9.816863e-01 1.736159e-02 9.504412e-04 1.693428e-06   Fold02\n363   VF   VF 9.822054e-01 1.686104e-02 9.316683e-04 1.893113e-06   Fold02\n364   VF   VF 9.884880e-01 1.109655e-02 4.150801e-04 3.641179e-07   Fold02\n365   VF   VF 9.775021e-01 2.134114e-02 1.153370e-03 3.394507e-06   Fold02\n366   VF   VF 9.726112e-01 2.502926e-02 2.352862e-03 6.720409e-06   Fold02\n367   VF   VF 9.805811e-01 1.819343e-02 1.223488e-03 2.016982e-06   Fold02\n368   VF   VF 9.801749e-01 1.857453e-02 1.248723e-03 1.884647e-06   Fold02\n369   VF   VF 9.729258e-01 2.544547e-02 1.621842e-03 6.850999e-06   Fold02\n370   VF   VF 9.706357e-01 2.738039e-02 1.973417e-03 1.052861e-05   Fold02\n371   VF   VF 9.814356e-01 1.777971e-02 7.830491e-04 1.668492e-06   Fold02\n372   VF   VF 9.244614e-01 6.351502e-02 1.063448e-02 1.389127e-03   Fold02\n373   VF   VF 9.886241e-01 1.094139e-02 4.339418e-04 5.656572e-07   Fold02\n374   VF   VF 9.668749e-01 3.021923e-02 2.892037e-03 1.379920e-05   Fold02\n375   VF   VF 9.791999e-01 1.943186e-02 1.366067e-03 2.203352e-06   Fold02\n376   VF   VF 9.830276e-01 1.628674e-02 6.841259e-04 1.575653e-06   Fold02\n377   VF   VF 7.014406e-01 2.713657e-01 2.709992e-02 9.376695e-05   Fold02\n378   VF   VF 6.369721e-01 3.257907e-01 3.702708e-02 2.101082e-04   Fold02\n379   VF   VF 6.322773e-01 3.369105e-01 3.056442e-02 2.476871e-04   Fold02\n380   VF   VF 6.203508e-01 3.319245e-01 4.746032e-02 2.643606e-04   Fold02\n381   VF   VF 7.151039e-01 2.631021e-01 2.171415e-02 7.981983e-05   Fold02\n382   VF   VF 6.191431e-01 3.238063e-01 5.678272e-02 2.678440e-04   Fold02\n383   VF   VF 5.627792e-01 3.864002e-01 5.002291e-02 7.976363e-04   Fold02\n384   VF   VF 6.605081e-01 3.080120e-01 3.127552e-02 2.044106e-04   Fold02\n385   VF   VF 7.276204e-01 2.515287e-01 2.076431e-02 8.663783e-05   Fold02\n386   VF   VF 6.633374e-01 3.057436e-01 3.072903e-02 1.900385e-04   Fold02\n387   VF   VF 7.439829e-01 2.218717e-01 3.405797e-02 8.743769e-05   Fold02\n388   VF   VF 8.212639e-01 1.640595e-01 1.465166e-02 2.499219e-05   Fold02\n389   VF   VF 8.873386e-01 1.058930e-01 6.761658e-03 6.718742e-06   Fold02\n390   VF   VF 8.266728e-01 1.581429e-01 1.514921e-02 3.500786e-05   Fold02\n391   VF   VF 8.565567e-01 1.347502e-01 8.673446e-03 1.969678e-05   Fold02\n392   VF   VF 8.942613e-01 9.976400e-02 5.968923e-03 5.776085e-06   Fold02\n393   VF    F 1.319817e-01 6.532315e-01 2.124915e-01 2.295341e-03   Fold02\n394   VF    F 1.578868e-01 6.487568e-01 1.909684e-01 2.388031e-03   Fold02\n395   VF    F 1.120367e-01 6.140491e-01 2.701824e-01 3.731767e-03   Fold02\n396   VF   VF 9.623908e-01 3.551062e-02 2.093784e-03 4.810624e-06   Fold02\n397   VF   VF 9.935280e-01 6.349190e-03 1.227248e-04 4.955180e-08   Fold02\n398   VF   VF 9.899627e-01 9.796279e-03 2.408796e-04 1.477022e-07   Fold02\n399   VF   VF 9.881459e-01 1.145432e-02 3.996262e-04 1.284713e-07   Fold02\n400   VF   VF 9.907346e-01 9.065933e-03 1.993730e-04 1.021842e-07   Fold02\n401   VF   VF 9.640417e-01 3.456528e-02 1.391007e-03 2.033527e-06   Fold02\n402   VF   VF 9.348205e-01 5.506303e-02 1.005462e-02 6.184648e-05   Fold02\n403   VF   VF 9.556890e-01 4.225700e-02 2.048080e-03 5.968322e-06   Fold02\n404   VF   VF 9.931306e-01 6.737728e-03 1.316406e-04 4.927377e-08   Fold02\n405   VF   VF 9.590621e-01 3.903457e-02 1.897683e-03 5.607987e-06   Fold02\n406   VF   VF 9.825806e-01 1.683898e-02 5.797981e-04 6.387659e-07   Fold02\n407   VF   VF 9.623393e-01 3.549000e-02 2.166718e-03 3.948187e-06   Fold02\n408   VF   VF 9.491463e-01 4.847133e-02 2.376224e-03 6.166182e-06   Fold02\n409   VF   VF 9.924491e-01 7.331232e-03 2.196052e-04 6.728706e-08   Fold02\n410   VF   VF 9.472081e-01 5.022245e-02 2.561338e-03 8.074188e-06   Fold02\n411   VF   VF 9.500427e-01 4.735223e-02 2.596009e-03 9.028863e-06   Fold02\n412   VF   VF 9.790948e-01 2.009903e-02 8.050499e-04 1.131054e-06   Fold02\n413   VF   VF 9.788184e-01 2.032442e-02 8.555545e-04 1.613734e-06   Fold02\n414   VF   VF 9.894083e-01 1.015608e-02 4.354148e-04 2.403840e-07   Fold02\n415   VF   VF 9.774361e-01 2.156228e-02 1.000656e-03 9.436831e-07   Fold02\n416   VF   VF 9.841789e-01 1.535520e-02 4.654007e-04 5.013316e-07   Fold02\n417   VF   VF 9.799535e-01 1.947840e-02 5.670985e-04 9.906829e-07   Fold02\n418   VF   VF 9.132297e-01 7.880853e-02 7.855828e-03 1.059837e-04   Fold02\n419   VF   VF 9.798805e-01 1.953308e-02 5.858438e-04 5.383499e-07   Fold02\n420   VF   VF 9.721206e-01 2.652469e-02 1.349283e-03 5.415316e-06   Fold02\n421   VF   VF 8.869423e-01 1.068298e-01 6.216196e-03 1.168894e-05   Fold02\n422   VF   VF 8.655679e-01 1.259751e-01 8.431411e-03 2.562060e-05   Fold02\n423   VF   VF 8.949100e-01 1.002857e-01 4.791267e-03 1.304236e-05   Fold02\n424   VF   VF 9.179435e-01 7.810563e-02 3.947459e-03 3.452704e-06   Fold02\n425   VF   VF 6.713405e-01 3.228358e-01 5.806723e-03 1.697331e-05   Fold02\n426   VF   VF 8.681524e-01 1.234825e-01 8.337278e-03 2.778107e-05   Fold02\n427   VF   VF 6.727988e-01 3.216458e-01 5.544556e-03 1.076954e-05   Fold02\n428   VF   VF 8.744381e-01 1.203201e-01 5.219914e-03 2.193793e-05   Fold02\n429   VF   VF 9.148481e-01 8.201837e-02 3.128398e-03 5.180481e-06   Fold02\n430   VF   VF 8.598002e-01 1.376134e-01 2.549184e-03 3.720961e-05   Fold02\n431   VF   VF 9.133000e-01 8.342110e-02 3.271903e-03 7.007512e-06   Fold02\n432   VF   VF 8.672375e-01 1.241379e-01 8.593192e-03 3.137136e-05   Fold02\n433   VF   VF 8.917659e-01 1.033957e-01 4.825536e-03 1.285237e-05   Fold02\n434   VF   VF 9.176491e-01 7.830028e-02 4.047207e-03 3.382761e-06   Fold02\n435   VF   VF 8.885116e-01 1.064167e-01 5.058754e-03 1.303120e-05   Fold02\n436   VF   VF 8.561191e-01 1.337242e-01 1.012019e-02 3.653272e-05   Fold02\n437   VF   VF 8.979300e-01 9.603680e-02 6.023683e-03 9.522402e-06   Fold02\n438   VF   VF 9.059634e-01 9.036612e-02 3.664604e-03 5.857743e-06   Fold02\n439   VF   VF 9.026699e-01 9.336250e-02 3.958805e-03 8.783136e-06   Fold02\n440   VF   VF 9.011111e-01 9.300817e-02 5.870732e-03 1.004196e-05   Fold02\n441   VF   VF 9.088555e-01 8.747825e-02 3.657275e-03 8.978887e-06   Fold02\n442   VF   VF 6.399458e-01 3.523235e-01 7.689636e-03 4.107191e-05   Fold02\n443   VF   VF 8.722396e-01 1.221261e-01 5.606501e-03 2.775968e-05   Fold02\n444   VF   VF 8.658305e-01 1.280394e-01 6.094239e-03 3.586795e-05   Fold02\n445   VF   VF 9.109901e-01 8.542539e-02 3.576151e-03 8.379436e-06   Fold02\n446   VF   VF 8.677419e-01 1.296362e-01 2.561238e-03 6.065251e-05   Fold02\n447   VF   VF 6.366535e-01 3.554292e-01 7.874250e-03 4.305485e-05   Fold02\n448   VF   VF 9.135297e-01 8.301764e-02 3.444346e-03 8.288715e-06   Fold02\n449   VF   VF 8.852686e-01 1.091012e-01 5.610555e-03 1.965348e-05   Fold02\n450   VF   VF 8.759383e-01 1.178098e-01 6.231620e-03 2.025520e-05   Fold02\n451   VF   VF 9.039180e-01 9.201638e-02 4.055159e-03 1.041931e-05   Fold02\n452   VF   VF 8.750187e-01 1.193526e-01 5.606590e-03 2.203195e-05   Fold02\n453   VF   VF 8.774378e-01 1.158965e-01 6.641870e-03 2.378966e-05   Fold02\n454   VF   VF 8.384292e-01 1.493110e-01 1.220381e-02 5.606950e-05   Fold02\n455   VF   VF 8.568954e-01 1.360430e-01 7.016126e-03 4.548441e-05   Fold02\n456   VF   VF 8.908956e-01 1.019468e-01 7.143038e-03 1.458013e-05   Fold02\n457   VF   VF 6.123497e-01 3.782664e-01 9.323538e-03 6.032761e-05   Fold02\n458   VF   VF 5.533978e-01 3.475408e-01 7.824624e-02 2.081507e-02   Fold02\n459   VF   VF 8.997931e-01 9.613297e-02 4.065071e-03 8.845146e-06   Fold02\n460   VF   VF 6.516807e-01 2.868333e-01 5.567374e-02 5.812239e-03   Fold02\n461   VF   VF 5.978975e-01 3.923591e-01 9.692574e-03 5.085671e-05   Fold02\n462   VF   VF 8.969996e-01 9.871829e-02 4.272277e-03 9.829512e-06   Fold02\n463   VF   VF 5.811334e-01 4.079691e-01 1.083375e-02 6.379053e-05   Fold02\n464   VF   VF 8.657865e-01 1.263913e-01 7.786435e-03 3.579123e-05   Fold02\n465   VF   VF 5.875310e-01 4.013740e-01 1.101067e-02 8.428505e-05   Fold02\n466   VF   VF 5.859721e-01 4.028244e-01 1.111751e-02 8.597736e-05   Fold02\n467   VF   VF 5.841853e-01 4.043794e-01 1.134229e-02 9.304117e-05   Fold02\n468   VF   VF 5.793755e-01 4.089237e-01 1.160700e-02 9.375931e-05   Fold02\n469   VF   VF 8.349337e-01 1.555209e-01 9.475753e-03 6.957572e-05   Fold02\n470   VF   VF 8.652677e-01 1.237528e-01 1.094377e-02 3.567355e-05   Fold02\n471   VF   VF 8.938884e-01 1.015392e-01 4.559910e-03 1.252024e-05   Fold02\n472   VF   VF 9.227006e-01 7.457426e-02 2.719913e-03 5.239942e-06   Fold02\n473   VF   VF 9.098991e-01 8.523642e-02 4.857212e-03 7.252739e-06   Fold02\n474   VF   VF 8.597071e-01 1.314605e-01 8.808218e-03 2.422368e-05   Fold02\n475   VF   VF 8.802138e-01 1.148186e-01 4.941526e-03 2.611767e-05   Fold02\n476   VF   VF 8.748615e-01 1.178193e-01 7.299947e-03 1.926647e-05   Fold02\n477   VF   VF 8.762783e-01 1.165069e-01 7.195688e-03 1.912051e-05   Fold02\n478   VF   VF 8.366678e-01 1.523969e-01 1.090046e-02 3.477376e-05   Fold02\n479   VF   VF 8.619630e-01 1.293888e-01 8.624808e-03 2.339865e-05   Fold02\n480   VF   VF 9.206736e-01 7.646214e-02 2.858676e-03 5.633997e-06   Fold02\n481   VF   VF 8.640669e-01 1.274155e-01 8.494637e-03 2.292996e-05   Fold02\n482   VF   VF 9.120775e-01 8.454403e-02 3.371626e-03 6.877527e-06   Fold02\n483   VF   VF 8.638588e-01 1.275796e-01 8.538173e-03 2.335009e-05   Fold02\n484   VF   VF 9.646843e-01 3.406696e-02 1.247917e-03 8.381428e-07   Fold02\n485   VF   VF 9.577756e-01 4.059647e-02 1.626338e-03 1.575429e-06   Fold02\n486   VF   VF 9.014167e-01 9.132900e-02 7.229224e-03 2.512618e-05   Fold02\n487   VF   VF 9.410553e-01 5.632757e-02 2.613679e-03 3.502706e-06   Fold02\n488   VF   VF 9.613750e-01 3.717781e-02 1.446094e-03 1.091766e-06   Fold02\n489   VF   VF 9.461803e-01 5.067612e-02 3.140445e-03 3.180952e-06   Fold02\n490   VF   VF 9.676560e-01 3.126576e-02 1.077432e-03 7.787704e-07   Fold02\n491   VF   VF 9.721129e-01 2.702262e-02 8.640418e-04 4.494552e-07   Fold02\n492   VF   VF 9.772282e-01 2.217017e-02 6.014444e-04 2.282313e-07   Fold02\n493   VF   VF 9.738293e-01 2.485784e-02 1.312342e-03 5.555556e-07   Fold02\n494   VF   VF 8.854273e-01 1.044543e-01 1.009270e-02 2.568889e-05   Fold02\n495   VF   VF 9.735859e-01 2.502162e-02 1.391760e-03 7.012472e-07   Fold02\n496   VF   VF 9.225397e-01 7.314578e-02 4.298455e-03 1.605168e-05   Fold02\n497   VF    F 9.891665e-02 7.923023e-01 1.083374e-01 4.436428e-04   Fold02\n498   VF    F 9.615077e-02 7.944174e-01 1.090301e-01 4.017447e-04   Fold02\n499   VF    F 9.071409e-02 7.652603e-01 1.436731e-01 3.524666e-04   Fold02\n500   VF    F 9.362804e-02 7.940470e-01 1.118918e-01 4.331388e-04   Fold02\n501   VF    F 1.077593e-01 7.541877e-01 1.376739e-01 3.790530e-04   Fold02\n502   VF   VF 7.966944e-01 1.866678e-01 1.662655e-02 1.127912e-05   Fold02\n503   VF   VF 8.052172e-01 1.787577e-01 1.601725e-02 7.862689e-06   Fold02\n504   VF    F 3.808753e-01 5.974494e-01 2.163396e-02 4.135462e-05   Fold02\n505   VF   VF 8.075835e-01 1.773012e-01 1.510052e-02 1.479613e-05   Fold02\n506   VF   VF 6.050811e-01 3.390456e-01 5.573077e-02 1.425297e-04   Fold02\n507   VF   VF 7.933887e-01 1.872798e-01 1.931592e-02 1.564763e-05   Fold02\n508   VF   VF 6.966068e-01 2.778978e-01 2.544573e-02 4.966381e-05   Fold02\n509   VF   VF 5.870442e-01 3.462513e-01 6.647342e-02 2.310908e-04   Fold02\n510   VF   VF 6.814658e-01 2.812545e-01 3.720395e-02 7.575593e-05   Fold02\n511   VF   VF 7.194497e-01 2.612959e-01 1.920661e-02 4.783959e-05   Fold02\n512   VF   VF 6.385601e-01 3.127748e-01 4.853846e-02 1.266552e-04   Fold02\n513   VF   VF 6.778155e-01 2.896317e-01 3.249082e-02 6.199087e-05   Fold02\n514   VF   VF 6.578773e-01 3.092501e-01 3.273109e-02 1.414856e-04   Fold02\n515   VF   VF 7.778379e-01 2.068688e-01 1.527608e-02 1.729233e-05   Fold02\n516   VF   VF 6.669539e-01 2.972571e-01 3.570671e-02 8.229915e-05   Fold02\n517   VF   VF 7.440361e-01 2.317735e-01 2.414695e-02 4.342562e-05   Fold02\n518   VF   VF 5.876107e-01 3.464737e-01 6.566927e-02 2.462677e-04   Fold02\n519   VF   VF 5.788706e-01 3.515087e-01 6.934658e-02 2.741279e-04   Fold02\n520   VF   VF 8.063869e-01 1.765658e-01 1.703379e-02 1.347681e-05   Fold02\n521   VF   VF 8.087230e-01 1.794730e-01 1.179160e-02 1.241174e-05   Fold02\n522   VF   VF 8.028081e-01 1.797376e-01 1.744134e-02 1.291490e-05   Fold02\n523   VF    F 9.088469e-02 7.500595e-01 1.567474e-01 2.308367e-03   Fold02\n524   VF    F 8.879143e-02 7.480867e-01 1.593707e-01 3.751221e-03   Fold02\n525    F    F 4.064392e-01 5.192439e-01 7.300694e-02 1.309958e-03   Fold02\n526    F   VF 6.236294e-01 3.438814e-01 3.217634e-02 3.128020e-04   Fold02\n527    F   VF 7.207876e-01 2.569065e-01 2.219231e-02 1.135647e-04   Fold02\n528    F    F 3.959107e-01 4.820750e-01 1.189352e-01 3.079148e-03   Fold02\n529    F   VF 7.656745e-01 1.957957e-01 3.846919e-02 6.061340e-05   Fold02\n530    F   VF 8.341053e-01 1.488405e-01 1.703052e-02 2.362207e-05   Fold02\n531    F   VF 8.504958e-01 1.379948e-01 1.148853e-02 2.088447e-05   Fold02\n532    F    M 2.264767e-02 1.286831e-01 5.025028e-01 3.461664e-01   Fold02\n533    F   VF 7.560049e-01 2.141113e-01 2.977884e-02 1.049495e-04   Fold02\n534    F   VF 8.951685e-01 9.893494e-02 5.890789e-03 5.806999e-06   Fold02\n535    F    F 1.406516e-01 6.517979e-01 2.051326e-01 2.417945e-03   Fold02\n536    F    F 1.798838e-01 6.321490e-01 1.859631e-01 2.004085e-03   Fold02\n537    F    F 1.342906e-01 5.977997e-01 2.655968e-01 2.312893e-03   Fold02\n538    F    F 1.356226e-01 5.965868e-01 2.653479e-01 2.442699e-03   Fold02\n539    F    F 1.222717e-01 6.295693e-01 2.444526e-01 3.706433e-03   Fold02\n540    F    F 1.005331e-01 6.671408e-01 2.269418e-01 5.384362e-03   Fold02\n541    F    F 1.176856e-01 6.377132e-01 2.402652e-01 4.335941e-03   Fold02\n542    F    F 8.373046e-02 5.910335e-01 3.189586e-01 6.277438e-03   Fold02\n543    F    F 1.146523e-01 6.434949e-01 2.380388e-01 3.813946e-03   Fold02\n544    F    F 1.090444e-01 5.967802e-01 2.901686e-01 4.006822e-03   Fold02\n545    F    F 1.488714e-01 6.533644e-01 1.956426e-01 2.121612e-03   Fold02\n546    F    F 1.908803e-01 6.467016e-01 1.611782e-01 1.239797e-03   Fold02\n547    F    F 5.784697e-02 5.362481e-01 3.939492e-01 1.195568e-02   Fold02\n548    F    F 7.166662e-02 6.151653e-01 2.982563e-01 1.491182e-02   Fold02\n549    F    F 1.657426e-01 3.991192e-01 3.225297e-01 1.126085e-01   Fold02\n550    F   VF 7.258770e-01 2.362982e-01 3.739437e-02 4.304325e-04   Fold02\n551    F   VF 8.005867e-01 1.820171e-01 1.731205e-02 8.411125e-05   Fold02\n552    F   VF 8.767429e-01 1.154079e-01 7.832883e-03 1.635613e-05   Fold02\n553    F   VF 8.327172e-01 1.502375e-01 1.699956e-02 4.574769e-05   Fold02\n554    F   VF 8.453146e-01 1.436240e-01 1.101829e-02 4.314354e-05   Fold02\n555    F   VF 8.624319e-01 1.238483e-01 1.369450e-02 2.521888e-05   Fold02\n556    F   VF 3.992397e-01 3.917702e-01 1.957093e-01 1.328071e-02   Fold02\n557    F    L 3.127208e-03 1.294986e-02 1.202146e-02 9.719015e-01   Fold02\n558    F   VF 8.872386e-01 1.052646e-01 7.480712e-03 1.602307e-05   Fold02\n559    F    L 1.120897e-02 3.230391e-02 2.087358e-02 9.356135e-01   Fold02\n560    F    L 8.762418e-03 2.956494e-02 2.954238e-02 9.321303e-01   Fold02\n561    F    L 1.093375e-02 3.138264e-02 2.031907e-02 9.373645e-01   Fold02\n562    F    L 1.043267e-02 3.038563e-02 1.995677e-02 9.392249e-01   Fold02\n563    F   VF 5.746233e-01 4.140088e-01 1.129850e-02 6.941587e-05   Fold02\n564    F   VF 5.620471e-01 4.256348e-01 1.223649e-02 8.157160e-05   Fold02\n565    F   VF 8.610198e-01 1.276057e-01 1.134059e-02 3.400389e-05   Fold02\n566    F   VF 5.381490e-01 4.475273e-01 1.421287e-02 1.108366e-04   Fold02\n567    F   VF 5.111945e-01 4.719165e-01 1.673382e-02 1.551305e-04   Fold02\n568    F   VF 7.699202e-01 2.001013e-01 2.970838e-02 2.701353e-04   Fold02\n569    F   VF 7.948845e-01 1.866775e-01 1.826235e-02 1.756498e-04   Fold02\n570    F   VF 9.795826e-01 1.874203e-02 1.671375e-03 3.992563e-06   Fold02\n571    F   VF 7.874256e-01 1.915075e-01 2.099317e-02 7.374726e-05   Fold02\n572    F   VF 7.769866e-01 1.907255e-01 3.205191e-02 2.359688e-04   Fold02\n573    F    F 8.942033e-02 7.909088e-01 1.190577e-01 6.131789e-04   Fold02\n574    F    F 7.474766e-02 8.101168e-01 1.143002e-01 8.353492e-04   Fold02\n575    F    F 8.114063e-02 7.922197e-01 1.259186e-01 7.211102e-04   Fold02\n576    F    F 9.173913e-02 7.956632e-01 1.121554e-01 4.422693e-04   Fold02\n577    F    F 1.062660e-01 7.889385e-01 1.044723e-01 3.232096e-04   Fold02\n578    F    F 9.156847e-02 7.970780e-01 1.109151e-01 4.384080e-04   Fold02\n579    F    F 6.668764e-02 7.431637e-01 1.893753e-01 7.732824e-04   Fold02\n580    F    F 6.697180e-02 7.407331e-01 1.914532e-01 8.419209e-04   Fold02\n581    F    F 6.486086e-02 7.980260e-01 1.357559e-01 1.357252e-03   Fold02\n582    F    F 8.143671e-02 7.444362e-01 1.735469e-01 5.801566e-04   Fold02\n583    F    F 8.048734e-02 7.826013e-01 1.362195e-01 6.918490e-04   Fold02\n584    F    F 1.414639e-02 8.936077e-01 9.115827e-02 1.087643e-03   Fold02\n585    F    F 2.086121e-02 9.216700e-01 5.721956e-02 2.492490e-04   Fold02\n586    F    F 2.091490e-02 9.216902e-01 5.714598e-02 2.488968e-04   Fold02\n587    F    F 6.406898e-02 7.729428e-01 1.624209e-01 5.673279e-04   Fold02\n588    F    F 7.854926e-02 7.978535e-01 1.229202e-01 6.770825e-04   Fold02\n589    F    F 4.876235e-02 7.798743e-01 1.688005e-01 2.562847e-03   Fold02\n590    F    F 1.131987e-01 7.890271e-01 9.743116e-02 3.429883e-04   Fold02\n591    F    F 3.671448e-02 7.458882e-01 2.140881e-01 3.309201e-03   Fold02\n592    F   VF 7.388899e-01 2.409270e-01 2.015112e-02 3.202613e-05   Fold02\n593    F   VF 7.114176e-01 2.521971e-01 3.635056e-02 3.470208e-05   Fold02\n594    F   VF 7.585858e-01 2.190040e-01 2.238740e-02 2.281452e-05   Fold02\n595    F   VF 6.139739e-01 3.363592e-01 4.953831e-02 1.285798e-04   Fold02\n596    F   VF 5.818455e-01 3.629288e-01 5.509122e-02 1.345015e-04   Fold02\n597    F    F 2.998387e-01 4.837705e-01 1.900607e-01 2.633013e-02   Fold02\n598    F   VF 7.241694e-01 2.440489e-01 3.175242e-02 2.925671e-05   Fold02\n599    F   VF 7.286605e-01 2.421100e-01 2.920051e-02 2.900695e-05   Fold02\n600    F    F 3.890649e-01 4.437612e-01 1.556593e-01 1.151457e-02   Fold02\n601    F    F 3.433363e-01 5.068771e-01 1.264373e-01 2.334919e-02   Fold02\n602    F   VF 7.167028e-01 2.484577e-01 3.478820e-02 5.130090e-05   Fold02\n603    F   VF 6.881027e-01 2.694130e-01 4.241398e-02 7.037093e-05   Fold02\n604    F   VF 6.158479e-01 3.339392e-01 5.003054e-02 1.823443e-04   Fold02\n605    F   VF 5.974018e-01 3.392405e-01 6.313442e-02 2.232891e-04   Fold02\n606    F   VF 5.696431e-01 3.651180e-01 6.489017e-02 3.486786e-04   Fold02\n607    F    F 7.915629e-02 7.828172e-01 1.321479e-01 5.878643e-03   Fold02\n608    F    F 4.727071e-02 7.281883e-01 2.111359e-01 1.340515e-02   Fold02\n609    F    F 6.755609e-02 7.261621e-01 1.992753e-01 7.006518e-03   Fold02\n610    F    F 5.450717e-02 7.794905e-01 1.505651e-01 1.543720e-02   Fold02\n611    F    F 6.798731e-02 7.292482e-01 1.953709e-01 7.393614e-03   Fold02\n612    F    F 6.108301e-02 7.689275e-01 1.603186e-01 9.670894e-03   Fold02\n613    F    F 4.008462e-02 7.209531e-01 2.004599e-01 3.850235e-02   Fold02\n614    F    F 5.847910e-02 7.691995e-01 1.641074e-01 8.214028e-03   Fold02\n615    F    F 6.657739e-02 7.280552e-01 1.978198e-01 7.547641e-03   Fold02\n616    F    F 5.917923e-02 7.698187e-01 1.619595e-01 9.042552e-03   Fold02\n617    F    F 5.798133e-02 7.705209e-01 1.629620e-01 8.535820e-03   Fold02\n618    F    F 5.740832e-02 7.644713e-01 1.686606e-01 9.459866e-03   Fold02\n619    F    F 5.816218e-02 7.657588e-01 1.658153e-01 1.026369e-02   Fold02\n620    F    F 5.539678e-02 7.644403e-01 1.708569e-01 9.306100e-03   Fold02\n621    F    F 3.765972e-02 7.097985e-01 2.083064e-01 4.423536e-02   Fold02\n622    F    F 2.950286e-02 6.338495e-01 2.881120e-01 4.853561e-02   Fold02\n623    F    F 3.748774e-02 7.071996e-01 2.092828e-01 4.602983e-02   Fold02\n624    F    F 5.685145e-02 7.659570e-01 1.672912e-01 9.900321e-03   Fold02\n625    F    F 6.375265e-02 7.231553e-01 2.046883e-01 8.403682e-03   Fold02\n626    F    F 5.325097e-02 7.296654e-01 2.095650e-01 7.518588e-03   Fold02\n627    F    F 6.276755e-02 7.233756e-01 2.070655e-01 6.791439e-03   Fold02\n628    F    F 9.893980e-02 7.853264e-01 1.129401e-01 2.793640e-03   Fold02\n629    F    F 7.081369e-02 7.657014e-01 1.556605e-01 7.824371e-03   Fold02\n630    F    F 8.384278e-02 7.856998e-01 1.254083e-01 5.049120e-03   Fold02\n631    F    F 8.892719e-02 7.497157e-01 1.576898e-01 3.667363e-03   Fold02\n632    F    F 5.632333e-02 7.503343e-01 1.853607e-01 7.981672e-03   Fold02\n633    M    F 3.983214e-01 5.256473e-01 7.465441e-02 1.376903e-03   Fold02\n634    M    M 7.651372e-02 1.325920e-01 5.020904e-01 2.888039e-01   Fold02\n635    M   VF 6.497912e-01 3.161153e-01 3.383116e-02 2.623379e-04   Fold02\n636    M   VF 5.809087e-01 3.588429e-01 5.984878e-02 3.995841e-04   Fold02\n637    M   VF 6.075920e-01 3.220632e-01 7.003494e-02 3.098231e-04   Fold02\n638    M    F 2.452603e-01 5.401834e-01 2.090279e-01 5.528439e-03   Fold02\n639    M    L 3.007043e-03 1.573121e-02 2.274223e-02 9.585195e-01   Fold02\n640    M    F 1.265774e-01 6.213291e-01 2.498779e-01 2.215535e-03   Fold02\n641    M    F 1.196205e-01 6.155522e-01 2.620966e-01 2.730691e-03   Fold02\n642    M    F 1.136184e-01 6.142928e-01 2.681346e-01 3.954289e-03   Fold02\n643    M    M 1.098018e-04 4.347828e-03 9.948806e-01 6.617344e-04   Fold02\n644    M   VF 8.586051e-01 1.156624e-01 2.571338e-02 1.913549e-05   Fold02\n645    M    L 6.024651e-03 1.987037e-02 1.574310e-02 9.583619e-01   Fold02\n646    M   VF 8.296687e-01 1.567095e-01 1.354236e-02 7.949161e-05   Fold02\n647    M    L 2.393823e-03 1.174486e-02 9.204439e-03 9.766569e-01   Fold02\n648    M    F 1.291783e-01 6.812891e-01 1.791814e-01 1.035116e-02   Fold02\n649    M    F 2.398213e-01 4.456009e-01 2.867785e-01 2.779924e-02   Fold02\n650    M    F 6.128864e-02 6.096166e-01 2.855401e-01 4.355460e-02   Fold02\n651    M    M 6.777802e-04 2.739801e-02 9.718307e-01 9.353896e-05   Fold02\n652    M    F 1.212227e-01 7.801304e-01 9.824369e-02 4.032484e-04   Fold02\n653    M    F 3.327596e-02 7.272549e-01 2.309759e-01 8.493172e-03   Fold02\n654    M    F 8.983263e-02 7.959676e-01 1.137239e-01 4.759090e-04   Fold02\n655    M    F 7.515527e-02 7.835037e-01 1.406318e-01 7.092342e-04   Fold02\n656    M    F 7.444434e-02 7.717680e-01 1.527219e-01 1.065738e-03   Fold02\n657    M    F 4.868711e-02 7.806771e-01 1.681730e-01 2.462760e-03   Fold02\n658    M    F 2.695811e-02 7.171030e-01 2.512020e-01 4.736846e-03   Fold02\n659    M    F 2.734568e-02 7.220950e-01 2.457406e-01 4.818753e-03   Fold02\n660    M    F 3.224273e-02 7.365918e-01 2.253597e-01 5.805701e-03   Fold02\n661    M    M 3.066435e-04 1.917751e-02 9.801098e-01 4.060053e-04   Fold02\n662    M    F 1.146995e-01 7.880227e-01 9.696402e-02 3.137385e-04   Fold02\n663    M    F 1.977585e-01 4.592753e-01 3.157922e-01 2.717404e-02   Fold02\n664    M    L 1.373166e-03 1.394217e-02 2.195217e-02 9.627325e-01   Fold02\n665    M    M 1.134322e-01 3.900928e-01 4.636648e-01 3.281022e-02   Fold02\n666    M    L 2.613576e-03 2.173369e-02 2.983233e-02 9.458204e-01   Fold02\n667    M    M 3.256553e-02 2.118359e-01 6.638327e-01 9.176585e-02   Fold02\n668    M    L 4.471984e-03 2.322972e-01 1.508328e-01 6.123980e-01   Fold02\n669    M    L 6.726248e-03 3.069938e-01 1.735241e-01 5.127559e-01   Fold02\n670    M    F 3.870958e-02 6.973682e-01 2.436004e-01 2.032188e-02   Fold02\n671    M    F 5.304247e-02 6.963184e-01 2.413112e-01 9.327967e-03   Fold02\n672    M    F 7.314746e-02 7.692408e-01 1.504190e-01 7.192694e-03   Fold02\n673    M    F 5.368413e-02 7.407970e-01 1.927339e-01 1.278498e-02   Fold02\n674    L    L 2.682881e-05 1.005804e-03 2.256919e-03 9.967104e-01   Fold02\n675    L    L 6.578103e-03 1.230803e-01 3.174556e-01 5.528861e-01   Fold02\n676    L    M 1.709187e-02 6.866608e-02 5.390337e-01 3.752083e-01   Fold02\n677    L    F 1.080682e-01 4.319136e-01 3.945128e-01 6.550538e-02   Fold02\n678    L    L 5.960813e-03 1.359096e-01 3.569841e-01 5.011455e-01   Fold02\n679    L    M 8.795239e-03 3.634550e-02 9.544583e-01 4.009878e-04   Fold02\n680    L    F 1.675067e-01 6.073871e-01 2.241777e-01 9.284058e-04   Fold02\n681    L    L 1.774776e-04 2.521971e-02 2.183373e-01 7.562655e-01   Fold02\n682    L    L 1.121212e-15 3.075492e-11 9.445284e-07 9.999991e-01   Fold02\n683    L   VF 4.776179e-01 3.752606e-01 1.385000e-01 8.621542e-03   Fold02\n684    L    F 2.965333e-01 3.990771e-01 2.707408e-01 3.364881e-02   Fold02\n685    L    F 8.237883e-02 8.070884e-01 1.100057e-01 5.270311e-04   Fold02\n686    L    M 4.145261e-10 4.866262e-06 9.713251e-01 2.867001e-02   Fold02\n687    L    F 4.476781e-02 7.478842e-01 2.049752e-01 2.372757e-03   Fold02\n688    L    M 1.655237e-02 2.070121e-01 5.156354e-01 2.608002e-01   Fold02\n689    L    F 1.002525e-02 4.207830e-01 3.838384e-01 1.853533e-01   Fold02\n690    L    L 4.428292e-08 3.819962e-05 4.199812e-04 9.995418e-01   Fold02\n691    L    L 6.079371e-07 2.164161e-04 7.289791e-04 9.990540e-01   Fold02\n692    L    L 1.155710e-06 3.388861e-04 1.001724e-03 9.986582e-01   Fold02\n693    L    L 5.217781e-07 1.980299e-04 7.044606e-04 9.990970e-01   Fold02\n694    L    L 2.422451e-08 2.457347e-05 6.508882e-05 9.999103e-01   Fold02\n695   VF   VF 9.392230e-01 5.564279e-02 5.126282e-03 7.903643e-06   Fold03\n696   VF   VF 9.279738e-01 6.424454e-02 7.766806e-03 1.482258e-05   Fold03\n697   VF   VF 9.268542e-01 6.526833e-02 7.862435e-03 1.500361e-05   Fold03\n698   VF   VF 9.558025e-01 4.096856e-02 3.225398e-03 3.588761e-06   Fold03\n699   VF    F 4.358237e-01 5.060691e-01 5.730369e-02 8.035155e-04   Fold03\n700   VF    F 4.020163e-01 5.120798e-01 8.475018e-02 1.153708e-03   Fold03\n701   VF   VF 9.613405e-01 3.296303e-02 5.693844e-03 2.644232e-06   Fold03\n702   VF   VF 8.472555e-01 1.182683e-01 3.439174e-02 8.450579e-05   Fold03\n703   VF   VF 8.402251e-01 1.288317e-01 3.082307e-02 1.200905e-04   Fold03\n704   VF   VF 9.581481e-01 3.669287e-02 5.156099e-03 2.896971e-06   Fold03\n705   VF    M 1.267927e-01 1.553620e-01 4.108299e-01 3.070154e-01   Fold03\n706   VF   VF 9.501685e-01 4.116818e-02 8.657078e-03 6.200444e-06   Fold03\n707   VF   VF 9.285738e-01 5.852711e-02 1.288159e-02 1.749436e-05   Fold03\n708   VF   VF 8.848870e-01 9.343222e-02 2.158779e-02 9.295748e-05   Fold03\n709   VF   VF 9.669487e-01 2.854314e-02 4.506335e-03 1.801954e-06   Fold03\n710   VF   VF 9.496143e-01 4.337365e-02 7.007050e-03 5.019080e-06   Fold03\n711   VF   VF 9.884603e-01 1.101876e-02 5.206515e-04 3.087186e-07   Fold03\n712   VF   VF 9.801738e-01 1.840550e-02 1.419072e-03 1.616664e-06   Fold03\n713   VF   VF 9.868503e-01 1.247554e-02 6.736414e-04 5.179917e-07   Fold03\n714   VF   VF 9.802129e-01 1.875819e-02 1.026683e-03 2.275079e-06   Fold03\n715   VF   VF 9.869620e-01 1.225224e-02 7.852858e-04 4.335800e-07   Fold03\n716   VF   VF 9.883945e-01 1.106485e-02 5.403483e-04 3.386952e-07   Fold03\n717   VF   VF 9.807026e-01 1.804614e-02 1.249204e-03 2.007531e-06   Fold03\n718   VF   VF 8.875990e-01 8.568285e-02 2.469079e-02 2.027392e-03   Fold03\n719   VF   VF 9.867302e-01 1.245872e-02 8.106536e-04 4.607080e-07   Fold03\n720   VF   VF 9.882153e-01 1.121579e-02 5.685413e-04 3.803888e-07   Fold03\n721   VF   VF 9.796699e-01 1.881504e-02 1.513193e-03 1.858814e-06   Fold03\n722   VF   VF 9.820615e-01 1.710486e-02 8.321449e-04 1.469724e-06   Fold03\n723   VF   VF 9.826882e-01 1.651403e-02 7.964481e-04 1.370977e-06   Fold03\n724   VF   VF 6.594747e-01 2.949310e-01 4.533126e-02 2.630411e-04   Fold03\n725   VF   VF 6.592468e-01 2.965471e-01 4.394435e-02 2.617817e-04   Fold03\n726   VF   VF 6.608818e-01 2.953026e-01 4.355896e-02 2.566478e-04   Fold03\n727   VF   VF 7.452897e-01 2.274627e-01 2.716565e-02 8.190427e-05   Fold03\n728   VF   VF 7.457022e-01 2.265311e-01 2.768051e-02 8.618131e-05   Fold03\n729   VF   VF 7.491082e-01 2.201834e-01 3.066233e-02 4.606636e-05   Fold03\n730   VF   VF 6.987303e-01 2.627552e-01 3.842949e-02 8.501460e-05   Fold03\n731   VF   VF 7.989117e-01 1.876049e-01 1.346623e-02 1.709659e-05   Fold03\n732   VF   VF 6.763675e-01 2.733648e-01 5.012965e-02 1.379981e-04   Fold03\n733   VF   VF 7.640487e-01 2.092697e-01 2.665498e-02 2.666085e-05   Fold03\n734   VF   VF 8.374506e-01 1.489598e-01 1.358379e-02 5.780933e-06   Fold03\n735   VF   VF 8.004533e-01 1.813132e-01 1.821620e-02 1.726164e-05   Fold03\n736   VF   VF 7.432825e-01 2.258068e-01 3.087039e-02 4.033248e-05   Fold03\n737   VF   VF 8.388702e-01 1.486197e-01 1.250147e-02 8.548363e-06   Fold03\n738   VF   VF 8.176418e-01 1.710762e-01 1.126872e-02 1.328076e-05   Fold03\n739   VF   VF 7.614249e-01 2.159714e-01 2.257310e-02 3.066625e-05   Fold03\n740   VF   VF 8.146284e-01 1.739305e-01 1.142757e-02 1.348164e-05   Fold03\n741   VF   VF 9.113213e-01 8.171942e-02 6.939740e-03 1.954126e-05   Fold03\n742   VF   VF 9.643057e-01 3.420873e-02 1.482391e-03 3.153675e-06   Fold03\n743   VF   VF 9.811015e-01 1.815922e-02 7.387606e-04 5.109052e-07   Fold03\n744   VF   VF 9.847973e-01 1.479319e-02 4.091296e-04 4.196520e-07   Fold03\n745   VF   VF 9.846547e-01 1.482882e-02 5.160670e-04 3.795033e-07   Fold03\n746   VF   VF 9.900181e-01 9.714996e-03 2.667659e-04 1.294148e-07   Fold03\n747   VF   VF 9.546495e-01 4.207893e-02 3.267929e-03 3.593721e-06   Fold03\n748   VF   VF 5.911263e-01 2.741069e-01 1.278836e-01 6.883176e-03   Fold03\n749   VF   VF 9.282367e-01 6.518282e-02 6.563286e-03 1.715652e-05   Fold03\n750   VF   VF 9.760305e-01 2.294782e-02 1.020833e-03 8.600715e-07   Fold03\n751   VF   VF 9.691215e-01 2.935466e-02 1.521685e-03 2.144241e-06   Fold03\n752   VF   VF 9.929682e-01 6.873015e-03 1.587575e-04 4.405447e-08   Fold03\n753   VF   VF 9.877471e-01 1.184856e-02 4.041479e-04 2.304148e-07   Fold03\n754   VF   VF 9.873311e-01 1.212555e-02 5.430330e-04 2.804092e-07   Fold03\n755   VF   VF 9.501511e-01 4.625279e-02 3.590335e-03 5.815424e-06   Fold03\n756   VF   VF 9.921400e-01 7.629045e-03 2.308779e-04 5.669937e-08   Fold03\n757   VF   VF 9.883805e-01 1.117214e-02 4.472339e-04 1.530048e-07   Fold03\n758   VF   VF 9.287307e-01 6.579097e-02 5.460968e-03 1.739852e-05   Fold03\n759   VF   VF 9.801709e-01 1.900260e-02 8.257498e-04 7.754086e-07   Fold03\n760   VF   VF 9.755256e-01 2.327062e-02 1.201810e-03 1.953329e-06   Fold03\n761   VF   VF 9.798449e-01 1.930250e-02 8.517956e-04 8.222916e-07   Fold03\n762   VF   VF 9.300129e-01 6.391093e-02 6.067488e-03 8.718683e-06   Fold03\n763   VF   VF 9.805046e-01 1.873738e-02 7.574714e-04 5.046502e-07   Fold03\n764   VF   VF 9.795468e-01 1.981689e-02 6.354304e-04 8.368963e-07   Fold03\n765   VF   VF 9.805469e-01 1.869470e-02 7.578769e-04 5.075936e-07   Fold03\n766   VF   VF 9.265481e-01 6.695094e-02 6.491344e-03 9.623311e-06   Fold03\n767   VF   VF 9.869379e-01 1.256675e-02 4.950177e-04 2.895791e-07   Fold03\n768   VF   VF 9.121322e-01 8.369869e-02 4.164247e-03 4.834862e-06   Fold03\n769   VF   VF 8.979620e-01 9.570816e-02 6.322900e-03 6.957500e-06   Fold03\n770   VF   VF 8.715645e-01 1.199055e-01 8.517494e-03 1.248711e-05   Fold03\n771   VF   VF 9.104444e-01 8.528634e-02 4.263968e-03 5.325609e-06   Fold03\n772   VF   VF 7.419438e-01 2.227862e-01 3.482566e-02 4.442902e-04   Fold03\n773   VF   VF 8.800780e-01 1.123520e-01 7.555535e-03 1.444826e-05   Fold03\n774   VF   VF 8.853884e-01 1.086927e-01 5.907520e-03 1.140008e-05   Fold03\n775   VF   VF 8.667098e-01 1.270105e-01 6.257848e-03 2.184434e-05   Fold03\n776   VF   VF 8.803371e-01 1.129420e-01 6.705961e-03 1.494930e-05   Fold03\n777   VF   VF 8.492308e-01 1.394479e-01 1.129388e-02 2.742322e-05   Fold03\n778   VF   VF 9.066880e-01 8.882851e-02 4.477689e-03 5.816374e-06   Fold03\n779   VF   VF 8.848504e-01 1.087866e-01 6.349113e-03 1.386669e-05   Fold03\n780   VF   VF 8.699768e-01 1.268590e-01 3.096513e-03 6.772825e-05   Fold03\n781   VF   VF 8.998407e-01 9.485890e-02 5.292233e-03 8.214716e-06   Fold03\n782   VF   VF 8.509021e-01 1.413763e-01 7.689166e-03 3.242639e-05   Fold03\n783   VF   VF 8.796325e-01 1.151424e-01 5.210034e-03 1.502907e-05   Fold03\n784   VF   VF 8.769491e-01 1.157187e-01 7.313540e-03 1.871126e-05   Fold03\n785   VF   VF 8.894992e-01 1.029685e-01 7.521891e-03 1.036536e-05   Fold03\n786   VF   VF 8.977795e-01 9.720317e-02 5.008882e-03 8.412984e-06   Fold03\n787   VF   VF 8.929136e-01 1.011669e-01 5.909464e-03 9.994019e-06   Fold03\n788   VF   VF 8.711450e-01 1.208872e-01 7.946911e-03 2.091696e-05   Fold03\n789   VF   VF 8.843023e-01 1.074765e-01 8.208851e-03 1.230764e-05   Fold03\n790   VF   VF 6.659727e-01 3.244629e-01 9.523925e-03 4.047995e-05   Fold03\n791   VF   VF 8.971974e-01 9.781915e-02 4.975277e-03 8.205929e-06   Fold03\n792   VF   VF 5.032426e-01 3.814974e-01 1.015945e-01 1.366545e-02   Fold03\n793   VF   VF 6.645134e-01 3.258245e-01 9.620714e-03 4.139244e-05   Fold03\n794   VF   VF 9.064028e-01 8.822294e-02 5.369146e-03 5.091921e-06   Fold03\n795   VF   VF 8.750559e-01 1.154066e-01 9.520975e-03 1.658778e-05   Fold03\n796   VF   VF 8.654768e-01 1.260426e-01 8.455876e-03 2.468839e-05   Fold03\n797   VF   VF 8.616184e-01 1.296156e-01 8.740117e-03 2.587981e-05   Fold03\n798   VF   VF 8.371978e-01 1.533981e-01 9.353928e-03 5.024660e-05   Fold03\n799   VF   VF 6.496066e-01 3.396914e-01 1.065022e-02 5.183943e-05   Fold03\n800   VF   VF 8.740537e-01 1.161750e-01 9.753580e-03 1.776014e-05   Fold03\n801   VF   VF 8.716347e-01 1.184471e-01 9.900273e-03 1.792111e-05   Fold03\n802   VF   VF 8.330411e-01 1.570524e-01 9.850802e-03 5.575315e-05   Fold03\n803   VF   VF 6.429843e-01 3.458306e-01 1.112802e-02 5.711923e-05   Fold03\n804   VF   VF 8.609694e-01 1.291667e-01 9.843335e-03 2.063052e-05   Fold03\n805   VF   VF 8.290460e-01 1.604630e-01 1.042711e-02 6.389821e-05   Fold03\n806   VF   VF 7.964457e-01 1.822667e-01 2.119342e-02 9.418904e-05   Fold03\n807   VF   VF 8.957732e-01 9.916889e-02 5.049535e-03 8.406956e-06   Fold03\n808   VF   VF 9.079104e-01 8.691073e-02 5.174128e-03 4.749185e-06   Fold03\n809   VF   VF 8.500978e-01 1.362361e-01 1.363081e-02 3.537794e-05   Fold03\n810   VF   VF 8.612644e-01 1.288733e-01 9.841424e-03 2.083000e-05   Fold03\n811   VF   VF 8.583328e-01 1.315883e-01 1.005757e-02 2.130047e-05   Fold03\n812   VF   VF 9.131736e-01 8.277826e-02 4.043321e-03 4.769412e-06   Fold03\n813   VF   VF 8.552671e-01 1.343370e-01 1.037345e-02 2.241287e-05   Fold03\n814   VF   VF 8.556431e-01 1.339977e-01 1.033695e-02 2.228085e-05   Fold03\n815   VF   VF 8.211794e-01 1.610783e-01 1.765680e-02 8.553195e-05   Fold03\n816   VF   VF 8.755137e-01 1.187721e-01 5.695689e-03 1.855613e-05   Fold03\n817   VF   VF 7.885334e-01 1.961051e-01 1.512463e-02 2.368031e-04   Fold03\n818   VF    M 1.263451e-01 1.140569e-01 7.586469e-01 9.510511e-04   Fold03\n819   VF   VF 7.249015e-01 2.691187e-01 5.968456e-03 1.129729e-05   Fold03\n820   VF   VF 8.842447e-01 1.084543e-01 7.291335e-03 9.678451e-06   Fold03\n821   VF   VF 9.529146e-01 4.474163e-02 2.342346e-03 1.458571e-06   Fold03\n822   VF   VF 9.422102e-01 5.289932e-02 4.886851e-03 3.642372e-06   Fold03\n823   VF   VF 9.417975e-01 5.324827e-02 4.950514e-03 3.739225e-06   Fold03\n824   VF   VF 9.519758e-01 4.558440e-02 2.438183e-03 1.592066e-06   Fold03\n825   VF   VF 9.248098e-01 7.139790e-02 3.787163e-03 5.116255e-06   Fold03\n826   VF   VF 9.301914e-01 6.349037e-02 6.307700e-03 1.057405e-05   Fold03\n827   VF   VF 9.597167e-01 3.878048e-02 1.501562e-03 1.290955e-06   Fold03\n828   VF   VF 9.172161e-01 7.713098e-02 5.646671e-03 6.279466e-06   Fold03\n829   VF   VF 9.746732e-01 2.442757e-02 8.989900e-04 2.350919e-07   Fold03\n830   VF   VF 9.700690e-01 2.881139e-02 1.119140e-03 4.266565e-07   Fold03\n831   VF   VF 9.709701e-01 2.793094e-02 1.098677e-03 3.180557e-07   Fold03\n832   VF   VF 6.839242e-01 2.453762e-01 6.737754e-02 3.322102e-03   Fold03\n833   VF   VF 6.819018e-01 2.465202e-01 6.816854e-02 3.409529e-03   Fold03\n834   VF   VF 9.253815e-01 6.958580e-02 5.027041e-03 5.681560e-06   Fold03\n835   VF   VF 9.728140e-01 2.614983e-02 1.035834e-03 3.072756e-07   Fold03\n836   VF   VF 9.445171e-01 5.309288e-02 2.387568e-03 2.411309e-06   Fold03\n837   VF   VF 9.372981e-01 5.868858e-02 4.008756e-03 4.548621e-06   Fold03\n838   VF   VF 9.183214e-01 7.660856e-02 5.058464e-03 1.160730e-05   Fold03\n839   VF   VF 9.831354e-01 1.635161e-02 5.128911e-04 1.091153e-07   Fold03\n840   VF    F 6.881919e-02 8.006391e-01 1.294338e-01 1.107907e-03   Fold03\n841   VF    F 1.051248e-01 7.560201e-01 1.384562e-01 3.988133e-04   Fold03\n842   VF    F 1.101269e-01 7.626259e-01 1.268403e-01 4.069053e-04   Fold03\n843   VF    F 9.869068e-02 7.736366e-01 1.271642e-01 5.084899e-04   Fold03\n844   VF   VF 7.462537e-01 2.323087e-01 2.141621e-02 2.140225e-05   Fold03\n845   VF   VF 6.251812e-01 3.124039e-01 6.231557e-02 9.931975e-05   Fold03\n846   VF   VF 6.229012e-01 3.129475e-01 6.405106e-02 1.002310e-04   Fold03\n847   VF   VF 7.323538e-01 2.480646e-01 1.954906e-02 3.252233e-05   Fold03\n848   VF   VF 7.886699e-01 1.912924e-01 2.002699e-02 1.064792e-05   Fold03\n849   VF   VF 8.035513e-01 1.815389e-01 1.490032e-02 9.475853e-06   Fold03\n850   VF   VF 8.237516e-01 1.640444e-01 1.219765e-02 6.307038e-06   Fold03\n851   VF   VF 7.668469e-01 2.084251e-01 2.471426e-02 1.369435e-05   Fold03\n852   VF   VF 7.022987e-01 2.729257e-01 2.472595e-02 4.958135e-05   Fold03\n853   VF   VF 6.988402e-01 2.755559e-01 2.555175e-02 5.210822e-05   Fold03\n854   VF   VF 6.394964e-01 3.106537e-01 4.977321e-02 7.667232e-05   Fold03\n855   VF   VF 7.756700e-01 2.067451e-01 1.756997e-02 1.496507e-05   Fold03\n856   VF   VF 7.797257e-01 2.036967e-01 1.656449e-02 1.318375e-05   Fold03\n857   VF   VF 7.465390e-01 2.303978e-01 2.304222e-02 2.089943e-05   Fold03\n858   VF   VF 7.215241e-01 2.504493e-01 2.798982e-02 3.676387e-05   Fold03\n859   VF   VF 7.399997e-01 2.288784e-01 3.109535e-02 2.653663e-05   Fold03\n860   VF   VF 7.034967e-01 2.632680e-01 3.320005e-02 3.532754e-05   Fold03\n861   VF   VF 6.445971e-01 3.194818e-01 3.580639e-02 1.148051e-04   Fold03\n862   VF   VF 7.320085e-01 2.398031e-01 2.815567e-02 3.279742e-05   Fold03\n863   VF   VF 7.331425e-01 2.438912e-01 2.294322e-02 2.310897e-05   Fold03\n864   VF   VF 7.700787e-01 2.115268e-01 1.837815e-02 1.635769e-05   Fold03\n865   VF   VF 8.223500e-01 1.649237e-01 1.271923e-02 7.058193e-06   Fold03\n866   VF   VF 7.316958e-01 2.445975e-01 2.368166e-02 2.498649e-05   Fold03\n867   VF   VF 5.702618e-01 3.586759e-01 7.087108e-02 1.912234e-04   Fold03\n868   VF   VF 7.998659e-01 1.821500e-01 1.797559e-02 8.492663e-06   Fold03\n869   VF   VF 7.075103e-01 2.815187e-01 1.086288e-02 1.081776e-04   Fold03\n870   VF    F 8.159334e-02 7.737307e-01 1.400741e-01 4.601857e-03   Fold03\n871   VF    F 1.046355e-01 7.644900e-01 1.280983e-01 2.776131e-03   Fold03\n872    F    F 3.540986e-01 5.308143e-01 1.136629e-01 1.424174e-03   Fold03\n873    F    F 3.533564e-01 5.312092e-01 1.139999e-01 1.434525e-03   Fold03\n874    F    F 2.353658e-01 6.112217e-01 1.450075e-01 8.404941e-03   Fold03\n875    F    L 4.372793e-03 1.201951e-02 4.561737e-02 9.379903e-01   Fold03\n876    F   VF 5.759077e-01 3.696989e-01 5.371752e-02 6.758832e-04   Fold03\n877    F    F 1.883441e-01 4.755853e-01 2.101694e-01 1.259011e-01   Fold03\n878    F   VF 6.685251e-01 2.831099e-01 4.806674e-02 2.981937e-04   Fold03\n879    F   VF 4.728610e-01 4.187487e-01 1.074421e-01 9.481711e-04   Fold03\n880    F   VF 7.043760e-01 2.438260e-01 5.173929e-02 5.875339e-05   Fold03\n881    F   VF 7.422347e-01 2.263846e-01 3.134546e-02 3.525053e-05   Fold03\n882    F    F 1.473783e-01 6.235839e-01 2.259827e-01 3.055166e-03   Fold03\n883    F    F 1.770840e-01 6.298784e-01 1.915425e-01 1.495065e-03   Fold03\n884    F    F 1.864512e-01 6.220353e-01 1.900090e-01 1.504523e-03   Fold03\n885    F    F 1.259547e-01 6.467871e-01 2.219376e-01 5.320594e-03   Fold03\n886    F    F 1.553489e-01 6.110715e-01 2.309690e-01 2.610576e-03   Fold03\n887    F    F 1.487979e-01 6.155093e-01 2.330665e-01 2.626335e-03   Fold03\n888    F    F 2.089614e-01 6.112888e-01 1.785086e-01 1.241261e-03   Fold03\n889    F    F 1.182466e-01 5.987673e-01 2.793565e-01 3.629574e-03   Fold03\n890    F    F 1.661052e-01 6.295469e-01 2.021633e-01 2.184594e-03   Fold03\n891    F    F 1.648194e-01 6.245039e-01 2.082812e-01 2.395464e-03   Fold03\n892    F    F 1.636482e-01 6.295576e-01 2.045294e-01 2.264787e-03   Fold03\n893    F    F 1.649903e-01 6.255494e-01 2.071084e-01 2.351929e-03   Fold03\n894    F    F 1.229100e-01 6.005473e-01 2.719365e-01 4.606283e-03   Fold03\n895    F    F 8.629696e-02 5.900552e-01 3.148407e-01 8.807080e-03   Fold03\n896    F    F 1.758394e-01 5.948332e-01 2.279126e-01 1.414840e-03   Fold03\n897    F   VF 4.420686e-01 3.508475e-01 1.766232e-01 3.046063e-02   Fold03\n898    F    M 1.029488e-02 2.819522e-01 5.202463e-01 1.875066e-01   Fold03\n899    F   VF 8.353851e-01 1.502623e-01 1.429772e-02 5.492664e-05   Fold03\n900    F   VF 7.396070e-01 2.275676e-01 3.261378e-02 2.115947e-04   Fold03\n901    F   VF 8.860341e-01 1.067541e-01 7.203563e-03 8.268316e-06   Fold03\n902    F   VF 5.458471e-01 3.640810e-01 7.816135e-02 1.191057e-02   Fold03\n903    F   VF 8.703835e-01 1.219420e-01 7.655743e-03 1.873776e-05   Fold03\n904    F   VF 8.618427e-01 1.277091e-01 1.042621e-02 2.191307e-05   Fold03\n905    F   VF 6.387919e-01 3.502259e-01 1.092870e-02 5.351545e-05   Fold03\n906    F    F 2.839755e-01 3.836801e-01 1.546675e-01 1.776769e-01   Fold03\n907    F   VF 6.248190e-01 3.631537e-01 1.196186e-02 6.546686e-05   Fold03\n908    F   VF 6.181529e-01 3.692961e-01 1.247898e-02 7.197495e-05   Fold03\n909    F   VF 8.368846e-01 1.510865e-01 1.198071e-02 4.815147e-05   Fold03\n910    F   VF 6.067329e-01 3.797811e-01 1.340145e-02 8.448944e-05   Fold03\n911    F   VF 8.055987e-01 1.803959e-01 1.390314e-02 1.022703e-04   Fold03\n912    F   VF 5.996193e-01 3.862708e-01 1.401635e-02 9.358922e-05   Fold03\n913    F    L 8.524922e-05 2.234945e-03 2.408019e-02 9.735996e-01   Fold03\n914    F   VF 5.742063e-01 4.093547e-01 1.630701e-02 1.319582e-04   Fold03\n915    F   VF 9.717652e-01 2.694631e-02 1.287533e-03 9.666119e-07   Fold03\n916    F   VF 8.363827e-01 1.471572e-01 1.642426e-02 3.579539e-05   Fold03\n917    F    F 1.158874e-01 7.625865e-01 1.211718e-01 3.543477e-04   Fold03\n918    F    F 1.114302e-01 7.507726e-01 1.375780e-01 2.191449e-04   Fold03\n919    F    F 8.669760e-02 7.401071e-01 1.727261e-01 4.692867e-04   Fold03\n920    F    F 9.113469e-02 7.668267e-01 1.415153e-01 5.232718e-04   Fold03\n921    F    F 3.530895e-02 7.564321e-01 1.997931e-01 8.465890e-03   Fold03\n922    F    F 6.166866e-02 7.389124e-01 1.982256e-01 1.193335e-03   Fold03\n923    F    F 7.371191e-02 7.655725e-01 1.598148e-01 9.008125e-04   Fold03\n924    F    F 5.088020e-02 7.295245e-01 2.179852e-01 1.610087e-03   Fold03\n925    F    F 5.061282e-02 7.165887e-01 2.307206e-01 2.077958e-03   Fold03\n926    F    F 7.380273e-02 7.286040e-01 1.967118e-01 8.815344e-04   Fold03\n927    F    F 5.512098e-02 7.506676e-01 1.923198e-01 1.891618e-03   Fold03\n928    F    F 1.174967e-01 7.713990e-01 1.108324e-01 2.718980e-04   Fold03\n929    F    F 5.924173e-02 7.422609e-01 1.973511e-01 1.146199e-03   Fold03\n930    F   VF 7.468682e-01 2.231821e-01 2.993311e-02 1.658765e-05   Fold03\n931    F   VF 7.946475e-01 1.895168e-01 1.582539e-02 1.027144e-05   Fold03\n932    F   VF 7.449534e-01 2.289364e-01 2.609149e-02 1.871477e-05   Fold03\n933    F   VF 7.247479e-01 2.390013e-01 3.622860e-02 2.222573e-05   Fold03\n934    F   VF 6.797407e-01 2.926026e-01 2.759423e-02 6.248056e-05   Fold03\n935    F   VF 5.648404e-01 3.686881e-01 6.633937e-02 1.321243e-04   Fold03\n936    F    F 4.132978e-01 4.567719e-01 1.253573e-01 4.573032e-03   Fold03\n937    F   VF 6.443989e-01 3.090027e-01 4.651065e-02 8.772436e-05   Fold03\n938    F   VF 6.922013e-01 2.807384e-01 2.699894e-02 6.140870e-05   Fold03\n939    F   VF 7.315882e-01 2.401492e-01 2.822952e-02 3.307213e-05   Fold03\n940    F   VF 7.213794e-01 2.490669e-01 2.951560e-02 3.805647e-05   Fold03\n941    F   VF 6.108565e-01 3.459362e-01 4.303741e-02 1.699160e-04   Fold03\n942    F    F 9.637341e-02 7.756533e-01 1.253351e-01 2.638176e-03   Fold03\n943    F    F 7.783564e-02 7.716949e-01 1.452918e-01 5.177637e-03   Fold03\n944    F    F 8.504192e-02 7.509727e-01 1.609365e-01 3.048814e-03   Fold03\n945    F    F 7.026779e-02 7.614137e-01 1.618097e-01 6.508742e-03   Fold03\n946    F    F 8.429041e-02 7.589022e-01 1.527088e-01 4.098520e-03   Fold03\n947    F    F 4.815380e-02 7.198692e-01 2.203600e-01 1.161697e-02   Fold03\n948    F    F 8.389576e-02 7.612911e-01 1.502971e-01 4.515984e-03   Fold03\n949    F    F 8.380880e-02 7.610330e-01 1.506374e-01 4.520850e-03   Fold03\n950    F    F 8.252936e-02 7.587367e-01 1.540052e-01 4.728691e-03   Fold03\n951    F    F 7.028264e-02 7.276529e-01 1.966984e-01 5.366100e-03   Fold03\n952    F    F 1.239095e-02 4.351246e-01 2.436475e-01 3.088370e-01   Fold03\n953    F    F 1.229227e-02 4.331899e-01 2.434650e-01 3.110529e-01   Fold03\n954    F    F 1.234697e-02 4.342977e-01 2.435751e-01 3.097802e-01   Fold03\n955    F    F 5.743368e-02 7.336986e-01 2.016725e-01 7.195292e-03   Fold03\n956    F    F 6.290046e-02 7.568948e-01 1.715253e-01 8.679464e-03   Fold03\n957    F    F 6.246914e-02 7.589677e-01 1.702362e-01 8.326922e-03   Fold03\n958    F    F 9.829972e-03 3.927726e-01 2.844917e-01 3.129057e-01   Fold03\n959    F    L 4.024299e-03 2.397707e-01 2.193841e-01 5.368209e-01   Fold03\n960    F    F 6.778540e-02 7.272993e-01 1.990189e-01 5.896387e-03   Fold03\n961    F    F 6.743404e-02 7.262747e-01 2.003532e-01 5.938047e-03   Fold03\n962    F    F 6.124635e-02 7.582358e-01 1.717287e-01 8.789146e-03   Fold03\n963    F    F 6.771751e-02 7.270321e-01 1.993249e-01 5.925492e-03   Fold03\n964    F    F 7.073383e-02 7.246412e-01 1.995330e-01 5.092033e-03   Fold03\n965    F    F 3.401149e-02 6.549582e-01 2.693798e-01 4.165057e-02   Fold03\n966    F    F 7.521999e-02 7.538390e-01 1.652866e-01 5.654493e-03   Fold03\n967    F    F 7.307687e-02 7.476104e-01 1.734450e-01 5.867740e-03   Fold03\n968    F    F 3.734269e-02 6.795467e-01 2.404207e-01 4.268994e-02   Fold03\n969    F    F 4.697655e-02 7.671940e-01 1.685934e-01 1.723608e-02   Fold03\n970    F    F 4.020351e-02 6.875439e-01 2.562089e-01 1.604369e-02   Fold03\n971    F    F 5.240731e-02 6.992286e-01 2.404752e-01 7.888891e-03   Fold03\n972    F    F 2.836140e-02 6.468776e-01 2.741945e-01 5.056644e-02   Fold03\n973    F    F 6.131289e-02 7.181065e-01 2.129501e-01 7.630520e-03   Fold03\n974    F    F 8.564945e-02 7.759095e-01 1.344031e-01 4.037910e-03   Fold03\n975    F    F 8.619696e-02 7.761259e-01 1.337077e-01 3.969458e-03   Fold03\n976    F    F 5.553452e-02 7.006927e-01 2.342750e-01 9.497798e-03   Fold03\n977    F    F 5.147259e-02 7.404170e-01 1.947041e-01 1.340626e-02   Fold03\n978    F    F 4.369032e-02 6.890095e-01 2.454337e-01 2.186646e-02   Fold03\n979    F    F 6.041874e-02 7.397875e-01 1.924210e-01 7.372818e-03   Fold03\n980    M    L 5.444004e-05 2.420574e-03 6.672767e-03 9.908522e-01   Fold03\n981    M    F 2.930861e-01 5.570176e-01 1.470793e-01 2.816969e-03   Fold03\n982    M    M 6.188884e-02 1.060684e-01 5.156488e-01 3.163940e-01   Fold03\n983    M   VF 5.745374e-01 3.567988e-01 6.802719e-02 6.366150e-04   Fold03\n984    M    M 1.503479e-01 2.091982e-01 6.389696e-01 1.484298e-03   Fold03\n985    M    L 4.039152e-03 3.599219e-02 6.135541e-02 8.986132e-01   Fold03\n986    M    F 8.957530e-02 5.740522e-01 3.299908e-01 6.381768e-03   Fold03\n987    M    M 2.004250e-02 3.377952e-01 4.574460e-01 1.847162e-01   Fold03\n988    M    L 6.484979e-05 9.793014e-03 1.057020e-01 8.844401e-01   Fold03\n989    M    F 6.876398e-02 5.584170e-01 3.612436e-01 1.157542e-02   Fold03\n990    M    F 4.933345e-02 5.324810e-01 3.968646e-01 2.132091e-02   Fold03\n991    M    L 1.151238e-16 3.571048e-12 2.363631e-08 1.000000e+00   Fold03\n992    M   VF 4.437889e-01 3.772830e-01 1.691550e-01 9.773100e-03   Fold03\n993    M   VF 8.098501e-01 1.776638e-01 1.240127e-02 8.487338e-05   Fold03\n994    M    F 5.035868e-02 7.292495e-01 2.187718e-01 1.620040e-03   Fold03\n995    M    F 1.591372e-02 5.559532e-01 3.744950e-01 5.363810e-02   Fold03\n996    M    F 1.034300e-02 5.053350e-01 4.073158e-01 7.700626e-02   Fold03\n997    M    F 7.343431e-02 7.142307e-01 2.115413e-01 7.936731e-04   Fold03\n998    M    M 1.560828e-03 8.709903e-02 9.090435e-01 2.296656e-03   Fold03\n999    M    F 4.385307e-02 7.585045e-01 1.935751e-01 4.067374e-03   Fold03\n1000   M    F 4.028271e-02 7.499593e-01 2.045453e-01 5.212762e-03   Fold03\n1001   M    M 2.509783e-04 2.213174e-02 9.753955e-01 2.221836e-03   Fold03\n1002   M    M 3.576303e-04 2.692344e-02 9.705451e-01 2.173871e-03   Fold03\n1003   M   VF 5.728841e-01 3.519188e-01 7.493481e-02 2.622893e-04   Fold03\n1004   M    M 3.174502e-01 2.621071e-01 4.202760e-01 1.667150e-04   Fold03\n1005   M    M 4.934287e-02 2.949898e-01 4.985497e-01 1.571176e-01   Fold03\n1006   M    M 2.727811e-02 2.487362e-01 3.989441e-01 3.250416e-01   Fold03\n1007   M    L 1.729866e-02 2.006728e-01 3.762784e-01 4.057501e-01   Fold03\n1008   M    F 1.076474e-02 4.369761e-01 3.612924e-01 1.909668e-01   Fold03\n1009   M    F 7.220459e-02 7.681644e-01 1.535758e-01 6.055216e-03   Fold03\n1010   M    F 4.449073e-02 7.164882e-01 2.265876e-01 1.243342e-02   Fold03\n1011   M    F 6.735155e-02 7.496803e-01 1.757775e-01 7.190654e-03   Fold03\n1012   M    M 1.281622e-02 2.943056e-01 6.825489e-01 1.032935e-02   Fold03\n1013   M    L 4.096351e-03 2.493710e-01 2.225870e-01 5.239457e-01   Fold03\n1014   M    F 2.130507e-02 6.302240e-01 2.457806e-01 1.026904e-01   Fold03\n1015   M    F 1.231547e-02 4.448805e-01 4.365100e-01 1.062940e-01   Fold03\n1016   M    F 3.491930e-02 6.578087e-01 2.634041e-01 4.386792e-02   Fold03\n1017   M    F 6.698115e-02 7.222661e-01 2.042124e-01 6.540262e-03   Fold03\n1018   M    L 7.415832e-06 2.999487e-03 2.120026e-02 9.757928e-01   Fold03\n1019   M    F 5.390159e-02 6.903476e-01 2.475205e-01 8.230259e-03   Fold03\n1020   M    M 1.909148e-04 1.859897e-02 9.661282e-01 1.508196e-02   Fold03\n1021   L    L 3.312804e-03 8.827622e-02 2.169937e-01 6.914173e-01   Fold03\n1022   L    L 5.637959e-03 1.180896e-01 3.247385e-01 5.515339e-01   Fold03\n1023   L    L 8.317836e-03 1.414187e-01 2.934367e-01 5.568268e-01   Fold03\n1024   L    L 4.329726e-03 3.187172e-02 2.231624e-01 7.406361e-01   Fold03\n1025   L    M 9.178392e-02 3.661547e-01 4.202717e-01 1.217896e-01   Fold03\n1026   L   VF 6.697401e-01 2.747438e-01 5.535763e-02 1.584428e-04   Fold03\n1027   L   VF 5.005163e-01 3.724797e-01 1.219454e-01 5.058590e-03   Fold03\n1028   L    L 1.939620e-06 7.135014e-04 3.137242e-03 9.961473e-01   Fold03\n1029   L    L 1.020992e-06 4.292575e-04 1.656421e-03 9.979133e-01   Fold03\n1030   L    L 1.986119e-06 7.325902e-04 3.188006e-03 9.960774e-01   Fold03\n1031   L    L 6.371432e-07 3.204722e-04 1.039003e-03 9.986399e-01   Fold03\n1032   L    L 1.896363e-06 6.550454e-04 2.380164e-03 9.969629e-01   Fold03\n1033   L    L 3.996971e-07 2.352830e-04 1.332700e-03 9.984316e-01   Fold03\n1034   L    L 2.625446e-07 1.755800e-04 1.148032e-03 9.986761e-01   Fold03\n1035   L    F 4.135671e-02 6.867167e-01 2.466560e-01 2.527066e-02   Fold03\n1036   L    F 3.468346e-02 6.462154e-01 2.939892e-01 2.511203e-02   Fold03\n1037   L    F 3.449856e-02 6.450260e-01 2.950125e-01 2.546292e-02   Fold03\n1038   L    L 1.572751e-07 1.186895e-04 9.338258e-04 9.989473e-01   Fold03\n1039   L    F 2.615199e-02 6.814479e-01 2.313285e-01 6.107164e-02   Fold03\n1040   L    L 1.415139e-07 1.105443e-04 8.941587e-04 9.989952e-01   Fold03\n1041   L    L 1.415870e-07 1.110914e-04 8.964077e-04 9.989924e-01   Fold03\n1042  VF   VF 9.490835e-01 4.692744e-02 3.979715e-03 9.354902e-06   Fold04\n1043  VF   VF 9.368736e-01 5.837632e-02 4.726538e-03 2.356136e-05   Fold04\n1044  VF   VF 9.419571e-01 5.289479e-02 5.133313e-03 1.479434e-05   Fold04\n1045  VF   VF 9.495949e-01 4.647279e-02 3.924226e-03 8.130057e-06   Fold04\n1046  VF   VF 9.560343e-01 4.094228e-02 3.018159e-03 5.219952e-06   Fold04\n1047  VF    F 4.024636e-01 5.201454e-01 7.346159e-02 3.929330e-03   Fold04\n1048  VF    F 3.983631e-01 5.229582e-01 7.419519e-02 4.483520e-03   Fold04\n1049  VF    F 4.485242e-01 4.566480e-01 9.241303e-02 2.414742e-03   Fold04\n1050  VF   VF 8.542799e-01 1.120422e-01 3.356851e-02 1.094160e-04   Fold04\n1051  VF   VF 9.428220e-01 4.867167e-02 8.493972e-03 1.236620e-05   Fold04\n1052  VF   VF 9.579077e-01 3.549387e-02 6.592321e-03 6.134470e-06   Fold04\n1053  VF   VF 9.538248e-01 4.084357e-02 5.318264e-03 1.336774e-05   Fold04\n1054  VF   VF 9.648597e-01 3.017644e-02 4.958754e-03 5.105878e-06   Fold04\n1055  VF   VF 8.463139e-01 1.204034e-01 3.316480e-02 1.178593e-04   Fold04\n1056  VF   VF 8.367207e-01 1.300700e-01 3.308588e-02 1.234324e-04   Fold04\n1057  VF   VF 8.169434e-01 1.431333e-01 3.976275e-02 1.605350e-04   Fold04\n1058  VF   VF 9.866742e-01 1.271735e-02 6.077234e-04 7.127221e-07   Fold04\n1059  VF   VF 9.841909e-01 1.496642e-02 8.414767e-04 1.242338e-06   Fold04\n1060  VF   VF 9.839288e-01 1.517245e-02 8.978547e-04 9.413513e-07   Fold04\n1061  VF   VF 9.208087e-01 6.300970e-02 1.591867e-02 2.629672e-04   Fold04\n1062  VF   VF 9.882166e-01 1.130100e-02 4.819190e-04 5.097991e-07   Fold04\n1063  VF   VF 9.876504e-01 1.167864e-02 6.703187e-04 6.703820e-07   Fold04\n1064  VF   VF 9.797472e-01 1.892384e-02 1.326329e-03 2.648737e-06   Fold04\n1065  VF   VF 9.797240e-01 1.894494e-02 1.328423e-03 2.631827e-06   Fold04\n1066  VF   VF 9.799453e-01 1.872219e-02 1.330063e-03 2.411200e-06   Fold04\n1067  VF   VF 9.706538e-01 2.665036e-02 2.686635e-03 9.161298e-06   Fold04\n1068  VF   VF 9.773411e-01 2.097939e-02 1.674717e-03 4.789429e-06   Fold04\n1069  VF   VF 9.587211e-01 3.616167e-02 5.090470e-03 2.679516e-05   Fold04\n1070  VF   VF 9.714356e-01 2.613455e-02 2.421085e-03 8.722617e-06   Fold04\n1071  VF   VF 9.798756e-01 1.878844e-02 1.333292e-03 2.623697e-06   Fold04\n1072  VF   VF 9.534074e-01 4.034023e-02 6.211159e-03 4.123400e-05   Fold04\n1073  VF   VF 9.554464e-01 3.868621e-02 5.830376e-03 3.698824e-05   Fold04\n1074  VF   VF 7.058981e-01 2.655423e-01 2.842160e-02 1.380323e-04   Fold04\n1075  VF   VF 6.730294e-01 2.907375e-01 3.599533e-02 2.378348e-04   Fold04\n1076  VF   VF 5.900136e-01 3.573006e-01 5.231706e-02 3.687319e-04   Fold04\n1077  VF   VF 7.519954e-01 2.129942e-01 3.493095e-02 7.944730e-05   Fold04\n1078  VF   VF 7.275729e-01 2.375997e-01 3.472494e-02 1.024028e-04   Fold04\n1079  VF   VF 8.120016e-01 1.724700e-01 1.550669e-02 2.169241e-05   Fold04\n1080  VF   VF 8.297790e-01 1.539733e-01 1.622910e-02 1.857475e-05   Fold04\n1081  VF   VF 8.656607e-01 1.262700e-01 8.062868e-03 6.487603e-06   Fold04\n1082  VF    F 1.390873e-01 6.779899e-01 1.809609e-01 1.961909e-03   Fold04\n1083  VF    F 1.742615e-01 6.797750e-01 1.446255e-01 1.337943e-03   Fold04\n1084  VF    F 7.885064e-02 6.346601e-01 2.791928e-01 7.296469e-03   Fold04\n1085  VF    F 1.061842e-01 6.632031e-01 2.271859e-01 3.426801e-03   Fold04\n1086  VF   VF 9.312361e-01 6.472675e-02 4.023824e-03 1.333134e-05   Fold04\n1087  VF   VF 9.625568e-01 3.600943e-02 1.428071e-03 5.655049e-06   Fold04\n1088  VF   VF 9.856584e-01 1.387037e-02 4.707727e-04 4.580338e-07   Fold04\n1089  VF   VF 9.567771e-01 4.065650e-02 2.561280e-03 5.070758e-06   Fold04\n1090  VF   VF 9.918604e-01 7.878278e-03 2.611400e-04 1.413577e-07   Fold04\n1091  VF   VF 9.305116e-01 6.441802e-02 5.051035e-03 1.933191e-05   Fold04\n1092  VF   VF 9.754840e-01 2.346389e-02 1.050510e-03 1.554109e-06   Fold04\n1093  VF   VF 9.463898e-01 5.081244e-02 2.789981e-03 7.811681e-06   Fold04\n1094  VF   VF 9.675186e-01 3.059948e-02 1.877954e-03 4.004145e-06   Fold04\n1095  VF   VF 9.845204e-01 1.479363e-02 6.852900e-04 6.432270e-07   Fold04\n1096  VF   VF 9.884730e-01 1.113541e-02 3.913484e-04 2.413772e-07   Fold04\n1097  VF   VF 9.596102e-01 3.819908e-02 2.185512e-03 5.239654e-06   Fold04\n1098  VF   VF 9.931033e-01 6.705364e-03 1.912957e-04 8.175935e-08   Fold04\n1099  VF   VF 9.573024e-01 3.979194e-02 2.896562e-03 9.098018e-06   Fold04\n1100  VF   VF 9.602619e-01 3.781867e-02 1.910162e-03 9.228143e-06   Fold04\n1101  VF   VF 9.803101e-01 1.887708e-02 8.116109e-04 1.251663e-06   Fold04\n1102  VF   VF 9.850779e-01 1.428703e-02 6.341889e-04 8.878616e-07   Fold04\n1103  VF   VF 9.887432e-01 1.096658e-02 2.900573e-04 2.059739e-07   Fold04\n1104  VF   VF 9.748119e-01 2.348315e-02 1.700756e-03 4.236806e-06   Fold04\n1105  VF   VF 9.053934e-01 9.010106e-02 4.495938e-03 9.644080e-06   Fold04\n1106  VF   VF 7.248399e-01 2.702064e-01 4.940653e-03 1.300953e-05   Fold04\n1107  VF   VF 8.916412e-01 1.037191e-01 4.620059e-03 1.965660e-05   Fold04\n1108  VF   VF 9.198160e-01 7.693374e-02 3.244697e-03 5.561639e-06   Fold04\n1109  VF   VF 8.871361e-01 1.065072e-01 6.344281e-03 1.238284e-05   Fold04\n1110  VF   VF 8.971669e-01 9.751790e-02 5.302857e-03 1.233415e-05   Fold04\n1111  VF   VF 9.048962e-01 9.054162e-02 4.551425e-03 1.077577e-05   Fold04\n1112  VF   VF 9.161118e-01 8.034624e-02 3.535332e-03 6.601958e-06   Fold04\n1113  VF   VF 9.138958e-01 8.222272e-02 3.874517e-03 6.918695e-06   Fold04\n1114  VF   VF 8.562803e-01 1.366468e-01 7.032004e-03 4.092568e-05   Fold04\n1115  VF   VF 9.197429e-01 7.611668e-02 4.132493e-03 7.925842e-06   Fold04\n1116  VF   VF 9.171247e-01 7.945476e-02 3.413403e-03 7.123946e-06   Fold04\n1117  VF   VF 9.114867e-01 8.345696e-02 5.047097e-03 9.204820e-06   Fold04\n1118  VF   VF 8.866229e-01 1.083170e-01 5.035278e-03 2.482225e-05   Fold04\n1119  VF   VF 6.690188e-01 3.231913e-01 7.754009e-03 3.597151e-05   Fold04\n1120  VF   VF 9.031394e-01 9.225776e-02 4.593474e-03 9.368788e-06   Fold04\n1121  VF   VF 8.645700e-01 1.260869e-01 9.310860e-03 3.215975e-05   Fold04\n1122  VF   VF 8.611848e-01 1.293649e-01 9.415456e-03 3.480975e-05   Fold04\n1123  VF   VF 8.923356e-01 1.018925e-01 5.756862e-03 1.503035e-05   Fold04\n1124  VF   VF 9.071108e-01 8.868490e-02 4.194812e-03 9.498352e-06   Fold04\n1125  VF   VF 8.920809e-01 1.020695e-01 5.833948e-03 1.564449e-05   Fold04\n1126  VF   VF 8.939562e-01 1.003313e-01 5.697618e-03 1.487891e-05   Fold04\n1127  VF   VF 9.077422e-01 8.797567e-02 4.274301e-03 7.870545e-06   Fold04\n1128  VF   VF 8.721365e-01 1.214859e-01 6.341910e-03 3.575367e-05   Fold04\n1129  VF   VF 9.070680e-01 8.859348e-02 4.330629e-03 7.906743e-06   Fold04\n1130  VF   VF 8.983386e-01 9.502278e-02 6.624341e-03 1.423832e-05   Fold04\n1131  VF   VF 5.622481e-01 3.430227e-01 8.326714e-02 1.146206e-02   Fold04\n1132  VF   VF 6.444029e-01 3.463090e-01 9.236766e-03 5.140102e-05   Fold04\n1133  VF   VF 8.763533e-01 1.159404e-01 7.682867e-03 2.345142e-05   Fold04\n1134  VF   VF 8.767258e-01 1.155498e-01 7.701141e-03 2.326043e-05   Fold04\n1135  VF   VF 8.764988e-01 1.157472e-01 7.731106e-03 2.289460e-05   Fold04\n1136  VF   VF 8.817641e-01 1.111410e-01 7.073198e-03 2.178251e-05   Fold04\n1137  VF   VF 8.785576e-01 1.139547e-01 7.464918e-03 2.280785e-05   Fold04\n1138  VF   VF 8.940590e-01 9.835130e-02 7.570600e-03 1.905690e-05   Fold04\n1139  VF   VF 6.183442e-01 3.705538e-01 1.102792e-02 7.404197e-05   Fold04\n1140  VF   VF 6.561446e-01 2.909416e-01 4.866451e-02 4.249312e-03   Fold04\n1141  VF   VF 6.138994e-01 3.746599e-01 1.136196e-02 7.880931e-05   Fold04\n1142  VF   VF 8.501724e-01 1.410172e-01 8.743783e-03 6.668391e-05   Fold04\n1143  VF   VF 6.123078e-01 3.761399e-01 1.147193e-02 8.039090e-05   Fold04\n1144  VF   VF 9.062039e-01 8.942879e-02 4.358655e-03 8.668332e-06   Fold04\n1145  VF   VF 9.101912e-01 8.573065e-02 4.070183e-03 7.917182e-06   Fold04\n1146  VF   VF 8.786620e-01 1.139258e-01 7.392613e-03 1.954940e-05   Fold04\n1147  VF   VF 8.981703e-01 9.778552e-02 4.027871e-03 1.635622e-05   Fold04\n1148  VF   VF 8.279667e-01 1.585132e-01 1.339939e-02 1.207447e-04   Fold04\n1149  VF   VF 8.551195e-01 1.341901e-01 1.064409e-02 4.634080e-05   Fold04\n1150  VF   VF 8.862436e-01 1.089078e-01 4.827013e-03 2.156779e-05   Fold04\n1151  VF   VF 9.199269e-01 7.686812e-02 3.199522e-03 5.431105e-06   Fold04\n1152  VF   VF 8.766466e-01 1.156226e-01 7.709056e-03 2.169074e-05   Fold04\n1153  VF   VF 8.725806e-01 1.192799e-01 8.116388e-03 2.309909e-05   Fold04\n1154  VF   VF 9.106451e-01 8.537347e-02 3.976342e-03 5.043963e-06   Fold04\n1155  VF   VF 8.582263e-01 1.324620e-01 9.234065e-03 7.763589e-05   Fold04\n1156  VF   VF 9.639168e-01 3.463475e-02 1.447392e-03 1.067201e-06   Fold04\n1157  VF   VF 9.519173e-01 4.583625e-02 2.244499e-03 1.972510e-06   Fold04\n1158  VF   VF 9.308866e-01 6.347862e-02 5.621150e-03 1.363726e-05   Fold04\n1159  VF   VF 9.411366e-01 5.585819e-02 3.002042e-03 3.192009e-06   Fold04\n1160  VF   VF 9.217358e-01 7.306731e-02 5.189266e-03 7.617206e-06   Fold04\n1161  VF   VF 9.621897e-01 3.622772e-02 1.581343e-03 1.256631e-06   Fold04\n1162  VF   VF 9.805010e-01 1.874137e-02 7.573463e-04 3.280279e-07   Fold04\n1163  VF   VF 9.159188e-01 7.988790e-02 4.181394e-03 1.188657e-05   Fold04\n1164  VF   VF 8.902166e-01 1.012664e-01 8.498562e-03 1.839318e-05   Fold04\n1165  VF   VF 9.773114e-01 2.201317e-02 6.751738e-04 2.919907e-07   Fold04\n1166  VF   VF 8.557715e-01 1.224370e-01 2.122121e-02 5.703154e-04   Fold04\n1167  VF   VF 7.793771e-01 1.804575e-01 3.877004e-02 1.395403e-03   Fold04\n1168  VF   VF 9.671867e-01 3.162426e-02 1.187242e-03 1.807285e-06   Fold04\n1169  VF   VF 8.370633e-01 1.494222e-01 1.347841e-02 3.613001e-05   Fold04\n1170  VF   VF 8.785756e-01 1.105459e-01 1.084748e-02 3.099234e-05   Fold04\n1171  VF   VF 9.765783e-01 2.239927e-02 1.021927e-03 4.527373e-07   Fold04\n1172  VF   VF 9.204351e-01 7.511103e-02 4.439415e-03 1.444528e-05   Fold04\n1173  VF   VF 7.490162e-01 2.291696e-01 2.173362e-02 8.063904e-05   Fold04\n1174  VF   VF 9.657924e-01 3.288728e-02 1.319455e-03 9.029590e-07   Fold04\n1175  VF   VF 9.252672e-01 6.895248e-02 5.768639e-03 1.172918e-05   Fold04\n1176  VF   VF 9.702317e-01 2.883237e-02 9.349599e-04 9.713495e-07   Fold04\n1177  VF    F 1.073563e-01 7.584374e-01 1.336799e-01 5.264300e-04   Fold04\n1178  VF    F 1.194957e-01 7.609080e-01 1.192022e-01 3.940765e-04   Fold04\n1179  VF    F 5.607753e-02 7.304695e-01 2.121608e-01 1.292225e-03   Fold04\n1180  VF    F 9.477878e-02 7.565455e-01 1.480428e-01 6.328549e-04   Fold04\n1181  VF    F 7.761712e-02 7.418849e-01 1.795383e-01 9.596590e-04   Fold04\n1182  VF   VF 7.281280e-01 2.540134e-01 1.781699e-02 4.161407e-05   Fold04\n1183  VF   VF 6.329862e-01 3.124071e-01 5.448141e-02 1.252252e-04   Fold04\n1184  VF   VF 6.058137e-01 3.409729e-01 5.305457e-02 1.588545e-04   Fold04\n1185  VF   VF 6.013546e-01 3.420630e-01 5.641810e-02 1.642219e-04   Fold04\n1186  VF   VF 7.540775e-01 2.240403e-01 2.186204e-02 2.016545e-05   Fold04\n1187  VF   VF 7.313800e-01 2.507736e-01 1.780594e-02 4.039136e-05   Fold04\n1188  VF   VF 6.341736e-01 3.371841e-01 2.855141e-02 9.096713e-05   Fold04\n1189  VF   VF 7.316192e-01 2.429638e-01 2.539223e-02 2.474358e-05   Fold04\n1190  VF   VF 6.605708e-01 3.032062e-01 3.616202e-02 6.093159e-05   Fold04\n1191  VF   VF 6.743991e-01 3.001131e-01 2.541639e-02 7.140347e-05   Fold04\n1192  VF   VF 6.722327e-01 3.022636e-01 2.542983e-02 7.385872e-05   Fold04\n1193  VF   VF 7.138924e-01 2.674151e-01 1.865088e-02 4.169724e-05   Fold04\n1194  VF   VF 7.953357e-01 1.923854e-01 1.226872e-02 1.018115e-05   Fold04\n1195  VF   VF 7.949116e-01 1.925073e-01 1.257137e-02 9.812698e-06   Fold04\n1196  VF   VF 7.656067e-01 2.177809e-01 1.659596e-02 1.645208e-05   Fold04\n1197  VF   VF 6.757429e-01 2.928918e-01 3.131422e-02 5.111241e-05   Fold04\n1198  VF   VF 7.023233e-01 2.645291e-01 3.310419e-02 4.338658e-05   Fold04\n1199  VF   VF 7.296353e-01 2.418644e-01 2.846242e-02 3.784495e-05   Fold04\n1200  VF   VF 5.759153e-01 3.666233e-01 5.731145e-02 1.499210e-04   Fold04\n1201  VF   VF 7.635380e-01 2.163151e-01 2.013032e-02 1.662757e-05   Fold04\n1202  VF   VF 7.351508e-01 2.466174e-01 1.821288e-02 1.897018e-05   Fold04\n1203  VF   VF 6.807021e-01 2.872875e-01 3.194549e-02 6.497676e-05   Fold04\n1204  VF   VF 6.242511e-01 3.399584e-01 3.564544e-02 1.450064e-04   Fold04\n1205  VF   VF 7.956550e-01 1.920022e-01 1.233245e-02 1.038600e-05   Fold04\n1206  VF   VF 6.796393e-01 2.802785e-01 4.001345e-02 6.875847e-05   Fold04\n1207  VF   VF 6.815216e-01 2.794469e-01 3.896348e-02 6.808197e-05   Fold04\n1208  VF   VF 5.850906e-01 3.567973e-01 5.792817e-02 1.840026e-04   Fold04\n1209  VF   VF 6.180350e-01 3.432023e-01 3.858186e-02 1.808121e-04   Fold04\n1210  VF   VF 7.776949e-01 2.038980e-01 1.839236e-02 1.468974e-05   Fold04\n1211  VF   VF 6.843413e-01 2.852120e-01 3.037272e-02 7.394554e-05   Fold04\n1212  VF   VF 6.820901e-01 2.870054e-01 3.082830e-02 7.616619e-05   Fold04\n1213  VF   VF 6.058583e-01 3.443860e-01 4.960420e-02 1.515114e-04   Fold04\n1214  VF   VF 7.703290e-01 2.145159e-01 1.514152e-02 1.358349e-05   Fold04\n1215  VF   VF 8.149603e-01 1.717983e-01 1.323496e-02 6.456374e-06   Fold04\n1216  VF   VF 7.154432e-01 2.588317e-01 2.569180e-02 3.329383e-05   Fold04\n1217  VF    F 9.024441e-02 7.936440e-01 1.123278e-01 3.783804e-03   Fold04\n1218  VF    F 8.113302e-02 7.881386e-01 1.269021e-01 3.826363e-03   Fold04\n1219   F   VF 9.128025e-01 7.843247e-02 8.680453e-03 8.456227e-05   Fold04\n1220   F    M 1.996093e-02 5.161912e-02 9.247519e-01 3.668002e-03   Fold04\n1221   F    F 4.319464e-01 4.922443e-01 7.259712e-02 3.212093e-03   Fold04\n1222   F    F 4.262223e-01 4.829283e-01 8.879164e-02 2.057863e-03   Fold04\n1223   F    F 3.532215e-01 5.284608e-01 1.140785e-01 4.239223e-03   Fold04\n1224   F    F 3.046457e-01 5.467679e-01 1.411733e-01 7.413074e-03   Fold04\n1225   F    M 1.498314e-01 1.450312e-01 4.496056e-01 2.555318e-01   Fold04\n1226   F    M 1.007316e-01 1.174164e-01 5.562805e-01 2.255715e-01   Fold04\n1227   F   VF 6.200583e-01 3.263891e-01 5.322428e-02 3.283264e-04   Fold04\n1228   F   VF 6.633522e-01 3.152356e-01 1.986624e-02 1.545986e-03   Fold04\n1229   F   VF 7.996571e-01 1.766726e-01 2.364665e-02 2.364052e-05   Fold04\n1230   F    F 9.292083e-02 6.561271e-01 2.466127e-01 4.339438e-03   Fold04\n1231   F    F 8.273533e-02 6.464751e-01 2.656857e-01 5.103827e-03   Fold04\n1232   F    F 1.213971e-01 6.817647e-01 1.939767e-01 2.861511e-03   Fold04\n1233   F    F 6.879513e-02 6.301348e-01 2.940248e-01 7.045216e-03   Fold04\n1234   F    F 1.300063e-01 6.766513e-01 1.911978e-01 2.144647e-03   Fold04\n1235   F    F 1.347806e-01 6.780367e-01 1.850224e-01 2.160433e-03   Fold04\n1236   F    F 1.304248e-01 6.769431e-01 1.904636e-01 2.168423e-03   Fold04\n1237   F    F 1.555538e-01 6.819247e-01 1.609369e-01 1.584638e-03   Fold04\n1238   F   VF 8.527355e-01 1.332586e-01 1.394458e-02 6.133981e-05   Fold04\n1239   F   VF 8.867293e-01 1.070325e-01 6.214211e-03 2.403136e-05   Fold04\n1240   F   VF 7.086915e-01 2.465422e-01 4.419842e-02 5.678569e-04   Fold04\n1241   F   VF 8.096465e-01 1.714833e-01 1.872457e-02 1.457144e-04   Fold04\n1242   F   VF 8.891818e-01 9.783132e-02 1.297105e-02 1.580939e-05   Fold04\n1243   F   VF 5.879235e-01 3.304124e-01 7.263709e-02 9.026970e-03   Fold04\n1244   F   VF 8.882438e-01 1.057035e-01 6.037820e-03 1.487386e-05   Fold04\n1245   F   VF 9.018274e-01 9.133489e-02 6.825806e-03 1.191152e-05   Fold04\n1246   F   VF 8.767679e-01 1.160621e-01 7.152208e-03 1.783365e-05   Fold04\n1247   F   VF 8.650968e-01 1.259030e-01 8.968789e-03 3.148272e-05   Fold04\n1248   F   VF 8.787157e-01 1.156548e-01 5.600874e-03 2.869510e-05   Fold04\n1249   F   VF 8.687502e-01 1.249336e-01 6.281697e-03 3.451903e-05   Fold04\n1250   F   VF 9.010662e-01 9.289632e-02 6.026014e-03 1.146598e-05   Fold04\n1251   F    L 1.936798e-02 6.230156e-02 7.461608e-02 8.437144e-01   Fold04\n1252   F    L 1.922193e-02 6.198429e-02 7.437094e-02 8.444228e-01   Fold04\n1253   F   VF 8.743062e-01 1.178153e-01 7.854142e-03 2.428535e-05   Fold04\n1254   F   VF 8.740579e-01 1.196471e-01 6.246719e-03 4.822180e-05   Fold04\n1255   F   VF 6.046067e-01 3.832935e-01 1.199832e-02 1.014281e-04   Fold04\n1256   F   VF 6.030975e-01 3.846830e-01 1.211591e-02 1.035081e-04   Fold04\n1257   F   VF 6.028416e-01 3.849261e-01 1.212855e-02 1.037016e-04   Fold04\n1258   F   VF 6.012674e-01 3.863643e-01 1.226212e-02 1.061686e-04   Fold04\n1259   F   VF 5.697516e-01 4.151412e-01 1.494652e-02 1.607459e-04   Fold04\n1260   F   VF 8.140425e-01 1.718580e-01 1.393191e-02 1.675634e-04   Fold04\n1261   F   VF 7.965992e-01 1.868631e-01 1.630376e-02 2.340099e-04   Fold04\n1262   F   VF 8.024273e-01 1.770249e-01 2.042866e-02 1.191161e-04   Fold04\n1263   F   VF 8.870843e-01 1.053146e-01 7.562421e-03 3.865051e-05   Fold04\n1264   F   VF 7.908417e-01 1.840245e-01 2.494983e-02 1.840392e-04   Fold04\n1265   F    F 1.066402e-01 7.508638e-01 1.419875e-01 5.085222e-04   Fold04\n1266   F    F 5.949727e-02 7.283098e-01 2.107849e-01 1.408053e-03   Fold04\n1267   F    F 1.042536e-01 7.575108e-01 1.377011e-01 5.344896e-04   Fold04\n1268   F    F 7.940113e-02 7.529101e-01 1.668303e-01 8.584882e-04   Fold04\n1269   F    F 3.529485e-02 7.179872e-01 2.354686e-01 1.124929e-02   Fold04\n1270   F    F 8.124147e-02 7.281058e-01 1.898149e-01 8.378103e-04   Fold04\n1271   F    F 4.286197e-02 6.679111e-01 2.870335e-01 2.193442e-03   Fold04\n1272   F    F 6.849180e-02 7.450171e-01 1.853938e-01 1.097342e-03   Fold04\n1273   F    F 7.843523e-02 7.546153e-01 1.660482e-01 9.012796e-04   Fold04\n1274   F    F 8.077909e-02 7.588665e-01 1.594701e-01 8.843804e-04   Fold04\n1275   F    F 4.592777e-02 7.098572e-01 2.420466e-01 2.168378e-03   Fold04\n1276   F    F 4.576100e-02 7.082984e-01 2.437354e-01 2.205175e-03   Fold04\n1277   F    F 5.686209e-02 7.335308e-01 2.081897e-01 1.417442e-03   Fold04\n1278   F    F 7.115522e-02 7.102867e-01 2.174243e-01 1.133868e-03   Fold04\n1279   F    F 8.113173e-02 7.127768e-01 2.049846e-01 1.106934e-03   Fold04\n1280   F    F 4.149896e-02 6.936841e-01 2.619102e-01 2.906704e-03   Fold04\n1281   F    F 3.970360e-02 6.881539e-01 2.692277e-01 2.914819e-03   Fold04\n1282   F    F 9.306375e-02 7.849439e-01 1.208512e-01 1.141115e-03   Fold04\n1283   F    F 6.768462e-02 7.411896e-01 1.896946e-01 1.431158e-03   Fold04\n1284   F    F 7.320005e-02 7.396520e-01 1.858772e-01 1.270728e-03   Fold04\n1285   F    F 9.966099e-02 7.595644e-01 1.401269e-01 6.476749e-04   Fold04\n1286   F    F 1.160321e-01 7.579404e-01 1.256277e-01 3.998270e-04   Fold04\n1287   F    F 6.308465e-02 6.946210e-01 2.406122e-01 1.682162e-03   Fold04\n1288   F    F 3.881023e-02 6.778948e-01 2.794784e-01 3.816520e-03   Fold04\n1289   F    F 1.092200e-01 7.295964e-01 1.606726e-01 5.110417e-04   Fold04\n1290   F   VF 7.505512e-01 2.306015e-01 1.882633e-02 2.092400e-05   Fold04\n1291   F   VF 6.238656e-01 3.336202e-01 4.243229e-02 8.186488e-05   Fold04\n1292   F   VF 6.208683e-01 3.352387e-01 4.380218e-02 9.075712e-05   Fold04\n1293   F   VF 6.611326e-01 2.936307e-01 4.518340e-02 5.336803e-05   Fold04\n1294   F    F 3.962879e-01 4.783813e-01 1.200847e-01 5.246080e-03   Fold04\n1295   F    F 3.966108e-01 4.783915e-01 1.197843e-01 5.213474e-03   Fold04\n1296   F    F 3.918280e-01 4.538552e-01 1.484856e-01 5.831220e-03   Fold04\n1297   F   VF 4.940496e-01 4.294582e-01 7.619582e-02 2.964472e-04   Fold04\n1298   F   VF 6.381803e-01 3.214420e-01 4.028412e-02 9.355571e-05   Fold04\n1299   F   VF 6.400545e-01 3.191639e-01 4.064742e-02 1.341889e-04   Fold04\n1300   F   VF 4.964986e-01 4.145982e-01 8.835640e-02 5.468109e-04   Fold04\n1301   F   VF 5.728023e-01 3.668794e-01 6.005250e-02 2.657819e-04   Fold04\n1302   F    F 7.098955e-02 7.842913e-01 1.388010e-01 5.918162e-03   Fold04\n1303   F    F 8.976533e-02 7.914803e-01 1.155898e-01 3.164590e-03   Fold04\n1304   F    F 6.999833e-02 7.576769e-01 1.651579e-01 7.166889e-03   Fold04\n1305   F    F 7.343974e-02 7.891014e-01 1.313486e-01 6.110205e-03   Fold04\n1306   F    F 7.379105e-02 7.893433e-01 1.308136e-01 6.052038e-03   Fold04\n1307   F    F 6.072570e-02 7.755125e-01 1.548759e-01 8.885965e-03   Fold04\n1308   F    F 5.855020e-02 7.737522e-01 1.585161e-01 9.181537e-03   Fold04\n1309   F    F 6.496296e-02 7.519173e-01 1.754434e-01 7.676244e-03   Fold04\n1310   F    F 6.006373e-02 7.758442e-01 1.551280e-01 8.964078e-03   Fold04\n1311   F    F 1.864899e-02 5.979236e-01 2.831926e-01 1.002348e-01   Fold04\n1312   F    F 5.756167e-02 7.723675e-01 1.606955e-01 9.375405e-03   Fold04\n1313   F    F 1.877998e-02 6.003741e-01 2.812219e-01 9.962403e-02   Fold04\n1314   F    F 6.321221e-02 7.495536e-01 1.789664e-01 8.267785e-03   Fold04\n1315   F    F 5.740976e-02 7.682985e-01 1.640102e-01 1.028158e-02   Fold04\n1316   F    F 3.386126e-02 6.998210e-01 2.170429e-01 4.927482e-02   Fold04\n1317   F    F 2.912383e-02 6.449474e-01 2.696263e-01 5.630243e-02   Fold04\n1318   F    F 2.912701e-02 6.464683e-01 2.676084e-01 5.679624e-02   Fold04\n1319   F    F 4.727081e-02 7.822910e-01 1.489883e-01 2.144985e-02   Fold04\n1320   F    F 8.769177e-02 7.988799e-01 1.097380e-01 3.690307e-03   Fold04\n1321   F    F 7.911210e-02 7.904392e-01 1.255010e-01 4.947685e-03   Fold04\n1322   F    F 3.654909e-02 7.630191e-01 1.692556e-01 3.117621e-02   Fold04\n1323   F    F 5.910244e-02 7.376326e-01 1.921041e-01 1.116087e-02   Fold04\n1324   F    F 8.089051e-02 7.900485e-01 1.242170e-01 4.843980e-03   Fold04\n1325   F    F 3.340599e-02 7.272090e-01 1.930365e-01 4.634851e-02   Fold04\n1326   F    M 4.307330e-04 2.495340e-02 9.558944e-01 1.872149e-02   Fold04\n1327   M    F 2.956260e-01 5.552362e-01 1.414121e-01 7.725724e-03   Fold04\n1328   M   VF 4.829384e-01 4.357774e-01 7.991814e-02 1.366030e-03   Fold04\n1329   M    M 5.066402e-02 8.067164e-02 5.054706e-01 3.631937e-01   Fold04\n1330   M    F 1.567516e-01 4.628297e-01 2.707526e-01 1.096661e-01   Fold04\n1331   M    F 2.742101e-01 5.420220e-01 1.778732e-01 5.894696e-03   Fold04\n1332   M    M 9.153815e-04 1.480698e-02 9.825682e-01 1.709436e-03   Fold04\n1333   M    F 7.595158e-02 6.399507e-01 2.779878e-01 6.109943e-03   Fold04\n1334   M    M 3.694707e-02 3.524006e-01 6.053621e-01 5.290215e-03   Fold04\n1335   M    L 3.303967e-04 3.629088e-02 2.830420e-01 6.803367e-01   Fold04\n1336   M    L 1.252217e-04 2.033098e-02 1.818637e-01 7.976801e-01   Fold04\n1337   M    F 1.074441e-01 6.272058e-01 2.613709e-01 3.979226e-03   Fold04\n1338   M    F 5.146101e-02 6.016517e-01 3.346702e-01 1.221705e-02   Fold04\n1339   M    M 1.782747e-04 6.840549e-03 9.884244e-01 4.556801e-03   Fold04\n1340   M    F 1.111652e-01 7.037419e-01 1.806681e-01 4.424861e-03   Fold04\n1341   M   VF 8.702941e-01 1.211282e-01 8.550503e-03 2.722259e-05   Fold04\n1342   M   VF 8.801721e-01 1.130406e-01 6.756414e-03 3.093120e-05   Fold04\n1343   M   VF 8.259951e-01 1.552294e-01 1.866082e-02 1.146301e-04   Fold04\n1344   M    M 1.954360e-02 3.009956e-01 4.330054e-01 2.464554e-01   Fold04\n1345   M    F 2.403299e-01 4.031951e-01 2.735298e-01 8.294520e-02   Fold04\n1346   M    F 1.541818e-02 4.999937e-01 4.166928e-01 6.789537e-02   Fold04\n1347   M    F 4.299353e-02 6.919680e-01 2.621376e-01 2.900884e-03   Fold04\n1348   M    F 9.182718e-02 7.846332e-01 1.223728e-01 1.166806e-03   Fold04\n1349   M    F 4.369252e-02 7.214107e-01 2.300306e-01 4.866203e-03   Fold04\n1350   M    F 2.481928e-02 6.570951e-01 3.122107e-01 5.874905e-03   Fold04\n1351   M    M 5.208726e-02 5.183565e-02 8.959241e-01 1.530123e-04   Fold04\n1352   M   VF 8.169950e-01 1.699907e-01 1.300712e-02 7.178258e-06   Fold04\n1353   M    F 1.164671e-01 4.417846e-01 4.149423e-01 2.680597e-02   Fold04\n1354   M    M 3.716724e-02 2.866859e-01 4.420779e-01 2.340689e-01   Fold04\n1355   M    M 4.509053e-02 2.663740e-01 5.661692e-01 1.223663e-01   Fold04\n1356   M    F 4.580473e-01 4.684776e-01 7.320374e-02 2.713666e-04   Fold04\n1357   M   VF 5.910763e-01 3.629874e-01 4.569391e-02 2.424085e-04   Fold04\n1358   M    F 7.344938e-02 7.956500e-01 1.260234e-01 4.877253e-03   Fold04\n1359   M    F 4.672467e-02 7.521511e-01 1.877590e-01 1.336524e-02   Fold04\n1360   M    F 4.376704e-02 7.495884e-01 1.922285e-01 1.441604e-02   Fold04\n1361   M    F 4.222282e-02 7.487576e-01 1.941984e-01 1.482117e-02   Fold04\n1362   M    F 2.548889e-02 5.553673e-01 3.971220e-01 2.202181e-02   Fold04\n1363   M    F 1.688410e-02 5.794174e-01 2.941242e-01 1.095743e-01   Fold04\n1364   M    F 2.742781e-02 6.141310e-01 3.037828e-01 5.465836e-02   Fold04\n1365   M    F 2.751481e-02 6.324279e-01 2.872637e-01 5.279361e-02   Fold04\n1366   M    F 3.762316e-02 7.211632e-01 2.193064e-01 2.190717e-02   Fold04\n1367   M    F 3.847080e-02 7.240698e-01 2.162147e-01 2.124472e-02   Fold04\n1368   L    L 3.080997e-03 3.035189e-02 2.378548e-01 7.287123e-01   Fold04\n1369   L    M 5.348338e-02 3.140373e-01 4.609642e-01 1.715151e-01   Fold04\n1370   L    M 4.170915e-02 2.936725e-01 4.669960e-01 1.976224e-01   Fold04\n1371   L    L 4.087309e-06 1.050384e-03 8.557372e-03 9.903882e-01   Fold04\n1372   L    F 9.033795e-02 6.617906e-01 2.434321e-01 4.439302e-03   Fold04\n1373   L    L 1.143324e-04 1.939195e-02 1.764739e-01 8.040198e-01   Fold04\n1374   L    L 1.782566e-04 2.729675e-02 2.060178e-01 7.665072e-01   Fold04\n1375   L   VF 5.084804e-01 3.587676e-01 1.242818e-01 8.470145e-03   Fold04\n1376   L    F 3.407465e-01 4.018003e-01 2.322666e-01 2.518663e-02   Fold04\n1377   L   VF 8.734669e-01 1.178920e-01 8.618906e-03 2.215615e-05   Fold04\n1378   L    M 2.970181e-02 2.641481e-01 5.369723e-01 1.691778e-01   Fold04\n1379   L    M 4.232681e-02 2.601169e-01 5.768431e-01 1.207132e-01   Fold04\n1380   L    F 2.443368e-02 6.095882e-01 3.122888e-01 5.368937e-02   Fold04\n1381   L    L 2.096729e-06 7.775258e-04 3.644602e-03 9.955758e-01   Fold04\n1382   L    L 7.758935e-07 3.751443e-04 1.330338e-03 9.982937e-01   Fold04\n1383   L    L 1.507153e-06 6.759062e-04 2.949761e-03 9.963728e-01   Fold04\n1384   L    L 1.490250e-06 6.705522e-04 2.934903e-03 9.963931e-01   Fold04\n1385   L    L 2.243900e-06 8.651784e-04 3.243278e-03 9.958893e-01   Fold04\n1386   L    L 2.241632e-06 8.646623e-04 3.241633e-03 9.958915e-01   Fold04\n1387   L    L 2.274129e-06 8.736200e-04 3.263240e-03 9.958609e-01   Fold04\n1388   L    L 2.244359e-06 8.614024e-04 3.212429e-03 9.959239e-01   Fold04\n1389  VF   VF 9.605942e-01 3.705498e-02 2.348191e-03 2.609714e-06   Fold05\n1390  VF   VF 9.587613e-01 3.752252e-02 3.711380e-03 4.783782e-06   Fold05\n1391  VF   VF 9.408962e-01 5.456370e-02 4.532461e-03 7.638048e-06   Fold05\n1392  VF   VF 9.387705e-01 5.394102e-02 7.270201e-03 1.825200e-05   Fold05\n1393  VF   VF 9.497857e-01 4.652623e-02 3.680317e-03 7.777788e-06   Fold05\n1394  VF   VF 9.496156e-01 4.668136e-02 3.695167e-03 7.834083e-06   Fold05\n1395  VF   VF 9.576951e-01 3.935446e-02 2.945176e-03 5.261404e-06   Fold05\n1396  VF    F 3.771941e-01 5.415848e-01 8.001618e-02 1.204926e-03   Fold05\n1397  VF   VF 8.173887e-01 1.520716e-01 3.041367e-02 1.260751e-04   Fold05\n1398  VF   VF 8.481571e-01 1.248791e-01 2.681460e-02 1.492027e-04   Fold05\n1399  VF   VF 8.709345e-01 1.049746e-01 2.388228e-02 2.086012e-04   Fold05\n1400  VF   VF 8.971459e-01 8.702092e-02 1.576983e-02 6.330088e-05   Fold05\n1401  VF   VF 8.993503e-01 8.397465e-02 1.655055e-02 1.244949e-04   Fold05\n1402  VF   VF 9.345176e-01 5.528516e-02 1.017261e-02 2.463040e-05   Fold05\n1403  VF   VF 9.799064e-01 1.855346e-02 1.538123e-03 2.055565e-06   Fold05\n1404  VF   VF 9.837202e-01 1.549661e-02 7.820243e-04 1.174037e-06   Fold05\n1405  VF   VF 9.828406e-01 1.631922e-02 8.391734e-04 1.028805e-06   Fold05\n1406  VF   VF 9.847565e-01 1.426197e-02 9.806897e-04 8.364112e-07   Fold05\n1407  VF   VF 9.868735e-01 1.240058e-02 7.254995e-04 4.679031e-07   Fold05\n1408  VF   VF 9.876059e-01 1.192559e-02 4.681797e-04 3.012926e-07   Fold05\n1409  VF   VF 9.874974e-01 1.202447e-02 4.778429e-04 3.134558e-07   Fold05\n1410  VF   VF 9.803766e-01 1.842529e-02 1.196068e-03 2.094174e-06   Fold05\n1411  VF   VF 9.024823e-01 7.342144e-02 2.201643e-02 2.079794e-03   Fold05\n1412  VF   VF 9.847317e-01 1.456726e-02 7.003237e-04 7.386442e-07   Fold05\n1413  VF   VF 9.673771e-01 2.962935e-02 2.981781e-03 1.177279e-05   Fold05\n1414  VF   VF 9.821668e-01 1.682260e-02 1.009938e-03 6.822031e-07   Fold05\n1415  VF   VF 6.385471e-01 3.203244e-01 4.092348e-02 2.049940e-04   Fold05\n1416  VF   VF 6.793888e-01 2.814869e-01 3.900031e-02 1.239612e-04   Fold05\n1417  VF   VF 5.405701e-01 4.002918e-01 5.874069e-02 3.975035e-04   Fold05\n1418  VF   VF 6.695289e-01 2.981114e-01 3.225483e-02 1.049423e-04   Fold05\n1419  VF   VF 6.819446e-01 2.852407e-01 3.270377e-02 1.108895e-04   Fold05\n1420  VF   VF 7.052342e-01 2.572191e-01 3.743872e-02 1.080039e-04   Fold05\n1421  VF   VF 6.014200e-01 3.338460e-01 6.438894e-02 3.450689e-04   Fold05\n1422  VF   VF 7.082463e-01 2.637462e-01 2.791801e-02 8.941041e-05   Fold05\n1423  VF   VF 4.609017e-01 4.301743e-01 1.064810e-01 2.442903e-03   Fold05\n1424  VF   VF 7.262305e-01 2.440273e-01 2.968968e-02 5.252876e-05   Fold05\n1425  VF   VF 6.730200e-01 2.890384e-01 3.784563e-02 9.596018e-05   Fold05\n1426  VF   VF 7.863537e-01 1.971076e-01 1.651541e-02 2.328541e-05   Fold05\n1427  VF   VF 6.461757e-01 2.854516e-01 6.818657e-02 1.861247e-04   Fold05\n1428  VF   VF 7.704952e-01 2.102571e-01 1.922144e-02 2.633121e-05   Fold05\n1429  VF   VF 7.882058e-01 1.968578e-01 1.492084e-02 1.553402e-05   Fold05\n1430  VF   VF 7.622867e-01 2.108317e-01 2.684758e-02 3.400789e-05   Fold05\n1431  VF   VF 8.172007e-01 1.650924e-01 1.769252e-02 1.440867e-05   Fold05\n1432  VF   VF 7.408474e-01 2.285996e-01 3.050875e-02 4.420517e-05   Fold05\n1433  VF   VF 8.479173e-01 1.440116e-01 8.066713e-03 4.375903e-06   Fold05\n1434  VF   VF 7.151165e-01 2.473422e-01 3.747496e-02 6.628851e-05   Fold05\n1435  VF   VF 8.092656e-01 1.773600e-01 1.335666e-02 1.782436e-05   Fold05\n1436  VF   VF 7.330243e-01 2.422536e-01 2.467551e-02 4.664131e-05   Fold05\n1437  VF    F 1.894177e-01 6.485925e-01 1.613006e-01 6.891717e-04   Fold05\n1438  VF    F 1.661467e-01 6.172565e-01 2.157185e-01 8.783162e-04   Fold05\n1439  VF    F 1.674387e-01 6.105224e-01 2.211068e-01 9.321038e-04   Fold05\n1440  VF   VF 9.541234e-01 4.137451e-02 4.484258e-03 1.780230e-05   Fold05\n1441  VF   VF 9.197979e-01 7.410014e-02 6.091978e-03 1.001248e-05   Fold05\n1442  VF   VF 9.793899e-01 1.993492e-02 6.743044e-04 8.563704e-07   Fold05\n1443  VF   VF 9.883019e-01 1.140376e-02 2.941569e-04 1.500502e-07   Fold05\n1444  VF   VF 8.985543e-01 9.162026e-02 9.796020e-03 2.944233e-05   Fold05\n1445  VF   VF 9.816609e-01 1.776049e-02 5.778515e-04 7.188157e-07   Fold05\n1446  VF   VF 9.929870e-01 6.885199e-03 1.277455e-04 3.876489e-08   Fold05\n1447  VF   VF 9.896612e-01 1.011202e-02 2.267202e-04 9.082600e-08   Fold05\n1448  VF   VF 9.493790e-01 4.814020e-02 2.474535e-03 6.273977e-06   Fold05\n1449  VF   VF 9.714688e-01 2.738950e-02 1.140221e-03 1.520983e-06   Fold05\n1450  VF   VF 9.447115e-01 5.105211e-02 4.227618e-03 8.821712e-06   Fold05\n1451  VF   VF 9.727513e-01 2.621851e-02 1.029171e-03 1.022096e-06   Fold05\n1452  VF   VF 9.860962e-01 1.352065e-02 3.828922e-04 2.368695e-07   Fold05\n1453  VF   VF 9.384028e-01 5.811573e-02 3.474913e-03 6.543775e-06   Fold05\n1454  VF   VF 9.555687e-01 3.966139e-02 4.639170e-03 1.306932e-04   Fold05\n1455  VF   VF 9.730387e-01 2.537844e-02 1.581120e-03 1.759652e-06   Fold05\n1456  VF   VF 9.882780e-01 1.127634e-02 4.453514e-04 2.667153e-07   Fold05\n1457  VF   VF 9.746762e-01 2.403484e-02 1.287701e-03 1.300389e-06   Fold05\n1458  VF   VF 7.964857e-01 1.782295e-01 2.517377e-02 1.110753e-04   Fold05\n1459  VF   VF 9.828241e-01 1.662624e-02 5.491622e-04 4.646083e-07   Fold05\n1460  VF   VF 9.921165e-01 7.664906e-03 2.185630e-04 7.096868e-08   Fold05\n1461  VF   VF 9.206628e-01 7.783154e-02 1.502193e-03 3.437319e-06   Fold05\n1462  VF   VF 9.818161e-01 1.754799e-02 6.352958e-04 6.234136e-07   Fold05\n1463  VF   VF 9.599543e-01 3.780587e-02 2.232317e-03 7.524170e-06   Fold05\n1464  VF   VF 9.766459e-01 2.260257e-02 7.509603e-04 5.494110e-07   Fold05\n1465  VF   VF 9.924279e-01 7.371807e-03 2.002420e-04 4.448679e-08   Fold05\n1466  VF   VF 9.059503e-01 8.839929e-02 5.636463e-03 1.398439e-05   Fold05\n1467  VF   VF 9.140985e-01 8.222634e-02 3.670766e-03 4.380510e-06   Fold05\n1468  VF   VF 8.762112e-01 1.150376e-01 8.733080e-03 1.813536e-05   Fold05\n1469  VF   VF 7.112422e-01 2.838292e-01 4.920770e-03 7.833260e-06   Fold05\n1470  VF   VF 7.654011e-01 2.072108e-01 2.724605e-02 1.420676e-04   Fold05\n1471  VF   VF 9.159304e-01 8.072922e-02 3.336338e-03 4.093723e-06   Fold05\n1472  VF   VF 9.257768e-01 7.046938e-02 3.751027e-03 2.748602e-06   Fold05\n1473  VF   VF 8.847080e-01 1.100048e-01 5.276939e-03 1.033452e-05   Fold05\n1474  VF   VF 8.841003e-01 1.084993e-01 7.390424e-03 9.910193e-06   Fold05\n1475  VF   VF 8.993905e-01 9.502287e-02 5.579824e-03 6.809443e-06   Fold05\n1476  VF   VF 9.109510e-01 8.545602e-02 3.588383e-03 4.638360e-06   Fold05\n1477  VF   VF 8.745916e-01 1.188221e-01 6.565409e-03 2.091870e-05   Fold05\n1478  VF   VF 8.809759e-01 1.140582e-01 4.958681e-03 7.218492e-06   Fold05\n1479  VF   VF 8.587343e-01 1.348737e-01 6.381421e-03 1.060105e-05   Fold05\n1480  VF   VF 6.925381e-01 3.000082e-01 7.428510e-03 2.517814e-05   Fold05\n1481  VF   VF 8.808677e-01 1.128306e-01 6.281471e-03 2.020287e-05   Fold05\n1482  VF   VF 8.754112e-01 1.184832e-01 6.087326e-03 1.821200e-05   Fold05\n1483  VF   VF 9.186737e-01 7.840727e-02 2.915949e-03 3.029723e-06   Fold05\n1484  VF   VF 8.524990e-01 1.364599e-01 1.101363e-02 2.744678e-05   Fold05\n1485  VF   VF 8.932771e-01 1.001445e-01 6.568757e-03 9.666910e-06   Fold05\n1486  VF   VF 8.818495e-01 1.113195e-01 6.817174e-03 1.389294e-05   Fold05\n1487  VF   VF 8.574410e-01 1.309165e-01 1.161023e-02 3.229571e-05   Fold05\n1488  VF   VF 8.765250e-01 1.165806e-01 6.869813e-03 2.452059e-05   Fold05\n1489  VF   VF 8.964766e-01 9.871244e-02 4.803151e-03 7.790342e-06   Fold05\n1490  VF   VF 8.767053e-01 1.134252e-01 9.858026e-03 1.155452e-05   Fold05\n1491  VF   VF 8.932061e-01 9.996749e-02 6.815724e-03 1.068617e-05   Fold05\n1492  VF   VF 6.709907e-01 3.202160e-01 8.757620e-03 3.571767e-05   Fold05\n1493  VF   VF 8.838187e-01 1.086327e-01 7.536174e-03 1.233523e-05   Fold05\n1494  VF   VF 6.621993e-01 2.743818e-01 5.853253e-02 4.886445e-03   Fold05\n1495  VF   VF 6.671190e-01 3.238258e-01 9.017184e-03 3.803642e-05   Fold05\n1496  VF   VF 8.842738e-01 1.077120e-01 7.999956e-03 1.424337e-05   Fold05\n1497  VF   VF 6.505171e-01 3.406680e-01 8.779914e-03 3.495373e-05   Fold05\n1498  VF   VF 9.118095e-01 8.360515e-02 4.580562e-03 4.776186e-06   Fold05\n1499  VF   VF 8.846058e-01 1.099580e-01 5.426397e-03 9.792412e-06   Fold05\n1500  VF   VF 8.609364e-01 1.308779e-01 8.152180e-03 3.352484e-05   Fold05\n1501  VF   VF 8.668486e-01 1.190399e-01 1.409095e-02 2.055350e-05   Fold05\n1502  VF   VF 6.673770e-01 2.812433e-01 4.674926e-02 4.630426e-03   Fold05\n1503  VF   VF 8.487269e-01 1.428918e-01 8.355656e-03 2.566329e-05   Fold05\n1504  VF   VF 8.425638e-01 1.476431e-01 9.746158e-03 4.696462e-05   Fold05\n1505  VF   VF 8.461867e-01 1.438029e-01 9.959979e-03 5.040364e-05   Fold05\n1506  VF   VF 8.759731e-01 1.170434e-01 6.967176e-03 1.634395e-05   Fold05\n1507  VF   VF 8.203597e-01 1.651854e-01 1.432286e-02 1.320663e-04   Fold05\n1508  VF   VF 9.198485e-01 7.705282e-02 3.095103e-03 3.545456e-06   Fold05\n1509  VF   VF 8.505175e-01 1.393315e-01 1.010949e-02 4.152188e-05   Fold05\n1510  VF   VF 9.208736e-01 7.603275e-02 3.090102e-03 3.567667e-06   Fold05\n1511  VF   VF 8.808526e-01 1.109139e-01 8.217377e-03 1.611968e-05   Fold05\n1512  VF   VF 9.098114e-01 8.532819e-02 4.854899e-03 5.468754e-06   Fold05\n1513  VF   VF 9.105394e-01 8.467918e-02 4.776144e-03 5.293215e-06   Fold05\n1514  VF   VF 8.625989e-01 1.285242e-01 8.859729e-03 1.718487e-05   Fold05\n1515  VF   VF 8.409200e-01 1.475410e-01 1.151286e-02 2.615026e-05   Fold05\n1516  VF   VF 9.119388e-01 8.611081e-02 1.920241e-03 3.016098e-05   Fold05\n1517  VF   VF 9.349984e-01 6.160214e-02 3.395645e-03 3.859036e-06   Fold05\n1518  VF   VF 9.139978e-01 8.090270e-02 5.092014e-03 7.497918e-06   Fold05\n1519  VF   VF 9.571141e-01 4.094096e-02 1.943632e-03 1.358213e-06   Fold05\n1520  VF   VF 9.075757e-01 8.652979e-02 5.887427e-03 7.063046e-06   Fold05\n1521  VF   VF 9.664118e-01 3.224440e-02 1.343141e-03 6.945932e-07   Fold05\n1522  VF   VF 8.907552e-01 9.791341e-02 1.131392e-02 1.748595e-05   Fold05\n1523  VF   VF 9.777296e-01 2.115665e-02 1.113329e-03 4.294257e-07   Fold05\n1524  VF   VF 9.231362e-01 6.981719e-02 7.037551e-03 9.062683e-06   Fold05\n1525  VF   VF 9.692676e-01 2.950482e-02 1.227063e-03 5.656424e-07   Fold05\n1526  VF   VF 9.302485e-01 6.610490e-02 3.643594e-03 2.980760e-06   Fold05\n1527  VF   VF 9.619674e-01 3.641353e-02 1.618187e-03 9.250590e-07   Fold05\n1528  VF   VF 9.552903e-01 4.265160e-02 2.056736e-03 1.333769e-06   Fold05\n1529  VF   VF 9.401260e-01 5.487620e-02 4.993309e-03 4.480448e-06   Fold05\n1530  VF   VF 9.446230e-01 5.195971e-02 3.411605e-03 5.689995e-06   Fold05\n1531  VF    F 1.012987e-01 7.785035e-01 1.198562e-01 3.415275e-04   Fold05\n1532  VF    F 6.314205e-02 7.845379e-01 1.515943e-01 7.257322e-04   Fold05\n1533  VF    F 6.726518e-02 7.698154e-01 1.617159e-01 1.203597e-03   Fold05\n1534  VF    F 1.078163e-01 7.862606e-01 1.056810e-01 2.421271e-04   Fold05\n1535  VF    F 1.127634e-01 7.842242e-01 1.027853e-01 2.271625e-04   Fold05\n1536  VF    F 8.362395e-02 7.520255e-01 1.638767e-01 4.738205e-04   Fold05\n1537  VF    F 1.107242e-01 7.782782e-01 1.107301e-01 2.675059e-04   Fold05\n1538  VF    F 7.180732e-02 7.425995e-01 1.848704e-01 7.228332e-04   Fold05\n1539  VF   VF 5.734712e-01 3.611507e-01 6.512092e-02 2.571826e-04   Fold05\n1540  VF   VF 5.972176e-01 3.472696e-01 5.537290e-02 1.399310e-04   Fold05\n1541  VF   VF 7.294933e-01 2.469908e-01 2.348102e-02 3.487842e-05   Fold05\n1542  VF   VF 5.698759e-01 3.594972e-01 7.041183e-02 2.150581e-04   Fold05\n1543  VF   VF 7.191930e-01 2.511945e-01 2.959074e-02 2.177645e-05   Fold05\n1544  VF   VF 8.040446e-01 1.841389e-01 1.181051e-02 5.998141e-06   Fold05\n1545  VF   VF 7.859920e-01 1.973514e-01 1.664469e-02 1.186477e-05   Fold05\n1546  VF   VF 7.079861e-01 2.628349e-01 2.912415e-02 5.490571e-05   Fold05\n1547  VF   VF 7.350729e-01 2.432742e-01 2.163413e-02 1.869924e-05   Fold05\n1548  VF   VF 7.966792e-01 1.900302e-01 1.328312e-02 7.420709e-06   Fold05\n1549  VF   VF 7.654159e-01 2.162333e-01 1.833607e-02 1.469189e-05   Fold05\n1550  VF   VF 7.722094e-01 2.095471e-01 1.822692e-02 1.657193e-05   Fold05\n1551  VF   VF 7.229130e-01 2.437891e-01 3.326433e-02 3.347647e-05   Fold05\n1552  VF   VF 7.714716e-01 2.102506e-01 1.826126e-02 1.657638e-05   Fold05\n1553  VF   VF 8.073547e-01 1.795085e-01 1.312922e-02 7.586973e-06   Fold05\n1554  VF   VF 6.864838e-01 2.561640e-01 5.730740e-02 4.487916e-05   Fold05\n1555  VF   VF 6.976394e-01 2.665111e-01 3.581256e-02 3.699206e-05   Fold05\n1556  VF   VF 7.308202e-01 2.429737e-01 2.617505e-02 3.107200e-05   Fold05\n1557  VF   VF 6.546905e-01 3.035712e-01 4.162224e-02 1.160153e-04   Fold05\n1558  VF   VF 6.553723e-01 3.032729e-01 4.124070e-02 1.141285e-04   Fold05\n1559  VF    F 3.522372e-01 6.119658e-01 3.568902e-02 1.080606e-04   Fold05\n1560  VF   VF 7.068983e-01 2.613462e-01 3.170934e-02 4.620307e-05   Fold05\n1561  VF   VF 6.369311e-01 3.184579e-01 4.447505e-02 1.359441e-04   Fold05\n1562  VF   VF 7.950711e-01 1.911030e-01 1.381776e-02 8.185663e-06   Fold05\n1563  VF   VF 4.679806e-01 4.207190e-01 1.108468e-01 4.535335e-04   Fold05\n1564  VF    F 9.311536e-02 7.848674e-01 1.205133e-01 1.504015e-03   Fold05\n1565  VF    F 6.920304e-02 8.091009e-01 1.176356e-01 4.060434e-03   Fold05\n1566   F   VF 8.779963e-01 9.700638e-02 2.479006e-02 2.072530e-04   Fold05\n1567   F   VF 8.822723e-01 9.389438e-02 2.364007e-02 1.932779e-04   Fold05\n1568   F    F 3.349775e-01 5.518220e-01 1.117687e-01 1.431762e-03   Fold05\n1569   F    L 1.770539e-03 3.534356e-03 1.314412e-02 9.815510e-01   Fold05\n1570   F    M 1.558408e-01 1.416183e-01 3.566438e-01 3.458970e-01   Fold05\n1571   F    F 1.877419e-01 4.428844e-01 2.314316e-01 1.379421e-01   Fold05\n1572   F   VF 5.616965e-01 3.670861e-01 7.081897e-02 3.984761e-04   Fold05\n1573   F   VF 4.534056e-01 4.385674e-01 1.071877e-01 8.393142e-04   Fold05\n1574   F   VF 6.210877e-01 3.340838e-01 4.449429e-02 3.341445e-04   Fold05\n1575   F    M 9.953713e-02 3.680775e-01 4.343582e-01 9.802712e-02   Fold05\n1576   F   VF 7.583330e-01 2.175034e-01 2.413800e-02 2.562687e-05   Fold05\n1577   F    F 1.246760e-01 6.010279e-01 2.725006e-01 1.795506e-03   Fold05\n1578   F    F 1.246343e-01 6.577669e-01 2.152157e-01 2.383074e-03   Fold05\n1579   F    F 1.008245e-01 5.834940e-01 3.128634e-01 2.818108e-03   Fold05\n1580   F    F 1.149662e-01 6.511118e-01 2.314467e-01 2.475242e-03   Fold05\n1581   F    F 1.440804e-01 6.104853e-01 2.443659e-01 1.068515e-03   Fold05\n1582   F    F 1.334188e-01 6.562808e-01 2.086359e-01 1.664502e-03   Fold05\n1583   F    F 1.465886e-01 6.445159e-01 2.076616e-01 1.233909e-03   Fold05\n1584   F    F 1.338323e-01 6.552021e-01 2.095341e-01 1.431474e-03   Fold05\n1585   F    F 1.847491e-01 6.487191e-01 1.657901e-01 7.416517e-04   Fold05\n1586   F    F 9.398133e-02 6.331141e-01 2.680145e-01 4.890067e-03   Fold05\n1587   F    F 1.635504e-01 6.164416e-01 2.190912e-01 9.168671e-04   Fold05\n1588   F    F 1.526410e-01 6.548950e-01 1.911813e-01 1.282621e-03   Fold05\n1589   F    F 1.316935e-01 6.648956e-01 2.014148e-01 1.996142e-03   Fold05\n1590   F    F 1.855697e-01 6.349933e-01 1.785543e-01 8.826132e-04   Fold05\n1591   F   VF 8.174628e-01 1.680666e-01 1.426866e-02 2.019485e-04   Fold05\n1592   F   VF 8.260969e-01 1.571368e-01 1.671550e-02 5.071842e-05   Fold05\n1593   F   VF 8.360326e-01 1.352326e-01 2.869783e-02 3.697617e-05   Fold05\n1594   F   VF 7.197431e-01 2.304134e-01 4.931421e-02 5.292593e-04   Fold05\n1595   F   VF 6.754309e-01 3.166712e-01 7.870652e-03 2.723348e-05   Fold05\n1596   F   VF 9.067097e-01 8.885914e-02 4.424036e-03 7.111620e-06   Fold05\n1597   F   VF 8.710230e-01 1.160843e-01 1.287231e-02 2.044354e-05   Fold05\n1598   F   VF 4.133653e-01 3.616799e-01 1.250005e-01 9.995428e-02   Fold05\n1599   F    L 1.468911e-02 3.901727e-02 3.723399e-02 9.090596e-01   Fold05\n1600   F    L 1.018560e-02 3.048121e-02 3.987403e-02 9.194592e-01   Fold05\n1601   F    F 3.171634e-01 3.386933e-01 1.345308e-01 2.096125e-01   Fold05\n1602   F   VF 6.267474e-01 3.628272e-01 1.037527e-02 5.013621e-05   Fold05\n1603   F   VF 8.288978e-01 1.549894e-01 1.605241e-02 6.036716e-05   Fold05\n1604   F   VF 6.200655e-01 3.690275e-01 1.085180e-02 5.525317e-05   Fold05\n1605   F   VF 6.206883e-01 3.684453e-01 1.081157e-02 5.483680e-05   Fold05\n1606   F   VF 8.646338e-01 1.265766e-01 8.758182e-03 3.134086e-05   Fold05\n1607   F   VF 6.049627e-01 3.829513e-01 1.201691e-02 6.907052e-05   Fold05\n1608   F   VF 7.685133e-01 2.061294e-01 2.495439e-02 4.029254e-04   Fold05\n1609   F   VF 8.235027e-01 1.658261e-01 1.061747e-02 5.379122e-05   Fold05\n1610   F   VF 5.915934e-01 3.952074e-01 1.311557e-02 8.367057e-05   Fold05\n1611   F   VF 7.983961e-01 1.786770e-01 2.280099e-02 1.259827e-04   Fold05\n1612   F   VF 7.868177e-01 1.823876e-01 3.060452e-02 1.901593e-04   Fold05\n1613   F   VF 8.631637e-01 1.282759e-01 8.539297e-03 2.114877e-05   Fold05\n1614   F   VF 8.409006e-01 1.426511e-01 1.642867e-02 1.956473e-05   Fold05\n1615   F   VF 5.652505e-01 2.393185e-01 1.768850e-01 1.854594e-02   Fold05\n1616   F    F 5.049370e-02 7.347832e-01 2.131579e-01 1.565155e-03   Fold05\n1617   F    F 5.054420e-02 7.760569e-01 1.720089e-01 1.389992e-03   Fold05\n1618   F    F 8.123199e-02 7.396645e-01 1.785592e-01 5.442942e-04   Fold05\n1619   F    F 3.397748e-02 7.219589e-01 2.356993e-01 8.364312e-03   Fold05\n1620   F    F 6.317629e-02 7.424550e-01 1.934305e-01 9.382704e-04   Fold05\n1621   F    F 5.764809e-02 7.191858e-01 2.219465e-01 1.219625e-03   Fold05\n1622   F    F 2.836298e-02 6.320360e-01 3.300490e-01 9.551997e-03   Fold05\n1623   F    F 3.396303e-02 7.106124e-01 2.460400e-01 9.384649e-03   Fold05\n1624   F    F 6.363450e-02 7.216877e-01 2.134723e-01 1.205464e-03   Fold05\n1625   F    F 6.647639e-02 7.706031e-01 1.620905e-01 8.299138e-04   Fold05\n1626   F    F 6.424375e-02 7.690508e-01 1.657323e-01 9.731962e-04   Fold05\n1627   F    F 4.788195e-02 7.756635e-01 1.751516e-01 1.302984e-03   Fold05\n1628   F    F 5.672918e-02 7.707794e-01 1.712847e-01 1.206747e-03   Fold05\n1629   F    F 5.104638e-02 7.437473e-01 2.027850e-01 2.421392e-03   Fold05\n1630   F    F 7.942600e-02 8.029075e-01 1.172633e-01 4.031983e-04   Fold05\n1631   F    F 9.594039e-02 8.040699e-01 9.976795e-02 2.217549e-04   Fold05\n1632   F    F 9.540081e-02 8.038557e-01 1.005171e-01 2.263237e-04   Fold05\n1633   F    F 1.028486e-01 7.857105e-01 1.111625e-01 2.783580e-04   Fold05\n1634   F    F 6.266008e-02 7.274483e-01 2.088855e-01 1.006104e-03   Fold05\n1635   F   VF 7.510918e-01 2.267271e-01 2.215864e-02 2.250136e-05   Fold05\n1636   F    F 3.383968e-01 4.366463e-01 2.225480e-01 2.408917e-03   Fold05\n1637   F    F 1.859150e-01 4.396696e-01 3.484466e-01 2.596874e-02   Fold05\n1638   F    F 4.130268e-01 4.540142e-01 1.270592e-01 5.899723e-03   Fold05\n1639   F    F 2.840690e-01 4.464797e-01 2.483417e-01 2.110960e-02   Fold05\n1640   F    F 3.748295e-01 4.401604e-01 1.770357e-01 7.974329e-03   Fold05\n1641   F    F 3.004964e-01 4.682547e-01 2.189565e-01 1.229251e-02   Fold05\n1642   F    F 3.382979e-01 4.705791e-01 1.722135e-01 1.890948e-02   Fold05\n1643   F   VF 7.548890e-01 2.259111e-01 1.918252e-02 1.739962e-05   Fold05\n1644   F   VF 7.210248e-01 2.428838e-01 3.605779e-02 3.356811e-05   Fold05\n1645   F    M 1.317491e-02 1.261485e-01 4.701503e-01 3.905263e-01   Fold05\n1646   F   VF 6.095174e-01 3.258095e-01 6.453949e-02 1.335568e-04   Fold05\n1647   F   VF 5.691745e-01 3.372208e-01 9.328464e-02 3.201022e-04   Fold05\n1648   F   VF 4.447318e-01 4.299763e-01 1.235605e-01 1.731419e-03   Fold05\n1649   F   VF 6.225645e-01 3.483224e-01 2.823452e-02 8.785499e-04   Fold05\n1650   F   VF 5.163841e-01 4.122781e-01 7.096960e-02 3.681891e-04   Fold05\n1651   F    F 6.850735e-02 8.035476e-01 1.221296e-01 5.815446e-03   Fold05\n1652   F    F 2.422913e-02 6.662403e-01 2.572168e-01 5.231376e-02   Fold05\n1653   F    F 1.075923e-01 7.991762e-01 9.194429e-02 1.287157e-03   Fold05\n1654   F    F 1.074270e-02 4.813058e-01 3.744453e-01 1.335062e-01   Fold05\n1655   F    F 6.498924e-02 7.928579e-01 1.362823e-01 5.870560e-03   Fold05\n1656   F    F 6.445382e-02 7.526662e-01 1.776394e-01 5.240609e-03   Fold05\n1657   F    F 6.551861e-02 7.590536e-01 1.702371e-01 5.190616e-03   Fold05\n1658   F    F 1.999015e-02 6.058792e-01 2.934148e-01 8.071592e-02   Fold05\n1659   F    F 5.826735e-02 7.923251e-01 1.425226e-01 6.885009e-03   Fold05\n1660   F    F 3.602227e-02 7.477384e-01 1.871580e-01 2.908128e-02   Fold05\n1661   F    F 7.114039e-02 8.010845e-01 1.239436e-01 3.831502e-03   Fold05\n1662   F    F 6.262384e-02 7.533561e-01 1.780860e-01 5.934002e-03   Fold05\n1663   F    F 3.115391e-02 6.617010e-01 2.618226e-01 4.532251e-02   Fold05\n1664   F    F 3.515671e-02 7.435688e-01 1.894242e-01 3.185034e-02   Fold05\n1665   F    F 2.958857e-02 6.817040e-01 2.530264e-01 3.568098e-02   Fold05\n1666   F    F 4.138581e-02 7.237451e-01 2.214463e-01 1.342278e-02   Fold05\n1667   F    F 5.860813e-02 7.665664e-01 1.693925e-01 5.432895e-03   Fold05\n1668   F    F 6.520672e-02 7.993621e-01 1.311664e-01 4.264746e-03   Fold05\n1669   F    F 6.810463e-02 8.012922e-01 1.263974e-01 4.205813e-03   Fold05\n1670   F    F 6.207966e-02 7.636693e-01 1.688134e-01 5.437611e-03   Fold05\n1671   F    F 3.171922e-02 7.122879e-01 2.386610e-01 1.733185e-02   Fold05\n1672   F    F 6.017198e-02 7.844193e-01 1.485567e-01 6.852022e-03   Fold05\n1673   F    F 2.702431e-02 6.243094e-01 3.141848e-01 3.448142e-02   Fold05\n1674   M   VF 9.050056e-01 8.246011e-02 1.245171e-02 8.263412e-05   Fold05\n1675   M    F 8.639698e-02 4.678800e-01 2.372195e-01 2.085035e-01   Fold05\n1676   M    L 1.304969e-01 1.352742e-01 3.567109e-01 3.775180e-01   Fold05\n1677   M    M 7.818056e-02 1.074127e-01 4.358633e-01 3.785435e-01   Fold05\n1678   M    L 7.516617e-02 1.136954e-01 3.467324e-01 4.644060e-01   Fold05\n1679   M    M 8.531959e-02 1.092673e-01 4.371871e-01 3.682260e-01   Fold05\n1680   M    M 8.613018e-02 1.108924e-01 4.376726e-01 3.653049e-01   Fold05\n1681   M    F 3.080992e-01 5.268263e-01 1.618664e-01 3.208062e-03   Fold05\n1682   M    F 2.128235e-01 4.990041e-01 2.817435e-01 6.428928e-03   Fold05\n1683   M    M 6.290807e-03 1.912821e-02 9.729794e-01 1.601585e-03   Fold05\n1684   M    M 3.903279e-02 2.256897e-01 5.460525e-01 1.892251e-01   Fold05\n1685   M    F 1.089408e-01 6.382318e-01 2.499321e-01 2.895241e-03   Fold05\n1686   M    M 1.489406e-02 3.040466e-01 3.785813e-01 3.024781e-01   Fold05\n1687   M    F 1.598039e-01 6.076506e-01 2.314867e-01 1.058775e-03   Fold05\n1688   M    F 1.795941e-01 6.501958e-01 1.694236e-01 7.865273e-04   Fold05\n1689   M   VF 8.674291e-01 1.095975e-01 2.295505e-02 1.840823e-05   Fold05\n1690   M   VF 8.663556e-01 1.207501e-01 1.287254e-02 2.183045e-05   Fold05\n1691   M    L 7.672887e-03 2.311847e-02 2.672252e-02 9.424861e-01   Fold05\n1692   M   VF 8.085719e-01 1.708141e-01 2.051824e-02 9.575320e-05   Fold05\n1693   M    F 7.086462e-02 7.807406e-01 1.477046e-01 6.901104e-04   Fold05\n1694   M    M 4.507493e-03 3.134386e-01 5.462480e-01 1.358059e-01   Fold05\n1695   M    F 3.896799e-02 7.053549e-01 2.538445e-01 1.832576e-03   Fold05\n1696   M    F 6.636427e-02 7.352486e-01 1.974683e-01 9.188587e-04   Fold05\n1697   M    F 6.086238e-02 7.146157e-01 2.235384e-01 9.834668e-04   Fold05\n1698   M    F 3.064479e-02 6.139930e-01 3.507351e-01 4.627154e-03   Fold05\n1699   M    M 3.634140e-04 2.122794e-02 9.766844e-01 1.724265e-03   Fold05\n1700   M    M 3.734893e-02 2.345582e-01 5.899057e-01 1.381872e-01   Fold05\n1701   M   VF 5.994612e-01 3.462342e-01 5.417020e-02 1.343753e-04   Fold05\n1702   M    F 7.608613e-02 8.030746e-01 1.173924e-01 3.446860e-03   Fold05\n1703   M    L 1.234719e-03 1.590845e-01 2.694979e-01 5.701829e-01   Fold05\n1704   M    L 7.534281e-03 3.563874e-01 1.892889e-01 4.467894e-01   Fold05\n1705   M    F 3.501065e-02 7.137776e-01 2.144977e-01 3.671407e-02   Fold05\n1706   M    F 3.301731e-02 7.231956e-01 2.100018e-01 3.378528e-02   Fold05\n1707   M    F 1.835874e-02 5.923488e-01 3.000215e-01 8.927095e-02   Fold05\n1708   M    F 2.749251e-02 6.254271e-01 3.060333e-01 4.104707e-02   Fold05\n1709   M    M 1.204371e-02 4.290766e-01 4.617113e-01 9.716839e-02   Fold05\n1710   M    F 3.588184e-02 7.226528e-01 2.037675e-01 3.769790e-02   Fold05\n1711   M    F 3.445833e-02 7.085605e-01 2.192341e-01 3.774715e-02   Fold05\n1712   M    L 1.968510e-04 3.729261e-02 1.104504e-01 8.520601e-01   Fold05\n1713   M    F 5.443022e-02 7.475930e-01 1.904780e-01 7.498781e-03   Fold05\n1714   M    F 3.064097e-02 7.204325e-01 2.245449e-01 2.438162e-02   Fold05\n1715   L    L 1.208849e-03 4.844665e-02 1.564660e-01 7.938785e-01   Fold05\n1716   L    L 9.572098e-03 1.669699e-01 3.223378e-01 5.011202e-01   Fold05\n1717   L    F 1.088122e-01 4.212986e-01 3.938672e-01 7.602200e-02   Fold05\n1718   L    M 9.074337e-02 4.075430e-01 4.124275e-01 8.928620e-02   Fold05\n1719   L    F 8.732260e-02 5.785409e-01 3.305767e-01 3.559738e-03   Fold05\n1720   L    F 1.091862e-01 6.365194e-01 2.506532e-01 3.641157e-03   Fold05\n1721   L    F 1.100983e-01 6.360801e-01 2.501768e-01 3.644869e-03   Fold05\n1722   L    F 1.026831e-01 6.583385e-01 2.357615e-01 3.216904e-03   Fold05\n1723   L    L 5.026882e-04 6.099789e-02 4.297158e-01 5.087836e-01   Fold05\n1724   L   VF 4.952782e-01 3.710892e-01 1.256785e-01 7.954096e-03   Fold05\n1725   L    F 2.438566e-01 3.886296e-01 3.064821e-01 6.103166e-02   Fold05\n1726   L    L 8.226731e-07 2.775420e-04 1.116063e-03 9.986056e-01   Fold05\n1727   L    L 4.108996e-07 1.877894e-04 1.078646e-03 9.987332e-01   Fold05\n1728   L    L 4.245366e-07 1.924611e-04 1.096053e-03 9.987111e-01   Fold05\n1729   L    L 4.445772e-03 3.250005e-01 3.231022e-01 3.474515e-01   Fold05\n1730   L    L 1.514420e-06 4.492441e-04 1.591140e-03 9.979581e-01   Fold05\n1731   L    F 3.835691e-02 7.336162e-01 2.152697e-01 1.275717e-02   Fold05\n1732   L    F 3.297060e-02 6.933894e-01 2.527752e-01 2.086479e-02   Fold05\n1733   L    F 2.516036e-02 7.015664e-01 2.275272e-01 4.574608e-02   Fold05\n1734   L    L 7.309981e-08 5.706510e-05 3.741870e-04 9.995687e-01   Fold05\n1735   L    F 2.489177e-02 6.998654e-01 2.286205e-01 4.662232e-02   Fold05\n1736  VF   VF 9.481529e-01 4.796182e-02 3.877481e-03 7.835153e-06   Fold06\n1737  VF   VF 9.448080e-01 5.106752e-02 4.116365e-03 8.132075e-06   Fold06\n1738  VF   VF 9.590757e-01 3.825165e-02 2.668882e-03 3.748604e-06   Fold06\n1739  VF    F 3.576745e-01 5.619005e-01 7.780405e-02 2.621006e-03   Fold06\n1740  VF    F 3.376900e-01 5.728823e-01 8.609712e-02 3.330586e-03   Fold06\n1741  VF   VF 9.075015e-01 7.444029e-02 1.801527e-02 4.294840e-05   Fold06\n1742  VF   VF 9.518569e-01 4.080840e-02 7.328765e-03 5.975556e-06   Fold06\n1743  VF   VF 8.693147e-01 9.789957e-02 3.266607e-02 1.196805e-04   Fold06\n1744  VF   VF 7.899832e-01 1.664066e-01 4.347725e-02 1.329462e-04   Fold06\n1745  VF   VF 4.401304e-01 3.459282e-01 2.121231e-01 1.818232e-03   Fold06\n1746  VF   VF 9.203043e-01 6.818378e-02 1.149568e-02 1.623378e-05   Fold06\n1747  VF   VF 7.541605e-01 1.797632e-01 6.581414e-02 2.621602e-04   Fold06\n1748  VF   VF 9.007753e-01 7.637549e-02 2.280582e-02 4.334319e-05   Fold06\n1749  VF   VF 9.354796e-01 5.510662e-02 9.399633e-03 1.410360e-05   Fold06\n1750  VF   VF 8.778988e-01 9.868870e-02 2.334634e-02 6.612766e-05   Fold06\n1751  VF   VF 9.276222e-01 6.156340e-02 1.080417e-02 1.018523e-05   Fold06\n1752  VF   VF 9.833211e-01 1.573354e-02 9.432487e-04 2.125782e-06   Fold06\n1753  VF   VF 9.891466e-01 1.040691e-02 4.461249e-04 3.545177e-07   Fold06\n1754  VF   VF 9.842204e-01 1.499044e-02 7.877921e-04 1.385357e-06   Fold06\n1755  VF   VF 9.808329e-01 1.796214e-02 1.201697e-03 3.255589e-06   Fold06\n1756  VF   VF 9.892565e-01 1.028202e-02 4.610510e-04 4.017010e-07   Fold06\n1757  VF   VF 9.895652e-01 9.971173e-03 4.631957e-04 4.361618e-07   Fold06\n1758  VF   VF 9.892313e-01 1.030719e-02 4.611165e-04 3.996987e-07   Fold06\n1759  VF   VF 9.377635e-01 5.027996e-02 1.131974e-02 6.367742e-04   Fold06\n1760  VF   VF 9.162990e-01 6.821857e-02 1.352725e-02 1.955219e-03   Fold06\n1761  VF   VF 9.875658e-01 1.179748e-02 6.358267e-04 8.780049e-07   Fold06\n1762  VF   VF 6.635110e-01 2.961887e-01 4.001875e-02 2.814975e-04   Fold06\n1763  VF   VF 7.146196e-01 2.477186e-01 3.754990e-02 1.119552e-04   Fold06\n1764  VF   VF 6.044835e-01 3.273338e-01 6.773814e-02 4.446332e-04   Fold06\n1765  VF   VF 6.615336e-01 2.817863e-01 5.640878e-02 2.713127e-04   Fold06\n1766  VF   VF 6.964234e-01 2.688744e-01 3.446404e-02 2.382142e-04   Fold06\n1767  VF   VF 6.872279e-01 2.755876e-01 3.690842e-02 2.760237e-04   Fold06\n1768  VF   VF 7.145499e-01 2.560798e-01 2.922677e-02 1.435055e-04   Fold06\n1769  VF   VF 8.743074e-01 1.172914e-01 8.394449e-03 6.696377e-06   Fold06\n1770  VF   VF 7.309179e-01 2.337452e-01 3.525349e-02 8.339776e-05   Fold06\n1771  VF   VF 5.536063e-01 3.339641e-01 1.116986e-01 7.309183e-04   Fold06\n1772  VF   VF 8.661055e-01 1.248517e-01 9.035637e-03 7.237582e-06   Fold06\n1773  VF   VF 8.416816e-01 1.451707e-01 1.312985e-02 1.781522e-05   Fold06\n1774  VF   VF 7.907311e-01 1.909931e-01 1.824345e-02 3.232365e-05   Fold06\n1775  VF   VF 8.411102e-01 1.416790e-01 1.719428e-02 1.658723e-05   Fold06\n1776  VF   VF 8.232336e-01 1.573991e-01 1.934869e-02 1.861452e-05   Fold06\n1777  VF    F 1.451247e-01 6.651651e-01 1.879631e-01 1.747129e-03   Fold06\n1778  VF    F 1.573200e-01 6.498501e-01 1.912912e-01 1.538742e-03   Fold06\n1779  VF    F 1.020449e-01 6.937596e-01 2.005671e-01 3.628350e-03   Fold06\n1780  VF    F 1.350379e-01 6.768905e-01 1.861787e-01 1.892896e-03   Fold06\n1781  VF    F 1.130849e-01 7.158657e-01 1.682551e-01 2.794361e-03   Fold06\n1782  VF    F 1.002441e-01 6.798685e-01 2.167177e-01 3.169751e-03   Fold06\n1783  VF   VF 9.855040e-01 1.399584e-02 4.995595e-04 5.560702e-07   Fold06\n1784  VF   VF 9.921464e-01 7.584435e-03 2.690365e-04 1.343705e-07   Fold06\n1785  VF   VF 9.878948e-01 1.176593e-02 3.388773e-04 3.601588e-07   Fold06\n1786  VF   VF 9.901912e-01 9.557738e-03 2.508648e-04 2.320116e-07   Fold06\n1787  VF   VF 9.813600e-01 1.770928e-02 9.298854e-04 8.385177e-07   Fold06\n1788  VF   VF 9.915922e-01 8.208544e-03 1.991139e-04 1.022930e-07   Fold06\n1789  VF   VF 7.565397e-01 2.139713e-01 2.914858e-02 3.404227e-04   Fold06\n1790  VF   VF 9.876147e-01 1.189986e-02 4.849141e-04 4.971787e-07   Fold06\n1791  VF   VF 9.741288e-01 2.448174e-02 1.387133e-03 2.334462e-06   Fold06\n1792  VF   VF 9.731873e-01 2.538073e-02 1.429598e-03 2.357061e-06   Fold06\n1793  VF   VF 9.773321e-01 2.144375e-02 1.222946e-03 1.240774e-06   Fold06\n1794  VF   VF 9.735851e-01 2.540351e-02 1.010046e-03 1.313254e-06   Fold06\n1795  VF   VF 9.800784e-01 1.912289e-02 7.978102e-04 8.626944e-07   Fold06\n1796  VF   VF 5.524029e-01 2.570955e-01 1.717060e-01 1.879566e-02   Fold06\n1797  VF   VF 9.878201e-01 1.180605e-02 3.735587e-04 2.976039e-07   Fold06\n1798  VF   VF 9.937644e-01 6.106797e-03 1.287131e-04 5.333882e-08   Fold06\n1799  VF   VF 8.713804e-01 9.594342e-02 3.124623e-02 1.429963e-03   Fold06\n1800  VF   VF 9.819019e-01 1.734857e-02 7.483469e-04 1.148858e-06   Fold06\n1801  VF   VF 9.517352e-01 4.538139e-02 2.872642e-03 1.073919e-05   Fold06\n1802  VF   VF 9.570318e-01 4.050872e-02 2.450860e-03 8.591131e-06   Fold06\n1803  VF   VF 9.661994e-01 3.165245e-02 2.143346e-03 4.837247e-06   Fold06\n1804  VF   VF 9.926792e-01 7.157168e-03 1.635280e-04 7.682395e-08   Fold06\n1805  VF   VF 9.442760e-01 5.122902e-02 4.447635e-03 4.730790e-05   Fold06\n1806  VF   VF 9.830949e-01 1.606690e-02 8.369317e-04 1.297162e-06   Fold06\n1807  VF   VF 9.792120e-01 1.974658e-02 1.040470e-03 9.261090e-07   Fold06\n1808  VF   VF 9.790921e-01 1.985730e-02 1.049692e-03 9.389690e-07   Fold06\n1809  VF   VF 9.899709e-01 9.607747e-03 4.210935e-04 2.994062e-07   Fold06\n1810  VF   VF 9.804736e-01 1.866741e-02 8.569612e-04 2.051302e-06   Fold06\n1811  VF   VF 9.740051e-01 2.480439e-02 1.188688e-03 1.816755e-06   Fold06\n1812  VF   VF 9.867432e-01 1.282424e-02 4.320961e-04 4.381659e-07   Fold06\n1813  VF   VF 9.583174e-01 3.943390e-02 2.243678e-03 5.015317e-06   Fold06\n1814  VF   VF 4.289678e-01 4.267278e-01 1.408089e-01 3.495467e-03   Fold06\n1815  VF   VF 8.898984e-01 1.021493e-01 7.915514e-03 3.686527e-05   Fold06\n1816  VF   VF 9.806036e-01 1.868934e-02 7.063581e-04 7.292967e-07   Fold06\n1817  VF   VF 9.750581e-01 2.356758e-02 1.369389e-03 4.905517e-06   Fold06\n1818  VF   VF 9.448776e-01 4.950164e-02 5.590022e-03 3.073106e-05   Fold06\n1819  VF   VF 9.028022e-01 9.243803e-02 4.748849e-03 1.096026e-05   Fold06\n1820  VF   VF 8.546347e-01 1.360610e-01 9.279841e-03 2.447180e-05   Fold06\n1821  VF   VF 9.037512e-01 9.178721e-02 4.451966e-03 9.576656e-06   Fold06\n1822  VF   VF 9.410227e-01 5.772217e-02 1.220412e-03 3.475804e-05   Fold06\n1823  VF   VF 8.913582e-01 1.034148e-01 5.215224e-03 1.175665e-05   Fold06\n1824  VF   VF 9.165970e-01 7.968642e-02 3.709920e-03 6.708335e-06   Fold06\n1825  VF   VF 6.779764e-01 3.150269e-01 6.971130e-03 2.553931e-05   Fold06\n1826  VF   VF 8.607008e-01 1.287924e-01 1.048191e-02 2.485776e-05   Fold06\n1827  VF   VF 8.961282e-01 9.858545e-02 5.275053e-03 1.133257e-05   Fold06\n1828  VF   VF 8.387929e-01 1.434649e-01 1.769869e-02 4.348993e-05   Fold06\n1829  VF   VF 8.989738e-01 9.667400e-02 4.345184e-03 6.969368e-06   Fold06\n1830  VF   VF 8.779102e-01 1.163426e-01 5.736668e-03 1.056950e-05   Fold06\n1831  VF   VF 9.016371e-01 9.248507e-02 5.869316e-03 8.471043e-06   Fold06\n1832  VF   VF 6.658492e-01 3.264935e-01 7.626563e-03 3.067566e-05   Fold06\n1833  VF   VF 8.617917e-01 1.302524e-01 7.921527e-03 3.442291e-05   Fold06\n1834  VF   VF 8.616280e-01 1.305534e-01 7.786043e-03 3.249461e-05   Fold06\n1835  VF   VF 6.683051e-01 3.233120e-01 8.341914e-03 4.096757e-05   Fold06\n1836  VF   VF 8.576352e-01 1.348486e-01 7.489291e-03 2.688023e-05   Fold06\n1837  VF   VF 8.899907e-01 1.042544e-01 5.739572e-03 1.529668e-05   Fold06\n1838  VF   VF 8.995224e-01 9.368757e-02 6.777447e-03 1.256856e-05   Fold06\n1839  VF   VF 5.296500e-01 3.659425e-01 8.751790e-02 1.688961e-02   Fold06\n1840  VF   VF 9.079653e-01 8.810224e-02 3.925249e-03 7.223259e-06   Fold06\n1841  VF   VF 9.191108e-01 7.780421e-02 3.080949e-03 4.073647e-06   Fold06\n1842  VF   VF 8.948413e-01 9.988477e-02 5.262453e-03 1.144259e-05   Fold06\n1843  VF   VF 8.975728e-01 9.530613e-02 7.107195e-03 1.392776e-05   Fold06\n1844  VF   VF 9.089281e-01 8.693397e-02 4.129239e-03 8.657682e-06   Fold06\n1845  VF   VF 9.007909e-01 9.422190e-02 4.976247e-03 1.093369e-05   Fold06\n1846  VF   VF 9.001866e-01 9.492280e-02 4.880390e-03 1.022695e-05   Fold06\n1847  VF   VF 6.396499e-01 3.510411e-01 9.262934e-03 4.608693e-05   Fold06\n1848  VF   VF 6.417912e-01 3.480894e-01 1.005936e-02 6.005751e-05   Fold06\n1849  VF   VF 8.809002e-01 1.118434e-01 7.230409e-03 2.594512e-05   Fold06\n1850  VF   VF 8.815500e-01 1.113654e-01 7.060253e-03 2.440883e-05   Fold06\n1851  VF   VF 8.830176e-01 1.083556e-01 8.608283e-03 1.851945e-05   Fold06\n1852  VF   VF 8.299554e-01 1.555504e-01 1.442726e-02 6.690479e-05   Fold06\n1853  VF   VF 8.593218e-01 1.317006e-01 8.955766e-03 2.188067e-05   Fold06\n1854  VF   VF 9.121136e-01 8.287203e-02 5.007573e-03 6.798681e-06   Fold06\n1855  VF   VF 8.630362e-01 1.286095e-01 8.325514e-03 2.879220e-05   Fold06\n1856  VF   VF 8.345698e-01 1.538556e-01 1.150317e-02 7.140202e-05   Fold06\n1857  VF   VF 8.328869e-01 1.559106e-01 1.113964e-02 6.286958e-05   Fold06\n1858  VF   VF 9.173047e-01 7.947514e-02 3.215714e-03 4.411899e-06   Fold06\n1859  VF   VF 7.640434e-01 2.113202e-01 2.430709e-02 3.293702e-04   Fold06\n1860  VF   VF 8.274651e-01 1.597511e-01 1.264515e-02 1.386567e-04   Fold06\n1861  VF   VF 9.148626e-01 8.175402e-02 3.378604e-03 4.797044e-06   Fold06\n1862  VF   VF 8.676402e-01 1.235298e-01 8.804006e-03 2.600499e-05   Fold06\n1863  VF   VF 8.679574e-01 1.234785e-01 8.540291e-03 2.376325e-05   Fold06\n1864  VF   VF 9.060788e-01 8.866262e-02 5.251867e-03 6.730641e-06   Fold06\n1865  VF   VF 8.789465e-01 1.147720e-01 6.258834e-03 2.271146e-05   Fold06\n1866  VF   VF 8.614847e-01 1.298415e-01 8.651805e-03 2.204099e-05   Fold06\n1867  VF   VF 8.506990e-01 1.396147e-01 9.660220e-03 2.603789e-05   Fold06\n1868  VF   VF 8.580774e-01 1.324519e-01 9.443290e-03 2.738887e-05   Fold06\n1869  VF   VF 9.468715e-01 5.222600e-02 8.895661e-04 1.295499e-05   Fold06\n1870  VF   VF 9.466099e-01 5.248291e-02 8.941449e-04 1.301786e-05   Fold06\n1871  VF   VF 8.585453e-01 1.319565e-01 9.426262e-03 7.192301e-05   Fold06\n1872  VF   VF 8.595084e-01 1.264458e-01 1.403319e-02 1.264456e-05   Fold06\n1873  VF   VF 9.652400e-01 3.332101e-02 1.437765e-03 1.254335e-06   Fold06\n1874  VF   VF 8.926621e-01 9.736516e-02 9.954501e-03 1.822149e-05   Fold06\n1875  VF   VF 8.716804e-01 1.152723e-01 1.301937e-02 2.784384e-05   Fold06\n1876  VF   VF 9.680933e-01 3.051582e-02 1.389846e-03 9.813743e-07   Fold06\n1877  VF   VF 8.273250e-01 1.543033e-01 1.833112e-02 4.055559e-05   Fold06\n1878  VF   VF 8.805077e-01 1.066896e-01 1.277167e-02 3.098999e-05   Fold06\n1879  VF   VF 9.667713e-01 3.156795e-02 1.659383e-03 1.334365e-06   Fold06\n1880  VF   VF 6.394375e-01 3.162265e-01 4.411316e-02 2.228714e-04   Fold06\n1881  VF    F 6.743769e-02 7.987536e-01 1.330086e-01 8.000583e-04   Fold06\n1882  VF    F 7.074850e-02 7.909554e-01 1.375086e-01 7.874587e-04   Fold06\n1883  VF    F 7.575013e-02 7.672482e-01 1.564578e-01 5.437979e-04   Fold06\n1884  VF    F 1.098636e-01 7.793424e-01 1.104575e-01 3.365639e-04   Fold06\n1885  VF    F 6.132819e-02 7.836033e-01 1.538157e-01 1.252823e-03   Fold06\n1886  VF   VF 6.889604e-01 2.654937e-01 4.549414e-02 5.170616e-05   Fold06\n1887  VF   VF 8.026263e-01 1.783146e-01 1.904545e-02 1.366007e-05   Fold06\n1888  VF   VF 8.234330e-01 1.616963e-01 1.486449e-02 6.206126e-06   Fold06\n1889  VF   VF 7.538511e-01 2.241086e-01 2.200851e-02 3.175399e-05   Fold06\n1890  VF   VF 7.598692e-01 2.225286e-01 1.757876e-02 2.342971e-05   Fold06\n1891  VF   VF 7.099197e-01 2.583800e-01 3.164372e-02 5.660453e-05   Fold06\n1892  VF   VF 7.926709e-01 1.916968e-01 1.561799e-02 1.433013e-05   Fold06\n1893  VF   VF 6.513471e-01 3.068053e-01 4.177610e-02 7.156318e-05   Fold06\n1894  VF   VF 8.258463e-01 1.632361e-01 1.091057e-02 7.102564e-06   Fold06\n1895  VF   VF 7.560523e-01 2.235759e-01 2.034975e-02 2.203660e-05   Fold06\n1896  VF   VF 8.234436e-01 1.654913e-01 1.105799e-02 7.169803e-06   Fold06\n1897  VF   VF 6.884956e-01 2.803163e-01 3.111300e-02 7.512549e-05   Fold06\n1898  VF   VF 8.139702e-01 1.699240e-01 1.609654e-02 9.323408e-06   Fold06\n1899  VF   VF 7.725257e-01 2.005717e-01 2.687340e-02 2.919755e-05   Fold06\n1900  VF   VF 6.614387e-01 2.926961e-01 4.576144e-02 1.037656e-04   Fold06\n1901  VF   VF 7.296192e-01 2.342785e-01 3.605872e-02 4.349209e-05   Fold06\n1902  VF   VF 7.360413e-01 2.380346e-01 2.588604e-02 3.813121e-05   Fold06\n1903  VF   VF 7.350930e-01 2.375296e-01 2.733129e-02 4.607592e-05   Fold06\n1904  VF   VF 5.904813e-01 3.452851e-01 6.404299e-02 1.906608e-04   Fold06\n1905  VF   VF 7.097344e-01 2.575520e-01 3.264005e-02 7.359499e-05   Fold06\n1906  VF   VF 8.198374e-01 1.684356e-01 1.171879e-02 8.184719e-06   Fold06\n1907  VF   VF 7.196850e-01 2.491914e-01 3.106342e-02 6.016400e-05   Fold06\n1908  VF   VF 8.426459e-01 1.454554e-01 1.189453e-02 4.106692e-06   Fold06\n1909  VF   VF 5.607278e-01 3.807010e-01 5.833744e-02 2.337648e-04   Fold06\n1910  VF   VF 7.414191e-01 2.340899e-01 2.446381e-02 2.724107e-05   Fold06\n1911  VF    F 7.488348e-02 7.800171e-01 1.405313e-01 4.568088e-03   Fold06\n1912  VF    F 6.375455e-02 7.989088e-01 1.297824e-01 7.554252e-03   Fold06\n1913   F    F 3.865793e-01 5.264143e-01 8.444438e-02 2.562023e-03   Fold06\n1914   F    F 3.068312e-01 5.763985e-01 1.105396e-01 6.230663e-03   Fold06\n1915   F   VF 8.570977e-01 9.730973e-02 4.361082e-02 1.981738e-03   Fold06\n1916   F   VF 6.068394e-01 3.089905e-01 8.375223e-02 4.179282e-04   Fold06\n1917   F    F 1.454957e-01 4.014841e-01 2.687609e-01 1.842593e-01   Fold06\n1918   F   VF 6.626491e-01 2.971070e-01 3.995041e-02 2.935551e-04   Fold06\n1919   F   VF 4.660898e-01 4.079171e-01 1.226141e-01 3.379072e-03   Fold06\n1920   F    M 4.934709e-03 1.554180e-02 9.773344e-01 2.189132e-03   Fold06\n1921   F   VF 6.293654e-01 2.865389e-01 8.373821e-02 3.575256e-04   Fold06\n1922   F   VF 7.713775e-01 2.029408e-01 2.563939e-02 4.232900e-05   Fold06\n1923   F   VF 7.280220e-01 2.356156e-01 3.627836e-02 8.408423e-05   Fold06\n1924   F    F 1.301303e-01 6.847478e-01 1.822660e-01 2.855865e-03   Fold06\n1925   F    F 1.530099e-01 6.854580e-01 1.600044e-01 1.527689e-03   Fold06\n1926   F    F 1.109330e-01 6.733099e-01 2.117493e-01 4.007811e-03   Fold06\n1927   F    F 1.221011e-01 6.830610e-01 1.925907e-01 2.247168e-03   Fold06\n1928   F    F 7.762115e-02 5.393908e-01 3.791155e-01 3.872550e-03   Fold06\n1929   F    F 6.914893e-02 6.772171e-01 2.450543e-01 8.579590e-03   Fold06\n1930   F    F 9.629563e-02 6.682203e-01 2.315110e-01 3.973088e-03   Fold06\n1931   F    F 2.414172e-02 8.320713e-01 1.376095e-01 6.177474e-03   Fold06\n1932   F    F 1.609587e-01 6.950640e-01 1.427448e-01 1.232519e-03   Fold06\n1933   F    F 1.102148e-01 7.122780e-01 1.743803e-01 3.126896e-03   Fold06\n1934   F   VF 9.855673e-01 1.398579e-02 4.465530e-04 3.506497e-07   Fold06\n1935   F   VF 8.163907e-01 1.492695e-01 2.961270e-02 4.727094e-03   Fold06\n1936   F    F 3.132730e-01 3.773021e-01 2.024259e-01 1.069991e-01   Fold06\n1937   F   VF 7.275933e-01 1.549427e-01 1.018345e-01 1.562951e-02   Fold06\n1938   F   VF 9.013458e-01 9.395726e-02 4.687571e-03 9.346345e-06   Fold06\n1939   F   VF 8.916595e-01 1.017953e-01 6.535724e-03 9.488443e-06   Fold06\n1940   F   VF 7.795235e-01 1.906202e-01 2.970146e-02 1.548231e-04   Fold06\n1941   F   VF 8.094014e-01 1.791635e-01 1.138540e-02 4.972626e-05   Fold06\n1942   F   VF 9.360062e-01 6.244334e-02 1.496855e-03 5.360733e-05   Fold06\n1943   F   VF 9.127472e-01 8.473364e-02 2.378068e-03 1.411117e-04   Fold06\n1944   F   VF 8.432445e-01 1.458577e-01 1.086441e-02 3.339378e-05   Fold06\n1945   F   VF 8.886252e-01 1.053961e-01 5.961991e-03 1.677347e-05   Fold06\n1946   F   VF 8.226462e-01 1.641507e-01 1.315771e-02 4.531432e-05   Fold06\n1947   F    L 4.834570e-03 1.985175e-02 2.019537e-02 9.551183e-01   Fold06\n1948   F   VF 8.947519e-01 9.844536e-02 6.791494e-03 1.124385e-05   Fold06\n1949   F   VF 8.783394e-01 1.141768e-01 7.456624e-03 2.716780e-05   Fold06\n1950   F   VF 7.839762e-01 1.958902e-01 1.980414e-02 3.295092e-04   Fold06\n1951   F   VF 5.936021e-01 3.937611e-01 1.255031e-02 8.647146e-05   Fold06\n1952   F   VF 5.907867e-01 3.963534e-01 1.277026e-02 8.963316e-05   Fold06\n1953   F   VF 8.057184e-01 1.789414e-01 1.522825e-02 1.119327e-04   Fold06\n1954   F   VF 5.638589e-01 4.209317e-01 1.508220e-02 1.271296e-04   Fold06\n1955   F   VF 6.581617e-01 2.872532e-01 5.433514e-02 2.499770e-04   Fold06\n1956   F    L 4.912433e-02 5.997767e-02 5.597512e-02 8.349229e-01   Fold06\n1957   F   VF 8.728420e-01 1.136285e-01 1.346586e-02 6.362722e-05   Fold06\n1958   F    F 5.358526e-02 7.760694e-01 1.697171e-01 6.281674e-04   Fold06\n1959   F    F 1.121279e-01 7.742860e-01 1.132159e-01 3.701789e-04   Fold06\n1960   F    F 6.479566e-02 7.963428e-01 1.379715e-01 8.900555e-04   Fold06\n1961   F    F 7.357506e-02 6.756566e-01 2.501718e-01 5.966082e-04   Fold06\n1962   F    F 1.132792e-01 7.742765e-01 1.121323e-01 3.120937e-04   Fold06\n1963   F    F 6.996199e-02 7.892399e-01 1.401932e-01 6.048640e-04   Fold06\n1964   F    F 8.240377e-02 7.860567e-01 1.309569e-01 5.826596e-04   Fold06\n1965   F    F 8.038491e-02 7.403070e-01 1.787378e-01 5.702877e-04   Fold06\n1966   F    F 6.174431e-02 7.935064e-01 1.437501e-01 9.991851e-04   Fold06\n1967   F    F 5.807901e-02 7.876129e-01 1.531209e-01 1.187266e-03   Fold06\n1968   F    F 7.219253e-02 7.973134e-01 1.297345e-01 7.595616e-04   Fold06\n1969   F    F 6.293129e-02 7.612058e-01 1.747561e-01 1.106738e-03   Fold06\n1970   F    F 1.024870e-01 7.794629e-01 1.175947e-01 4.554351e-04   Fold06\n1971   F    F 7.473554e-02 7.295464e-01 1.948528e-01 8.652697e-04   Fold06\n1972   F    F 6.392931e-02 7.709611e-01 1.643244e-01 7.852369e-04   Fold06\n1973   F    F 8.696028e-02 7.922070e-01 1.204313e-01 4.014872e-04   Fold06\n1974   F    F 1.004191e-01 7.776373e-01 1.214419e-01 5.017373e-04   Fold06\n1975   F    F 1.126553e-01 7.823921e-01 1.046657e-01 2.869248e-04   Fold06\n1976   F    F 6.322658e-02 7.163710e-01 2.191675e-01 1.234898e-03   Fold06\n1977   F    F 5.624339e-02 7.601632e-01 1.820658e-01 1.527681e-03   Fold06\n1978   F    F 9.997751e-02 7.687299e-01 1.310368e-01 2.557964e-04   Fold06\n1979   F    F 9.553950e-02 7.955512e-01 1.085720e-01 3.373443e-04   Fold06\n1980   F    F 1.139461e-01 7.826056e-01 1.031731e-01 2.752617e-04   Fold06\n1981   F    F 1.045719e-01 7.503654e-01 1.446960e-01 3.666245e-04   Fold06\n1982   F   VF 6.408849e-01 3.142005e-01 4.486607e-02 4.856880e-05   Fold06\n1983   F   VF 7.549713e-01 2.067590e-01 3.824449e-02 2.516763e-05   Fold06\n1984   F   VF 7.784107e-01 1.982209e-01 2.334863e-02 1.980623e-05   Fold06\n1985   F   VF 8.180672e-01 1.689929e-01 1.293049e-02 9.454018e-06   Fold06\n1986   F   VF 6.478791e-01 3.044374e-01 4.760280e-02 8.067403e-05   Fold06\n1987   F   VF 7.012751e-01 2.520588e-01 4.660522e-02 6.092000e-05   Fold06\n1988   F   VF 7.359519e-01 2.418329e-01 2.217776e-02 3.747273e-05   Fold06\n1989   F    F 4.388022e-01 4.483900e-01 1.076454e-01 5.162450e-03   Fold06\n1990   F   VF 7.175530e-01 2.542021e-01 2.819526e-02 4.960269e-05   Fold06\n1991   F    F 2.896151e-01 4.977747e-01 1.953167e-01 1.729352e-02   Fold06\n1992   F   VF 7.450695e-01 2.307415e-01 2.415068e-02 3.838146e-05   Fold06\n1993   F   VF 6.205951e-01 3.246765e-01 5.459065e-02 1.377008e-04   Fold06\n1994   F   VF 6.450437e-01 3.124337e-01 4.238608e-02 1.365103e-04   Fold06\n1995   F   VF 5.465188e-01 3.489741e-01 1.041202e-01 3.868805e-04   Fold06\n1996   F    F 4.232980e-01 4.241936e-01 1.506824e-01 1.825971e-03   Fold06\n1997   F   VF 5.283343e-01 4.023946e-01 6.893659e-02 3.345401e-04   Fold06\n1998   F   VF 5.940040e-01 3.485171e-01 5.728668e-02 1.922202e-04   Fold06\n1999   F   VF 4.825848e-01 4.190410e-01 9.795699e-02 4.171541e-04   Fold06\n2000   F    F 1.887320e-02 6.216605e-01 3.011684e-01 5.829795e-02   Fold06\n2001   F    F 6.315932e-02 7.799922e-01 1.521340e-01 4.714522e-03   Fold06\n2002   F    F 5.508999e-02 7.949090e-01 1.415556e-01 8.445440e-03   Fold06\n2003   F    F 5.423780e-02 7.900209e-01 1.453067e-01 1.043467e-02   Fold06\n2004   F    F 4.607422e-02 7.572616e-01 1.855054e-01 1.115878e-02   Fold06\n2005   F    F 5.220192e-02 7.996738e-01 1.392674e-01 8.856880e-03   Fold06\n2006   F    F 6.606254e-02 7.843375e-01 1.430201e-01 6.579902e-03   Fold06\n2007   F    F 6.358760e-02 7.874888e-01 1.417747e-01 7.148955e-03   Fold06\n2008   F    F 6.117648e-02 7.797291e-01 1.504567e-01 8.637662e-03   Fold06\n2009   F    F 1.433520e-02 5.235861e-01 3.643542e-01 9.772449e-02   Fold06\n2010   F    F 9.362462e-03 3.955070e-01 2.419715e-01 3.531591e-01   Fold06\n2011   F    F 6.019828e-02 7.775143e-01 1.537074e-01 8.579931e-03   Fold06\n2012   F    F 6.871213e-02 7.847288e-01 1.403156e-01 6.243420e-03   Fold06\n2013   F    F 6.737718e-02 7.903625e-01 1.366518e-01 5.608545e-03   Fold06\n2014   F    F 6.700322e-02 7.894779e-01 1.378250e-01 5.693898e-03   Fold06\n2015   F    F 3.365520e-02 7.228860e-01 2.016500e-01 4.180887e-02   Fold06\n2016   F    F 2.043907e-02 6.597777e-01 2.286092e-01 9.117401e-02   Fold06\n2017   F    F 6.771897e-02 7.874741e-01 1.388271e-01 5.979787e-03   Fold06\n2018   F    F 5.708289e-02 7.362646e-01 1.980042e-01 8.648345e-03   Fold06\n2019   F    F 3.773113e-02 7.627472e-01 1.799942e-01 1.952754e-02   Fold06\n2020   F    F 9.239168e-02 7.958982e-01 1.086682e-01 3.041913e-03   Fold06\n2021   M    F 2.571930e-01 6.033523e-01 1.343059e-01 5.148850e-03   Fold06\n2022   M    M 1.235445e-01 1.587574e-01 5.195034e-01 1.981947e-01   Fold06\n2023   M    M 6.195636e-02 1.234604e-01 5.963385e-01 2.182448e-01   Fold06\n2024   M    F 4.185533e-01 4.715301e-01 1.080550e-01 1.861557e-03   Fold06\n2025   M   VF 4.479141e-01 4.283795e-01 1.214537e-01 2.252728e-03   Fold06\n2026   M    F 7.313612e-02 6.567018e-01 2.644220e-01 5.740117e-03   Fold06\n2027   M    F 9.939780e-02 6.405206e-01 2.558962e-01 4.185340e-03   Fold06\n2028   M    L 8.345076e-03 2.858394e-01 3.329729e-01 3.728426e-01   Fold06\n2029   M    M 1.759871e-02 3.653900e-01 3.865012e-01 2.305101e-01   Fold06\n2030   M    M 1.765324e-02 3.794453e-01 3.881333e-01 2.147681e-01   Fold06\n2031   M    L 1.066578e-02 3.148382e-01 2.810476e-01 3.934484e-01   Fold06\n2032   M    L 2.398019e-04 3.116502e-02 2.012398e-01 7.673554e-01   Fold06\n2033   M    L 8.482343e-05 1.806427e-02 1.680794e-01 8.137715e-01   Fold06\n2034   M    F 3.956028e-02 5.678232e-01 3.740120e-01 1.860449e-02   Fold06\n2035   M    F 3.536910e-02 5.258985e-01 4.200716e-01 1.866086e-02   Fold06\n2036   M   VF 7.658834e-01 2.080928e-01 2.582937e-02 1.944968e-04   Fold06\n2037   M    M 9.832688e-03 4.631839e-01 4.794979e-01 4.748547e-02   Fold06\n2038   M    M 1.113436e-02 4.718605e-01 4.721213e-01 4.488384e-02   Fold06\n2039   M    F 3.576906e-02 7.365249e-01 2.257424e-01 1.963640e-03   Fold06\n2040   M    F 8.835108e-02 7.431351e-01 1.679542e-01 5.595843e-04   Fold06\n2041   M    F 9.888216e-02 7.781679e-01 1.225069e-01 4.430315e-04   Fold06\n2042   M    F 9.131547e-03 4.833095e-01 3.711861e-01 1.363729e-01   Fold06\n2043   M    M 6.759294e-04 1.258582e-01 4.875033e-01 3.859626e-01   Fold06\n2044   M    F 7.928941e-02 7.378276e-01 1.821796e-01 7.033415e-04   Fold06\n2045   M    F 6.922647e-02 7.299378e-01 1.999256e-01 9.101868e-04   Fold06\n2046   M    F 6.678837e-02 7.655240e-01 1.668457e-01 8.419744e-04   Fold06\n2047   M    F 2.239219e-02 6.205152e-01 3.522271e-01 4.865508e-03   Fold06\n2048   M   VF 7.304196e-01 2.261707e-01 4.336766e-02 4.205913e-05   Fold06\n2049   M   VF 6.287329e-01 2.722335e-01 9.894518e-02 8.842739e-05   Fold06\n2050   M    M 1.449089e-02 1.803635e-01 5.796469e-01 2.254987e-01   Fold06\n2051   M   VF 5.139329e-01 4.019478e-01 8.374271e-02 3.765704e-04   Fold06\n2052   M   VF 6.297759e-01 3.153266e-01 5.471086e-02 1.865672e-04   Fold06\n2053   M    L 7.684780e-03 3.562277e-01 1.814900e-01 4.545976e-01   Fold06\n2054   M    L 3.723315e-03 2.553863e-01 1.818909e-01 5.589995e-01   Fold06\n2055   M    L 3.693620e-03 2.539638e-01 1.814596e-01 5.608830e-01   Fold06\n2056   M    F 3.398050e-02 7.104917e-01 2.078574e-01 4.767045e-02   Fold06\n2057   M    F 1.699957e-02 5.935308e-01 2.772192e-01 1.122504e-01   Fold06\n2058   M    F 2.802277e-02 6.285678e-01 2.926800e-01 5.072946e-02   Fold06\n2059   M    F 2.665217e-02 6.063211e-01 3.154876e-01 5.153917e-02   Fold06\n2060   M    F 5.397073e-02 7.733061e-01 1.621036e-01 1.061951e-02   Fold06\n2061   M    F 2.214798e-02 6.400538e-01 2.879029e-01 4.989537e-02   Fold06\n2062   L    M 5.085915e-02 1.279589e-01 5.346177e-01 2.865643e-01   Fold06\n2063   L    L 1.511928e-03 2.881694e-02 3.045803e-01 6.650909e-01   Fold06\n2064   L    M 9.271214e-02 1.420267e-01 5.228025e-01 2.424587e-01   Fold06\n2065   L    F 8.507716e-02 7.026335e-01 2.072259e-01 5.063476e-03   Fold06\n2066   L    L 6.999926e-05 1.618126e-02 1.579062e-01 8.258426e-01   Fold06\n2067   L    L 2.441309e-04 3.271910e-02 2.745594e-01 6.924774e-01   Fold06\n2068   L    F 8.133032e-02 6.648205e-01 2.471010e-01 6.748124e-03   Fold06\n2069   L    L 1.447982e-16 7.030129e-12 1.412199e-07 9.999999e-01   Fold06\n2070   L    L 9.787038e-06 3.231026e-03 1.072371e-02 9.860355e-01   Fold06\n2071   L    M 4.523283e-04 1.389815e-01 5.896570e-01 2.709091e-01   Fold06\n2072   L    F 4.216652e-02 7.234277e-01 2.321265e-01 2.279252e-03   Fold06\n2073   L    F 4.155053e-02 7.235652e-01 2.326002e-01 2.284010e-03   Fold06\n2074   L    M 1.924973e-02 2.230522e-01 5.260555e-01 2.316426e-01   Fold06\n2075   L    L 4.397054e-07 2.614061e-04 1.096824e-03 9.986413e-01   Fold06\n2076   L    L 9.336702e-07 3.656366e-04 1.172643e-03 9.984608e-01   Fold06\n2077   L    L 1.500937e-06 5.391212e-04 1.574478e-03 9.978849e-01   Fold06\n2078   L    L 1.442868e-06 5.238754e-04 1.546203e-03 9.979285e-01   Fold06\n2079   L    L 4.190614e-05 1.598216e-02 8.168084e-02 9.022951e-01   Fold06\n2080   L    F 3.904202e-02 7.398036e-01 2.026970e-01 1.845733e-02   Fold06\n2081   L    F 2.287801e-02 6.853287e-01 2.402746e-01 5.151863e-02   Fold06\n2082   L    F 2.311580e-02 6.874308e-01 2.389720e-01 5.048140e-02   Fold06\n2083  VF   VF 9.534535e-01 4.237568e-02 4.163943e-03 6.843660e-06   Fold07\n2084  VF   VF 9.616171e-01 3.551236e-02 2.867298e-03 3.288119e-06   Fold07\n2085  VF   VF 9.693959e-01 2.845582e-02 2.146341e-03 1.973602e-06   Fold07\n2086  VF   VF 9.529592e-01 4.352765e-02 3.505685e-03 7.504453e-06   Fold07\n2087  VF   VF 9.700753e-01 2.769779e-02 2.224777e-03 2.138509e-06   Fold07\n2088  VF   VF 8.917249e-01 8.251027e-02 2.559267e-02 1.721312e-04   Fold07\n2089  VF    F 3.394970e-01 5.540628e-01 1.049883e-01 1.451882e-03   Fold07\n2090  VF    F 3.560519e-01 5.488736e-01 9.357257e-02 1.501916e-03   Fold07\n2091  VF    F 2.975172e-01 5.803266e-01 1.178941e-01 4.262080e-03   Fold07\n2092  VF   VF 9.568821e-01 3.771361e-02 5.401962e-03 2.316976e-06   Fold07\n2093  VF   VF 8.580494e-01 1.214592e-01 2.044815e-02 4.325762e-05   Fold07\n2094  VF   VF 8.573182e-01 1.223916e-01 2.024792e-02 4.224684e-05   Fold07\n2095  VF   VF 9.527470e-01 4.286796e-02 4.381072e-03 3.967520e-06   Fold07\n2096  VF   VF 8.612858e-01 1.083148e-01 3.035813e-02 4.125327e-05   Fold07\n2097  VF   VF 8.264198e-01 1.388856e-01 3.462888e-02 6.577056e-05   Fold07\n2098  VF   VF 9.454045e-01 4.718392e-02 7.407541e-03 4.013281e-06   Fold07\n2099  VF    M 9.864111e-02 1.180734e-01 4.291796e-01 3.541059e-01   Fold07\n2100  VF   VF 8.466955e-01 1.150186e-01 3.820955e-02 7.627524e-05   Fold07\n2101  VF   VF 9.525558e-01 4.265266e-02 4.789138e-03 2.394556e-06   Fold07\n2102  VF   VF 5.614100e-01 3.216417e-01 1.166651e-01 2.831326e-04   Fold07\n2103  VF   VF 9.842582e-01 1.487010e-02 8.711071e-04 5.733196e-07   Fold07\n2104  VF   VF 9.889453e-01 1.061109e-02 4.433320e-04 2.508709e-07   Fold07\n2105  VF   VF 9.296270e-01 5.837150e-02 1.149564e-02 5.058429e-04   Fold07\n2106  VF   VF 9.752478e-01 2.317931e-02 1.568451e-03 4.458081e-06   Fold07\n2107  VF   VF 8.684573e-01 1.146737e-01 1.676231e-02 1.067452e-04   Fold07\n2108  VF   VF 9.853637e-01 1.399944e-02 6.364340e-04 4.759609e-07   Fold07\n2109  VF   VF 9.863799e-01 1.291096e-02 7.087394e-04 3.878930e-07   Fold07\n2110  VF   VF 9.839176e-01 1.518122e-02 9.007060e-04 4.703050e-07   Fold07\n2111  VF   VF 7.085333e-01 2.513129e-01 3.998474e-02 1.690102e-04   Fold07\n2112  VF   VF 7.502161e-01 2.216706e-01 2.801770e-02 9.554160e-05   Fold07\n2113  VF   VF 6.931023e-01 2.729810e-01 3.378069e-02 1.359898e-04   Fold07\n2114  VF   VF 5.748839e-01 3.539078e-01 7.037739e-02 8.308952e-04   Fold07\n2115  VF   VF 4.365163e-01 3.822112e-01 1.775288e-01 3.743670e-03   Fold07\n2116  VF   VF 7.133479e-01 2.560825e-01 3.050556e-02 6.408563e-05   Fold07\n2117  VF   VF 7.922640e-01 1.919029e-01 1.581701e-02 1.608612e-05   Fold07\n2118  VF   VF 8.041380e-01 1.840202e-01 1.182778e-02 1.396583e-05   Fold07\n2119  VF   VF 8.435367e-01 1.486958e-01 7.761367e-03 6.133112e-06   Fold07\n2120  VF   VF 8.136323e-01 1.717163e-01 1.463417e-02 1.721634e-05   Fold07\n2121  VF    F 9.608062e-02 7.136576e-01 1.880363e-01 2.225425e-03   Fold07\n2122  VF    F 9.527628e-02 7.136636e-01 1.888184e-01 2.241646e-03   Fold07\n2123  VF    F 1.157205e-01 6.996244e-01 1.834804e-01 1.174706e-03   Fold07\n2124  VF    F 1.424086e-01 6.663262e-01 1.905679e-01 6.973455e-04   Fold07\n2125  VF    F 1.364615e-01 6.878136e-01 1.747904e-01 9.345910e-04   Fold07\n2126  VF    F 1.387216e-01 6.688690e-01 1.916914e-01 7.181081e-04   Fold07\n2127  VF    F 1.384738e-01 6.767472e-01 1.841202e-01 6.587669e-04   Fold07\n2128  VF    F 1.396212e-01 6.724650e-01 1.872345e-01 6.792673e-04   Fold07\n2129  VF   VF 9.874039e-01 1.225537e-02 3.405547e-04 1.657815e-07   Fold07\n2130  VF   VF 9.746147e-01 2.436470e-02 1.020217e-03 4.160279e-07   Fold07\n2131  VF   VF 9.939357e-01 5.926148e-03 1.380801e-04 4.122459e-08   Fold07\n2132  VF   VF 9.922723e-01 7.573278e-03 1.543614e-04 5.585060e-08   Fold07\n2133  VF   VF 9.878851e-01 1.169552e-02 4.192385e-04 1.630668e-07   Fold07\n2134  VF   VF 9.869574e-01 1.264688e-02 3.953381e-04 3.848384e-07   Fold07\n2135  VF   VF 9.369669e-01 5.413620e-02 8.828239e-03 6.864540e-05   Fold07\n2136  VF   VF 9.561630e-01 4.128327e-02 2.551094e-03 2.620662e-06   Fold07\n2137  VF   VF 9.876393e-01 1.188301e-02 4.772843e-04 4.306855e-07   Fold07\n2138  VF   VF 9.775238e-01 2.162021e-02 8.547104e-04 1.317222e-06   Fold07\n2139  VF   VF 9.558417e-01 4.179105e-02 2.363509e-03 3.709587e-06   Fold07\n2140  VF   VF 9.549471e-01 4.297451e-02 2.073495e-03 4.923440e-06   Fold07\n2141  VF   VF 9.583073e-01 3.963157e-02 2.058220e-03 2.945566e-06   Fold07\n2142  VF   VF 9.909740e-01 8.803821e-03 2.221027e-04 7.694042e-08   Fold07\n2143  VF   VF 9.870960e-01 1.229148e-02 6.120870e-04 4.123293e-07   Fold07\n2144  VF   VF 9.806142e-01 1.861698e-02 7.681177e-04 7.018003e-07   Fold07\n2145  VF   VF 9.900497e-01 9.589610e-03 3.604984e-04 1.540707e-07   Fold07\n2146  VF   VF 9.778475e-01 2.122596e-02 9.260818e-04 4.981956e-07   Fold07\n2147  VF   VF 9.901065e-01 9.516702e-03 3.766527e-04 1.749394e-07   Fold07\n2148  VF   VF 9.924225e-01 7.380885e-03 1.965153e-04 6.071993e-08   Fold07\n2149  VF   VF 9.582274e-01 3.913765e-02 2.631897e-03 3.012068e-06   Fold07\n2150  VF   VF 8.853128e-01 1.064641e-01 8.203704e-03 1.943839e-05   Fold07\n2151  VF   VF 9.837603e-01 1.556490e-02 6.741459e-04 6.234511e-07   Fold07\n2152  VF   VF 9.868565e-01 1.277079e-02 3.723751e-04 3.077440e-07   Fold07\n2153  VF   VF 9.883765e-01 1.132849e-02 2.948557e-04 1.279585e-07   Fold07\n2154  VF   VF 9.820419e-01 1.740559e-02 5.519137e-04 5.723541e-07   Fold07\n2155  VF   VF 9.718381e-01 2.686257e-02 1.295930e-03 3.349106e-06   Fold07\n2156  VF   VF 9.851933e-01 1.417026e-02 6.360579e-04 3.562744e-07   Fold07\n2157  VF   VF 9.849377e-01 1.435769e-02 7.041460e-04 4.300119e-07   Fold07\n2158  VF   VF 9.895968e-01 1.008570e-02 3.173636e-04 1.456078e-07   Fold07\n2159  VF   VF 9.928862e-01 6.929173e-03 1.845527e-04 4.413808e-08   Fold07\n2160  VF   VF 9.312515e-01 6.427153e-02 4.471306e-03 5.671270e-06   Fold07\n2161  VF   VF 9.198092e-01 7.570647e-02 4.476049e-03 8.240480e-06   Fold07\n2162  VF   VF 9.895271e-01 1.004529e-02 4.272487e-04 3.244662e-07   Fold07\n2163  VF   VF 9.757796e-01 2.303265e-02 1.186802e-03 9.082533e-07   Fold07\n2164  VF   VF 9.930878e-01 6.746554e-03 1.655969e-04 5.351954e-08   Fold07\n2165  VF   VF 8.582291e-01 1.321783e-01 9.575545e-03 1.711020e-05   Fold07\n2166  VF   VF 8.969707e-01 9.764559e-02 5.378718e-03 5.012970e-06   Fold07\n2167  VF   VF 9.052681e-01 9.045760e-02 4.270573e-03 3.696863e-06   Fold07\n2168  VF   VF 8.948614e-01 1.003686e-01 4.765395e-03 4.613323e-06   Fold07\n2169  VF   VF 8.942553e-01 1.009680e-01 4.769914e-03 6.791877e-06   Fold07\n2170  VF   VF 8.932471e-01 1.016086e-01 5.135892e-03 8.393978e-06   Fold07\n2171  VF   VF 8.693164e-01 1.217390e-01 8.928571e-03 1.609297e-05   Fold07\n2172  VF   VF 9.090522e-01 8.760374e-02 3.340188e-03 3.825499e-06   Fold07\n2173  VF    M 3.129911e-01 1.151795e-01 5.717299e-01 9.950676e-05   Fold07\n2174  VF   VF 9.082746e-01 8.789480e-02 3.825665e-03 4.939182e-06   Fold07\n2175  VF   VF 8.979164e-01 9.673320e-02 5.344284e-03 6.130130e-06   Fold07\n2176  VF   VF 9.074820e-01 8.832460e-02 4.187546e-03 5.886735e-06   Fold07\n2177  VF   VF 9.138143e-01 8.243132e-02 3.749450e-03 4.921895e-06   Fold07\n2178  VF   VF 8.502431e-01 1.402114e-01 9.527871e-03 1.767849e-05   Fold07\n2179  VF   VF 8.481888e-01 1.421600e-01 9.633297e-03 1.792063e-05   Fold07\n2180  VF   VF 8.553585e-01 1.382639e-01 6.357677e-03 1.999272e-05   Fold07\n2181  VF   VF 9.041158e-01 9.196834e-02 3.912750e-03 3.122034e-06   Fold07\n2182  VF   VF 6.954998e-01 2.976096e-01 6.871765e-03 1.881780e-05   Fold07\n2183  VF   VF 8.887095e-01 1.052144e-01 6.068333e-03 7.800299e-06   Fold07\n2184  VF   VF 8.023725e-01 1.820103e-01 1.546156e-02 1.555817e-04   Fold07\n2185  VF   VF 8.473131e-01 1.409514e-01 1.170818e-02 2.731898e-05   Fold07\n2186  VF   VF 8.585674e-01 1.307415e-01 1.066752e-02 2.364632e-05   Fold07\n2187  VF   VF 8.751336e-01 1.137950e-01 1.106006e-02 1.142566e-05   Fold07\n2188  VF   VF 8.896849e-01 1.040448e-01 6.261810e-03 8.471645e-06   Fold07\n2189  VF   VF 8.585063e-01 1.346258e-01 6.843705e-03 2.419104e-05   Fold07\n2190  VF   VF 6.382628e-01 3.091657e-01 4.921889e-02 3.352632e-03   Fold07\n2191  VF   VF 6.778470e-01 3.142508e-01 7.876663e-03 2.558238e-05   Fold07\n2192  VF   VF 8.805941e-01 1.126848e-01 6.712047e-03 9.017484e-06   Fold07\n2193  VF   VF 5.775746e-01 3.557902e-01 5.560476e-02 1.103049e-02   Fold07\n2194  VF   VF 9.063012e-01 8.943409e-02 4.260946e-03 3.753001e-06   Fold07\n2195  VF   VF 8.826291e-01 1.100190e-01 7.340144e-03 1.181814e-05   Fold07\n2196  VF   VF 8.800868e-01 1.124301e-01 7.470923e-03 1.218257e-05   Fold07\n2197  VF   VF 9.135571e-01 8.313740e-02 3.302034e-03 3.499793e-06   Fold07\n2198  VF   VF 6.656638e-01 3.241953e-01 1.009713e-02 4.374884e-05   Fold07\n2199  VF   VF 8.868464e-01 1.065637e-01 6.584814e-03 5.112427e-06   Fold07\n2200  VF   VF 6.647898e-01 3.250083e-01 1.015763e-02 4.432734e-05   Fold07\n2201  VF   VF 8.862434e-01 1.072045e-01 6.538031e-03 1.409642e-05   Fold07\n2202  VF   VF 6.589537e-01 3.303792e-01 1.061820e-02 4.888503e-05   Fold07\n2203  VF   VF 9.018466e-01 9.419617e-02 3.952154e-03 5.028731e-06   Fold07\n2204  VF   VF 9.242625e-01 7.272751e-02 3.006823e-03 3.159516e-06   Fold07\n2205  VF   VF 8.276147e-01 1.611150e-01 1.119260e-02 7.770332e-05   Fold07\n2206  VF   VF 8.749043e-01 1.199100e-01 5.172926e-03 1.275924e-05   Fold07\n2207  VF   VF 8.424772e-01 1.454285e-01 1.206966e-02 2.465187e-05   Fold07\n2208  VF   VF 9.222081e-01 7.483270e-02 2.956148e-03 3.018625e-06   Fold07\n2209  VF   VF 8.529068e-01 1.356149e-01 1.144923e-02 2.910903e-05   Fold07\n2210  VF   VF 8.718236e-01 1.207921e-01 7.373638e-03 1.065008e-05   Fold07\n2211  VF   VF 9.059310e-01 8.989062e-02 4.174690e-03 3.694560e-06   Fold07\n2212  VF   VF 9.217143e-01 7.511933e-02 3.162863e-03 3.502875e-06   Fold07\n2213  VF   VF 8.733309e-01 1.193211e-01 7.337122e-03 1.082614e-05   Fold07\n2214  VF   VF 8.952588e-01 9.987005e-02 4.863626e-03 7.488473e-06   Fold07\n2215  VF   VF 9.730933e-01 2.597099e-02 9.354167e-04 3.217169e-07   Fold07\n2216  VF   VF 9.509212e-01 4.696995e-02 2.107647e-03 1.203490e-06   Fold07\n2217  VF   VF 9.610850e-01 3.763571e-02 1.278370e-03 9.238715e-07   Fold07\n2218  VF   VF 9.122870e-01 8.308868e-02 4.620225e-03 4.066526e-06   Fold07\n2219  VF   VF 9.708751e-01 2.813041e-02 9.941381e-04 3.609594e-07   Fold07\n2220  VF   VF 9.309798e-01 6.403500e-02 4.981260e-03 3.936048e-06   Fold07\n2221  VF   VF 9.519267e-01 4.628195e-02 1.789738e-03 1.637597e-06   Fold07\n2222  VF   VF 9.128224e-01 8.278647e-02 4.384371e-03 6.754654e-06   Fold07\n2223  VF   VF 9.630377e-01 3.566861e-02 1.293221e-03 4.783504e-07   Fold07\n2224  VF   VF 8.517998e-01 1.357989e-01 1.238741e-02 1.390796e-05   Fold07\n2225  VF   VF 4.802893e-01 4.311897e-01 8.814824e-02 3.728280e-04   Fold07\n2226  VF   VF 9.797277e-01 1.949633e-02 7.755850e-04 3.377212e-07   Fold07\n2227  VF   VF 9.686247e-01 3.038095e-02 9.940140e-04 3.524198e-07   Fold07\n2228  VF   VF 9.683558e-01 3.064852e-02 9.953497e-04 3.495497e-07   Fold07\n2229  VF   VF 9.147545e-01 7.989480e-02 5.343605e-03 7.122217e-06   Fold07\n2230  VF   VF 9.620878e-01 3.640145e-02 1.510093e-03 6.591975e-07   Fold07\n2231  VF    F 8.833487e-02 7.772245e-01 1.340706e-01 3.700076e-04   Fold07\n2232  VF    F 8.227526e-02 7.706643e-01 1.466248e-01 4.356182e-04   Fold07\n2233  VF    F 1.066043e-01 7.503329e-01 1.428250e-01 2.377198e-04   Fold07\n2234  VF    F 1.262514e-01 7.613052e-01 1.122384e-01 2.050759e-04   Fold07\n2235  VF    F 9.588809e-02 7.798339e-01 1.239829e-01 2.950987e-04   Fold07\n2236  VF   VF 8.113921e-01 1.756566e-01 1.294458e-02 6.772152e-06   Fold07\n2237  VF   VF 8.146732e-01 1.723142e-01 1.300579e-02 6.777062e-06   Fold07\n2238  VF   VF 7.903209e-01 1.943378e-01 1.533450e-02 6.779010e-06   Fold07\n2239  VF   VF 7.163324e-01 2.608264e-01 2.281878e-02 2.239427e-05   Fold07\n2240  VF   VF 7.795908e-01 2.052897e-01 1.511294e-02 6.563314e-06   Fold07\n2241  VF   VF 7.789987e-01 2.057838e-01 1.521098e-02 6.561891e-06   Fold07\n2242  VF   VF 8.070862e-01 1.803906e-01 1.251647e-02 6.799602e-06   Fold07\n2243  VF   VF 7.612034e-01 2.234031e-01 1.538315e-02 1.033860e-05   Fold07\n2244  VF   VF 8.244354e-01 1.659519e-01 9.608662e-03 4.078993e-06   Fold07\n2245  VF   VF 8.257835e-01 1.645644e-01 9.647904e-03 4.126310e-06   Fold07\n2246  VF   VF 7.273848e-01 2.496881e-01 2.290638e-02 2.065453e-05   Fold07\n2247  VF   VF 7.245380e-01 2.493035e-01 2.614098e-02 1.755674e-05   Fold07\n2248  VF   VF 6.915077e-01 2.871757e-01 2.128504e-02 3.158193e-05   Fold07\n2249  VF   VF 6.945551e-01 2.780277e-01 2.738574e-02 3.150471e-05   Fold07\n2250  VF   VF 6.758065e-01 2.900987e-01 3.406646e-02 2.829956e-05   Fold07\n2251  VF   VF 7.703472e-01 2.157602e-01 1.388448e-02 8.088824e-06   Fold07\n2252  VF   VF 7.795412e-01 2.075271e-01 1.292453e-02 7.179371e-06   Fold07\n2253  VF   VF 7.773885e-01 2.094070e-01 1.319712e-02 7.479921e-06   Fold07\n2254  VF   VF 6.924973e-01 2.736977e-01 3.376969e-02 3.533574e-05   Fold07\n2255  VF   VF 8.046750e-01 1.841350e-01 1.118460e-02 5.331669e-06   Fold07\n2256  VF   VF 7.952259e-01 1.906119e-01 1.415372e-02 8.435924e-06   Fold07\n2257  VF    F 8.949814e-02 7.754776e-01 1.326077e-01 2.416601e-03   Fold07\n2258  VF    F 8.127567e-02 7.823114e-01 1.344283e-01 1.984583e-03   Fold07\n2259   F   VF 9.539072e-01 4.167564e-02 4.409392e-03 7.758133e-06   Fold07\n2260   F   VF 9.504847e-01 4.448049e-02 5.024765e-03 9.995157e-06   Fold07\n2261   F   VF 9.502466e-01 4.478075e-02 4.962907e-03 9.724210e-06   Fold07\n2262   F   VF 8.877380e-01 8.516486e-02 2.691044e-02 1.867428e-04   Fold07\n2263   F    F 3.102963e-01 5.478796e-01 1.399038e-01 1.920231e-03   Fold07\n2264   F    F 3.479192e-01 5.546338e-01 9.587374e-02 1.573221e-03   Fold07\n2265   F    L 6.434805e-03 1.358419e-02 4.495763e-02 9.350234e-01   Fold07\n2266   F    F 2.001517e-01 3.941323e-01 3.765893e-01 2.912659e-02   Fold07\n2267   F   VF 5.850412e-01 3.534134e-01 6.079488e-02 7.505179e-04   Fold07\n2268   F    F 1.491164e-01 3.965946e-01 3.120777e-01 1.422112e-01   Fold07\n2269   F    F 2.442840e-01 4.511395e-01 2.439287e-01 6.064785e-02   Fold07\n2270   F   VF 6.571374e-01 3.011723e-01 4.149363e-02 1.966206e-04   Fold07\n2271   F   VF 5.968741e-01 3.241775e-01 7.802543e-02 9.229070e-04   Fold07\n2272   F   VF 7.869832e-01 1.876034e-01 2.539465e-02 1.867778e-05   Fold07\n2273   F    F 1.219498e-01 6.846127e-01 1.922040e-01 1.233450e-03   Fold07\n2274   F    F 1.253544e-01 6.631100e-01 2.105996e-01 9.360219e-04   Fold07\n2275   F    F 1.172559e-01 6.685628e-01 2.125938e-01 1.587478e-03   Fold07\n2276   F    F 6.805486e-02 6.027293e-01 3.256267e-01 3.589092e-03   Fold07\n2277   F    F 1.143110e-01 6.749192e-01 2.092496e-01 1.520126e-03   Fold07\n2278   F    F 1.145108e-01 6.635106e-01 2.202440e-01 1.734546e-03   Fold07\n2279   F    F 5.540221e-02 5.972923e-01 3.428214e-01 4.484109e-03   Fold07\n2280   F    F 9.009470e-02 6.499208e-01 2.569727e-01 3.011760e-03   Fold07\n2281   F    F 5.978621e-02 6.483774e-01 2.837815e-01 8.054807e-03   Fold07\n2282   F    F 6.050331e-02 6.536883e-01 2.781795e-01 7.628923e-03   Fold07\n2283   F    L 3.222640e-16 6.374744e-13 1.361531e-08 1.000000e+00   Fold07\n2284   F   VF 4.810723e-01 4.352002e-01 8.261963e-02 1.107803e-03   Fold07\n2285   F   VF 6.049618e-01 1.730641e-01 1.634772e-01 5.849687e-02   Fold07\n2286   F    L 3.109786e-01 1.240764e-01 2.404486e-01 3.244964e-01   Fold07\n2287   F   VF 6.130802e-01 2.458268e-01 1.093083e-01 3.178469e-02   Fold07\n2288   F   VF 8.812641e-01 1.103590e-01 8.368089e-03 8.786008e-06   Fold07\n2289   F   VF 8.162403e-01 1.667268e-01 1.697911e-02 5.374443e-05   Fold07\n2290   F   VF 7.906233e-01 1.869563e-01 2.226153e-02 1.589052e-04   Fold07\n2291   F   VF 7.900642e-01 1.875959e-01 2.218437e-02 1.556082e-04   Fold07\n2292   F   VF 8.671803e-01 1.262107e-01 6.587387e-03 2.168785e-05   Fold07\n2293   F   VF 8.839279e-01 1.105213e-01 5.541111e-03 9.728738e-06   Fold07\n2294   F   VF 8.408449e-01 1.473267e-01 1.180229e-02 2.612568e-05   Fold07\n2295   F    L 1.293028e-02 4.131919e-02 3.519344e-02 9.105571e-01   Fold07\n2296   F    L 1.313102e-02 4.139627e-02 3.541970e-02 9.100530e-01   Fold07\n2297   F    M 1.020339e-01 2.348420e-01 3.336162e-01 3.295079e-01   Fold07\n2298   F   VF 6.682954e-01 3.231796e-01 8.494774e-03 3.018457e-05   Fold07\n2299   F   VF 6.559748e-01 3.346703e-01 9.317949e-03 3.697065e-05   Fold07\n2300   F   VF 7.975499e-01 1.895410e-01 1.283192e-02 7.723827e-05   Fold07\n2301   F   VF 6.379876e-01 3.513524e-01 1.061072e-02 4.919989e-05   Fold07\n2302   F   VF 6.344235e-01 3.546306e-01 1.089370e-02 5.218966e-05   Fold07\n2303   F   VF 6.261234e-01 3.622705e-01 1.154673e-02 5.934163e-05   Fold07\n2304   F   VF 6.211074e-01 3.668696e-01 1.195880e-02 6.415550e-05   Fold07\n2305   F   VF 6.036477e-01 3.827981e-01 1.347049e-02 8.364984e-05   Fold07\n2306   F   VF 5.961134e-01 3.896297e-01 1.416331e-02 9.358182e-05   Fold07\n2307   F   VF 8.369144e-01 1.504011e-01 1.265520e-02 2.931517e-05   Fold07\n2308   F    M 2.316165e-01 3.351302e-01 3.849293e-01 4.832393e-02   Fold07\n2309   F    L 6.377337e-02 7.353100e-02 6.678145e-02 7.959142e-01   Fold07\n2310   F   VF 7.256981e-01 2.467866e-01 2.743554e-02 7.975653e-05   Fold07\n2311   F    F 1.148678e-01 7.442885e-01 1.404979e-01 3.458427e-04   Fold07\n2312   F    F 8.024232e-02 7.448362e-01 1.745104e-01 4.111646e-04   Fold07\n2313   F    F 8.646726e-02 7.654451e-01 1.476311e-01 4.565385e-04   Fold07\n2314   F    F 8.049794e-02 7.406780e-01 1.784002e-01 4.238846e-04   Fold07\n2315   F    F 1.096257e-01 7.496030e-01 1.405947e-01 1.765991e-04   Fold07\n2316   F    F 8.418816e-02 7.750503e-01 1.403372e-01 4.242539e-04   Fold07\n2317   F    F 6.220570e-02 7.112362e-01 2.256689e-01 8.891655e-04   Fold07\n2318   F    F 8.066484e-02 7.923519e-01 1.264335e-01 5.497890e-04   Fold07\n2319   F    F 5.100372e-02 7.194244e-01 2.285639e-01 1.008013e-03   Fold07\n2320   F    F 6.651854e-02 7.471796e-01 1.855720e-01 7.299439e-04   Fold07\n2321   F    F 1.181118e-01 7.645950e-01 1.170649e-01 2.282005e-04   Fold07\n2322   F    F 8.073453e-02 7.550415e-01 1.636440e-01 5.799911e-04   Fold07\n2323   F    F 5.598527e-02 7.609156e-01 1.815828e-01 1.516313e-03   Fold07\n2324   F    F 5.882860e-02 6.625339e-01 2.779752e-01 6.623148e-04   Fold07\n2325   F    F 6.412035e-02 7.392904e-01 1.959916e-01 5.976428e-04   Fold07\n2326   F    F 4.451864e-02 6.981239e-01 2.558665e-01 1.490960e-03   Fold07\n2327   F    F 7.645327e-02 7.468806e-01 1.758830e-01 7.831595e-04   Fold07\n2328   F    F 7.144733e-02 7.393692e-01 1.882207e-01 9.627264e-04   Fold07\n2329   F    F 3.924645e-02 6.520903e-01 3.064453e-01 2.217974e-03   Fold07\n2330   F    F 7.195726e-02 7.148826e-01 2.124569e-01 7.032834e-04   Fold07\n2331   F   VF 7.536093e-01 2.268794e-01 1.949823e-02 1.312777e-05   Fold07\n2332   F   VF 6.782604e-01 2.700223e-01 5.168180e-02 3.547657e-05   Fold07\n2333   F   VF 6.987017e-01 2.672746e-01 3.399381e-02 2.990204e-05   Fold07\n2334   F   VF 7.226936e-01 2.477086e-01 2.957248e-02 2.531464e-05   Fold07\n2335   F   VF 5.980833e-01 3.499872e-01 5.186252e-02 6.704995e-05   Fold07\n2336   F   VF 6.358401e-01 3.090627e-01 5.504803e-02 4.915266e-05   Fold07\n2337   F    F 2.998885e-01 4.993787e-01 1.897101e-01 1.102271e-02   Fold07\n2338   F   VF 6.510877e-01 3.128747e-01 3.595529e-02 8.227535e-05   Fold07\n2339   F   VF 5.835896e-01 3.724464e-01 4.382406e-02 1.399082e-04   Fold07\n2340   F   VF 6.258918e-01 3.319179e-01 4.209554e-02 9.473695e-05   Fold07\n2341   F    F 8.829615e-02 7.761960e-01 1.332747e-01 2.233175e-03   Fold07\n2342   F    F 7.016484e-02 7.671005e-01 1.601796e-01 2.555015e-03   Fold07\n2343   F    F 7.006903e-02 7.708640e-01 1.564316e-01 2.635375e-03   Fold07\n2344   F    F 4.871224e-02 7.459778e-01 1.986797e-01 6.630256e-03   Fold07\n2345   F    F 5.384421e-02 8.136337e-01 1.263525e-01 6.169546e-03   Fold07\n2346   F    F 4.749898e-02 7.352208e-01 2.096744e-01 7.605825e-03   Fold07\n2347   F    F 7.714459e-02 7.828405e-01 1.364693e-01 3.545570e-03   Fold07\n2348   F    F 6.447928e-02 7.651083e-01 1.665841e-01 3.828361e-03   Fold07\n2349   F    F 6.441770e-02 7.653339e-01 1.664236e-01 3.824741e-03   Fold07\n2350   F    F 5.973253e-02 7.973743e-01 1.385880e-01 4.305129e-03   Fold07\n2351   F    F 1.696226e-02 5.444799e-01 3.763487e-01 6.220919e-02   Fold07\n2352   F    F 5.784111e-02 7.945485e-01 1.428867e-01 4.723627e-03   Fold07\n2353   F    F 5.205369e-02 7.957382e-01 1.436564e-01 8.551767e-03   Fold07\n2354   F    F 6.088764e-02 7.622135e-01 1.725633e-01 4.335566e-03   Fold07\n2355   F    F 2.237478e-02 6.741745e-01 2.223332e-01 8.111753e-02   Fold07\n2356   F    F 2.205889e-02 6.922369e-01 2.129548e-01 7.274939e-02   Fold07\n2357   F    F 7.228489e-02 7.801132e-01 1.434333e-01 4.168562e-03   Fold07\n2358   F    F 6.011680e-02 7.665304e-01 1.694227e-01 3.930165e-03   Fold07\n2359   F    F 9.700475e-02 7.968634e-01 1.044756e-01 1.656264e-03   Fold07\n2360   F    F 9.738602e-02 7.971013e-01 1.038868e-01 1.625950e-03   Fold07\n2361   F    F 6.124676e-02 7.720605e-01 1.630582e-01 3.634548e-03   Fold07\n2362   F    F 8.376521e-02 7.843976e-01 1.300024e-01 1.834766e-03   Fold07\n2363   F    F 1.033400e-01 7.881767e-01 1.067837e-01 1.699663e-03   Fold07\n2364   F    F 9.657030e-02 8.001466e-01 1.016904e-01 1.592706e-03   Fold07\n2365   F    F 1.026250e-01 7.806206e-01 1.147670e-01 1.987353e-03   Fold07\n2366   M    M 4.947858e-02 8.721043e-02 5.304680e-01 3.328430e-01   Fold07\n2367   M    M 3.731210e-02 7.237551e-02 5.109462e-01 3.793662e-01   Fold07\n2368   M   VF 4.811100e-01 3.901067e-01 1.272622e-01 1.521138e-03   Fold07\n2369   M    L 5.652663e-03 3.275032e-02 4.914992e-02 9.124471e-01   Fold07\n2370   M    L 2.373169e-03 1.384210e-01 4.149017e-01 4.443041e-01   Fold07\n2371   M    M 1.730942e-02 4.196160e-01 4.293397e-01 1.337349e-01   Fold07\n2372   M    F 1.602580e-02 4.192334e-01 3.859298e-01 1.788109e-01   Fold07\n2373   M    L 1.197859e-04 1.766814e-02 1.929450e-01 7.892670e-01   Fold07\n2374   M    L 1.343216e-04 1.965695e-02 2.019965e-01 7.782122e-01   Fold07\n2375   M    F 1.316831e-01 6.777698e-01 1.898217e-01 7.254534e-04   Fold07\n2376   M    F 8.391164e-02 6.225796e-01 2.891008e-01 4.408012e-03   Fold07\n2377   M    M 2.229758e-04 9.621402e-03 9.876429e-01 2.512744e-03   Fold07\n2378   M   VF 7.499963e-01 2.237314e-01 2.601711e-02 2.551905e-04   Fold07\n2379   M    L 8.612143e-03 3.311301e-02 3.643511e-02 9.218397e-01   Fold07\n2380   M   VF 8.664274e-01 1.146302e-01 1.893198e-02 1.040481e-05   Fold07\n2381   M    L 4.118856e-18 1.122439e-13 3.959066e-09 1.000000e+00   Fold07\n2382   M   VF 6.612540e-01 2.755539e-01 6.144490e-02 1.747157e-03   Fold07\n2383   M   VF 7.690233e-01 2.041924e-01 2.666338e-02 1.208733e-04   Fold07\n2384   M    F 2.387948e-01 4.004246e-01 2.713438e-01 8.943683e-02   Fold07\n2385   M    F 4.411397e-02 5.578830e-01 3.718878e-01 2.611521e-02   Fold07\n2386   M    F 5.715773e-02 7.176410e-01 2.242590e-01 9.423217e-04   Fold07\n2387   M    F 5.812844e-02 7.101204e-01 2.307619e-01 9.892770e-04   Fold07\n2388   M    F 8.569340e-03 4.437723e-01 4.363735e-01 1.112849e-01   Fold07\n2389   M    F 1.045136e-02 5.091815e-01 3.389608e-01 1.414063e-01   Fold07\n2390   M   VF 6.986111e-01 2.602014e-01 4.116240e-02 2.511790e-05   Fold07\n2391   M    F 2.092228e-01 4.598480e-01 3.156068e-01 1.532240e-02   Fold07\n2392   M   VF 6.722226e-01 3.011366e-01 2.658807e-02 5.274851e-05   Fold07\n2393   M   VF 7.125964e-01 2.505140e-01 3.686689e-02 2.264840e-05   Fold07\n2394   M   VF 7.079481e-01 2.630313e-01 2.899547e-02 2.513262e-05   Fold07\n2395   M   VF 4.831740e-01 4.312469e-01 8.531646e-02 2.626397e-04   Fold07\n2396   M    L 5.700488e-03 3.261028e-01 2.035418e-01 4.646549e-01   Fold07\n2397   M    F 4.950208e-02 7.434711e-01 2.003498e-01 6.677071e-03   Fold07\n2398   M    F 6.092352e-02 7.576835e-01 1.771881e-01 4.204805e-03   Fold07\n2399   M    L 4.056701e-03 2.587853e-01 2.336759e-01 5.034821e-01   Fold07\n2400   M    F 3.138623e-02 6.703100e-01 2.645800e-01 3.372368e-02   Fold07\n2401   M    F 3.571051e-02 6.985828e-01 2.285926e-01 3.711406e-02   Fold07\n2402   M    F 1.416546e-02 4.834330e-01 4.276390e-01 7.476250e-02   Fold07\n2403   M    F 1.946294e-02 4.830286e-01 4.570116e-01 4.049685e-02   Fold07\n2404   M    F 3.431808e-02 6.825518e-01 2.696606e-01 1.346952e-02   Fold07\n2405   M    F 5.969176e-02 7.640684e-01 1.717148e-01 4.525055e-03   Fold07\n2406   M    F 7.078801e-02 7.705030e-01 1.561012e-01 2.607800e-03   Fold07\n2407   L    L 1.485886e-03 1.880705e-02 1.843891e-01 7.953180e-01   Fold07\n2408   L    L 2.787186e-03 2.314553e-02 3.063527e-01 6.677146e-01   Fold07\n2409   L    M 1.122223e-02 3.425308e-01 3.926504e-01 2.535966e-01   Fold07\n2410   L    F 7.425046e-02 6.304922e-01 2.925769e-01 2.680361e-03   Fold07\n2411   L    F 7.466270e-02 6.159487e-01 3.064356e-01 2.952931e-03   Fold07\n2412   L    L 3.960424e-06 7.522564e-04 5.326332e-03 9.939175e-01   Fold07\n2413   L    F 4.048561e-02 5.864220e-01 3.523854e-01 2.070700e-02   Fold07\n2414   L    L 9.134281e-05 1.509516e-02 1.426840e-01 8.421295e-01   Fold07\n2415   L    F 5.635604e-02 5.786854e-01 3.596354e-01 5.323202e-03   Fold07\n2416   L   VF 4.354015e-01 3.751938e-01 1.707597e-01 1.864495e-02   Fold07\n2417   L    L 9.416483e-09 1.141630e-05 1.182831e-04 9.998703e-01   Fold07\n2418   L    L 1.375752e-06 5.380026e-04 2.157067e-03 9.973036e-01   Fold07\n2419   L    L 1.382686e-06 5.401087e-04 2.163045e-03 9.972955e-01   Fold07\n2420   L    L 8.843666e-07 3.635608e-04 1.390413e-03 9.982451e-01   Fold07\n2421   L    L 1.430946e-06 5.670438e-04 2.205321e-03 9.972262e-01   Fold07\n2422   L    L 2.726872e-03 2.090869e-01 3.038397e-01 4.843465e-01   Fold07\n2423   L    L 8.530185e-04 9.775303e-02 2.597648e-01 6.416291e-01   Fold07\n2424   L    L 1.266099e-06 4.299550e-04 1.600291e-03 9.979685e-01   Fold07\n2425   L    F 3.095648e-02 6.832924e-01 2.655135e-01 2.023768e-02   Fold07\n2426   L    F 3.106732e-02 6.831885e-01 2.655647e-01 2.017945e-02   Fold07\n2427   L    F 2.339090e-02 6.918453e-01 2.362037e-01 4.856011e-02   Fold07\n2428  VF   VF 9.464211e-01 4.965031e-02 3.920335e-03 8.270067e-06   Fold08\n2429  VF   VF 9.384909e-01 5.752868e-02 3.969261e-03 1.114173e-05   Fold08\n2430  VF   VF 9.398544e-01 5.633315e-02 3.802108e-03 1.037496e-05   Fold08\n2431  VF   VF 9.407888e-01 5.542072e-02 3.780092e-03 1.037273e-05   Fold08\n2432  VF   VF 9.596045e-01 3.845659e-02 1.936151e-03 2.726087e-06   Fold08\n2433  VF   VF 9.583385e-01 3.942703e-02 2.230920e-03 3.563415e-06   Fold08\n2434  VF   VF 9.282725e-01 6.548653e-02 6.226149e-03 1.478026e-05   Fold08\n2435  VF   VF 9.579877e-01 3.984326e-02 2.165635e-03 3.362603e-06   Fold08\n2436  VF   VF 9.308508e-01 6.363363e-02 5.503888e-03 1.172822e-05   Fold08\n2437  VF    F 3.870040e-01 5.428815e-01 6.746913e-02 2.645336e-03   Fold08\n2438  VF   VF 5.521346e-01 4.104413e-01 3.690669e-02 5.174924e-04   Fold08\n2439  VF   VF 9.404565e-01 5.337128e-02 6.157699e-03 1.449705e-05   Fold08\n2440  VF   VF 8.807248e-01 8.773486e-02 3.139826e-02 1.421217e-04   Fold08\n2441  VF   VF 9.656542e-01 3.056963e-02 3.772752e-03 3.388958e-06   Fold08\n2442  VF   VF 9.144314e-01 6.870744e-02 1.683243e-02 2.868842e-05   Fold08\n2443  VF   VF 9.612352e-01 3.500190e-02 3.755409e-03 7.517966e-06   Fold08\n2444  VF   VF 4.542364e-01 3.624780e-01 1.820954e-01 1.190228e-03   Fold08\n2445  VF   VF 8.565258e-01 1.079602e-01 3.540835e-02 1.056805e-04   Fold08\n2446  VF   VF 8.701212e-01 9.742648e-02 3.235405e-02 9.822600e-05   Fold08\n2447  VF   VF 9.379075e-01 5.408941e-02 7.993537e-03 9.577261e-06   Fold08\n2448  VF    L 1.857902e-16 1.572671e-12 6.750471e-08 9.999999e-01   Fold08\n2449  VF   VF 9.880199e-01 1.151083e-02 4.688541e-04 4.550914e-07   Fold08\n2450  VF   VF 9.232908e-01 6.385753e-02 1.209450e-02 7.571435e-04   Fold08\n2451  VF   VF 9.890781e-01 1.044588e-02 4.756761e-04 3.166468e-07   Fold08\n2452  VF   VF 9.708034e-01 2.743842e-02 1.748532e-03 9.631984e-06   Fold08\n2453  VF   VF 9.473787e-01 4.448419e-02 8.081817e-03 5.527133e-05   Fold08\n2454  VF   VF 6.330782e-01 3.291702e-01 3.751868e-02 2.329616e-04   Fold08\n2455  VF   VF 6.902408e-01 2.778896e-01 3.171462e-02 1.549816e-04   Fold08\n2456  VF   VF 6.222140e-01 3.472903e-01 3.020872e-02 2.869738e-04   Fold08\n2457  VF   VF 7.335811e-01 2.461098e-01 2.025672e-02 5.228693e-05   Fold08\n2458  VF   VF 6.635018e-01 3.013940e-01 3.495641e-02 1.477825e-04   Fold08\n2459  VF   VF 6.031086e-01 3.531331e-01 4.345997e-02 2.983738e-04   Fold08\n2460  VF   VF 6.958211e-01 2.791499e-01 2.493556e-02 9.350210e-05   Fold08\n2461  VF   VF 7.187334e-01 2.589741e-01 2.221493e-02 7.756467e-05   Fold08\n2462  VF   VF 4.978214e-01 4.323640e-01 6.937398e-02 4.406174e-04   Fold08\n2463  VF   VF 4.707561e-01 4.315212e-01 9.669841e-02 1.024310e-03   Fold08\n2464  VF   VF 6.558641e-01 3.135869e-01 3.039349e-02 1.555330e-04   Fold08\n2465  VF   VF 7.845923e-01 1.873052e-01 2.804881e-02 5.366398e-05   Fold08\n2466  VF   VF 7.501821e-01 2.178179e-01 3.192530e-02 7.467604e-05   Fold08\n2467  VF   VF 8.581105e-01 1.292965e-01 1.258168e-02 1.128687e-05   Fold08\n2468  VF   VF 7.126949e-01 2.382726e-01 4.886978e-02 1.627152e-04   Fold08\n2469  VF   VF 5.617923e-01 3.214772e-01 1.161340e-01 5.965130e-04   Fold08\n2470  VF   VF 8.538675e-01 1.319450e-01 1.417327e-02 1.426565e-05   Fold08\n2471  VF   VF 7.612506e-01 2.063313e-01 3.236679e-02 5.129648e-05   Fold08\n2472  VF   VF 8.748255e-01 1.153783e-01 9.789142e-03 7.075220e-06   Fold08\n2473  VF   VF 8.507782e-01 1.361152e-01 1.309902e-02 7.634335e-06   Fold08\n2474  VF    F 1.351200e-01 6.687794e-01 1.936007e-01 2.499958e-03   Fold08\n2475  VF    F 1.523523e-01 6.663412e-01 1.793708e-01 1.935772e-03   Fold08\n2476  VF    F 1.004216e-01 7.048185e-01 1.891357e-01 5.624185e-03   Fold08\n2477  VF    F 1.540101e-01 6.627115e-01 1.810885e-01 2.189860e-03   Fold08\n2478  VF    F 1.072474e-01 6.275188e-01 2.608445e-01 4.389299e-03   Fold08\n2479  VF   VF 9.682057e-01 3.050938e-02 1.283435e-03 1.515116e-06   Fold08\n2480  VF   VF 9.636711e-01 3.460814e-02 1.718350e-03 2.429687e-06   Fold08\n2481  VF   VF 9.827490e-01 1.682149e-02 4.287207e-04 7.698783e-07   Fold08\n2482  VF   VF 9.898653e-01 9.826797e-03 3.077257e-04 2.100531e-07   Fold08\n2483  VF   VF 9.418406e-01 4.992581e-02 8.138044e-03 9.552620e-05   Fold08\n2484  VF   VF 9.858282e-01 1.381183e-02 3.594164e-04 5.808029e-07   Fold08\n2485  VF   VF 9.151603e-01 7.840883e-02 6.412669e-03 1.819286e-05   Fold08\n2486  VF   VF 6.352623e-01 2.536438e-01 1.034465e-01 7.647369e-03   Fold08\n2487  VF   VF 9.582421e-01 3.899924e-02 2.752667e-03 5.973524e-06   Fold08\n2488  VF   VF 7.652764e-01 2.125785e-01 2.204696e-02 9.812877e-05   Fold08\n2489  VF   VF 9.536445e-01 4.306011e-02 3.287213e-03 8.217476e-06   Fold08\n2490  VF   VF 9.798147e-01 1.943687e-02 7.471999e-04 1.199009e-06   Fold08\n2491  VF    L 1.316128e-01 9.664238e-02 2.957417e-01 4.760031e-01   Fold08\n2492  VF   VF 7.811060e-01 1.994010e-01 1.934303e-02 1.500087e-04   Fold08\n2493  VF   VF 9.620283e-01 3.613062e-02 1.836357e-03 4.710996e-06   Fold08\n2494  VF   VF 9.509917e-01 4.635185e-02 2.648341e-03 8.075769e-06   Fold08\n2495  VF   VF 8.426357e-01 1.500463e-01 7.274674e-03 4.326453e-05   Fold08\n2496  VF   VF 9.885086e-01 1.112539e-02 3.657581e-04 2.524446e-07   Fold08\n2497  VF   VF 9.860797e-01 1.332694e-02 5.924195e-04 9.049387e-07   Fold08\n2498  VF   VF 9.581556e-01 4.040256e-02 1.437532e-03 4.327609e-06   Fold08\n2499  VF   VF 9.754574e-01 2.257061e-02 1.966292e-03 5.712408e-06   Fold08\n2500  VF   VF 8.998058e-01 9.603259e-02 4.153167e-03 8.426934e-06   Fold08\n2501  VF   VF 8.684951e-01 1.239867e-01 7.503178e-03 1.503617e-05   Fold08\n2502  VF   VF 7.809808e-01 2.045051e-01 1.437474e-02 1.393658e-04   Fold08\n2503  VF   VF 9.167471e-01 7.993800e-02 3.311685e-03 3.245135e-06   Fold08\n2504  VF   VF 8.595220e-01 1.306921e-01 9.763996e-03 2.185036e-05   Fold08\n2505  VF   VF 8.575199e-01 1.337000e-01 8.760517e-03 1.959747e-05   Fold08\n2506  VF   VF 8.559786e-01 1.350706e-01 8.930618e-03 2.018390e-05   Fold08\n2507  VF   VF 8.567906e-01 1.340973e-01 9.091169e-03 2.088960e-05   Fold08\n2508  VF   VF 8.710216e-01 1.239108e-01 5.048472e-03 1.907525e-05   Fold08\n2509  VF   VF 8.769993e-01 1.185246e-01 4.458515e-03 1.752818e-05   Fold08\n2510  VF   VF 9.141059e-01 8.247798e-02 3.410616e-03 5.519023e-06   Fold08\n2511  VF   VF 9.090455e-01 8.693441e-02 4.012852e-03 7.278900e-06   Fold08\n2512  VF   VF 9.081740e-01 8.854048e-02 3.280616e-03 4.918781e-06   Fold08\n2513  VF   VF 9.010726e-01 9.480974e-02 4.108988e-03 8.634462e-06   Fold08\n2514  VF   VF 8.745052e-01 1.203637e-01 5.108296e-03 2.275439e-05   Fold08\n2515  VF   VF 9.130060e-01 8.327500e-02 3.712500e-03 6.492317e-06   Fold08\n2516  VF   VF 8.864245e-01 1.086063e-01 4.957722e-03 1.149787e-05   Fold08\n2517  VF   VF 8.736871e-01 1.214422e-01 4.850586e-03 2.009986e-05   Fold08\n2518  VF   VF 8.738337e-01 1.207044e-01 5.436315e-03 2.560080e-05   Fold08\n2519  VF   VF 8.999299e-01 9.497368e-02 5.086959e-03 9.442424e-06   Fold08\n2520  VF   VF 8.551480e-01 1.359369e-01 8.894166e-03 2.090158e-05   Fold08\n2521  VF   VF 8.673070e-01 1.269150e-01 5.749787e-03 2.814212e-05   Fold08\n2522  VF   VF 8.900124e-01 1.045577e-01 5.415829e-03 1.411947e-05   Fold08\n2523  VF   VF 8.451219e-01 1.437651e-01 1.108426e-02 2.870052e-05   Fold08\n2524  VF   VF 8.943550e-01 1.004979e-01 5.134023e-03 1.307509e-05   Fold08\n2525  VF   VF 9.007799e-01 9.372645e-02 5.482390e-03 1.123050e-05   Fold08\n2526  VF   VF 8.905027e-01 1.043573e-01 5.126895e-03 1.304029e-05   Fold08\n2527  VF   VF 8.992877e-01 9.491224e-02 5.787631e-03 1.241127e-05   Fold08\n2528  VF   VF 9.253117e-01 7.206411e-02 2.620834e-03 3.357339e-06   Fold08\n2529  VF   VF 8.987987e-01 9.639917e-02 4.791989e-03 1.017992e-05   Fold08\n2530  VF   VF 9.154717e-01 8.075981e-02 3.763994e-03 4.499903e-06   Fold08\n2531  VF   VF 9.049901e-01 9.084416e-02 4.157690e-03 8.070192e-06   Fold08\n2532  VF   VF 6.423797e-01 3.489389e-01 8.635670e-03 4.571275e-05   Fold08\n2533  VF   VF 8.967351e-01 9.718888e-02 6.062497e-03 1.355697e-05   Fold08\n2534  VF   VF 6.213650e-01 3.216455e-01 5.264193e-02 4.347639e-03   Fold08\n2535  VF   VF 6.351105e-01 3.557488e-01 9.089928e-03 5.079786e-05   Fold08\n2536  VF   VF 6.301796e-01 3.615691e-01 8.209643e-03 4.170393e-05   Fold08\n2537  VF   VF 6.329617e-01 3.577687e-01 9.217345e-03 5.223922e-05   Fold08\n2538  VF   VF 6.279284e-01 3.636868e-01 8.341742e-03 4.311783e-05   Fold08\n2539  VF   VF 6.287398e-01 3.617067e-01 9.497969e-03 5.559674e-05   Fold08\n2540  VF   VF 8.858352e-01 1.066798e-01 7.466287e-03 1.872084e-05   Fold08\n2541  VF   VF 6.213903e-01 3.698410e-01 8.721523e-03 4.724566e-05   Fold08\n2542  VF   VF 6.188948e-01 3.721830e-01 8.873286e-03 4.895810e-05   Fold08\n2543  VF   VF 8.870742e-01 1.062772e-01 6.632733e-03 1.590648e-05   Fold08\n2544  VF   VF 6.200476e-01 3.698028e-01 1.008675e-02 6.285897e-05   Fold08\n2545  VF   VF 6.146058e-01 3.762171e-01 9.125214e-03 5.186494e-05   Fold08\n2546  VF   VF 8.202193e-01 1.656647e-01 1.406959e-02 4.635769e-05   Fold08\n2547  VF   VF 8.287830e-01 1.568658e-01 1.429870e-02 5.248194e-05   Fold08\n2548  VF   VF 8.890127e-01 1.072369e-01 3.737991e-03 1.245555e-05   Fold08\n2549  VF   VF 5.996023e-01 3.887882e-01 1.152658e-02 8.291685e-05   Fold08\n2550  VF   VF 8.759582e-01 1.160361e-01 7.983072e-03 2.269992e-05   Fold08\n2551  VF   VF 9.105342e-01 8.622281e-02 3.237588e-03 5.428413e-06   Fold08\n2552  VF   VF 9.098795e-01 8.650170e-02 3.612119e-03 6.681297e-06   Fold08\n2553  VF   VF 8.824764e-01 1.112731e-01 6.233065e-03 1.741472e-05   Fold08\n2554  VF   VF 8.274524e-01 1.611432e-01 1.130739e-02 9.705482e-05   Fold08\n2555  VF   VF 8.185697e-01 1.702948e-01 1.103667e-02 9.880865e-05   Fold08\n2556  VF   VF 6.877098e-01 3.069599e-01 5.313091e-03 1.725419e-05   Fold08\n2557  VF   VF 8.685677e-01 1.240032e-01 7.414666e-03 1.440287e-05   Fold08\n2558  VF   VF 9.145652e-01 8.158372e-02 3.845542e-03 5.494572e-06   Fold08\n2559  VF   VF 9.164947e-01 7.962530e-02 3.874146e-03 5.813275e-06   Fold08\n2560  VF   VF 9.225256e-01 7.435624e-02 3.113476e-03 4.655701e-06   Fold08\n2561  VF   VF 8.718024e-01 1.202473e-01 7.933580e-03 1.673247e-05   Fold08\n2562  VF   VF 8.675408e-01 1.257205e-01 6.726730e-03 1.205170e-05   Fold08\n2563  VF   VF 9.226112e-01 7.437230e-02 3.012092e-03 4.430631e-06   Fold08\n2564  VF   VF 8.538043e-01 1.362593e-01 9.912592e-03 2.385642e-05   Fold08\n2565  VF   VF 8.516601e-01 1.381835e-01 1.013158e-02 2.480046e-05   Fold08\n2566  VF   VF 9.376818e-01 6.061430e-02 1.701352e-03 2.555131e-06   Fold08\n2567  VF   VF 9.078716e-01 8.530474e-02 6.783112e-03 4.053456e-05   Fold08\n2568  VF   VF 9.506970e-01 4.621402e-02 3.084161e-03 4.816088e-06   Fold08\n2569  VF   VF 9.305917e-01 6.303975e-02 6.352700e-03 1.581090e-05   Fold08\n2570  VF   VF 8.290180e-01 1.580290e-01 1.293101e-02 2.198861e-05   Fold08\n2571  VF   VF 9.276231e-01 6.783700e-02 4.535228e-03 4.693039e-06   Fold08\n2572  VF   VF 9.834236e-01 1.604649e-02 5.296829e-04 2.390808e-07   Fold08\n2573  VF   VF 9.130185e-01 8.205119e-02 4.922985e-03 7.332405e-06   Fold08\n2574  VF   VF 9.405419e-01 5.726386e-02 2.190327e-03 3.941521e-06   Fold08\n2575  VF   VF 9.101667e-01 8.605721e-02 3.767447e-03 8.618745e-06   Fold08\n2576  VF   VF 9.316075e-01 6.468867e-02 3.698918e-03 4.941339e-06   Fold08\n2577  VF   VF 9.784382e-01 2.082581e-02 7.354901e-04 4.591377e-07   Fold08\n2578  VF   VF 6.710028e-01 3.065477e-01 2.233660e-02 1.129001e-04   Fold08\n2579  VF   VF 6.466660e-01 2.863919e-01 6.337801e-02 3.564072e-03   Fold08\n2580  VF   VF 7.596428e-01 2.099118e-01 2.801992e-02 2.425428e-03   Fold08\n2581  VF   VF 9.761905e-01 2.272483e-02 1.083939e-03 7.083897e-07   Fold08\n2582  VF   VF 9.655274e-01 3.228023e-02 2.190291e-03 2.045764e-06   Fold08\n2583  VF   VF 9.383242e-01 5.857688e-02 3.090110e-03 8.780077e-06   Fold08\n2584  VF    F 1.161383e-01 7.549975e-01 1.284717e-01 3.925106e-04   Fold08\n2585  VF    F 1.262755e-01 7.661281e-01 1.073366e-01 2.597991e-04   Fold08\n2586  VF   VF 7.870977e-01 1.998596e-01 1.302796e-02 1.471002e-05   Fold08\n2587  VF   VF 7.422348e-01 2.426083e-01 1.512488e-02 3.200631e-05   Fold08\n2588  VF   VF 7.333497e-01 2.515513e-01 1.506469e-02 3.426794e-05   Fold08\n2589  VF   VF 7.467415e-01 2.392917e-01 1.393593e-02 3.078972e-05   Fold08\n2590  VF    F 4.166434e-01 4.300336e-01 1.522080e-01 1.114963e-03   Fold08\n2591  VF   VF 6.957452e-01 2.718627e-01 3.234377e-02 4.824151e-05   Fold08\n2592  VF   VF 8.012719e-01 1.850939e-01 1.362193e-02 1.223679e-05   Fold08\n2593  VF   VF 7.728932e-01 2.122612e-01 1.483008e-02 1.554237e-05   Fold08\n2594  VF   VF 6.767692e-01 2.896275e-01 3.355076e-02 5.258816e-05   Fold08\n2595  VF   VF 7.503774e-01 2.298206e-01 1.976924e-02 3.274250e-05   Fold08\n2596  VF   VF 8.167018e-01 1.715246e-01 1.176452e-02 9.104123e-06   Fold08\n2597  VF   VF 8.213311e-01 1.669157e-01 1.174402e-02 9.186360e-06   Fold08\n2598  VF   VF 6.782854e-01 2.962403e-01 2.538611e-02 8.820992e-05   Fold08\n2599  VF   VF 7.476575e-01 2.335589e-01 1.876494e-02 1.857857e-05   Fold08\n2600  VF   VF 5.799732e-01 3.597366e-01 6.011582e-02 1.743471e-04   Fold08\n2601  VF   VF 8.448738e-01 1.463398e-01 8.782022e-03 4.380442e-06   Fold08\n2602  VF   VF 6.843066e-01 2.835443e-01 3.210345e-02 4.566619e-05   Fold08\n2603  VF    F 1.003621e-01 7.780904e-01 1.193424e-01 2.205155e-03   Fold08\n2604  VF    F 8.764778e-02 7.701021e-01 1.382607e-01 3.989436e-03   Fold08\n2605   F    F 3.250082e-01 5.590534e-01 1.122984e-01 3.640026e-03   Fold08\n2606   F    L 8.507209e-05 4.687913e-03 3.731225e-02 9.579148e-01   Fold08\n2607   F    F 3.523256e-01 5.273191e-01 1.157233e-01 4.631937e-03   Fold08\n2608   F   VF 8.805140e-01 8.811000e-02 3.123539e-02 1.405696e-04   Fold08\n2609   F   VF 8.224122e-01 1.449463e-01 3.252723e-02 1.142278e-04   Fold08\n2610   F   VF 5.522690e-01 3.738508e-01 7.348627e-02 3.939795e-04   Fold08\n2611   F   VF 5.916296e-01 3.530030e-01 5.499628e-02 3.710817e-04   Fold08\n2612   F   VF 7.523770e-01 2.160056e-01 3.154231e-02 7.504425e-05   Fold08\n2613   F   VF 7.937710e-01 1.821460e-01 2.405374e-02 2.925247e-05   Fold08\n2614   F   VF 8.747611e-01 1.172364e-01 7.996353e-03 6.228099e-06   Fold08\n2615   F   VF 8.405183e-01 1.434256e-01 1.603813e-02 1.793352e-05   Fold08\n2616   F   VF 7.943076e-01 1.814937e-01 2.417419e-02 2.443803e-05   Fold08\n2617   F   VF 6.863566e-01 2.719951e-01 4.138829e-02 2.599985e-04   Fold08\n2618   F    F 1.360282e-01 6.612616e-01 1.998552e-01 2.855006e-03   Fold08\n2619   F    F 1.589140e-01 6.487743e-01 1.902022e-01 2.109402e-03   Fold08\n2620   F    F 1.061059e-01 7.085737e-01 1.804061e-01 4.914328e-03   Fold08\n2621   F    F 1.265759e-01 6.395208e-01 2.309788e-01 2.924408e-03   Fold08\n2622   F    F 7.465000e-02 5.971354e-01 3.226819e-01 5.532728e-03   Fold08\n2623   F    F 7.060374e-02 6.063278e-01 3.168210e-01 6.247443e-03   Fold08\n2624   F    F 1.088271e-01 6.642587e-01 2.228864e-01 4.027843e-03   Fold08\n2625   F    F 8.403929e-02 6.873558e-01 2.200600e-01 8.544986e-03   Fold08\n2626   F    F 8.761766e-02 6.147331e-01 2.919970e-01 5.652214e-03   Fold08\n2627   F    F 1.718148e-01 6.666662e-01 1.600826e-01 1.436504e-03   Fold08\n2628   F    F 9.178771e-02 6.127666e-01 2.909328e-01 4.512851e-03   Fold08\n2629   F    F 6.172077e-02 6.124382e-01 3.136522e-01 1.218881e-02   Fold08\n2630   F    F 1.107339e-01 7.361541e-01 1.492824e-01 3.829579e-03   Fold08\n2631   F   VF 8.450624e-01 1.430081e-01 1.187996e-02 4.950756e-05   Fold08\n2632   F   VF 9.156991e-01 8.084351e-02 3.452013e-03 5.405146e-06   Fold08\n2633   F   VF 9.007293e-01 9.277723e-02 6.483399e-03 1.003246e-05   Fold08\n2634   F   VF 8.663952e-01 1.286561e-01 4.928186e-03 2.057033e-05   Fold08\n2635   F   VF 8.990423e-01 9.563191e-02 5.315594e-03 1.020574e-05   Fold08\n2636   F   VF 8.943739e-01 1.004676e-01 5.149003e-03 9.476535e-06   Fold08\n2637   F   VF 8.797975e-01 1.140752e-01 6.110057e-03 1.721249e-05   Fold08\n2638   F   VF 7.498647e-01 1.403134e-01 1.097416e-01 8.027007e-05   Fold08\n2639   F   VF 6.236792e-01 3.661805e-01 1.007771e-02 6.256275e-05   Fold08\n2640   F   VF 8.126656e-01 1.777189e-01 9.549139e-03 6.637456e-05   Fold08\n2641   F   VF 8.343728e-01 1.574899e-01 8.085328e-03 5.196641e-05   Fold08\n2642   F   VF 8.314291e-01 1.601688e-01 8.347461e-03 5.470830e-05   Fold08\n2643   F   VF 6.091494e-01 3.813231e-01 9.471538e-03 5.602545e-05   Fold08\n2644   F   VF 8.164560e-01 1.679410e-01 1.554267e-02 6.034933e-05   Fold08\n2645   F   VF 5.935120e-01 3.959380e-01 1.048089e-02 6.909003e-05   Fold08\n2646   F    L 3.586914e-03 3.418134e-02 1.751913e-01 7.870405e-01   Fold08\n2647   F   VF 5.909461e-01 3.983153e-01 1.066687e-02 7.169727e-05   Fold08\n2648   F   VF 5.860730e-01 4.028407e-01 1.100968e-02 7.660049e-05   Fold08\n2649   F   VF 5.765586e-01 4.116644e-01 1.169015e-02 8.680269e-05   Fold08\n2650   F   VF 5.724106e-01 4.155051e-01 1.199269e-02 9.152591e-05   Fold08\n2651   F   VF 8.012856e-01 1.866249e-01 1.188036e-02 2.091206e-04   Fold08\n2652   F    M 4.886930e-02 2.940904e-02 9.213231e-01 3.985408e-04   Fold08\n2653   F   VF 6.659941e-01 2.759124e-01 5.410291e-02 3.990632e-03   Fold08\n2654   F   VF 8.241101e-01 1.513135e-01 2.447398e-02 1.024197e-04   Fold08\n2655   F    F 2.181860e-02 9.040511e-01 7.355658e-02 5.736893e-04   Fold08\n2656   F    F 5.262948e-02 7.026159e-01 2.426917e-01 2.062935e-03   Fold08\n2657   F    F 9.425196e-02 7.623862e-01 1.427524e-01 6.094979e-04   Fold08\n2658   F    F 8.706585e-02 7.724806e-01 1.398508e-01 6.026930e-04   Fold08\n2659   F    F 1.161495e-01 7.617840e-01 1.217055e-01 3.608728e-04   Fold08\n2660   F    F 5.361275e-02 7.438978e-01 1.995585e-01 2.930964e-03   Fold08\n2661   F    F 1.250860e-01 7.648775e-01 1.097648e-01 2.717181e-04   Fold08\n2662   F    F 6.896988e-02 7.864800e-01 1.432019e-01 1.348195e-03   Fold08\n2663   F    F 8.326201e-02 7.495285e-01 1.666383e-01 5.711996e-04   Fold08\n2664   F    F 1.055506e-01 7.601293e-01 1.339615e-01 3.586289e-04   Fold08\n2665   F    F 9.146757e-02 7.653919e-01 1.424952e-01 6.453368e-04   Fold08\n2666   F    F 1.229772e-01 7.666286e-01 1.101191e-01 2.750996e-04   Fold08\n2667   F    F 8.053200e-02 7.578089e-01 1.609289e-01 7.301241e-04   Fold08\n2668   F    F 6.031305e-02 7.533959e-01 1.850446e-01 1.246409e-03   Fold08\n2669   F    F 6.732523e-02 7.396644e-01 1.922360e-01 7.743872e-04   Fold08\n2670   F    F 9.222759e-02 7.274411e-01 1.797196e-01 6.116840e-04   Fold08\n2671   F    F 1.014580e-01 7.640908e-01 1.339445e-01 5.066443e-04   Fold08\n2672   F    F 6.307255e-02 7.126795e-01 2.227648e-01 1.483085e-03   Fold08\n2673   F    F 3.876439e-02 7.635017e-01 1.942433e-01 3.490600e-03   Fold08\n2674   F    F 5.726244e-02 7.629264e-01 1.792824e-01 5.287861e-04   Fold08\n2675   F   VF 7.793030e-01 2.048440e-01 1.583339e-02 1.960862e-05   Fold08\n2676   F   VF 7.623587e-01 2.207815e-01 1.683868e-02 2.117391e-05   Fold08\n2677   F   VF 7.596410e-01 2.212210e-01 1.911820e-02 1.976660e-05   Fold08\n2678   F   VF 7.337134e-01 2.404480e-01 2.580636e-02 3.224453e-05   Fold08\n2679   F    F 4.290561e-01 4.700914e-01 9.661384e-02 4.238666e-03   Fold08\n2680   F    F 2.788358e-01 5.248024e-01 1.841721e-01 1.218964e-02   Fold08\n2681   F    M 1.607171e-02 1.864848e-01 5.573972e-01 2.400462e-01   Fold08\n2682   F    M 3.507008e-02 2.366652e-01 5.355466e-01 1.927181e-01   Fold08\n2683   F    F 4.432864e-01 4.451734e-01 1.096922e-01 1.848083e-03   Fold08\n2684   F    M 1.706024e-02 2.668827e-02 9.559252e-01 3.263129e-04   Fold08\n2685   F   VF 6.330742e-01 3.276522e-01 3.915346e-02 1.201020e-04   Fold08\n2686   F    F 8.024038e-02 7.600028e-01 1.551336e-01 4.623294e-03   Fold08\n2687   F    F 8.553289e-02 7.903652e-01 1.203270e-01 3.774849e-03   Fold08\n2688   F    F 1.009230e-01 7.795518e-01 1.173311e-01 2.194135e-03   Fold08\n2689   F    F 7.699228e-02 7.803934e-01 1.372860e-01 5.328300e-03   Fold08\n2690   F    F 7.680915e-02 7.867290e-01 1.315724e-01 4.889441e-03   Fold08\n2691   F    F 7.603893e-02 7.844266e-01 1.344760e-01 5.058491e-03   Fold08\n2692   F    F 7.570570e-02 7.866988e-01 1.325751e-01 5.020384e-03   Fold08\n2693   F    F 6.644972e-02 7.526290e-01 1.732783e-01 7.642984e-03   Fold08\n2694   F    F 6.566348e-02 7.516886e-01 1.748006e-01 7.847289e-03   Fold08\n2695   F    F 6.554340e-02 7.516678e-01 1.749228e-01 7.866062e-03   Fold08\n2696   F    F 3.641556e-02 7.263727e-01 2.009963e-01 3.621548e-02   Fold08\n2697   F    F 1.751391e-02 6.008033e-01 2.977512e-01 8.393158e-02   Fold08\n2698   F    F 6.330608e-02 7.480190e-01 1.801664e-01 8.508482e-03   Fold08\n2699   F    F 6.291272e-02 7.456913e-01 1.825756e-01 8.820454e-03   Fold08\n2700   F    F 3.395615e-02 7.072309e-01 2.169551e-01 4.185786e-02   Fold08\n2701   F    F 3.757737e-02 7.097126e-01 2.364426e-01 1.626741e-02   Fold08\n2702   F    F 6.674457e-02 7.628363e-01 1.638643e-01 6.554889e-03   Fold08\n2703   F    F 6.442880e-02 7.454198e-01 1.833699e-01 6.781497e-03   Fold08\n2704   F    F 1.845003e-02 6.266042e-01 2.478760e-01 1.070698e-01   Fold08\n2705   F    F 5.455932e-02 7.553647e-01 1.827189e-01 7.357102e-03   Fold08\n2706   F    F 5.108267e-02 7.586407e-01 1.827517e-01 7.524971e-03   Fold08\n2707   F    F 1.013793e-01 7.836772e-01 1.119423e-01 3.001211e-03   Fold08\n2708   F    F 1.012784e-01 7.923164e-01 1.038128e-01 2.592430e-03   Fold08\n2709   F    F 2.518465e-02 6.670126e-01 2.593829e-01 4.841988e-02   Fold08\n2710   F    F 7.830673e-02 7.909167e-01 1.259722e-01 4.804391e-03   Fold08\n2711   F    F 9.718902e-02 7.859468e-01 1.137200e-01 3.144133e-03   Fold08\n2712   F    F 8.346097e-02 7.680352e-01 1.440326e-01 4.471196e-03   Fold08\n2713   M   VF 8.859705e-01 1.011508e-01 1.278193e-02 9.683605e-05   Fold08\n2714   M    L 1.208134e-03 3.372610e-02 1.188927e-01 8.461731e-01   Fold08\n2715   M    F 2.681581e-01 5.805866e-01 1.444426e-01 6.812643e-03   Fold08\n2716   M    M 7.248691e-02 1.205176e-01 4.350604e-01 3.719351e-01   Fold08\n2717   M    F 1.882425e-01 5.106265e-01 2.362756e-01 6.485547e-02   Fold08\n2718   M    F 2.239372e-01 5.836353e-01 1.867755e-01 5.652079e-03   Fold08\n2719   M    M 3.410486e-03 1.251394e-02 9.825428e-01 1.532726e-03   Fold08\n2720   M    F 7.337486e-02 6.073815e-01 3.132312e-01 6.012451e-03   Fold08\n2721   M    F 1.179747e-01 6.513740e-01 2.271022e-01 3.549062e-03   Fold08\n2722   M    M 8.331369e-03 2.812742e-01 4.068442e-01 3.035503e-01   Fold08\n2723   M    M 1.034221e-03 2.085531e-02 9.743591e-01 3.751411e-03   Fold08\n2724   M    F 1.724035e-01 6.570945e-01 1.689055e-01 1.596473e-03   Fold08\n2725   M    F 7.491715e-02 6.019019e-01 3.087728e-01 1.440815e-02   Fold08\n2726   M    L 1.199697e-02 5.063268e-02 5.213600e-02 8.852343e-01   Fold08\n2727   M   VF 8.711648e-01 1.212750e-01 7.542633e-03 1.757058e-05   Fold08\n2728   M    L 6.151887e-03 3.199974e-02 3.338070e-02 9.284677e-01   Fold08\n2729   M   VF 8.003195e-01 1.836458e-01 1.589650e-02 1.381095e-04   Fold08\n2730   M    M 2.754143e-04 3.474477e-01 4.662660e-01 1.860109e-01   Fold08\n2731   M    F 2.903561e-02 6.384803e-01 3.130641e-01 1.942000e-02   Fold08\n2732   M    M 1.086122e-02 1.760632e-01 8.120871e-01 9.884129e-04   Fold08\n2733   M    F 6.057773e-02 7.320116e-01 2.062453e-01 1.165319e-03   Fold08\n2734   M    F 4.130536e-02 6.889771e-01 2.675756e-01 2.141940e-03   Fold08\n2735   M    F 5.839518e-02 7.221628e-01 2.184960e-01 9.460417e-04   Fold08\n2736   M    F 5.232476e-02 7.593163e-01 1.857747e-01 2.584271e-03   Fold08\n2737   M   VF 5.204042e-01 3.834363e-01 9.555165e-02 6.078308e-04   Fold08\n2738   M   VF 7.945106e-01 1.923384e-01 1.313965e-02 1.137938e-05   Fold08\n2739   M    F 1.920589e-01 4.777972e-01 3.146857e-01 1.545810e-02   Fold08\n2740   M    M 2.900700e-02 2.170816e-01 5.514709e-01 2.024405e-01   Fold08\n2741   M    M 2.757061e-02 2.254249e-01 5.075507e-01 2.394538e-01   Fold08\n2742   M    M 2.848641e-02 2.231324e-01 5.492218e-01 1.991594e-01   Fold08\n2743   M    M 2.898929e-02 2.236860e-01 5.492416e-01 1.980832e-01   Fold08\n2744   M   VF 5.715329e-01 3.554954e-01 7.265088e-02 3.207543e-04   Fold08\n2745   M   VF 5.885191e-01 3.497360e-01 6.150534e-02 2.395542e-04   Fold08\n2746   M    F 8.009929e-02 7.631393e-01 1.516457e-01 5.115643e-03   Fold08\n2747   M    F 3.977086e-02 7.488638e-01 2.006777e-01 1.068759e-02   Fold08\n2748   M    F 6.203872e-02 7.740385e-01 1.551355e-01 8.787300e-03   Fold08\n2749   M    F 2.085917e-02 6.783664e-01 2.004760e-01 1.002984e-01   Fold08\n2750   M    F 2.014213e-02 6.492808e-01 2.136626e-01 1.169145e-01   Fold08\n2751   M    F 3.413977e-02 7.136108e-01 2.110328e-01 4.121664e-02   Fold08\n2752   M    F 3.082031e-02 6.538528e-01 2.641475e-01 5.117937e-02   Fold08\n2753   M    F 5.493372e-02 7.495924e-01 1.854522e-01 1.002167e-02   Fold08\n2754   M    F 3.760863e-02 7.123490e-01 2.267199e-01 2.332244e-02   Fold08\n2755   L    L 1.864357e-03 2.609525e-02 3.537901e-01 6.182503e-01   Fold08\n2756   L    F 1.922481e-01 6.603861e-01 1.450378e-01 2.328029e-03   Fold08\n2757   L    F 1.783137e-01 6.558467e-01 1.617209e-01 4.118721e-03   Fold08\n2758   L    L 2.881471e-06 7.132784e-04 5.181816e-03 9.941020e-01   Fold08\n2759   L    L 7.262086e-12 1.558945e-08 1.249030e-05 9.999875e-01   Fold08\n2760   L    F 2.714319e-01 4.121605e-01 2.829717e-01 3.343592e-02   Fold08\n2761   L   VF 4.350734e-01 3.915295e-01 1.605647e-01 1.283237e-02   Fold08\n2762   L    M 4.771883e-02 2.142368e-01 4.216452e-01 3.163992e-01   Fold08\n2763   L    L 9.258804e-08 1.527864e-03 3.074160e-02 9.677304e-01   Fold08\n2764   L    M 1.077460e-02 4.646060e-01 5.021146e-01 2.250481e-02   Fold08\n2765   L    M 5.207665e-03 4.099134e-01 4.527588e-01 1.321202e-01   Fold08\n2766   L    M 1.923176e-02 2.015502e-01 5.017830e-01 2.774350e-01   Fold08\n2767   L    L 4.619928e-08 6.088433e-05 4.691444e-04 9.994699e-01   Fold08\n2768   L    L 1.678422e-08 3.106861e-05 1.198070e-04 9.998491e-01   Fold08\n2769   L    L 1.631124e-06 7.190900e-04 2.762572e-03 9.965167e-01   Fold08\n2770   L    L 5.819943e-07 4.044906e-04 2.032719e-03 9.975622e-01   Fold08\n2771   L    L 5.994774e-07 3.745515e-04 1.124534e-03 9.985003e-01   Fold08\n2772   L    L 2.274412e-03 2.039117e-01 3.087879e-01 4.850260e-01   Fold08\n2773   L    F 3.899267e-02 7.210286e-01 2.176173e-01 2.236147e-02   Fold08\n2774   L    L 4.629758e-08 6.564526e-05 1.892774e-04 9.997450e-01   Fold08\n2775   L    L 9.709405e-08 1.051000e-04 6.678651e-04 9.992269e-01   Fold08\n2776  VF   VF 9.399712e-01 5.579105e-02 4.228538e-03 9.180935e-06   Fold09\n2777  VF   VF 9.532388e-01 4.384571e-02 2.910178e-03 5.354773e-06   Fold09\n2778  VF   VF 9.403719e-01 5.496889e-02 4.648928e-03 1.031191e-05   Fold09\n2779  VF   VF 9.600075e-01 3.804551e-02 1.944769e-03 2.194442e-06   Fold09\n2780  VF   VF 9.389143e-01 5.620018e-02 4.874313e-03 1.124373e-05   Fold09\n2781  VF   VF 9.426079e-01 5.209919e-02 5.283423e-03 9.463407e-06   Fold09\n2782  VF   VF 9.428437e-01 5.317823e-02 3.970011e-03 8.027906e-06   Fold09\n2783  VF   VF 9.430623e-01 5.297092e-02 3.958811e-03 7.966000e-06   Fold09\n2784  VF   VF 9.235479e-01 6.899301e-02 7.439464e-03 1.964565e-05   Fold09\n2785  VF   VF 9.220405e-01 7.032157e-02 7.617273e-03 2.066097e-05   Fold09\n2786  VF   VF 4.829568e-01 4.547236e-01 6.197821e-02 3.414327e-04   Fold09\n2787  VF    F 3.630383e-01 5.471667e-01 8.714509e-02 2.649884e-03   Fold09\n2788  VF    F 3.000263e-01 5.622204e-01 1.351650e-01 2.588281e-03   Fold09\n2789  VF   VF 9.308594e-01 6.142122e-02 7.689851e-03 2.949034e-05   Fold09\n2790  VF   VF 9.525404e-01 4.312138e-02 4.326048e-03 1.221606e-05   Fold09\n2791  VF   VF 9.534790e-01 4.103169e-02 5.480941e-03 8.377839e-06   Fold09\n2792  VF   VF 7.774346e-01 1.647265e-01 5.749932e-02 3.396343e-04   Fold09\n2793  VF   VF 8.404618e-01 1.316186e-01 2.778885e-02 1.308125e-04   Fold09\n2794  VF   VF 7.982905e-01 1.516923e-01 4.971233e-02 3.049493e-04   Fold09\n2795  VF   VF 9.327288e-01 5.680698e-02 1.044576e-02 1.844916e-05   Fold09\n2796  VF   VF 9.125367e-01 7.218018e-02 1.524741e-02 3.570882e-05   Fold09\n2797  VF   VF 9.873897e-01 1.206176e-02 5.478255e-04 7.152366e-07   Fold09\n2798  VF   VF 9.855222e-01 1.359803e-02 8.787373e-04 1.023682e-06   Fold09\n2799  VF   VF 9.811921e-01 1.775422e-02 1.049777e-03 3.917675e-06   Fold09\n2800  VF   VF 7.703857e-01 1.269287e-01 9.507454e-02 7.611080e-03   Fold09\n2801  VF   VF 9.887503e-01 1.084450e-02 4.047838e-04 4.275620e-07   Fold09\n2802  VF   VF 9.857636e-01 1.354796e-02 6.875189e-04 8.832654e-07   Fold09\n2803  VF   VF 9.832546e-01 1.581048e-02 9.329710e-04 1.973676e-06   Fold09\n2804  VF   VF 9.853480e-01 1.397197e-02 6.791101e-04 8.911330e-07   Fold09\n2805  VF   VF 9.764766e-01 2.132656e-02 2.192104e-03 4.770989e-06   Fold09\n2806  VF   VF 5.430722e-01 3.345343e-01 1.215615e-01 8.319772e-04   Fold09\n2807  VF   VF 6.624602e-01 2.948105e-01 4.244099e-02 2.882905e-04   Fold09\n2808  VF   VF 6.739023e-01 2.799942e-01 4.581675e-02 2.867590e-04   Fold09\n2809  VF    M 9.058835e-03 2.006131e-02 9.684046e-01 2.475251e-03   Fold09\n2810  VF   VF 7.546264e-01 2.239087e-01 2.139129e-02 7.361055e-05   Fold09\n2811  VF   VF 7.557299e-01 2.225452e-01 2.165047e-02 7.437970e-05   Fold09\n2812  VF   VF 6.194039e-01 3.061031e-01 7.392671e-02 5.663276e-04   Fold09\n2813  VF   VF 6.202304e-01 3.070868e-01 7.214173e-02 5.411342e-04   Fold09\n2814  VF   VF 7.406857e-01 2.341292e-01 2.506113e-02 1.238924e-04   Fold09\n2815  VF   VF 8.755458e-01 1.185185e-01 5.929153e-03 6.611115e-06   Fold09\n2816  VF   VF 8.318636e-01 1.597779e-01 8.330465e-03 2.810742e-05   Fold09\n2817  VF   VF 8.644707e-01 1.271290e-01 8.392338e-03 7.957968e-06   Fold09\n2818  VF   VF 8.535356e-01 1.381632e-01 8.292889e-03 8.300649e-06   Fold09\n2819  VF   VF 8.294102e-01 1.578899e-01 1.267916e-02 2.082008e-05   Fold09\n2820  VF    F 1.344154e-01 6.344969e-01 2.284739e-01 2.613842e-03   Fold09\n2821  VF    F 1.406737e-01 6.851633e-01 1.720029e-01 2.160120e-03   Fold09\n2822  VF    F 1.090653e-01 6.543515e-01 2.311143e-01 5.468880e-03   Fold09\n2823  VF    F 1.501612e-01 6.352904e-01 2.125133e-01 2.035132e-03   Fold09\n2824  VF    F 9.832909e-02 6.419956e-01 2.552636e-01 4.411748e-03   Fold09\n2825  VF   VF 9.871426e-01 1.248286e-02 3.741562e-04 3.392093e-07   Fold09\n2826  VF   VF 9.841030e-01 1.531829e-02 5.780385e-04 6.527838e-07   Fold09\n2827  VF   VF 9.840025e-01 1.541621e-02 5.806535e-04 6.585718e-07   Fold09\n2828  VF   VF 9.680927e-01 3.030092e-02 1.603171e-03 3.255607e-06   Fold09\n2829  VF   VF 9.894022e-01 1.034505e-02 2.523630e-04 3.919746e-07   Fold09\n2830  VF   VF 7.063664e-01 2.138699e-01 7.683478e-02 2.928880e-03   Fold09\n2831  VF   VF 9.861855e-01 1.338979e-02 4.242474e-04 4.410498e-07   Fold09\n2832  VF   VF 9.275705e-01 6.775844e-02 4.652351e-03 1.874030e-05   Fold09\n2833  VF   VF 9.914029e-01 8.411158e-03 1.857931e-04 1.170619e-07   Fold09\n2834  VF   VF 9.911137e-01 8.693131e-03 1.930838e-04 1.241724e-07   Fold09\n2835  VF   VF 9.592793e-01 3.855461e-02 2.154802e-03 1.132067e-05   Fold09\n2836  VF   VF 7.235043e-01 2.295029e-01 4.628390e-02 7.088859e-04   Fold09\n2837  VF   VF 4.819877e-01 3.876804e-01 1.261031e-01 4.228810e-03   Fold09\n2838  VF   VF 9.314052e-01 6.290051e-02 5.670685e-03 2.364281e-05   Fold09\n2839  VF   VF 9.853615e-01 1.403231e-02 6.054781e-04 6.951604e-07   Fold09\n2840  VF   VF 5.879786e-01 2.259903e-01 1.662901e-01 1.974098e-02   Fold09\n2841  VF   VF 9.918074e-01 8.027100e-03 1.653444e-04 1.904058e-07   Fold09\n2842  VF   VF 9.668384e-01 3.089886e-02 2.257288e-03 5.483149e-06   Fold09\n2843  VF   VF 9.913144e-01 8.465285e-03 2.201643e-04 1.751871e-07   Fold09\n2844  VF   VF 9.558171e-01 4.140778e-02 2.766602e-03 8.536429e-06   Fold09\n2845  VF   VF 9.877346e-01 1.189147e-02 3.731062e-04 7.852504e-07   Fold09\n2846  VF   VF 9.803187e-01 1.898742e-02 6.929841e-04 8.564028e-07   Fold09\n2847  VF   VF 9.520709e-01 4.479764e-02 3.123106e-03 8.320581e-06   Fold09\n2848  VF   VF 9.913482e-01 8.347339e-03 3.042833e-04 1.491134e-07   Fold09\n2849  VF   VF 9.822918e-01 1.677014e-02 9.368596e-04 1.235131e-06   Fold09\n2850  VF   VF 9.878453e-01 1.182565e-02 3.285406e-04 5.388699e-07   Fold09\n2851  VF   VF 9.876484e-01 1.200616e-02 3.451267e-04 2.925847e-07   Fold09\n2852  VF   VF 9.684467e-01 3.010743e-02 1.443174e-03 2.705556e-06   Fold09\n2853  VF   VF 9.941327e-01 5.749537e-03 1.176949e-04 5.638522e-08   Fold09\n2854  VF   VF 9.848868e-01 1.444785e-02 6.642892e-04 1.055045e-06   Fold09\n2855  VF   VF 8.215162e-01 1.564869e-01 2.180548e-02 1.914092e-04   Fold09\n2856  VF   VF 7.077506e-01 2.874213e-01 4.815941e-03 1.213407e-05   Fold09\n2857  VF   VF 9.088823e-01 8.908039e-02 1.992047e-03 4.523638e-05   Fold09\n2858  VF   VF 9.187098e-01 7.802986e-02 3.254344e-03 5.976959e-06   Fold09\n2859  VF   VF 9.143897e-01 8.262926e-02 2.975429e-03 5.576676e-06   Fold09\n2860  VF   VF 8.856369e-01 1.092528e-01 5.085780e-03 2.454318e-05   Fold09\n2861  VF   VF 8.814312e-01 1.134277e-01 5.116076e-03 2.496190e-05   Fold09\n2862  VF   VF 8.835107e-01 1.111326e-01 5.343491e-03 1.315414e-05   Fold09\n2863  VF   VF 8.616752e-01 1.281041e-01 1.018969e-02 3.101163e-05   Fold09\n2864  VF   VF 8.615328e-01 1.285993e-01 9.837773e-03 3.015743e-05   Fold09\n2865  VF   VF 8.866767e-01 1.074443e-01 5.864284e-03 1.470297e-05   Fold09\n2866  VF   VF 9.103583e-01 8.654287e-02 3.092639e-03 6.239275e-06   Fold09\n2867  VF   VF 8.941165e-01 1.017440e-01 4.122747e-03 1.678934e-05   Fold09\n2868  VF   VF 8.763451e-01 1.170439e-01 6.573895e-03 3.702208e-05   Fold09\n2869  VF   VF 8.736682e-01 1.200789e-01 6.218002e-03 3.482837e-05   Fold09\n2870  VF   VF 8.785298e-01 1.155400e-01 5.898063e-03 3.215202e-05   Fold09\n2871  VF   VF 8.780642e-01 1.158876e-01 6.014944e-03 3.320224e-05   Fold09\n2872  VF   VF 8.838802e-01 1.105541e-01 5.551158e-03 1.449253e-05   Fold09\n2873  VF   VF 8.769480e-01 1.151662e-01 7.867303e-03 1.851316e-05   Fold09\n2874  VF   VF 8.888467e-01 1.067125e-01 4.425692e-03 1.511304e-05   Fold09\n2875  VF   VF 9.036399e-01 9.200399e-02 4.347702e-03 8.414703e-06   Fold09\n2876  VF   VF 8.938372e-01 9.930980e-02 6.838680e-03 1.431049e-05   Fold09\n2877  VF   VF 6.395005e-01 3.523341e-01 8.121586e-03 4.378398e-05   Fold09\n2878  VF   VF 6.373864e-01 3.543151e-01 8.253270e-03 4.523981e-05   Fold09\n2879  VF   VF 8.732006e-01 1.193572e-01 7.418186e-03 2.403728e-05   Fold09\n2880  VF   VF 8.965082e-01 9.889279e-02 4.586561e-03 1.246423e-05   Fold09\n2881  VF   VF 6.357948e-01 3.543305e-01 9.816502e-03 5.822883e-05   Fold09\n2882  VF   VF 8.723874e-01 1.196293e-01 7.956882e-03 2.637935e-05   Fold09\n2883  VF   VF 8.616162e-01 1.307188e-01 7.611716e-03 5.324806e-05   Fold09\n2884  VF   VF 6.214246e-01 3.692494e-01 9.268620e-03 5.731012e-05   Fold09\n2885  VF   VF 8.540560e-01 1.372831e-01 8.594522e-03 6.630485e-05   Fold09\n2886  VF   VF 8.558186e-01 1.356968e-01 8.420595e-03 6.410276e-05   Fold09\n2887  VF   VF 6.227186e-01 3.664648e-01 1.074645e-02 7.013741e-05   Fold09\n2888  VF   VF 6.200962e-01 3.688503e-01 1.098034e-02 7.313748e-05   Fold09\n2889  VF   VF 8.477632e-01 1.423027e-01 9.851621e-03 8.251125e-05   Fold09\n2890  VF   VF 8.844712e-01 1.103076e-01 5.197047e-03 2.411265e-05   Fold09\n2891  VF   VF 9.105457e-01 8.473580e-02 4.711332e-03 7.153698e-06   Fold09\n2892  VF   VF 9.217091e-01 7.524578e-02 3.039840e-03 5.246440e-06   Fold09\n2893  VF   VF 8.682759e-01 1.230327e-01 8.667577e-03 2.378368e-05   Fold09\n2894  VF   VF 7.893979e-01 1.866619e-01 2.380131e-02 1.388227e-04   Fold09\n2895  VF   VF 8.643746e-01 1.271464e-01 8.455495e-03 2.351059e-05   Fold09\n2896  VF   VF 9.045543e-01 9.037927e-02 5.058340e-03 8.112333e-06   Fold09\n2897  VF   VF 8.989809e-01 9.608209e-02 4.921690e-03 1.532538e-05   Fold09\n2898  VF   VF 8.613687e-01 1.291847e-01 9.419203e-03 2.735526e-05   Fold09\n2899  VF   VF 8.684634e-01 1.223566e-01 9.154499e-03 2.554584e-05   Fold09\n2900  VF   VF 8.623994e-01 1.283042e-01 9.220208e-03 7.621788e-05   Fold09\n2901  VF   VF 9.754492e-01 2.376012e-02 7.903064e-04 4.144074e-07   Fold09\n2902  VF   VF 9.522711e-01 4.479755e-02 2.927377e-03 3.929022e-06   Fold09\n2903  VF   VF 9.709685e-01 2.790683e-02 1.123316e-03 1.379053e-06   Fold09\n2904  VF   VF 9.761278e-01 2.322108e-02 6.508601e-04 3.076348e-07   Fold09\n2905  VF   VF 9.848568e-01 1.480209e-02 3.410460e-04 1.096326e-07   Fold09\n2906  VF   VF 9.794627e-01 1.997210e-02 5.649578e-04 2.392984e-07   Fold09\n2907  VF   VF 9.759938e-01 2.289365e-02 1.112006e-03 5.641352e-07   Fold09\n2908  VF    F 4.415518e-01 4.454314e-01 1.120335e-01 9.833464e-04   Fold09\n2909  VF   VF 9.267095e-01 6.693902e-02 6.340378e-03 1.107729e-05   Fold09\n2910  VF   VF 9.463202e-01 5.117456e-02 2.502921e-03 2.354355e-06   Fold09\n2911  VF   VF 9.848905e-01 1.457271e-02 5.366206e-04 1.449820e-07   Fold09\n2912  VF   VF 9.650739e-01 3.361417e-02 1.311005e-03 9.030409e-07   Fold09\n2913  VF   VF 9.510005e-01 4.440124e-02 4.590734e-03 7.517473e-06   Fold09\n2914  VF    F 7.624403e-02 7.988438e-01 1.240423e-01 8.698090e-04   Fold09\n2915  VF    F 1.046580e-01 7.958902e-01 9.913979e-02 3.120450e-04   Fold09\n2916  VF    F 7.956870e-02 7.578948e-01 1.620325e-01 5.040092e-04   Fold09\n2917  VF    F 8.779238e-02 7.734261e-01 1.383836e-01 3.978778e-04   Fold09\n2918  VF    F 9.120986e-02 8.004179e-01 1.080341e-01 3.380852e-04   Fold09\n2919  VF    F 1.145305e-01 7.993631e-01 8.587987e-02 2.265080e-04   Fold09\n2920  VF    F 1.053964e-01 7.733958e-01 1.209428e-01 2.649839e-04   Fold09\n2921  VF   VF 6.013366e-01 3.453226e-01 5.319490e-02 1.458558e-04   Fold09\n2922  VF   VF 7.546991e-01 2.208620e-01 2.441792e-02 2.098478e-05   Fold09\n2923  VF   VF 7.867115e-01 1.985194e-01 1.475477e-02 1.432008e-05   Fold09\n2924  VF   VF 7.919319e-01 1.930260e-01 1.502807e-02 1.402517e-05   Fold09\n2925  VF   VF 7.106629e-01 2.616628e-01 2.760309e-02 7.117905e-05   Fold09\n2926  VF   VF 6.887443e-01 2.855626e-01 2.561660e-02 7.646848e-05   Fold09\n2927  VF   VF 6.378371e-01 3.147318e-01 4.734378e-02 8.727547e-05   Fold09\n2928  VF   VF 7.784815e-01 2.032877e-01 1.821800e-02 1.283809e-05   Fold09\n2929  VF   VF 6.272711e-01 3.221065e-01 5.052543e-02 9.690799e-05   Fold09\n2930  VF   VF 7.523572e-01 2.276107e-01 2.000735e-02 2.477693e-05   Fold09\n2931  VF   VF 7.511132e-01 2.309382e-01 1.791267e-02 3.596958e-05   Fold09\n2932  VF   VF 7.725046e-01 2.115755e-01 1.590535e-02 1.456118e-05   Fold09\n2933  VF   VF 7.412103e-01 2.378194e-01 2.094257e-02 2.772570e-05   Fold09\n2934  VF   VF 6.241941e-01 3.222248e-01 5.347373e-02 1.073997e-04   Fold09\n2935  VF   VF 6.849625e-01 2.841291e-01 3.080817e-02 1.002921e-04   Fold09\n2936  VF   VF 7.998608e-01 1.840215e-01 1.610728e-02 1.039932e-05   Fold09\n2937  VF   VF 6.619187e-01 2.997966e-01 3.821589e-02 6.875610e-05   Fold09\n2938  VF   VF 7.218078e-01 2.428006e-01 3.534976e-02 4.185808e-05   Fold09\n2939  VF   VF 6.763860e-01 2.932945e-01 3.021036e-02 1.091686e-04   Fold09\n2940  VF   VF 6.051013e-01 3.351005e-01 5.965356e-02 1.446380e-04   Fold09\n2941  VF   VF 6.584390e-01 3.039954e-01 3.741236e-02 1.532331e-04   Fold09\n2942  VF   VF 8.109067e-01 1.772818e-01 1.180200e-02 9.439481e-06   Fold09\n2943  VF   VF 7.664865e-01 2.150995e-01 1.839611e-02 1.786094e-05   Fold09\n2944  VF   VF 6.424773e-01 3.180827e-01 3.925878e-02 1.812777e-04   Fold09\n2945  VF   VF 7.938679e-01 1.880558e-01 1.806373e-02 1.258982e-05   Fold09\n2946  VF   VF 6.023094e-01 3.427356e-01 5.461322e-02 3.418138e-04   Fold09\n2947  VF   VF 6.872860e-01 2.793948e-01 3.327355e-02 4.570549e-05   Fold09\n2948  VF    F 6.842116e-02 8.244206e-01 1.033474e-01 3.810834e-03   Fold09\n2949  VF    F 8.754319e-02 8.050499e-01 1.056067e-01 1.800261e-03   Fold09\n2950  VF    F 8.190155e-02 8.250275e-01 9.007223e-02 2.998753e-03   Fold09\n2951  VF    F 8.364137e-02 8.185516e-01 9.458825e-02 3.218798e-03   Fold09\n2952  VF    F 6.655034e-02 8.221456e-01 1.074843e-01 3.819758e-03   Fold09\n2953   F    F 3.044691e-01 5.581997e-01 1.348026e-01 2.528551e-03   Fold09\n2954   F    F 3.052249e-01 5.579878e-01 1.342846e-01 2.502756e-03   Fold09\n2955   F   VF 5.847529e-01 2.093094e-01 1.825095e-01 2.342816e-02   Fold09\n2956   F    F 3.222262e-01 4.108290e-01 2.585258e-01 8.419025e-03   Fold09\n2957   F   VF 5.966737e-01 2.974332e-01 1.054318e-01 4.612830e-04   Fold09\n2958   F   VF 6.799034e-01 2.724607e-01 4.741473e-02 2.211843e-04   Fold09\n2959   F    M 7.597496e-02 2.670726e-01 4.195205e-01 2.374319e-01   Fold09\n2960   F   VF 6.999918e-01 2.528384e-01 4.695877e-02 2.110410e-04   Fold09\n2961   F   VF 5.689378e-01 3.500870e-01 8.025611e-02 7.190736e-04   Fold09\n2962   F   VF 8.555515e-01 1.287587e-01 1.567813e-02 1.163448e-05   Fold09\n2963   F   VF 7.915449e-01 1.855987e-01 2.281935e-02 3.704979e-05   Fold09\n2964   F   VF 8.276730e-01 1.589785e-01 1.332628e-02 2.220177e-05   Fold09\n2965   F   VF 8.441143e-01 1.435692e-01 1.229810e-02 1.840404e-05   Fold09\n2966   F   VF 7.886291e-01 1.914958e-01 1.977815e-02 9.694337e-05   Fold09\n2967   F    F 1.383968e-01 6.372290e-01 2.220364e-01 2.337848e-03   Fold09\n2968   F    F 1.154427e-01 6.890654e-01 1.894525e-01 6.039400e-03   Fold09\n2969   F    F 1.591286e-01 6.577436e-01 1.818840e-01 1.243744e-03   Fold09\n2970   F    L 2.705969e-04 2.689725e-02 2.395774e-01 7.332547e-01   Fold09\n2971   F    F 8.919176e-02 6.532580e-01 2.454080e-01 1.214226e-02   Fold09\n2972   F    F 1.000640e-01 6.439074e-01 2.518136e-01 4.215017e-03   Fold09\n2973   F    L 2.526308e-05 4.800168e-03 8.822822e-02 9.069464e-01   Fold09\n2974   F    F 8.593627e-02 5.810340e-01 3.271995e-01 5.830240e-03   Fold09\n2975   F    F 1.362640e-01 6.850510e-01 1.763681e-01 2.316876e-03   Fold09\n2976   F    F 1.738410e-01 6.779041e-01 1.466508e-01 1.604121e-03   Fold09\n2977   F    F 7.113471e-02 6.392423e-01 2.722608e-01 1.736218e-02   Fold09\n2978   F    L 7.483836e-02 3.945731e-02 5.966672e-02 8.260376e-01   Fold09\n2979   F   VF 8.998556e-01 7.965692e-02 1.904474e-02 1.442778e-03   Fold09\n2980   F   VF 6.011922e-01 2.052879e-01 1.403892e-01 5.313067e-02   Fold09\n2981   F   VF 4.451629e-01 1.943156e-01 2.678622e-01 9.265926e-02   Fold09\n2982   F    L 1.914928e-04 1.952795e-03 3.420599e-02 9.636497e-01   Fold09\n2983   F    L 1.045781e-06 1.006890e-04 5.387912e-03 9.945104e-01   Fold09\n2984   F   VF 8.190935e-01 1.665585e-01 1.405517e-02 2.928626e-04   Fold09\n2985   F   VF 8.468522e-01 1.418790e-01 1.121383e-02 5.493804e-05   Fold09\n2986   F   VF 7.959108e-01 1.865314e-01 1.734087e-02 2.169018e-04   Fold09\n2987   F    M 2.226956e-01 7.161382e-02 7.054752e-01 2.154047e-04   Fold09\n2988   F   VF 9.085166e-01 8.737595e-02 4.098746e-03 8.651379e-06   Fold09\n2989   F   VF 8.874718e-01 1.066538e-01 5.859655e-03 1.467617e-05   Fold09\n2990   F   VF 8.544969e-01 1.366725e-01 8.802046e-03 2.855871e-05   Fold09\n2991   F   VF 8.984035e-01 9.719930e-02 4.386331e-03 1.089752e-05   Fold09\n2992   F   VF 3.850398e-01 3.810905e-01 2.145042e-01 1.936547e-02   Fold09\n2993   F    L 1.390652e-02 3.773591e-02 6.520270e-02 8.831549e-01   Fold09\n2994   F    L 1.400706e-02 3.795384e-02 6.540925e-02 8.826298e-01   Fold09\n2995   F    L 1.833694e-02 4.388057e-02 5.638027e-02 8.814022e-01   Fold09\n2996   F   VF 7.752829e-01 1.536197e-01 7.101324e-02 8.412220e-05   Fold09\n2997   F    L 1.810954e-02 4.403015e-02 5.563104e-02 8.822293e-01   Fold09\n2998   F   VF 5.937616e-01 3.949387e-01 1.121482e-02 8.484735e-05   Fold09\n2999   F   VF 5.805688e-01 4.070782e-01 1.225119e-02 1.018118e-04   Fold09\n3000   F   VF 5.520425e-01 4.330748e-01 1.473303e-02 1.496229e-04   Fold09\n3001   F   VF 5.487098e-01 4.360771e-01 1.505653e-02 1.566354e-04   Fold09\n3002   F   VF 8.435713e-01 1.421370e-01 1.425740e-02 3.435744e-05   Fold09\n3003   F   VF 7.581732e-01 2.085254e-01 3.315584e-02 1.455763e-04   Fold09\n3004   F    F 3.767735e-01 4.951596e-01 1.241692e-01 3.897653e-03   Fold09\n3005   F   VF 4.681197e-01 4.138585e-01 1.166559e-01 1.365887e-03   Fold09\n3006   F    F 2.917125e-01 3.866736e-01 2.907292e-01 3.088477e-02   Fold09\n3007   F    L 6.031987e-02 6.055909e-02 1.026041e-01 7.765169e-01   Fold09\n3008   F    F 7.530469e-02 8.039068e-01 1.198638e-01 9.247096e-04   Fold09\n3009   F    F 7.941536e-02 7.771105e-01 1.430458e-01 4.283598e-04   Fold09\n3010   F    F 1.132479e-02 5.072392e-01 4.629534e-01 1.848264e-02   Fold09\n3011   F    F 5.232829e-02 7.376475e-01 2.089571e-01 1.067134e-03   Fold09\n3012   F    F 9.211559e-02 7.926633e-01 1.148097e-01 4.113320e-04   Fold09\n3013   F    F 6.799611e-02 7.929433e-01 1.377818e-01 1.278794e-03   Fold09\n3014   F    F 4.549973e-02 7.447722e-01 2.083748e-01 1.353298e-03   Fold09\n3015   F    F 6.475090e-02 7.890007e-01 1.455265e-01 7.219219e-04   Fold09\n3016   F    F 9.027554e-02 7.907297e-01 1.186176e-01 3.772080e-04   Fold09\n3017   F    F 6.159713e-02 7.809023e-01 1.559276e-01 1.572978e-03   Fold09\n3018   F    F 6.393756e-02 7.403661e-01 1.950326e-01 6.636970e-04   Fold09\n3019   F    F 5.891374e-02 7.856335e-01 1.537286e-01 1.724093e-03   Fold09\n3020   F    F 5.773214e-02 7.891011e-01 1.513105e-01 1.856283e-03   Fold09\n3021   F    F 5.560235e-02 7.433320e-01 2.001273e-01 9.382598e-04   Fold09\n3022   F    F 7.187327e-02 8.013737e-01 1.262608e-01 4.922656e-04   Fold09\n3023   F    F 8.735172e-02 7.901417e-01 1.220970e-01 4.095883e-04   Fold09\n3024   F    F 7.751850e-02 8.293537e-01 9.247187e-02 6.559563e-04   Fold09\n3025   F    F 9.864444e-02 8.542448e-01 4.555845e-02 1.552291e-03   Fold09\n3026   F    F 5.538209e-02 7.618595e-01 1.814283e-01 1.330193e-03   Fold09\n3027   F   VF 8.002386e-01 1.857817e-01 1.396778e-02 1.185528e-05   Fold09\n3028   F   VF 7.205647e-01 2.535005e-01 2.588191e-02 5.285373e-05   Fold09\n3029   F   VF 6.213937e-01 3.303100e-01 4.822813e-02 6.813194e-05   Fold09\n3030   F   VF 7.061312e-01 2.695257e-01 2.428356e-02 5.953161e-05   Fold09\n3031   F    F 3.381472e-01 4.447021e-01 2.146076e-01 2.543060e-03   Fold09\n3032   F   VF 7.395848e-01 2.379744e-01 2.238833e-02 5.240954e-05   Fold09\n3033   F   VF 4.392063e-01 4.348324e-01 1.205492e-01 5.412172e-03   Fold09\n3034   F   VF 4.386171e-01 4.350806e-01 1.208585e-01 5.443749e-03   Fold09\n3035   F   VF 6.678148e-01 2.837957e-01 4.827624e-02 1.132276e-04   Fold09\n3036   F    F 4.029741e-01 4.191431e-01 1.714238e-01 6.459057e-03   Fold09\n3037   F   VF 7.314332e-01 2.323774e-01 3.615711e-02 3.224852e-05   Fold09\n3038   F    F 3.527995e-01 4.610041e-01 1.673016e-01 1.889494e-02   Fold09\n3039   F    F 3.500007e-01 4.626098e-01 1.682109e-01 1.917858e-02   Fold09\n3040   F   VF 6.774138e-01 2.874825e-01 3.505631e-02 4.742526e-05   Fold09\n3041   F   VF 6.806180e-01 2.862553e-01 3.300437e-02 1.222808e-04   Fold09\n3042   F    F 7.984564e-02 8.083305e-01 1.088649e-01 2.958965e-03   Fold09\n3043   F    F 6.469550e-02 8.103287e-01 1.201127e-01 4.863050e-03   Fold09\n3044   F    F 6.641043e-02 7.635618e-01 1.655879e-01 4.439898e-03   Fold09\n3045   F    F 6.781904e-02 7.937491e-01 1.342058e-01 4.226022e-03   Fold09\n3046   F    F 5.590173e-02 8.237041e-01 1.104268e-01 9.967460e-03   Fold09\n3047   F    F 5.719455e-02 8.122551e-01 1.245114e-01 6.039015e-03   Fold09\n3048   F    F 6.124552e-02 7.880451e-01 1.453497e-01 5.359671e-03   Fold09\n3049   F    F 7.104445e-02 8.181823e-01 1.062562e-01 4.517101e-03   Fold09\n3050   F    F 1.310780e-02 4.982083e-01 3.973056e-01 9.137836e-02   Fold09\n3051   F    F 5.222560e-02 8.133435e-01 1.274668e-01 6.964098e-03   Fold09\n3052   F    F 1.184582e-02 4.610589e-01 2.031852e-01 3.239100e-01   Fold09\n3053   F    F 5.084929e-02 8.081521e-01 1.333637e-01 7.634949e-03   Fold09\n3054   F    F 3.422652e-02 7.487231e-01 1.775841e-01 3.946638e-02   Fold09\n3055   F    F 3.633304e-02 7.449079e-01 2.032619e-01 1.549714e-02   Fold09\n3056   F    F 3.685369e-02 7.472600e-01 2.008926e-01 1.499374e-02   Fold09\n3057   F    F 3.670330e-02 7.456104e-01 2.024770e-01 1.520933e-02   Fold09\n3058   F    F 6.511812e-02 8.194983e-01 1.100458e-01 5.337790e-03   Fold09\n3059   F    F 3.428996e-02 7.497495e-01 1.769219e-01 3.903856e-02   Fold09\n3060   F    F 4.068773e-02 7.677610e-01 1.653214e-01 2.622981e-02   Fold09\n3061   M    M 7.155176e-02 1.002124e-01 5.431120e-01 2.851238e-01   Fold09\n3062   M    M 1.031897e-01 1.210380e-01 4.139541e-01 3.618182e-01   Fold09\n3063   M   VF 8.033616e-01 1.812899e-01 1.530473e-02 4.369350e-05   Fold09\n3064   M   VF 7.921806e-01 1.904370e-01 1.734455e-02 3.782830e-05   Fold09\n3065   M   VF 7.919321e-01 1.906453e-01 1.738459e-02 3.799163e-05   Fold09\n3066   M   VF 7.881023e-01 1.939767e-01 1.788144e-02 3.952626e-05   Fold09\n3067   M    M 7.652060e-02 2.890910e-01 3.734261e-01 2.609623e-01   Fold09\n3068   M    F 1.937012e-01 6.713061e-01 1.339702e-01 1.022423e-03   Fold09\n3069   M    M 8.169701e-03 2.160225e-01 4.077350e-01 3.680728e-01   Fold09\n3070   M    L 2.293211e-04 2.525961e-02 2.430033e-01 7.315078e-01   Fold09\n3071   M    F 9.146686e-02 6.058880e-01 2.972285e-01 5.416638e-03   Fold09\n3072   M    F 1.474748e-01 6.526049e-01 1.980145e-01 1.905824e-03   Fold09\n3073   M    F 5.564258e-02 5.963860e-01 3.323806e-01 1.559079e-02   Fold09\n3074   M    F 9.291633e-02 6.478259e-01 2.545714e-01 4.686358e-03   Fold09\n3075   M    F 4.888165e-02 4.760127e-01 4.179091e-01 5.719655e-02   Fold09\n3076   M    F 5.163201e-02 7.502890e-01 1.965162e-01 1.562812e-03   Fold09\n3077   M    F 5.315519e-02 6.653500e-01 2.808374e-01 6.574022e-04   Fold09\n3078   M    F 3.308398e-02 7.785636e-01 1.868234e-01 1.529091e-03   Fold09\n3079   M    F 9.865592e-03 4.859068e-01 4.025036e-01 1.017240e-01   Fold09\n3080   M    F 6.468475e-02 7.860637e-01 1.484475e-01 8.040829e-04   Fold09\n3081   M    F 6.278392e-02 7.403023e-01 1.960268e-01 8.869233e-04   Fold09\n3082   M    F 7.380439e-02 7.563929e-01 1.691383e-01 6.643957e-04   Fold09\n3083   M    F 5.892666e-02 7.907055e-01 1.485316e-01 1.836240e-03   Fold09\n3084   M    F 7.382064e-02 7.862605e-01 1.391616e-01 7.572529e-04   Fold09\n3085   M    F 5.046410e-02 7.292536e-01 2.179550e-01 2.327336e-03   Fold09\n3086   M    F 2.044804e-01 4.610804e-01 3.209104e-01 1.352875e-02   Fold09\n3087   M    M 3.556389e-02 2.135077e-01 6.186306e-01 1.322979e-01   Fold09\n3088   M    F 4.876141e-02 7.797634e-01 1.638589e-01 7.616301e-03   Fold09\n3089   M    M 5.229260e-04 2.020596e-02 9.754873e-01 3.783810e-03   Fold09\n3090   M    F 5.691944e-02 8.250694e-01 1.080621e-01 9.949122e-03   Fold09\n3091   M    F 3.990752e-02 7.707606e-01 1.780682e-01 1.126364e-02   Fold09\n3092   M    L 4.927611e-03 2.605232e-01 1.462656e-01 5.882836e-01   Fold09\n3093   M    F 2.225047e-02 6.590935e-01 2.050389e-01 1.136171e-01   Fold09\n3094   M    F 2.839088e-02 6.887525e-01 2.374251e-01 4.543149e-02   Fold09\n3095   M    F 2.444126e-02 6.139674e-01 3.142641e-01 4.732718e-02   Fold09\n3096   M    F 3.067627e-02 6.864103e-01 2.363844e-01 4.652900e-02   Fold09\n3097   M    F 3.349867e-02 7.494133e-01 1.775927e-01 3.949530e-02   Fold09\n3098   M    F 5.768277e-02 7.752278e-01 1.602284e-01 6.861075e-03   Fold09\n3099   M    F 4.355500e-02 7.834942e-01 1.646656e-01 8.285158e-03   Fold09\n3100   M    F 4.762625e-02 7.875491e-01 1.544011e-01 1.042353e-02   Fold09\n3101   M    M 1.997023e-04 1.478912e-02 9.728664e-01 1.214481e-02   Fold09\n3102   L    L 3.599973e-03 8.254870e-02 3.295524e-01 5.842989e-01   Fold09\n3103   L    F 7.578033e-02 6.295435e-01 2.878722e-01 6.803947e-03   Fold09\n3104   L    M 1.627056e-01 3.145099e-01 3.570698e-01 1.657148e-01   Fold09\n3105   L    L 2.338712e-05 5.504847e-03 2.739960e-02 9.670722e-01   Fold09\n3106   L    L 1.189884e-05 3.533848e-03 2.286841e-02 9.735858e-01   Fold09\n3107   L    L 8.963581e-05 2.945136e-02 2.674465e-01 7.030125e-01   Fold09\n3108   L    F 3.355886e-02 6.920330e-01 2.714073e-01 3.000763e-03   Fold09\n3109   L    F 3.219717e-02 6.913808e-01 2.726879e-01 3.734085e-03   Fold09\n3110   L    M 1.374424e-02 1.635275e-01 4.439600e-01 3.787682e-01   Fold09\n3111   L    L 7.360614e-08 5.826236e-05 7.346419e-04 9.992070e-01   Fold09\n3112   L    L 4.412389e-08 3.721722e-05 5.274408e-04 9.994353e-01   Fold09\n3113   L    L 1.941808e-06 6.004316e-04 3.468040e-03 9.959296e-01   Fold09\n3114   L    F 5.120046e-02 8.093935e-01 1.324682e-01 6.937845e-03   Fold09\n3115   L    L 2.025675e-06 6.294111e-04 3.495364e-03 9.958732e-01   Fold09\n3116   L    L 1.343929e-06 4.732826e-04 2.479341e-03 9.970460e-01   Fold09\n3117   L    L 1.038833e-03 1.335763e-01 2.803882e-01 5.849967e-01   Fold09\n3118   L    L 9.922434e-04 1.345295e-01 2.734747e-01 5.910035e-01   Fold09\n3119   L    L 2.261222e-06 6.570032e-04 2.773033e-03 9.965677e-01   Fold09\n3120   L    L 1.620529e-04 3.417920e-02 1.068429e-01 8.588159e-01   Fold09\n3121   L    L 1.555944e-07 1.030788e-04 1.046124e-03 9.988506e-01   Fold09\n3122  VF   VF 9.514221e-01 4.530554e-02 3.265587e-03 6.767586e-06   Fold10\n3123  VF   VF 9.568102e-01 4.074899e-02 2.436014e-03 4.792772e-06   Fold10\n3124  VF   VF 9.291768e-01 6.415580e-02 6.648588e-03 1.878517e-05   Fold10\n3125  VF   VF 9.542097e-01 4.285761e-02 2.928108e-03 4.538548e-06   Fold10\n3126  VF   VF 9.424877e-01 5.253284e-02 4.967264e-03 1.218337e-05   Fold10\n3127  VF   VF 9.539190e-01 4.334142e-02 2.734514e-03 5.096001e-06   Fold10\n3128  VF   VF 9.539798e-01 4.306550e-02 2.949139e-03 5.582539e-06   Fold10\n3129  VF    F 3.051345e-01 5.763936e-01 1.156068e-01 2.865124e-03   Fold10\n3130  VF    F 3.059256e-01 5.771329e-01 1.140598e-01 2.881769e-03   Fold10\n3131  VF    F 3.343705e-01 5.896899e-01 7.224146e-02 3.698135e-03   Fold10\n3132  VF   VF 8.954744e-01 8.867221e-02 1.582015e-02 3.321604e-05   Fold10\n3133  VF   VF 9.617978e-01 3.512873e-02 3.070558e-03 2.939357e-06   Fold10\n3134  VF   VF 8.130249e-01 1.462838e-01 4.055096e-02 1.404274e-04   Fold10\n3135  VF   VF 9.449739e-01 4.944682e-02 5.570691e-03 8.618843e-06   Fold10\n3136  VF   VF 8.654076e-01 1.064289e-01 2.808360e-02 7.987037e-05   Fold10\n3137  VF   VF 9.524250e-01 4.134834e-02 6.189532e-03 3.712844e-05   Fold10\n3138  VF   VF 9.873147e-01 1.221845e-02 4.663728e-04 4.617135e-07   Fold10\n3139  VF   VF 9.883752e-01 1.119965e-02 4.247770e-04 3.717456e-07   Fold10\n3140  VF   VF 9.883205e-01 1.129336e-02 3.857927e-04 3.306408e-07   Fold10\n3141  VF   VF 9.863060e-01 1.307976e-02 6.136382e-04 6.086910e-07   Fold10\n3142  VF   VF 9.875955e-01 1.183610e-02 5.680278e-04 4.106255e-07   Fold10\n3143  VF   VF 9.832349e-01 1.589041e-02 8.735099e-04 1.231191e-06   Fold10\n3144  VF   VF 9.882197e-01 1.137279e-02 4.071665e-04 3.576893e-07   Fold10\n3145  VF   VF 9.881919e-01 1.139885e-02 4.089310e-04 3.608608e-07   Fold10\n3146  VF   VF 9.710775e-01 2.646343e-02 2.452878e-03 6.167748e-06   Fold10\n3147  VF   VF 9.872222e-01 1.213333e-02 6.439404e-04 5.008502e-07   Fold10\n3148  VF   VF 9.688941e-01 2.830724e-02 2.788264e-03 1.039504e-05   Fold10\n3149  VF   VF 9.577471e-01 3.698798e-02 5.238573e-03 2.633376e-05   Fold10\n3150  VF   VF 9.811184e-01 1.781809e-02 1.062277e-03 1.280137e-06   Fold10\n3151  VF   VF 6.281864e-01 3.255770e-01 4.593123e-02 3.054329e-04   Fold10\n3152  VF   VF 6.189651e-01 3.299256e-01 5.078587e-02 3.233464e-04   Fold10\n3153  VF   VF 6.631933e-01 2.992065e-01 3.739432e-02 2.058539e-04   Fold10\n3154  VF   VF 6.584101e-01 3.032385e-01 3.814091e-02 2.104754e-04   Fold10\n3155  VF   VF 6.999611e-01 2.745512e-01 2.536641e-02 1.213124e-04   Fold10\n3156  VF    F 2.572819e-01 4.691768e-01 2.148868e-01 5.865450e-02   Fold10\n3157  VF   VF 5.930160e-01 3.542788e-01 5.234501e-02 3.602053e-04   Fold10\n3158  VF   VF 5.661458e-01 3.687834e-01 6.460525e-02 4.655278e-04   Fold10\n3159  VF   VF 7.531268e-01 2.203777e-01 2.643213e-02 6.340267e-05   Fold10\n3160  VF   VF 6.992592e-01 2.757589e-01 2.481356e-02 1.683653e-04   Fold10\n3161  VF   VF 7.538301e-01 2.212435e-01 2.486730e-02 5.910245e-05   Fold10\n3162  VF   VF 7.681959e-01 2.123296e-01 1.943970e-02 3.481800e-05   Fold10\n3163  VF   VF 8.752764e-01 1.197254e-01 4.993461e-03 4.661045e-06   Fold10\n3164  VF   VF 8.454331e-01 1.468888e-01 7.666968e-03 1.103832e-05   Fold10\n3165  VF   VF 7.217801e-01 2.503299e-01 2.780625e-02 8.378787e-05   Fold10\n3166  VF   VF 8.312228e-01 1.588680e-01 9.900599e-03 8.657324e-06   Fold10\n3167  VF    F 1.412455e-01 7.003857e-01 1.568964e-01 1.472364e-03   Fold10\n3168  VF    F 1.144852e-01 6.905127e-01 1.927344e-01 2.267585e-03   Fold10\n3169  VF    F 1.631928e-01 6.981869e-01 1.375864e-01 1.033909e-03   Fold10\n3170  VF    F 9.871402e-02 6.737052e-01 2.252124e-01 2.368358e-03   Fold10\n3171  VF    F 1.499231e-01 6.735086e-01 1.754776e-01 1.090742e-03   Fold10\n3172  VF    F 1.491029e-01 6.772259e-01 1.725887e-01 1.082497e-03   Fold10\n3173  VF    F 1.011149e-01 6.821096e-01 2.145862e-01 2.189355e-03   Fold10\n3174  VF   VF 9.849743e-01 1.453340e-02 4.918709e-04 4.205055e-07   Fold10\n3175  VF   VF 9.883642e-01 1.132387e-02 3.116954e-04 2.232618e-07   Fold10\n3176  VF   VF 9.680502e-01 2.959425e-02 2.345404e-03 1.010421e-05   Fold10\n3177  VF   VF 9.904326e-01 9.265616e-03 3.016537e-04 1.510847e-07   Fold10\n3178  VF   VF 9.848644e-01 1.469313e-02 4.421188e-04 3.918460e-07   Fold10\n3179  VF   VF 9.281075e-01 6.246247e-02 9.337937e-03 9.212001e-05   Fold10\n3180  VF   VF 9.882386e-01 1.146142e-02 2.997163e-04 2.195609e-07   Fold10\n3181  VF   VF 9.731552e-01 2.547481e-02 1.368123e-03 1.861074e-06   Fold10\n3182  VF   VF 9.783048e-01 2.094747e-02 7.467845e-04 9.470801e-07   Fold10\n3183  VF   VF 9.646973e-01 3.345445e-02 1.845001e-03 3.219963e-06   Fold10\n3184  VF   VF 7.392687e-01 1.919953e-01 6.399283e-02 4.743186e-03   Fold10\n3185  VF   VF 9.237454e-01 7.036131e-02 5.871696e-03 2.157344e-05   Fold10\n3186  VF   VF 9.851032e-01 1.432518e-02 5.710278e-04 5.601634e-07   Fold10\n3187  VF   VF 9.862158e-01 1.331658e-02 4.673414e-04 2.792190e-07   Fold10\n3188  VF   VF 9.784322e-01 2.062670e-02 9.402754e-04 7.929473e-07   Fold10\n3189  VF   VF 9.808739e-01 1.848564e-02 6.398543e-04 6.083373e-07   Fold10\n3190  VF   VF 9.890717e-01 1.056385e-02 3.641483e-04 2.519430e-07   Fold10\n3191  VF   VF 9.930071e-01 6.822660e-03 1.702009e-04 6.105509e-08   Fold10\n3192  VF   VF 9.791389e-01 2.012059e-02 7.397187e-04 7.650041e-07   Fold10\n3193  VF   VF 9.843271e-01 1.516033e-02 5.121061e-04 4.285837e-07   Fold10\n3194  VF   VF 9.574003e-01 4.021529e-02 2.371759e-03 1.261105e-05   Fold10\n3195  VF   VF 9.225513e-01 7.232964e-02 5.103770e-03 1.524394e-05   Fold10\n3196  VF   VF 9.217023e-01 6.941999e-02 8.823514e-03 5.423598e-05   Fold10\n3197  VF   VF 9.200507e-01 7.660454e-02 3.341999e-03 2.787769e-06   Fold10\n3198  VF   VF 9.100671e-01 8.545909e-02 4.467688e-03 6.107734e-06   Fold10\n3199  VF   VF 9.142292e-01 8.259240e-02 3.173201e-03 5.214556e-06   Fold10\n3200  VF   VF 7.451132e-01 2.508837e-01 3.995402e-03 7.755209e-06   Fold10\n3201  VF   VF 8.818977e-01 1.138044e-01 4.281372e-03 1.648486e-05   Fold10\n3202  VF   VF 9.161274e-01 8.101692e-02 2.852641e-03 3.071419e-06   Fold10\n3203  VF   VF 8.879266e-01 1.101127e-01 1.923372e-03 3.735555e-05   Fold10\n3204  VF   VF 9.137139e-01 8.333563e-02 2.947260e-03 3.245870e-06   Fold10\n3205  VF   VF 8.861806e-01 1.081490e-01 5.662578e-03 7.867341e-06   Fold10\n3206  VF   VF 8.969003e-01 9.826763e-02 4.822675e-03 9.446285e-06   Fold10\n3207  VF   VF 8.971352e-01 9.831552e-02 4.540480e-03 8.817279e-06   Fold10\n3208  VF   VF 9.063464e-01 8.888691e-02 4.759782e-03 6.913858e-06   Fold10\n3209  VF   VF 8.766856e-01 1.183169e-01 4.978957e-03 1.853647e-05   Fold10\n3210  VF   VF 9.042690e-01 9.112703e-02 4.597124e-03 6.801036e-06   Fold10\n3211  VF   VF 9.024593e-01 9.365663e-02 3.877260e-03 6.790875e-06   Fold10\n3212  VF   VF 8.920650e-01 1.027028e-01 5.222530e-03 9.654491e-06   Fold10\n3213  VF   VF 8.732340e-01 1.214774e-01 5.265142e-03 2.339197e-05   Fold10\n3214  VF   VF 8.246509e-01 1.672341e-01 8.068366e-03 4.663232e-05   Fold10\n3215  VF   VF 9.050825e-01 9.136082e-02 3.550011e-03 6.716442e-06   Fold10\n3216  VF   VF 8.753873e-01 1.194647e-01 5.125292e-03 2.264083e-05   Fold10\n3217  VF   VF 8.858971e-01 1.097612e-01 4.332684e-03 8.977267e-06   Fold10\n3218  VF   VF 9.033086e-01 9.213333e-02 4.551145e-03 6.897518e-06   Fold10\n3219  VF   VF 8.757775e-01 1.189575e-01 5.241604e-03 2.333741e-05   Fold10\n3220  VF   VF 9.080271e-01 8.837620e-02 3.589947e-03 6.779953e-06   Fold10\n3221  VF   VF 8.743503e-01 1.203844e-01 5.241787e-03 2.358993e-05   Fold10\n3222  VF   VF 7.001047e-01 2.936634e-01 6.208482e-03 2.342360e-05   Fold10\n3223  VF   VF 8.970493e-01 9.753311e-02 5.408550e-03 9.045693e-06   Fold10\n3224  VF   VF 9.012768e-01 9.437902e-02 4.335107e-03 9.040578e-06   Fold10\n3225  VF   VF 8.458709e-01 1.439358e-01 1.016394e-02 2.940540e-05   Fold10\n3226  VF   VF 8.440360e-01 1.455497e-01 1.038377e-02 3.050488e-05   Fold10\n3227  VF   VF 8.986242e-01 9.559348e-02 5.772299e-03 9.996981e-06   Fold10\n3228  VF   VF 9.058249e-01 9.049562e-02 3.673460e-03 5.986375e-06   Fold10\n3229  VF   VF 8.858638e-01 1.083711e-01 5.751013e-03 1.403198e-05   Fold10\n3230  VF   VF 8.639772e-01 1.298856e-01 6.104873e-03 3.232113e-05   Fold10\n3231  VF   VF 6.850308e-01 3.069512e-01 7.982283e-03 3.580066e-05   Fold10\n3232  VF   VF 9.023474e-01 9.314208e-02 4.503708e-03 6.797660e-06   Fold10\n3233  VF   VF 8.935143e-01 1.004101e-01 6.064225e-03 1.142885e-05   Fold10\n3234  VF   VF 6.441512e-01 3.028195e-01 4.738832e-02 5.640970e-03   Fold10\n3235  VF   VF 5.243918e-01 3.706282e-01 9.039268e-02 1.458730e-02   Fold10\n3236  VF   VF 8.845133e-01 1.085741e-01 6.898513e-03 1.411259e-05   Fold10\n3237  VF   VF 8.498040e-01 1.411139e-01 9.041676e-03 4.043085e-05   Fold10\n3238  VF   VF 6.741292e-01 3.181391e-01 7.695431e-03 3.625886e-05   Fold10\n3239  VF   VF 6.612989e-01 3.301978e-01 8.459010e-03 4.422868e-05   Fold10\n3240  VF   VF 8.508736e-01 1.422148e-01 6.869257e-03 4.236146e-05   Fold10\n3241  VF   VF 8.497420e-01 1.428955e-01 7.315485e-03 4.700475e-05   Fold10\n3242  VF   VF 8.674893e-01 1.267562e-01 5.730809e-03 2.374933e-05   Fold10\n3243  VF   VF 9.042352e-01 9.198056e-02 3.777884e-03 6.325411e-06   Fold10\n3244  VF   VF 9.032656e-01 9.253871e-02 4.188472e-03 7.255467e-06   Fold10\n3245  VF    M 9.958636e-04 9.965516e-04 9.980064e-01 1.166570e-06   Fold10\n3246  VF   VF 6.335764e-01 3.560600e-01 1.029661e-02 6.702278e-05   Fold10\n3247  VF   VF 8.246315e-01 1.624608e-01 1.279654e-02 1.111397e-04   Fold10\n3248  VF   VF 9.043795e-01 9.117832e-02 4.434344e-03 7.825596e-06   Fold10\n3249  VF   VF 8.730556e-01 1.201315e-01 6.798761e-03 1.411977e-05   Fold10\n3250  VF   VF 8.713242e-01 1.215762e-01 7.084642e-03 1.498045e-05   Fold10\n3251  VF   VF 8.664885e-01 1.263236e-01 7.172238e-03 1.566667e-05   Fold10\n3252  VF   VF 8.694431e-01 1.231717e-01 7.369189e-03 1.606589e-05   Fold10\n3253  VF   VF 9.137397e-01 8.316989e-02 3.085499e-03 4.925504e-06   Fold10\n3254  VF   VF 8.691694e-01 1.232085e-01 7.605268e-03 1.682985e-05   Fold10\n3255  VF   VF 8.814067e-01 1.140497e-01 4.526358e-03 1.730094e-05   Fold10\n3256  VF   VF 8.556349e-01 1.354395e-01 8.903709e-03 2.185067e-05   Fold10\n3257  VF   VF 9.460621e-01 5.078562e-02 3.149838e-03 2.405500e-06   Fold10\n3258  VF   VF 9.696622e-01 2.926659e-02 1.070694e-03 5.517008e-07   Fold10\n3259  VF   VF 9.687882e-01 3.008486e-02 1.126344e-03 5.940453e-07   Fold10\n3260  VF   VF 9.315239e-01 6.407315e-02 4.398569e-03 4.392856e-06   Fold10\n3261  VF   VF 9.368366e-01 6.003122e-02 3.126373e-03 5.806580e-06   Fold10\n3262  VF   VF 8.837864e-01 1.055924e-01 1.060427e-02 1.688499e-05   Fold10\n3263  VF   VF 9.158996e-01 7.974884e-02 4.341606e-03 9.997086e-06   Fold10\n3264  VF   VF 9.366717e-01 5.944152e-02 3.882792e-03 3.993117e-06   Fold10\n3265  VF   VF 9.748292e-01 2.407932e-02 1.090994e-03 4.682561e-07   Fold10\n3266  VF   VF 7.864407e-01 1.823930e-01 2.856552e-02 2.600798e-03   Fold10\n3267  VF   VF 8.956022e-01 9.542830e-02 8.954392e-03 1.515661e-05   Fold10\n3268  VF   VF 9.298445e-01 6.550212e-02 4.649214e-03 4.153736e-06   Fold10\n3269  VF   VF 9.611417e-01 3.707765e-02 1.779088e-03 1.538885e-06   Fold10\n3270  VF   VF 9.850822e-01 1.450082e-02 4.169373e-04 6.539769e-08   Fold10\n3271  VF   VF 9.663309e-01 3.236095e-02 1.307463e-03 7.101709e-07   Fold10\n3272  VF   VF 9.422418e-01 5.275113e-02 4.998689e-03 8.362538e-06   Fold10\n3273  VF    F 8.255248e-02 7.944479e-01 1.220650e-01 9.346355e-04   Fold10\n3274  VF    F 1.104197e-01 7.709640e-01 1.182243e-01 3.919524e-04   Fold10\n3275  VF    F 1.252981e-01 7.688806e-01 1.055356e-01 2.857870e-04   Fold10\n3276  VF    F 6.854194e-02 7.674827e-01 1.625842e-01 1.391099e-03   Fold10\n3277  VF   VF 7.535396e-01 2.262756e-01 2.016570e-02 1.915448e-05   Fold10\n3278  VF   VF 5.527293e-01 3.925880e-01 5.435950e-02 3.231230e-04   Fold10\n3279  VF   VF 7.170689e-01 2.644267e-01 1.846426e-02 4.015327e-05   Fold10\n3280  VF   VF 7.121354e-01 2.649163e-01 2.292135e-02 2.703703e-05   Fold10\n3281  VF   VF 7.861125e-01 2.004479e-01 1.343114e-02 8.416641e-06   Fold10\n3282  VF   VF 8.055184e-01 1.840793e-01 1.039452e-02 7.744884e-06   Fold10\n3283  VF   VF 8.109529e-01 1.793186e-01 9.721466e-03 7.025266e-06   Fold10\n3284  VF   VF 6.318595e-01 3.262930e-01 4.177794e-02 6.960469e-05   Fold10\n3285  VF   VF 6.612790e-01 3.022457e-01 3.642015e-02 5.520327e-05   Fold10\n3286  VF   VF 7.308872e-01 2.497852e-01 1.930285e-02 2.477368e-05   Fold10\n3287  VF   VF 8.022719e-01 1.867704e-01 1.094931e-02 8.341282e-06   Fold10\n3288  VF   VF 7.970929e-01 1.885540e-01 1.434414e-02 8.941798e-06   Fold10\n3289  VF   VF 6.408531e-01 3.149738e-01 4.408812e-02 8.490988e-05   Fold10\n3290  VF   VF 6.568930e-01 3.118862e-01 3.110534e-02 1.154806e-04   Fold10\n3291  VF   VF 6.481586e-01 3.227658e-01 2.896539e-02 1.101607e-04   Fold10\n3292  VF   VF 6.892240e-01 2.748133e-01 3.590692e-02 5.586125e-05   Fold10\n3293  VF   VF 6.883761e-01 2.761112e-01 3.545829e-02 5.446438e-05   Fold10\n3294  VF   VF 5.709919e-01 3.689732e-01 5.986903e-02 1.659350e-04   Fold10\n3295  VF   VF 7.833825e-01 2.034302e-01 1.317584e-02 1.154022e-05   Fold10\n3296  VF    F 2.845935e-01 6.733221e-01 4.180295e-02 2.814639e-04   Fold10\n3297  VF    F 7.513177e-02 7.578536e-01 1.639884e-01 3.026218e-03   Fold10\n3298  VF    F 8.591125e-02 7.622860e-01 1.489805e-01 2.822241e-03   Fold10\n3299   F   VF 9.396376e-01 5.549891e-02 4.849121e-03 1.439688e-05   Fold10\n3300   F    F 3.989024e-01 5.080768e-01 9.176893e-02 1.251873e-03   Fold10\n3301   F    F 3.897505e-01 5.289050e-01 7.972614e-02 1.618429e-03   Fold10\n3302   F   VF 5.813145e-01 3.384096e-01 7.991871e-02 3.571687e-04   Fold10\n3303   F    F 1.401751e-01 4.434608e-01 2.750020e-01 1.413621e-01   Fold10\n3304   F    F 4.545646e-01 4.581804e-01 8.470999e-02 2.545026e-03   Fold10\n3305   F    F 2.946675e-01 4.540168e-01 2.372589e-01 1.405689e-02   Fold10\n3306   F    F 4.290040e-01 4.378192e-01 1.216478e-01 1.152898e-02   Fold10\n3307   F    F 2.721393e-02 4.858864e-01 4.654333e-01 2.146640e-02   Fold10\n3308   F    F 9.308164e-02 7.135020e-01 1.886244e-01 4.791935e-03   Fold10\n3309   F    F 7.227280e-02 6.661839e-01 2.528087e-01 8.734591e-03   Fold10\n3310   F    F 1.053506e-01 6.920031e-01 2.001612e-01 2.485122e-03   Fold10\n3311   F    F 1.169096e-01 7.018513e-01 1.790969e-01 2.142216e-03   Fold10\n3312   F    F 7.260131e-02 6.929962e-01 2.257434e-01 8.659067e-03   Fold10\n3313   F    F 8.076367e-02 6.268018e-01 2.879678e-01 4.466764e-03   Fold10\n3314   F   VF 8.897631e-01 9.372955e-02 1.533873e-02 1.168577e-03   Fold10\n3315   F   VF 8.918551e-01 9.125487e-02 1.590425e-02 9.857425e-04   Fold10\n3316   F    L 3.133772e-04 3.028098e-03 3.465810e-02 9.620004e-01   Fold10\n3317   F    F 2.832124e-01 4.704341e-01 2.320579e-01 1.429550e-02   Fold10\n3318   F   VF 8.323011e-01 1.538041e-01 1.384326e-02 5.157988e-05   Fold10\n3319   F   VF 7.875113e-01 1.968431e-01 1.547723e-02 1.684445e-04   Fold10\n3320   F   VF 6.215988e-01 3.099467e-01 6.699588e-02 1.458601e-03   Fold10\n3321   F   VF 8.239046e-01 1.680340e-01 8.015460e-03 4.600077e-05   Fold10\n3322   F   VF 8.983125e-01 9.645535e-02 5.223840e-03 8.346597e-06   Fold10\n3323   F   VF 8.481582e-01 1.427856e-01 9.030780e-03 2.537743e-05   Fold10\n3324   F   VF 8.650916e-01 1.293128e-01 5.568448e-03 2.718898e-05   Fold10\n3325   F   VF 8.483372e-01 1.416329e-01 1.000034e-02 2.959235e-05   Fold10\n3326   F   VF 8.901412e-01 1.035146e-01 6.332022e-03 1.218735e-05   Fold10\n3327   F   VF 6.714788e-01 3.204429e-01 8.041069e-03 3.726736e-05   Fold10\n3328   F    L 1.573486e-02 4.740018e-02 3.710253e-02 8.997624e-01   Fold10\n3329   F   VF 8.738234e-01 1.187894e-01 7.365747e-03 2.139840e-05   Fold10\n3330   F   VF 8.828109e-01 1.099026e-01 7.270133e-03 1.631239e-05   Fold10\n3331   F   VF 6.509873e-01 3.398449e-01 9.116016e-03 5.177438e-05   Fold10\n3332   F   VF 6.496901e-01 3.410536e-01 9.203494e-03 5.280067e-05   Fold10\n3333   F   VF 6.464061e-01 3.441252e-01 9.413366e-03 5.541514e-05   Fold10\n3334   F   VF 6.489429e-01 3.417564e-01 9.247336e-03 5.337555e-05   Fold10\n3335   F   VF 8.412956e-01 1.509971e-01 7.653294e-03 5.404386e-05   Fold10\n3336   F   VF 6.226108e-01 3.662135e-01 1.109706e-02 7.859632e-05   Fold10\n3337   F   VF 6.014630e-01 3.856655e-01 1.276538e-02 1.061279e-04   Fold10\n3338   F   VF 6.012050e-01 3.859028e-01 1.278576e-02 1.064837e-04   Fold10\n3339   F   VF 5.939840e-01 3.925008e-01 1.339738e-02 1.178285e-04   Fold10\n3340   F   VF 7.984191e-01 1.842712e-01 1.720019e-02 1.095323e-04   Fold10\n3341   F   VF 7.902885e-01 1.904373e-01 1.918150e-02 9.268433e-05   Fold10\n3342   F   VF 5.242455e-01 3.403977e-01 1.200729e-01 1.528388e-02   Fold10\n3343   F   VF 7.983731e-01 1.644183e-01 3.587742e-02 1.331156e-03   Fold10\n3344   F    M 7.312116e-02 3.483290e-01 3.832883e-01 1.952616e-01   Fold10\n3345   F    M 2.555879e-01 2.341758e-01 3.497437e-01 1.604926e-01   Fold10\n3346   F    F 8.366084e-02 7.128418e-01 2.029940e-01 5.033150e-04   Fold10\n3347   F    F 5.455765e-02 7.257311e-01 2.177578e-01 1.953482e-03   Fold10\n3348   F    F 5.489096e-02 7.249775e-01 2.182330e-01 1.898510e-03   Fold10\n3349   F    F 6.211730e-02 7.508877e-01 1.857944e-01 1.200580e-03   Fold10\n3350   F    F 6.234568e-02 7.294374e-01 2.071938e-01 1.023134e-03   Fold10\n3351   F    F 1.116787e-01 7.742443e-01 1.137007e-01 3.762649e-04   Fold10\n3352   F    F 7.844218e-02 7.760475e-01 1.443064e-01 1.203893e-03   Fold10\n3353   F    F 5.124790e-02 7.260106e-01 2.214398e-01 1.301630e-03   Fold10\n3354   F    F 5.883685e-02 7.378920e-01 2.021681e-01 1.103084e-03   Fold10\n3355   F    F 7.010212e-02 7.266660e-01 2.023327e-01 8.991028e-04   Fold10\n3356   F    F 6.454112e-02 7.901735e-01 1.438295e-01 1.455847e-03   Fold10\n3357   F    F 5.220094e-02 7.268542e-01 2.196148e-01 1.330084e-03   Fold10\n3358   F    F 8.559942e-02 7.756783e-01 1.381299e-01 5.924269e-04   Fold10\n3359   F    F 8.510178e-02 7.892683e-01 1.247705e-01 8.594018e-04   Fold10\n3360   F    F 7.796203e-02 7.459881e-01 1.751820e-01 8.678454e-04   Fold10\n3361   F    F 7.718889e-02 7.447205e-01 1.772070e-01 8.835278e-04   Fold10\n3362   F    F 6.509583e-02 7.797231e-01 1.535227e-01 1.658412e-03   Fold10\n3363   F    F 8.751113e-02 7.658210e-01 1.459440e-01 7.238220e-04   Fold10\n3364   F    F 6.015870e-02 7.268962e-01 2.120989e-01 8.461477e-04   Fold10\n3365   F    F 4.607267e-02 6.996932e-01 2.521956e-01 2.038538e-03   Fold10\n3366   F    F 9.584826e-02 7.685018e-01 1.351899e-01 4.599994e-04   Fold10\n3367   F    F 7.111726e-02 7.729838e-01 1.552725e-01 6.264806e-04   Fold10\n3368   F    F 5.151406e-02 7.562598e-01 1.891719e-01 3.054227e-03   Fold10\n3369   F    F 5.112324e-02 7.560314e-01 1.898088e-01 3.036559e-03   Fold10\n3370   F    F 4.265938e-02 6.449072e-01 3.090580e-01 3.375343e-03   Fold10\n3371   F    F 2.137335e-02 6.098192e-01 3.465878e-01 2.221966e-02   Fold10\n3372   F    F 2.042137e-02 6.018540e-01 3.536670e-01 2.405765e-02   Fold10\n3373   F    F 6.553183e-02 7.506878e-01 1.829557e-01 8.246216e-04   Fold10\n3374   F    F 8.634013e-02 7.615047e-01 1.517715e-01 3.837093e-04   Fold10\n3375   F   VF 6.429703e-01 3.065373e-01 5.043875e-02 5.364224e-05   Fold10\n3376   F   VF 7.322487e-01 2.449792e-01 2.274680e-02 2.529930e-05   Fold10\n3377   F   VF 6.882275e-01 2.907604e-01 2.095664e-02 5.545821e-05   Fold10\n3378   F   VF 5.947168e-01 3.569154e-01 4.829175e-02 7.605327e-05   Fold10\n3379   F    F 3.737139e-01 4.645942e-01 1.539867e-01 7.705155e-03   Fold10\n3380   F   VF 6.000980e-01 3.474251e-01 5.235793e-02 1.189918e-04   Fold10\n3381   F   VF 6.259384e-01 3.348401e-01 3.905451e-02 1.669312e-04   Fold10\n3382   F   VF 5.727101e-01 3.739135e-01 5.317118e-02 2.052810e-04   Fold10\n3383   F    F 7.854496e-02 7.561767e-01 1.619136e-01 3.364775e-03   Fold10\n3384   F    F 7.252697e-02 7.798738e-01 1.434528e-01 4.146412e-03   Fold10\n3385   F    M 3.645518e-04 1.785099e-02 9.810190e-01 7.654082e-04   Fold10\n3386   F    F 4.683787e-02 7.387130e-01 2.060180e-01 8.431142e-03   Fold10\n3387   F    F 6.034584e-02 7.453832e-01 1.889526e-01 5.318335e-03   Fold10\n3388   F    F 6.031108e-02 7.633028e-01 1.700231e-01 6.363034e-03   Fold10\n3389   F    F 4.242350e-02 7.257230e-01 2.217225e-01 1.013092e-02   Fold10\n3390   F    F 6.436310e-02 7.370142e-01 1.930997e-01 5.522972e-03   Fold10\n3391   F    F 6.436861e-02 7.370488e-01 1.930657e-01 5.516947e-03   Fold10\n3392   F    F 1.111912e-02 4.250841e-01 2.242538e-01 3.395430e-01   Fold10\n3393   F    F 7.530117e-02 7.863404e-01 1.340068e-01 4.351563e-03   Fold10\n3394   F    F 3.620114e-02 7.235443e-01 2.051559e-01 3.509871e-02   Fold10\n3395   F    F 3.576042e-02 7.186762e-01 2.106502e-01 3.491323e-02   Fold10\n3396   F    F 3.617975e-02 7.214570e-01 2.078053e-01 3.455796e-02   Fold10\n3397   F    F 5.815600e-02 7.572231e-01 1.776518e-01 6.969088e-03   Fold10\n3398   F    F 5.505858e-02 7.561949e-01 1.809024e-01 7.844189e-03   Fold10\n3399   F    F 5.448253e-02 7.485584e-01 1.888842e-01 8.074848e-03   Fold10\n3400   F    F 3.405383e-02 7.068634e-01 2.184265e-01 4.065631e-02   Fold10\n3401   F    F 6.633425e-02 7.780817e-01 1.498679e-01 5.716156e-03   Fold10\n3402   F    F 6.666098e-02 7.813643e-01 1.464482e-01 5.526542e-03   Fold10\n3403   F    F 7.820025e-02 7.825834e-01 1.358249e-01 3.391397e-03   Fold10\n3404   F    F 3.428402e-02 6.941636e-01 2.557650e-01 1.578734e-02   Fold10\n3405   F    F 3.191901e-02 7.153514e-01 2.156967e-01 3.703295e-02   Fold10\n3406   M   VF 9.000477e-01 8.779851e-02 1.207123e-02 8.255188e-05   Fold10\n3407   M    F 3.200817e-01 6.002819e-01 7.558022e-02 4.056202e-03   Fold10\n3408   M    M 9.023987e-02 1.460182e-01 5.176402e-01 2.461018e-01   Fold10\n3409   M    M 1.235607e-01 1.573909e-01 5.041599e-01 2.148885e-01   Fold10\n3410   M    M 1.004217e-01 1.480523e-01 5.099637e-01 2.415623e-01   Fold10\n3411   M    M 6.487370e-02 1.210072e-01 5.138978e-01 3.002212e-01   Fold10\n3412   M   VF 4.873922e-01 4.318217e-01 8.016319e-02 6.229664e-04   Fold10\n3413   M   VF 7.004299e-01 2.680803e-01 3.132883e-02 1.609393e-04   Fold10\n3414   M   VF 6.692603e-01 2.816374e-01 4.891119e-02 1.911035e-04   Fold10\n3415   M   VF 4.793778e-01 4.151271e-01 9.613941e-02 9.355695e-03   Fold10\n3416   M    M 1.086074e-01 3.642969e-01 3.913196e-01 1.357761e-01   Fold10\n3417   M    F 7.959453e-02 6.559188e-01 2.608542e-01 3.632485e-03   Fold10\n3418   M    F 7.536563e-02 6.651094e-01 2.556567e-01 3.868286e-03   Fold10\n3419   M    F 1.038730e-01 6.854122e-01 2.081433e-01 2.571422e-03   Fold10\n3420   M    F 1.976981e-02 4.167811e-01 3.899671e-01 1.734821e-01   Fold10\n3421   M    L 1.224025e-04 2.202331e-02 1.841624e-01 7.936919e-01   Fold10\n3422   M    L 1.150979e-04 2.102706e-02 1.782157e-01 8.006422e-01   Fold10\n3423   M    L 3.025311e-04 3.593680e-02 2.601299e-01 7.036308e-01   Fold10\n3424   M    F 1.050967e-01 6.561263e-01 2.360976e-01 2.679372e-03   Fold10\n3425   M    F 9.254355e-02 6.833771e-01 2.215459e-01 2.533484e-03   Fold10\n3426   M   VF 7.705587e-01 2.046493e-01 2.451008e-02 2.819165e-04   Fold10\n3427   M   VF 7.892612e-01 1.908725e-01 1.971855e-02 1.477261e-04   Fold10\n3428   M    F 4.032799e-01 4.188101e-01 1.490769e-01 2.883308e-02   Fold10\n3429   M   VF 8.302380e-01 1.588654e-01 1.085402e-02 4.260128e-05   Fold10\n3430   M    F 1.370829e-02 5.120313e-01 4.400808e-01 3.417956e-02   Fold10\n3431   M    F 7.163479e-02 7.676417e-01 1.599732e-01 7.503257e-04   Fold10\n3432   M    F 4.315250e-02 7.166167e-01 2.385972e-01 1.633618e-03   Fold10\n3433   M    F 5.278915e-02 7.725977e-01 1.725827e-01 2.030386e-03   Fold10\n3434   M    F 3.888084e-02 7.229395e-01 2.364466e-01 1.733099e-03   Fold10\n3435   M    F 1.798937e-02 5.486894e-01 3.722467e-01 6.107456e-02   Fold10\n3436   M    F 1.204258e-02 5.164570e-01 3.146866e-01 1.568139e-01   Fold10\n3437   M    F 4.813479e-02 7.499385e-01 1.985993e-01 3.327385e-03   Fold10\n3438   M   VF 6.658218e-01 2.948182e-01 3.930095e-02 5.896477e-05   Fold10\n3439   M    M 1.141483e-01 4.224569e-01 4.342112e-01 2.918353e-02   Fold10\n3440   M    M 4.623456e-02 2.804910e-01 5.394355e-01 1.338390e-01   Fold10\n3441   M    F 1.588206e-01 4.591816e-01 3.213826e-01 6.061522e-02   Fold10\n3442   M    M 3.026016e-03 1.011372e-02 9.868036e-01 5.666579e-05   Fold10\n3443   M    F 7.106207e-02 7.760795e-01 1.486179e-01 4.240589e-03   Fold10\n3444   M    L 1.520639e-03 1.527062e-01 2.205268e-01 6.252464e-01   Fold10\n3445   M    F 2.427786e-02 5.863709e-01 3.507483e-01 3.860300e-02   Fold10\n3446   M    M 8.880259e-03 3.674734e-01 5.514802e-01 7.216619e-02   Fold10\n3447   M    M 1.645507e-02 4.484980e-01 4.953824e-01 3.966449e-02   Fold10\n3448   L    L 2.901247e-02 8.932486e-02 3.335124e-01 5.481503e-01   Fold10\n3449   L    L 2.101474e-02 7.820178e-02 4.494369e-01 4.513466e-01   Fold10\n3450   L    F 7.512525e-02 3.697475e-01 3.192041e-01 2.359232e-01   Fold10\n3451   L    F 1.099139e-01 6.929609e-01 1.957551e-01 1.370081e-03   Fold10\n3452   L    M 1.171274e-02 3.287034e-01 3.652292e-01 2.943546e-01   Fold10\n3453   L    M 2.839876e-03 5.066844e-02 9.457296e-01 7.620754e-04   Fold10\n3454   L    F 5.637572e-02 6.216107e-01 3.148691e-01 7.144522e-03   Fold10\n3455   L    F 5.645592e-02 6.234120e-01 3.130495e-01 7.082616e-03   Fold10\n3456   L    L 1.046709e-01 3.093231e-01 2.616896e-01 3.243165e-01   Fold10\n3457   L    M 3.312802e-03 2.880069e-01 5.832242e-01 1.254561e-01   Fold10\n3458   L    F 4.378049e-02 6.972524e-01 2.568080e-01 2.159094e-03   Fold10\n3459   L    L 1.782569e-06 5.863452e-04 2.701501e-03 9.967104e-01   Fold10\n3460   L    L 1.206733e-03 1.456164e-01 3.141510e-01 5.390258e-01   Fold10\n3461   L    L 4.725859e-07 2.517555e-04 1.462692e-03 9.982851e-01   Fold10\n3462   L    L 1.587719e-06 5.269729e-04 2.000793e-03 9.974706e-01   Fold10\n3463   L    F 2.762672e-02 6.629804e-01 2.549542e-01 5.443863e-02   Fold10\n3464   L    F 3.712051e-02 6.731643e-01 2.640418e-01 2.567334e-02   Fold10\n3465   L    M 8.635688e-11 1.271806e-06 8.259137e-01 1.740851e-01   Fold10\n3466   L    L 6.189979e-08 9.829959e-05 3.335443e-03 9.965662e-01   Fold10\n3467   L    F 3.203979e-02 6.495508e-01 2.938443e-01 2.456504e-02   Fold10"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-11",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-11",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\n\n&gt; # load the data and convert it to a tibble\n&gt; hpc_cv &lt;- modeldata::hpc_cv %&gt;% tibble::tibble()\n&gt; # compute accuracy (same as binary case)\n&gt; yardstick::accuracy(hpc_cv, obs, pred)\n&gt; # compute matthews correlation coefficient (same as binary case)\n&gt; yardstick::mcc(hpc_cv, obs, pred)\n&gt; \n&gt; # apply the sensitivity metrics\n&gt; yardstick::sensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n&gt; yardstick::sensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n&gt; yardstick::sensitivity(hpc_cv, obs, pred, estimator = \"micro\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-12",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-yardstick-12",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: yardstick",
    "text": "Tidymodels: yardstick\nRegression metrics: multi-class targets\n\n&gt; # multi-class estimates for probability metrics\n&gt; hpc_cv %&gt;% yardstick::roc_auc(obs, VF, F, M, L)\n&gt; # multi-class estimates with estimator (one of \"hand_till\", \"macro\", or \"macro_weighted\")\n&gt; hpc_cv %&gt;% yardstick::roc_auc(\n+   obs, VF, F, M, L, estimator = \"macro_weighted\"\n+ )\n&gt; \n&gt; # show metrics by groups (re-sampling in this case)\n&gt; hpc_cv %&gt;% \n+   dplyr::group_by(Resample) %&gt;% \n+   yardstick::accuracy(obs, pred)\n&gt; \n&gt; hpc_cv %&gt;% \n+   dplyr::group_by(Resample) %&gt;% \n+   yardstick::roc_curve(obs, VF, F, M, L) %&gt;% \n+   autoplot()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: performance evaluation",
    "text": "Tidymodels: performance evaluation\nRe-substitution: comparison using same training data\n\n&gt; # create  random forest model object\n&gt; rf_model &lt;- \n+   parsnip::rand_forest(trees = 1000) %&gt;% \n+   parsnip::set_engine(\"ranger\") %&gt;% \n+   parsnip::set_mode(\"regression\")\n&gt; \n&gt; # create a workflow using the random forest model\n&gt; rf_wflow &lt;- \n+   workflows::workflow() %&gt;% \n+   workflows::add_formula(\n+     Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n+       Latitude + Longitude) %&gt;% \n+   workflows::add_model(rf_model) \n&gt; \n&gt; # fit the random forest model with the ames training set\n&gt; rf_fit &lt;- rf_wflow %&gt;% parsnip::fit(data = ames_train)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: performance evaluation",
    "text": "Tidymodels: performance evaluation\nA function to compare models\n\n&gt; estimate_perf &lt;- function(model, dat) {\n+   # Capture the names of the `model` and `dat` objects\n+   cl &lt;- match.call()\n+   obj_name &lt;- as.character(cl$model)         # get the model name\n+   data_name &lt;- as.character(cl$dat)          # get the dataset name\n+   data_name &lt;- gsub(\"ames_\", \"\", data_name)  # replace underlines\n+   \n+   # Estimate these metrics:\n+   reg_metrics &lt;- \n+     yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n+   \n+   model %&gt;%\n+     stats::predict(dat) %&gt;%                  # predict\n+     dplyr::bind_cols(dat %&gt;% dplyr::select(Sale_Price)) %&gt;% \n+     reg_metrics(Sale_Price, .pred) %&gt;%\n+     dplyr::select(-.estimator) %&gt;%\n+     dplyr::mutate(object = obj_name, data = data_name)\n+ }"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-2",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-2",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: performance evaluation",
    "text": "Tidymodels: performance evaluation\nUse the function on the random forest and linear models\n\nrand forest - trainlinear -trainlinear - test\n\n\n\n&gt; # get performance of the random forest model (train)\n&gt; estimate_perf(rf_fit, ames_train)\n\n# A tibble: 2 Ã— 4\n  .metric .estimate object data \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1 rmse       0.0366 rf_fit train\n2 rsq        0.960  rf_fit train\n\n\n\n\n\n&gt; # get performance of the linear model (train)\n&gt; estimate_perf(lm_fit, ames_train)\n\n# A tibble: 2 Ã— 4\n  .metric .estimate object data \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1 rmse        0.160 lm_fit train\n2 rsq         0.168 lm_fit train\n\n\n\n\n\n&gt; # get performance of the linear model  (test)\n&gt; estimate_perf(rf_fit, ames_test)\n\n# A tibble: 2 Ã— 4\n  .metric .estimate object data \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1 rmse       0.0702 rf_fit test \n2 rsq        0.852  rf_fit test"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-3",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-performance-evaluation-3",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: performance evaluation",
    "text": "Tidymodels: performance evaluation\nSummarize and present the performance comparison\n\n\nCode\n&gt; # get performance of the random forest model (train)\n&gt; dplyr::bind_rows(\n+   estimate_perf(rf_fit, ames_train)\n+   , estimate_perf(lm_fit, ames_train)\n+   , estimate_perf(rf_fit, ames_test)\n+   , estimate_perf(lm_fit, ames_test)\n+ ) %&gt;% \n+   dplyr::filter(.metric == 'rmse') %&gt;% \n+   dplyr::select(-.metric) %&gt;% \n+   tidyr::pivot_wider(\n+     names_from = data\n+     , values_from = .estimate\n+   ) %&gt;% \n+   gt::gt() %&gt;% \n+   gt::fmt_number(decimals=4) %&gt;% \n+   gt::tab_header(title = \"Performance statistics\", subtitle = \"metric: rmse\") %&gt;% \n+   gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nPerformance statistics\n\n\nmetric: rmse\n\n\nobject\ntrain\ntest\n\n\n\n\nrf_fit\n0.0366\n0.0702\n\n\nlm_fit\n0.1603\n0.1638"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling",
    "href": "slides/BSMM_8740_lec_04.html#sampling",
    "title": "The Tidymodels Framework",
    "section": "Sampling",
    "text": "Sampling\nThe primary approach for empirical model validation is to split the existing pool of data into two distinct sets, the training set and the test set.\nThe training set is used to develop and optimize the model and is usually the majority of the data.\nThe test set is the remainder of the data, held in reserve to determine the efficacy of the model. It is critical to look at the test set only once; otherwise, it becomes part of the modeling process."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling-1",
    "href": "slides/BSMM_8740_lec_04.html#sampling-1",
    "title": "The Tidymodels Framework",
    "section": "Sampling",
    "text": "Sampling\nThe rsample package has tools for making data splits, as follows\n\n&gt; set.seed(501)\n&gt; \n&gt; # Save the split information for an 80/20 split of the data\n&gt; ames_split &lt;- rsample::initial_split(ames, prop = 0.80)\n&gt; ames_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2344/586/2930&gt;\n\n\nThe functions training and testing extract the corresponding data.\n\n&gt; ames_train &lt;- rsample::training(ames_split)\n&gt; ames_test  &lt;- rsample::testing(ames_split)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling-class-imbalance",
    "href": "slides/BSMM_8740_lec_04.html#sampling-class-imbalance",
    "title": "The Tidymodels Framework",
    "section": "Sampling: class imbalance",
    "text": "Sampling: class imbalance\nSimple random sampling is appropriate in many cases but there are exceptions. When there is a dramatic class imbalance in classification problems, one class occurs much less frequently than another. To avoid this, stratified sampling can be used. For regession problems the outcome data can be binned and then stratified sampling will keep the distributions of the outcome similar between the training and test set.\n\n&gt; set.seed(502)\n&gt; ames_split &lt;- \n+   rsample::initial_split(ames, prop = 0.80, strata = Sale_Price)\n&gt; ames_train &lt;- rsample::training(ames_split)\n&gt; ames_test  &lt;- rsample::testing(ames_split)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling-time-variables",
    "href": "slides/BSMM_8740_lec_04.html#sampling-time-variables",
    "title": "The Tidymodels Framework",
    "section": "Sampling: time variables",
    "text": "Sampling: time variables\nWith time variables, random sampling is not the best choice. The rsample package contains a function called initial_time_split() that is very similar to initial_split() .\nThe prop argument denotes what proportion of the first part of the data should be used as the training set; the function assumes that the data have been pre-sorted by time in an appropriate order."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#sampling-validation-sets",
    "href": "slides/BSMM_8740_lec_04.html#sampling-validation-sets",
    "title": "The Tidymodels Framework",
    "section": "Sampling: validation sets",
    "text": "Sampling: validation sets\nValidation sets are the answer to the question: â€œHow can we tell what is best if we donâ€™t measure performance until the test set?â€ The validation set is a means to get a rough sense of how well the model performed prior to the test set.\nValidation sets are a special case of resampling methods.\n\n&gt; set.seed(52)\n&gt; # To put 60% into training, 20% in validation, and 20% in testing:\n&gt; ames_val_split &lt;- \n+   rsample::initial_validation_split(ames, prop = c(0.6, 0.2))\n&gt; ames_val_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;1758/586/586/2930&gt;\n\n\nThe functions validation extracts the corresponding data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling",
    "href": "slides/BSMM_8740_lec_04.html#resampling",
    "title": "The Tidymodels Framework",
    "section": "Resampling",
    "text": "Resampling\nWhile the test set is used for obtaining an unbiased estimate of performance, we usually need to understand the performance of a model or even multiple models before using the test set."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-1",
    "href": "slides/BSMM_8740_lec_04.html#resampling-1",
    "title": "The Tidymodels Framework",
    "section": "Resampling",
    "text": "Resampling\nResampling is conducted only on the training set; the test set is not involved."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-cross-validation",
    "href": "slides/BSMM_8740_lec_04.html#resampling-cross-validation",
    "title": "The Tidymodels Framework",
    "section": "Resampling: cross-validation",
    "text": "Resampling: cross-validation\nWhile there are a number of variations, the most common cross-validation method is V-fold cross-validation. The data are randomly partitioned into V sets of roughly equal size (called the folds).\nIn the example of 3-fold cross validation. In each of 3 iterations, one fold is held out for assessment statistics and the remaining folds are used for modeling.\nThe final resampling estimate of performance averages each of the V replicates.\nIn practice, values of V are most often 5 or 10."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-cv-example",
    "href": "slides/BSMM_8740_lec_04.html#resampling-cv-example",
    "title": "The Tidymodels Framework",
    "section": "Resampling: cv example",
    "text": "Resampling: cv example\n\n&gt; set.seed(1001)\n&gt; ames_folds &lt;- rsample::vfold_cv(ames_train, v = 10)\n&gt; ames_folds |&gt; dplyr::slice_head(n=5)\n\n# A tibble: 5 Ã— 2\n  splits             id    \n  &lt;list&gt;             &lt;chr&gt; \n1 &lt;split [2107/235]&gt; Fold01\n2 &lt;split [2107/235]&gt; Fold02\n3 &lt;split [2108/234]&gt; Fold03\n4 &lt;split [2108/234]&gt; Fold04\n5 &lt;split [2108/234]&gt; Fold05\n\n\nThe rsample::analysis() and rsample::assessment() functions return the corresponding data frames:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-repeated-cv",
    "href": "slides/BSMM_8740_lec_04.html#resampling-repeated-cv",
    "title": "The Tidymodels Framework",
    "section": "Resampling: repeated cv",
    "text": "Resampling: repeated cv\n\nThe most important variation on cross-validation is repeated V-fold cross-validation. Repeated V-fold cv, reduce noise in the estimate by using more data, i.e.Â averaging more V statistics.\nR repetitions of V-fold cross-validation reduces the standard error variance by a factor of \\(1/\\sqrt{\\text{R}}\\).\n\n&gt; rsample::vfold_cv(ames_train, v = 10, repeats = 5) \n\n\n\n#  10-fold cross-validation repeated 5 times \n# A tibble: 50 Ã— 3\n   splits             id      id2   \n   &lt;list&gt;             &lt;chr&gt;   &lt;chr&gt; \n 1 &lt;split [2107/235]&gt; Repeat1 Fold01\n 2 &lt;split [2107/235]&gt; Repeat1 Fold02\n 3 &lt;split [2108/234]&gt; Repeat1 Fold03\n 4 &lt;split [2108/234]&gt; Repeat1 Fold04\n 5 &lt;split [2108/234]&gt; Repeat1 Fold05\n 6 &lt;split [2108/234]&gt; Repeat1 Fold06\n 7 &lt;split [2108/234]&gt; Repeat1 Fold07\n 8 &lt;split [2108/234]&gt; Repeat1 Fold08\n 9 &lt;split [2108/234]&gt; Repeat1 Fold09\n10 &lt;split [2108/234]&gt; Repeat1 Fold10\n# â„¹ 40 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-loo-cv",
    "href": "slides/BSMM_8740_lec_04.html#resampling-loo-cv",
    "title": "The Tidymodels Framework",
    "section": "Resampling: LOO cv",
    "text": "Resampling: LOO cv\nOne variation of cross-validation is leave-one-out (LOO) cross-validation. If there are n training set samples, n models are fit using nâˆ’1 rows of the training set.\nEach model predicts the single excluded data point. At the end of resampling, the n predictions are pooled to produce a single performance statistic. LOO cv is created using rsample::loo_cv().\nLeave-one-out methods are deficient compared to almost any other method. For anything but pathologically small samples, LOO is computationally excessive."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-mc-cv",
    "href": "slides/BSMM_8740_lec_04.html#resampling-mc-cv",
    "title": "The Tidymodels Framework",
    "section": "Resampling: MC cv",
    "text": "Resampling: MC cv\nAnother variant of V-fold cross-validation is Monte Carlo cross-validation (MCCV).\nThe difference between MCCV and regular cross-validation is that, for MCCV, the input proportion of the data is randomly selected each time.\nMCCV is performed using rsample::mv_cv()."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-boostrapping",
    "href": "slides/BSMM_8740_lec_04.html#resampling-boostrapping",
    "title": "The Tidymodels Framework",
    "section": "Resampling: Boostrapping",
    "text": "Resampling: Boostrapping\nRe-sampling: bootstrapping:\nA bootstrap sample of the training set is a sample that is the same size as the training set but is drawn with replacement.\nWhen bootstrapping, the assessment set is often called theÂ out-of-bagÂ sample.\n&gt; rsample::bootstraps(ames_train, times = 5)\n\n\n\n# Bootstrap sampling \n# A tibble: 5 Ã— 2\n  splits             id        \n  &lt;list&gt;             &lt;chr&gt;     \n1 &lt;split [2342/882]&gt; Bootstrap1\n2 &lt;split [2342/882]&gt; Bootstrap2\n3 &lt;split [2342/858]&gt; Bootstrap3\n4 &lt;split [2342/877]&gt; Bootstrap4\n5 &lt;split [2342/837]&gt; Bootstrap5"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-boostrapping-1",
    "href": "slides/BSMM_8740_lec_04.html#resampling-boostrapping-1",
    "title": "The Tidymodels Framework",
    "section": "Resampling: Boostrapping",
    "text": "Resampling: Boostrapping\nEach data point has a 63.2% chance of inclusion in the training set at least once.\nThe assessment set contains all of the training set samples that were not selected for the analysis set (on average, with 36.8% of the training set), and so they can vary in size.\nBootstrap samples produce performance estimates that have very low variance (unlike cross-validation) but have significant pessimistic bias. This means that, if the true accuracy of a model is 90%, the bootstrap would tend to estimate the value to be less than 90%."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\nRe-sampling: resampling time:\nWhen the data have a strong time component, a resampling method needs to support modeling to estimate seasonal and other temporal trends within the data.Â \nFor this type of resampling, the size of the initial analysis and assessment sets are specified and subsequent iterations are shifted in time"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables-1",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables-1",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\nRolling forecast orgin resampling:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables-2",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables-2",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\nTwo different configurations of rolling forecast orgin resampling:\n\nThe analysis set can cumulatively grow (as opposed to remaining the same size). After the first initial analysis set, new samples can accrue without discarding the earlier data.\nThe resamples need not increment by one. For example, for large data sets, the incremental block could be a week or month instead of a day."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables-3",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables-3",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\nFor a yearâ€™s worth of data, suppose that six sets of 30-day blocks define the analysis set. For assessment sets of 30 days with a 29-day skip, we can use theÂ rsampleÂ package to specify:\n\n&gt; time_slices &lt;- \n+   tibble::tibble(x = 1:365) %&gt;% \n+   rsample::rolling_origin(\n+     initial = 6 * 30\n+     , assess = 30\n+     , skip = 29\n+     , cumulative = FALSE\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#resampling-time-variables-4",
    "href": "slides/BSMM_8740_lec_04.html#resampling-time-variables-4",
    "title": "The Tidymodels Framework",
    "section": "Resampling: time variables",
    "text": "Resampling: time variables\n\nanalysisassessment\n\n\n\n&gt; # pull out first and last data points in the analysis dataset\n&gt; time_slices$splits %&gt;% \n+   purrr::map_dfr( \n+     .f = ~rsample::analysis(.x) %&gt;% \n+       dplyr::summarize(first = min(.), last = max(.))\n+   )\n\n# A tibble: 6 Ã— 2\n  first  last\n  &lt;int&gt; &lt;int&gt;\n1     1   180\n2    31   210\n3    61   240\n4    91   270\n5   121   300\n6   151   330\n\n\n\n\n\n&gt; # pull out first and last data points in the assessment dataset\n&gt; time_slices$splits %&gt;% \n+   purrr::map_dfr(\n+     .f = ~rsample::assessment(.x) %&gt;% \n+       dplyr::summarize(first = min(.), last = max(.))\n+   )\n\n# A tibble: 6 Ã— 2\n  first  last\n  &lt;int&gt; &lt;int&gt;\n1   181   210\n2   211   240\n3   241   270\n4   271   300\n5   301   330\n6   331   360"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-7",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-7",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\n\n\nDuring resampling, the analysis set is used to preprocess the data, apply the pre-processing to itself, and use these processed data to fit the model.\nThe pre-processing statistics produced by the analysis set are applied to the assessment set. The predictions from the assessment set estimate performance on new data.\n\nThis sequence repeats for every resample."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-8",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-8",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\n\nAny of the resampling methods discussed here can be used to evaluate the modeling process (including preprocessing, model fitting, etc). These methods are effective because different groups of data are used to train the model and assess the model. To reiterate, the process to use resampling is:\n\nDuring resampling, the analysis set is used to preprocess the data, apply the preprocessing to itself, and use these processed data to fit the model.\nThe preprocessing statistics produced by the analysis set are applied to the assessment set. The predictions from the assessment set estimate performance on new data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-9",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-9",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nThe function tune::fit_resamples is like parsnip::fit with a resamples argument instead of a data argument:\n\n&gt; #\n&gt; model_spec %&gt;% tune::fit_resamples(formula,  resamples, ...)\n&gt; \n&gt; model_spec %&gt;% tune::fit_resamples(recipe,   resamples, ...)\n&gt; \n&gt; workflow   %&gt;% tune::fit_resamples(          resamples, ...)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-10",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-10",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nOptional arguments are:\n\nmetrics: A metric set of performance statistics to compute. By default, regression models use RMSE and R2 while classification models compute the area under the ROC curve and overall accuracy.\ncontrol: A list created by tune::control_resamples() with various options."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-11",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-11",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nControl arguments are:\n\nverbose: A logical for printing logging.\nextract: A function for retaining objects from each model iteration (discussed later).\nsave_pred: A logical for saving the assessment set predictions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-12",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-12",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nSave the predictions in order to visualize the model fit and residuals:\n\nCode\n&gt; keep_pred &lt;- \n+   tune::control_resamples(save_pred = TRUE, save_workflow = TRUE)\n&gt; \n&gt; set.seed(1003)\n&gt; rf_res &lt;- \n+   rf_wflow %&gt;% \n+   tune::fit_resamples(resamples = ames_folds, control = keep_pred)\n&gt; rf_res\n\n\n\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 Ã— 5\n   splits             id     .metrics         .notes           .predictions      \n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n 1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [235 Ã— 4]&gt;\n 2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [235 Ã— 4]&gt;\n 3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [234 Ã— 4]&gt;\n 4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [234 Ã— 4]&gt;\n 5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [234 Ã— 4]&gt;\n 6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [234 Ã— 4]&gt;\n 7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [234 Ã— 4]&gt;\n 8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [234 Ã— 4]&gt;\n 9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [234 Ã— 4]&gt;\n10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [234 Ã— 4]&gt;"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-13",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-13",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nThe return value is a tibble similar to the input resamples, along with some extra columns:\n\n.metrics is a list column of tibbles containing the assessment set performance statistics.\n.notes is list column of tibbles cataloging any warnings/errors generated during resampling.\n.predictions is present when save_pred = TRUE. This list column contains tibbles with the out-of-sample predictions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-14",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-14",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels",
    "text": "Tidymodels\nPerformance evaluation:\nWhile the list columns may look daunting, they can be easily reconfigured using tidyr or with convenience functions that tidymodels provides. For example, to return the performance metrics in a more usable format:\n\n&gt; rf_res %&gt;% tune::collect_metrics()\n&gt; \n&gt; rf_res %&gt;% tune::collect_predictions()\n&gt; \n&gt; val_res %&gt;% tune::collect_metrics()\n\nWhat is collected are the resampling estimates averaged over the individual replicates. To get the metrics for each resample, use the option summarize = FALSE."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-parallelism",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-parallelism",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: parallelism",
    "text": "Tidymodels: parallelism\nThe models created during resampling are independent of one another and can be processed in parallel across processors on the same computer.\nThe code below determines the number of of possible worker processes.\n\n&gt; # The number of physical cores in the hardware:\n&gt; parallel::detectCores(logical = FALSE)\n&gt; \n&gt; # The number of possible independent processes that can \n&gt; # be simultaneously used:  \n&gt; parallel::detectCores(logical = TRUE)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#tidymodels-parallelism-1",
    "href": "slides/BSMM_8740_lec_04.html#tidymodels-parallelism-1",
    "title": "The Tidymodels Framework",
    "section": "Tidymodels: parallelism",
    "text": "Tidymodels: parallelism\nFor fit_resamples() and other functions in tune, parallel processing occurs when the user registers a parallel backend package.\nThese R backend packages define how to execute parallel processing."
  },
  {
    "objectID": "slides/BSMM_8740_lec_04.html#recap",
    "href": "slides/BSMM_8740_lec_04.html#recap",
    "title": "The Tidymodels Framework",
    "section": "Recap",
    "text": "Recap\n\nIn this section we have worked with the tidymodels package to build a workflow that facilitates building and evaluating multiple models.\nCombined with the recipes package we now have a complete data modeling framework.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSMM-8740: Data Analytic Methods & Algorithms",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the Fall 2024 semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\n\nLab\nLab Solution\nExam\nProject\n\n\n\n\n1\nWed, Sep 11\nTidyverse, EDA & Git\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 1\n\n\n\nğŸ’»\nâœ…\n\n\n\n\n2\nWed, Sep 18\nThe Recipes Package\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 2\n\n\n\nğŸ’»\nâœ…\n\n\n\n\n3\nWed, Sep 25\nRegression Methods\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 3\n\n\n\nğŸ’»\nâœ…\n\n\n\n\n4\nWed, Oct 02\nThe TidyModels Framework\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 4\n\n\n\nğŸ’»\nâœ…\n\n\n\n\n5\nWed, Oct 09\nClassification & Clustering Methods\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 5\n\n\n\nğŸ’»\nâœ…\nâ›”\n\n\n\n6\nWed, Oct 23\nTime Series Methods\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 6\n\n\n\nâ›”\nâ›”\n\n\n\n\n7\nWed, Oct 30\nCausality: DAGs\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 7\n\n\n\nâ›”\nâ›”\n\n\n\n\n8\nWed, Nov 06\nCausality: Methods\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 8\n\n\n\nâ›”\nâ›”\nâ›”\n\n\n\n9\nWed, Nov 13\nMonte Carlo Methods\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 9\n\n\n\nâ›”\n\n\n\n\n\n10\nWed, Nov 20\nBayesian methods [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 10\n\n\n\nâ›”\n\nâ›”\n\n\n\n11\nWed, Nov 27\nAdvanced Topics [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 11\n\n\n\nâ›”\n\n\n\n\n\n12\nWed, Dec 04\nFinal Exam [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 12\n\n\n\nâ›”",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, youâ€™ll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance youâ€™ll use for the course."
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#data-sources",
    "href": "project-tips-resources.html#data-sources",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, youâ€™re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and thatâ€™s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called â€œReferencesâ€ at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called â€œAppendixâ€.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf youâ€™re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\nâŒ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nâœ… YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %&gt;%\n  count(manufacturer) %&gt;%\n  mutate(manufacturer = str_to_title(manufacturer)) %&gt;%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDonâ€™t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\nâŒ There is a negative linear relationship between mpg and hp.\nâœ… There is a negative linear relationship between a carâ€™s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Donâ€™t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the â€œso whatâ€: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e.Â what you want the audience to know about your topic after reading your report or viewing your presentation.\n\nâŒ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\nâœ… If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if itâ€™s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html",
    "href": "ae/ae-10-flight-delays.html",
    "title": "AE 10: Flight delays",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-10-flight-delays-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#packages",
    "href": "ae/ae-10-flight-delays.html#packages",
    "title": "AE 10: Flight delays",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#data",
    "href": "ae/ae-10-flight-delays.html#data",
    "title": "AE 10: Flight delays",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of 25,000 randomly sampled flights that departed one of three NYC airports (JFK, LGA, EWR) in 2013.\n\nflight_data &lt;- read_csv(\"data/flight-data.csv\")\n\nRows: 25000 Columns: 10\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr  (4): origin, dest, carrier, arr_delay\ndbl  (4): dep_time, flight, air_time, distance\ndttm (1): time_hour\ndate (1): date\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nConvert arr_delay to factor with levels \"late\" (first level) and \"on_time\" (second level). This variable is our outcome and it indicates whether the flightâ€™s arrival was more than 30 minutes.\n\n\nflight_data &lt;- flight_data %&gt;%\n  mutate(arr_delay = as.factor(arr_delay))\n\nlevels(flight_data$arr_delay)\n\n[1] \"late\"    \"on_time\"\n\n\n\nLetâ€™s get started with some data prep: Convert all variables that are character strings to factors.\n\n\n#flight_data &lt;- flight_data %&gt;%\n#  mutate(\n#    origin = as.factor(origin),\n#    carrier = as.factor(carrier),\n#    dest = as.factor(dest)\n#    )\n\nflight_data &lt;- flight_data %&gt;%\n  #go across all columns and convert that are characters to factors\n  #go across all columns and convert if is.character = TRUE to factors\n  #go across all columns and if is.character apply as.factor\n  mutate(across(where(is.character), as.factor))"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#modeling-prep",
    "href": "ae/ae-10-flight-delays.html#modeling-prep",
    "title": "AE 10: Flight delays",
    "section": "Modeling prep",
    "text": "Modeling prep\n\nSplit the data into testing (75%) and training (25%), and save each subset.\n\n\nset.seed(222)\n\nflight_split &lt;- initial_split(flight_data)\n\nflight_train &lt;- training(flight_split)\nflight_test &lt;- testing(flight_split)\n\n\nSpecify a logistic regression model that uses the \"glm\" engine.\n\n\nflight_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nNext, weâ€™ll create two recipes and workflows and compare them to each other."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "href": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "title": "AE 10: Flight delays",
    "section": "Model 1: Everything and the kitchen sink",
    "text": "Model 1: Everything and the kitchen sink\n\nDefine a recipe that predicts arr_delay using all variables except for flight and time_hour, which, in combination, can be used to identify a flight. Also make sure this recipe handles dummy coding as well as issues that can arise due to having categorical variables with some levels apparent in the training set but not in the testing set. Call this recipe flights_rec1.\n\n\nflights_rec1 &lt;- recipe(arr_delay ~ ., data = flight_train) %&gt;%\n  update_role(flight, time_hour, new_role = \"id\") %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nCreate a workflow that uses flights_rec1 and the model you specified.\n\n\nflight_wflow1 &lt;- workflow() %&gt;%\n  add_recipe(flights_rec1) %&gt;%\n  add_model(flight_spec)\n\n\nFit the this model to the training data using your workflow and display a tidy summary of the model fit.\n\n\nflight_fit1 &lt;- flight_wflow1 %&gt;%\n  fit(data = flight_train)\n\ntidy(flight_fit1)\n\n# A tibble: 119 Ã— 5\n   term          estimate   std.error statistic   p.value\n   &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  13.3      287.          0.0464  9.63e-  1\n 2 dep_time     -0.00164    0.0000504 -32.6     1.04e-233\n 3 air_time     -0.0349     0.00179   -19.5     1.75e- 84\n 4 distance      0.00533    0.00523     1.02    3.08e-  1\n 5 date          0.000227   0.000198    1.15    2.51e-  1\n 6 origin_JFK    0.0830     0.102       0.815   4.15e-  1\n 7 origin_LGA   -0.0360     0.0983     -0.366   7.14e-  1\n 8 dest_ACK    -12.4      287.         -0.0434  9.65e-  1\n 9 dest_ALB    -12.4      287.         -0.0433  9.65e-  1\n10 dest_ANC     -3.75     928.         -0.00404 9.97e-  1\n# â€¦ with 109 more rows\n\n\n\nPredict arr_delay for the testing data using this model.\n\n\nflight_aug1 &lt;- augment(flight_fit1, flight_test)\n\n\nPlot the ROC curve and find the area under the curve. Comment on how well you think this model has done for predicting arrival delay.\n\n\nflight_aug1 %&gt;%\n  roc_curve(\n    truth = arr_delay,\n    .pred_late\n  ) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nflight_aug1 %&gt;%\n  roc_auc(\n    truth = arr_delay,\n    .pred_late\n  )\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.734"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "href": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "title": "AE 10: Flight delays",
    "section": "Model 2: Letâ€™s be a bit more thoughtful",
    "text": "Model 2: Letâ€™s be a bit more thoughtful\n\nDefine a new recipe, flights_rec2, that, in addition to what was done in flights_rec1, adds features for day of week and month based on date and also adds indicators for all US holidays (also based on date). A list of these holidays can be found in timeDate::listHolidays(\"US\"). Once these features are added, date should be removed from the data. Then, create a new workflow, fit the same model (logistic regression) to the training data, and do predictions on the testing data. Finally, draw another ROC curve and find the area under the curve. Compare the predictive performance of this new model to the previous one. Based on the area under the curve statistic, which model does better?"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "href": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "title": "AE 10: Flight delays",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\nCreate an ROC curve that plots both models, in different colors, and adds a legend indicating which model is which."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#acknowledgement",
    "href": "ae/ae-10-flight-delays.html#acknowledgement",
    "title": "AE 10: Flight delays",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://www.tidymodels.org/start/recipes/."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html",
    "href": "ae/ae-3-duke-forest.html",
    "title": "AE 3: Duke Forest houses",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-3-duke-forest-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#packages",
    "href": "ae/ae-3-duke-forest.html#packages",
    "title": "AE 3: Duke Forest houses",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "href": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "title": "AE 3: Duke Forest houses",
    "section": "Predict sale price from area",
    "text": "Predict sale price from area\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#model-conditions",
    "href": "ae/ae-3-duke-forest.html#model-conditions",
    "title": "AE 3: Duke Forest houses",
    "section": "Model conditions",
    "text": "Model conditions\n\nExercise 1\nThe following code produces the residuals vs.Â fitted values plot for this model. Comment out the layer that defines the y-axis limits and re-create the plot. How does the plot change? Why might we want to define the limits explicitly?\n\ndf_aug &lt;- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n\n\n\n\n\n\n\n\n\n\nExercise 2\nImprove how the values on the axes of the plot are displayed by modifying the code below.\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html",
    "href": "ae/ae-7-exam-2-review.html",
    "title": "AE 7: Exam 2 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-7-exam-2-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#packages",
    "href": "ae/ae-7-exam-2-review.html#packages",
    "title": "AE 7: Exam 2 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\n# fix data!\nloans_full_schema &lt;- droplevels(loans_full_schema)"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#goal",
    "href": "ae/ae-7-exam-2-review.html#goal",
    "title": "AE 7: Exam 2 Review",
    "section": "Goal",
    "text": "Goal\nCreate a model for precicting interest_rate."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#view-data",
    "href": "ae/ae-7-exam-2-review.html#view-data",
    "title": "AE 7: Exam 2 Review",
    "section": "View data",
    "text": "View data\nNote the dimensions of the data and the variable names. Review the data dictionary.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "href": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "title": "AE 7: Exam 2 Review",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#write-the-model",
    "href": "ae/ae-7-exam-2-review.html#write-the-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Write the model",
    "text": "Write the model\nWrite the model for predicting interest rate (interest_rate) from debt to income ratio (debt_to_income), the term of loan (term), the number of inquiries (credit checks) into the applicantâ€™s credit during the last 12 months (inquiries_last_12m), whether there are any bankruptcies listed in the public record for this applicant (bankrupt), and the type of application (application_type). The model should allow for the effect of to income ratio on interest rate to vary by application type.\nAdd model here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#exploration",
    "href": "ae/ae-7-exam-2-review.html#exploration",
    "title": "AE 7: Exam 2 Review",
    "section": "Exploration",
    "text": "Exploration\nExplore characteristics of the variables youâ€™ll use for the model using the training data only.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#specify-model",
    "href": "ae/ae-7-exam-2-review.html#specify-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-recipe",
    "href": "ae/ae-7-exam-2-review.html#create-recipe",
    "title": "AE 7: Exam 2 Review",
    "section": "Create recipe",
    "text": "Create recipe\n\nPredict interest_rate from debt_to_income, term, inquiries_last_12m, public_record_bankrupt, and application_type.\nMean center debt_to_income.\nMake term a factor.\nCreate a new variable: bankrupt that takes on the value â€œnoâ€ if public_record_bankrupt is 0 and the value â€œyesâ€ if public_record_bankrupt is 1 or higher. Then, remove public_record_bankrupt.\nInteract application_type with debt_to_income.\nCreate dummy variables where needed and drop any zero variance variables.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-workflow",
    "href": "ae/ae-7-exam-2-review.html#create-workflow",
    "title": "AE 7: Exam 2 Review",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#cross-validation",
    "href": "ae/ae-7-exam-2-review.html#cross-validation",
    "title": "AE 7: Exam 2 Review",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "href": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "title": "AE 7: Exam 2 Review",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here\n\nWhy are we focusing on R-squared and RMSE instead of adjusted R-squared, AIC, BIC?\n[Add response here]"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#next-steps",
    "href": "ae/ae-7-exam-2-review.html#next-steps",
    "title": "AE 7: Exam 2 Review",
    "section": "Next stepsâ€¦",
    "text": "Next stepsâ€¦\nDepending on time, either\n\nCreate a workflow for another model with a new recipe (omitting the interaction variable), conduct CV, do model selection between these two, and then interpret the coefficients for the selected model.\nOr interpret the coefficients for the one model you fit.\n\nMake sure to interpret the intercept and slope coefficient for at least one numerical, one categorical, and one interaction predictor."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html",
    "href": "ae/ae-12-exam-3-review.html",
    "title": "AE 12: Exam 3 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-12-exam-3-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#packages",
    "href": "ae/ae-12-exam-3-review.html#packages",
    "title": "AE 12: Exam 3 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data)\nlibrary(rms)\nlibrary(nnet)"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#data",
    "href": "ae/ae-12-exam-3-review.html#data",
    "title": "AE 12: Exam 3 Review",
    "section": "Data",
    "text": "Data\nAs part of a study of the effects of predatory intertidal crab species on snail populations, researchers measured the mean closing forces and the propodus heights of the claws on several crabs of three species.\n\n\nclaws &lt;- read_csv(here::here(\"ae\", \"data/claws.csv\"))\n\nWe will use the following variables:\n\nforce: Closing force of claw (newtons)\nheight: Propodus height (mm)\nspecies: Crab species - Cp(Cancer productus), Hn (Hemigrapsus nudus), Lb(Lophopanopeus bellus)\nlb: 1 if Lophopanopeus bellus species, 0 otherwise\nhn: 1 if Hemigrapsus nudus species, 0 otherwise\ncp: 1 if Cancer productus species, 0 otherwise\nforce_cent: mean centered force\nheight_cent: mean centered height\n\nBefore we get started, letâ€™s make the categorical and indicator variables factors.\n\nclaws &lt;- claws %&gt;%\n  mutate(\n    species = as_factor(species),\n    lb = as_factor(lb),\n    hn = as_factor(hn),\n    cp = as_factor(cp)\n  )"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "href": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "title": "AE 12: Exam 3 Review",
    "section": "Probabilities vs.Â odds vs.Â log-odds",
    "text": "Probabilities vs.Â odds vs.Â log-odds\nWhy we use log-odds as response variable: https://sta210-s22.github.io/website/slides/lec-18.html#/do-teenagers-get-7-hours-of-sleep"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-1",
    "href": "ae/ae-12-exam-3-review.html#exercise-1",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 1",
    "text": "Exercise 1\nFill in the blanks:\n\nUse log-odds to fit the model (outcome)\nUse odds to interpret model results\nUse probabilities to make predictions for individual observations and ultimately to make classification decisions"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-2",
    "href": "ae/ae-12-exam-3-review.html#exercise-2",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 2",
    "text": "Exercise 2\nSuppose we want to use force to determine whether or not a crab is from the Lophopanopeus bellus (Lb) species. Why should we use a logistic regression model for this analysis?\n\nclaws %&gt;%\n  distinct(lb)\n\n# A tibble: 2 Ã— 1\n  lb   \n  &lt;fct&gt;\n1 0    \n2 1"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-3",
    "href": "ae/ae-12-exam-3-review.html#exercise-3",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe will use the mean-centered variables for force in the model. The model output is below. Write the equation of the model produced by R. Donâ€™t forget to fill in the blanks for â€¦.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.798\n0.358\n-2.233\n0.026\n-1.542\n-0.123\n\n\nforce_cent\n0.043\n0.039\n1.090\n0.276\n-0.034\n0.123\n\n\n\n\n\nLet Ï€\\pi be probability that a crab is from Lb species.\nlog(Ï€Ì‚1âˆ’Ï€Ì‚)=âˆ’0.798+0.043*force_cent\n\\log\\Big(\\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\Big) = -0.798 + 0.043 * force\\_cent"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-4",
    "href": "ae/ae-12-exam-3-review.html#exercise-4",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 4",
    "text": "Exercise 4\nInterpret the intercept in the context of the data.\n\nmean_force &lt;- round(mean(claws$force), 2)\n\nFor crabs with average closing force (12.13 newtons), we expect odds of the crab being Lophopanopeus bellus is 0.45 (exp(-0.798))."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-5",
    "href": "ae/ae-12-exam-3-review.html#exercise-5",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the effect of force in the context of the data.\nWhen x goes up by 1 unit, we expect y to change by (slope) units.\nFor each additional unit increase in closing force, the odds of crab being from lb species multiplies on average by a factor of 1.0439379."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-6",
    "href": "ae/ae-12-exam-3-review.html#exercise-6",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 6",
    "text": "Exercise 6\nNow letâ€™s consider adding height_cent to the model. Fit the model that includes height_cent. Then use AIC to choose the model that best fits the data.\n\nlb_fit_2 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(lb ~ force_cent + height_cent, data = claws)\n\ntidy(lb_fit_2, conf.int = TRUE)\n\n# A tibble: 3 Ã— 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -1.13     0.463      -2.44  0.0146  -2.17      -0.306\n2 force_cent     0.211    0.0925      2.28  0.0227   0.0563     0.424\n3 height_cent   -0.895    0.398      -2.25  0.0245  -1.82      -0.234\n\nglance(lb_fit_1)$AIC\n\n[1] 50.19535\n\nglance(lb_fit_2)$AIC\n\n[1] 44.11812"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-7",
    "href": "ae/ae-12-exam-3-review.html#exercise-7",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 7",
    "text": "Exercise 7\nWhat do the following mean in the context of this data. Explain and calculate them.\n\nSensitivity: P(predict lb | actual lb) = 6 / 12\nSpecificity: P(predict not lb | actual not lb) = 4/ 26\nNegative predictive power: P(actual not lb | predict not lb) = 22 / 28\nPositive predictive power: P(actual lb | predict lb) = 6 / 10\n\n\n\n\nActual\nPredict lb\nPredict not lb\nTOTAL predicted\n\n\n\n\nLb\n6\n6\n12\n\n\nNot lb\n4\n22\n26\n\n\nTOTAL actual\n10\n28\n38"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-8",
    "href": "ae/ae-12-exam-3-review.html#exercise-8",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 8",
    "text": "Exercise 8\nWrite the equation of the model.\nlog(Ï€Ì‚HnÏ€Ì‚Cp)=\\log\\Big(\\frac{\\hat{\\pi}_{Hn}}{\\hat{\\pi}_{Cp}}\\Big) = \nlog(Ï€Ì‚LbÏ€Ì‚Cp)=\\log\\Big(\\frac{\\hat{\\pi}_{Lb}}{\\hat{\\pi}_{Cp}}\\Big) ="
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-9",
    "href": "ae/ae-12-exam-3-review.html#exercise-9",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nInterpret the intercept for the odds a crab is Hn vs.Â Cp species.\nInterpret the effect of force on the odds a crab is Lb vs.Â Cp species."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-10",
    "href": "ae/ae-12-exam-3-review.html#exercise-10",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the effect of force on the odds a crab is in the Hn vs.Â Lb species.\nCAUTION: We can write an interpretation based on the estimated coefficients; however, we canâ€™t make any inferential conclusions for this question based on the current model. We would need to refit the model with Lb as the baseline category to do so."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-11",
    "href": "ae/ae-12-exam-3-review.html#exercise-11",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 11",
    "text": "Exercise 11\nConditions for multinomial logistic (and logistic models as well):\n\nIndependence:\nRandomness:\nLinearity:\n\nemplogitplot1(lb ~ force, data = claws, ngroups = 10)\nemplogitplot1(lb ~ height, data = claws, ngroups = 10)\n\n\n\n\n\n\n\n\n\n\n# add code here for other species here\n\n\n\n# add code here for other species here"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "href": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "title": "AE 12: Exam 3 Review",
    "section": "Checking for multicollinearity in logistic and multinomial logistic",
    "text": "Checking for multicollinearity in logistic and multinomial logistic\nSimilar to multiple linear regression, we can also check for multicollinearity in logistic and multinomial logistic models.\n\nUse the vif function to check for multicollinearity in logistic regression.\n\n\nThe vif function doesnâ€™t work for the multinomial logistic regression models, so we can look at a correlation matrix of the predictors as a way to assess if the predictors are highly correlated:"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html",
    "href": "ae/ae-8-rail-trail.html",
    "title": "AE 8: Rail Trail",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-8-rail-trail-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#packages-and-data",
    "href": "ae/ae-8-rail-trail.html#packages-and-data",
    "title": "AE 8: Rail Trail",
    "section": "Packages and data",
    "text": "Packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrail_trail &lt;- read_csv(\"data/rail_trail.csv\")"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-1",
    "href": "ae/ae-8-rail-trail.html#exercise-1",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model predicting volume from hightemp and season.\n\nrt_mlr_main_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit)\n\n# A tibble: 4 Ã— 5\n  term         estimate std.error statistic       p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   -125.       71.7     -1.75  0.0841       \n2 hightemp         7.54      1.17     6.43  0.00000000692\n3 seasonSpring     5.13     34.3      0.150 0.881        \n4 seasonSummer   -76.8      47.7     -1.61  0.111        \n\n\nRecreate the following visualization which displays the three regression lines we can draw based on the results of this model.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-2",
    "href": "ae/ae-8-rail-trail.html#exercise-2",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 2",
    "text": "Exercise 2\nAdd an interaction effect between hightemp and season and comment on the significance of the interaction predictors. Time permitting, visualize the interaction model as well.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-3",
    "href": "ae/ae-8-rail-trail.html#exercise-3",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a model predicting volume from all available predictors.\n\nrt_full_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ ., data = rail_trail)\n\ntidy(rt_full_fit)\n\n# A tibble: 8 Ã— 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\nRecreate the following visualization which displays a histogram of residuals and a normal density curve overlaid.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html",
    "href": "ae/ae-4-exam-1-review.html",
    "title": "AE 4: Exam 1 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-4-exam-1-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#packages",
    "href": "ae/ae-4-exam-1-review.html#packages",
    "title": "AE 4: Exam 1 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "href": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "title": "AE 4: Exam 1 Review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St.Â Olaf who worked at a local restaurant.1\nThe variables weâ€™ll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips &lt;- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "href": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "title": "AE 4: Exam 1 Review",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\nVisualize, summarize, and describe the relationship between Party and Tip.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#modeling",
    "href": "ae/ae-4-exam-1-review.html#modeling",
    "title": "AE 4: Exam 1 Review",
    "section": "Modeling",
    "text": "Modeling\nLetâ€™s start by fitting a model using Party to predict the Tip at this restaurant.\n\nWrite the statistical model.\nFit the regression line and write the regression equation. Name the model tips_fit and display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret the slope.\nDoes it make sense to interpret the intercept? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#inference",
    "href": "ae/ae-4-exam-1-review.html#inference",
    "title": "AE 4: Exam 1 Review",
    "section": "Inference",
    "text": "Inference\n\nInference for the slope\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though weâ€™ll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\nset.seed(1234)\n\nboot_dist &lt;- tips %&gt;%\n  specify(Tip ~ Party) %&gt;%\n  generate(reps = 100, type = \"bootstrap\") %&gt;%\n  fit()\n\n\nUse the bootstrap distribution created in Exercise 6, boot_dist, to construct a 90% confidence interval for the slope using bootstrapping and the percentile method and interpret it in context of the data.\n\n\n# add your code here\n\n\nConduct a hypothesis test at the equivalent significance level using permutation. State the hypotheses and the significance level youâ€™re using explicitly. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercises 7 and 8. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\nNow repeat Exercises 7 and 8 using approaches based on mathematical models.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercise 9. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\n\nInference for a prediction\n\nBased on your model, predict the tip for a party of 4.\n\n\n# add your code here\n\n\nSuppose youâ€™re asked to construct a confidence and a prediction interval for your finding in Exercise 11. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals from Exercise 12 and comment on whether your guess is confirmed.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "href": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "title": "AE 4: Exam 1 Review",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nLeverage (Outliers in x direction)\n\nWhat is the threshold used to identify observations with high leverage? Calculate the threshold and save the value as leverage_threshold.\n\n\n# add your code here\n\n\nMake a plot of the standardized residuals vs.Â leverage (you can do this with ggplot() or with autoplot(which = 5)). Use geom_vline() to add a vertical line to help identify points with high leverage.\n\n\n# add your code here\n\n\nLetâ€™s dig into the data further. Which observations have high leverage? Why do these points have high leverage?\n\n\n# add your code here\n\n\n\nIdentifying outliers (outliers in y direction)\n\nMake a plot of the residuals vs.Â fitted values and a plot of the square root of the absolute value of standardized residuals vs.Â fitted (You can use autoplot(which = c(1, 3)) to display the plots side-by-side).\n\n\nHow are the plots similar? How do they differ?\nWhat is an advantage of using the plot of the residuals vs.Â fitted to check conditions and model diagnostics?\nWhat is an advantage of using the plot of the |standardized residuals|\\sqrt{|\\text{standardized residuals}|} vs.Â fitted to check conditions and model diagnostics?\n\n\n# add your code here\n\n\nAre there any observations that are outliers?\n\n\n# add your code here\n\n\n\nCookâ€™s distance\n\nMake a plot to check Cookâ€™s distance (autoplot(which = 4)). Based on this plot, are there any points that have a strong influence on the model coefficients?\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "href": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "title": "AE 4: Exam 1 Review",
    "section": "Adding another variable",
    "text": "Adding another variable\n\nAdd another variable, Alcohol, to your exploratory visualization. Describe any patterns that emerge.\n\n\n# add your code here\n\n\nFit a multiple linear regression model predicting Tip from Party and Alcohol. Display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret each of the slopes.\nDoes it make sense to interpret the intercept? Explain your reasoning.\nAccording to this model, is the rate of change in tip amount the same for various sizes of parties regardless of alcohol consumption or are they different? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#footnotes",
    "href": "ae/ae-4-exam-1-review.html#footnotes",
    "title": "AE 4: Exam 1 Review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. â€œThe Effects of Credit Cards on Tipping.â€ Project for Statistics 212-Statistics for the Sciences, St.Â Olaf College.â†©ï¸"
  },
  {
    "objectID": "ae/ae-5-the-office.html",
    "href": "ae/ae-5-the-office.html",
    "title": "AE 5: The Office",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-5-the-office-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-5-the-office.html#packages",
    "href": "ae/ae-5-the-office.html#packages",
    "title": "AE 5: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-5-the-office.html#load-data",
    "href": "ae/ae-5-the-office.html#load-data",
    "title": "AE 5: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\n\nRows: 188 Columns: 6\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr  (1): title\ndbl  (4): season, episode, imdb_rating, total_votes\ndate (1): air_date\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "href": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "title": "AE 5: The Office",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nRecreate at least one of the exploratory visualizations from class."
  },
  {
    "objectID": "ae/ae-5-the-office.html#testtrain-split",
    "href": "ae/ae-5-the-office.html#testtrain-split",
    "title": "AE 5: The Office",
    "section": "Test/train split",
    "text": "Test/train split\nSplit your data into testing and training sets."
  },
  {
    "objectID": "ae/ae-5-the-office.html#build-a-recipe",
    "href": "ae/ae-5-the-office.html#build-a-recipe",
    "title": "AE 5: The Office",
    "section": "Build a recipe",
    "text": "Build a recipe\nBuild the recipe from class.\n\nTime permittingâ€¦"
  },
  {
    "objectID": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "href": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "title": "AE 5: The Office",
    "section": "Workflows and model fitting",
    "text": "Workflows and model fitting\nBuild the modeling workflow and fit the model to the training data after feature engineering with the recipe."
  },
  {
    "objectID": "hw/hw-2.html",
    "href": "hw/hw-2.html",
    "title": "HW 2 - Multiple linear regression",
    "section": "",
    "text": "In this assignment, youâ€™ll get to put into practice the multiple linear regression skills youâ€™ve developed.\n\n\nIn this assignment, you willâ€¦\n\nFit and interpret multiple linear regression models with main and interaction effects.\nCompare multiple linear regression models.\nReason around multiple linear regression concepts.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-2. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-2.html#introduction",
    "href": "hw/hw-2.html#introduction",
    "title": "HW 2 - Multiple linear regression",
    "section": "",
    "text": "In this assignment, youâ€™ll get to put into practice the multiple linear regression skills youâ€™ve developed.\n\n\nIn this assignment, you willâ€¦\n\nFit and interpret multiple linear regression models with main and interaction effects.\nCompare multiple linear regression models.\nReason around multiple linear regression concepts.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-2. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-2.html#part-1---conceptual",
    "href": "hw/hw-2.html#part-1---conceptual",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 1 - Conceptual",
    "text": "Part 1 - Conceptual\n\nDealing with categorical predictors. Two friends, Elliott and Adrian, want to build a model predicting typing speed (average number of words typed per minute) from whether the person wears glasses or not. Before building the model they want to conduct some exploratory analysis to evaluate the strength of the association between these two variables, but theyâ€™re in disagreement about how to evaluate how strongly a categorical predictor is associated with a numerical outcome. Elliott claims that it is not possible to calculate a correlation coefficient to summarize the relationship between a categorical predictor and a numerical outcome, however theyâ€™re not sure what a better alternative is. Adrian claims that you can recode a binary predictor as a 0/1 variable (assign one level to be 0 and the other to be 1), thus converting it to a numerical variable. According to Adrian, you can then calculate the correlation coefficient between the predictor and the outcome. Who is right: Elliott or Adrian? If you pick Elliott, can you suggest a better alternative for evaluating the association between the categorical predictor and the numerical outcome?\nHigh correlation, good or bad? Two friends, Frances and Annika, are in disagreement about whether high correlation values are always good in the context of regression. Frances claims that itâ€™s desirable for all variables in the dataset to be highly correlated to each other when building linear models. Annika claims that while itâ€™s desirable for each of the predictors to be highly correlated with the outcome, it is not desirable for the predictors to be highly correlated with each other. Who is right: Frances, Annika, both, or neither? Explain your reasoning using appropriate terminology.\nTraining for the 5K. Nico signs up for a 5K (a 5,000 metre running race) 30 days prior to the race. They decide to run a 5K every day to train for it, and each day they record the following information: days_since_start (number of days since starting training), days_till_race (number of days left until the race), mood (poor, good, awesome), tiredness (1-not tired to 10-very tired), and time (time it takes to run 5K, recorded as mm:ss). Top few rows of the data they collect is shown below.\n\n\n\ndays_since_start\ndays_till_race\nmood\ntiredness\ntime\n\n\n\n\n1\n29\ngood\n3\n25:45\n\n\n2\n28\npoor\n5\n27:13\n\n\n3\n27\nawesome\n4\n24:13\n\n\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\n\n\n\nUsing these data Nico wants to build a model predicting time from the other variables. Should they include all variables shown above in their model? Why or why not?\nMultiple regression fact checking. Determine which of the following statements are true and false. For each statement that is false, explain why it is false.\n\nIf predictors are colinear, then removing one variable will have no influence on the point estimate of another variableâ€™s coefficient.\nSuppose a numerical predictor xx has a coefficient of Î²Ì‚1=2.5\\hat{\\beta}_1 = 2.5 in a multiple regression model. Suppose also that the first observation has x1,1=7.2x_{1,1} = 7.2, the second observation has a value of x2,1=8.2x_{2,1} = 8.2, and these two observations have the same values for all other predictors. Then the predicted value of the second observation will be 2.5 higher than the prediction of the first observation based on the multiple regression model.\nIf a regression modelâ€™s first predictor has a coefficient of Î²Ì‚1=5.7\\hat{\\beta}_1 = 5.7 and if we are able to influence the data so that an observation will have its x1x_1 be 1 larger than it would otherwise, the value yÌ‚1\\hat{y}_1 for this observation would increase by 5.7."
  },
  {
    "objectID": "hw/hw-2.html#part-2---palmer-penguins",
    "href": "hw/hw-2.html#part-2---palmer-penguins",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 2 - Palmer penguins",
    "text": "Part 2 - Palmer penguins\nData were collected and made available by Dr.Â Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. (Gorman, Williams, and Fraser 2014)\n\n\n\nArtwork by @allison_horst\n\n\nThese data can be found in the palmerpenguins package. Weâ€™re going to be working with the penguins dataset from this package. The dataset contains data for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n\nBody mass. Our first goal is to fit a model predicting body mass (which is more difficult to measure) from bill length, bill depth, flipper length, species, and sex.\n\nFit a model predicting body mass (which is more difficult to measure) from the other variables listed above.\nWrite the equation of the regression model.\nInterpret each one of the slopes in this context.\nCalculate the residual for a male Adelie penguin that weighs 3750 grams with the following body measurements: bill_length_mm = 39.1, bill_depth_mm = 18.7, flipper_length_mm = 181. Does the model overpredict or underpredict this penguinâ€™s weight?\nFind the R2R^2 of this model and interpret this value in context of the data and the model.\n\n\n\n\nBill depth. Next weâ€™ll be focusing on bill depth and bill length and also considering species.\n\nFit a model predicting bill depth from bill length. Find the adjusted R-squared, AIC, and BIC for this model.\nThen, add a new predictor: species. Fit another model predicting bill depth from bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nFinally, add one more predictor: the interaction between bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nUsing the three criteria you recorded for these three models, and with the goal of parsimony, which model is the â€œbestâ€ for predicting bill depth from bill length and/or species. Explain your reasoning.\nCreate a visualization representing your model from part a. Hint: Make a scatterplot of bill depth vs.Â bill length and add the linear model.\nCreate a visualization representing your model from part b. Hint: Same as part (e), but think about how many lines to plot and whether their slopes should be the same or different.\nCreate a visualization representing your model from part c. Hint: Same as part (f), but think about how many lines to plot and whether their slopes should be the same or different.\nBased on your visualizations from parts e - g, and with the goal of parsimony, is your answer for which model is the â€œbestâ€ for predicting bill depth from bill length and/or species the same as your answer in part d? Explain your reasoning."
  },
  {
    "objectID": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "href": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 3 - Perceived threat of Covid-19",
    "text": "Part 3 - Perceived threat of Covid-19\nGarbe, Rau, and Toppe (2020), published in June 2020, aims to examine the relationship between personality traits, perceived threat of Covid-19 and stockpiling toilet paper. For this study titled Influence of perceived threat of Covid-19 and HEXACO personality traits on toilet paper stockpiling, researchers conducted an online survey March 23 - 29, 2020 and used the results to fit multiple linear regression models to draw conclusions about their research questions. From their survey, they collected data on adults across 35 countries. Given the small number of responses from people outside of the United States, Canada, and Europe, only responses from people in these three locations were included in the regression analysis.\nLetâ€™s consider their results for the model looking at the effect on perceived threat of Covid-19. The model can be found on page 6 of the paper. The perceived threat of Covid was quantified using the responses to the following survey question:\n\nHow threatened do you feel by Coronavirus? [Users select on a 10-point visual analogue scale (Not at all threatened to Extremely Threatened)]\n\n\nInterpret the coefficient of Age (0.072) in the context of the analysis.\nInterpret the coefficient of Place of residence in the context of the analysis.\nThe model includes an interaction between Place of residence and Emotionality (capturing differential tendencies in to worry and be anxious).\n\nWhat does the coefficient for the interaction (0.101) mean in the context of the data?\nInterpret the estimated effect of Emotionality for a person who lives in the US/Canada.\nInterpret the estimated effect of Emotionality for a person who lives in Europe."
  },
  {
    "objectID": "hw/hw-2.html#submission",
    "href": "hw/hw-2.html#submission",
    "title": "HW 2 - Multiple linear regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember â€“ you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials â¡ï¸ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and youâ€™ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be â€œcheckedâ€).\nSelect the first page of your PDF submission to be associated with the â€œWorkflow & formattingâ€ section."
  },
  {
    "objectID": "hw/hw-2.html#grading",
    "href": "hw/hw-2.html#grading",
    "title": "HW 2 - Multiple linear regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-2.html#footnotes",
    "href": "hw/hw-2.html#footnotes",
    "title": "HW 2 - Multiple linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe â€œWorkflow & formattingâ€ grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.â†©ï¸"
  },
  {
    "objectID": "hw/hw-4.html",
    "href": "hw/hw-4.html",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "",
    "text": "In this assignment, youâ€™ll get to put into practice the logistic regression skills youâ€™ve developed.\n\n\nIn this assignment, you willâ€¦\n\nFit and interpret multinomial logistic regression models.\nEvaluate model conditions\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-4. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "hw/hw-4.html#introduction",
    "href": "hw/hw-4.html#introduction",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "",
    "text": "In this assignment, youâ€™ll get to put into practice the logistic regression skills youâ€™ve developed.\n\n\nIn this assignment, you willâ€¦\n\nFit and interpret multinomial logistic regression models.\nEvaluate model conditions\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-4. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "hw/hw-4.html#data",
    "href": "hw/hw-4.html#data",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Data",
    "text": "Data\nFor this assignment, we will analyze data from the eye witness identification experiment in Carlson and Carlson (2014). In this experiment, participants were asked to watch a video of a mock crime (from the first person perspective), spend a few minutes completing a random task, and then identify the perpetrator of the mock crime from a line up shown on the screen. Every lineup in this analysis included the true perpetrator from the video. After viewing the line-up , each participant could make one of the following decisions (id):\n\ncorrect: correctly identified the true perpetrator\nfoil: incorrectly identified the â€œfoilâ€, i.e.Â a person who looks very similar to the perpetrator\nreject: incorrectly concluded the true perpetrator is not in the lineup\n\nThe main objective of the analysis is to understand how different conditions of the mock crime and suspect lineup affect the decision made by the participant. We will consider the following conditions to describe the decisions:\n\nlineup: How potential suspects are shown to the participants\n\nSimultaneous Lineup: Participants were shown photos of all 6 potential suspects at the same time and were required to make a single decision (identify someone from the lineup or reject the lineup).\nSequential 5 Lineup: Photos of the 6 suspects were shown one at a time. The participant was required to make a decision (choose or donâ€™t choose) as each photo was shown. Once a decision was made, participants were not allowed to reexamine a photo. If the participant made an identification, the remaining photos were not shown. In each of these lineups the true perpetrator was always the 5th photo in the lineup.\n\nweapon: Whether or not a weapon was present in the video of the mock crime.\nfeature: Whether or not the perpetrator had a distinctive marking on his face. In this experiment, the distinctive feature was a large â€œNâ€ sticker on one cheek. (The letter â€œNâ€ was chosen to represent the first authorâ€™s alma mater - University of Nebraska.)\n\nThe data may be found in eyewitness.csv in the data folder.\n\new &lt;- read_csv(here::here(\"hw\", \"data/eyewitness.csv\"))\new &lt;- ew %&gt;%\n  mutate(id = as_factor(id))"
  },
  {
    "objectID": "hw/hw-4.html#exercises",
    "href": "hw/hw-4.html#exercises",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Exercises",
    "text": "Exercises\n\nLetâ€™s begin by doing some exploratory data analysis. The univariate (single variable) plots for each of the predictor variables and the response variable are shown below.\n\n\n\n\n\n\n\n\n\n\nComplete the exploratory data analysis by creating the plots and/or summary statistics to examine the relationship between the response variable (id) and each of the explanatory variables (lineup, weapon, and feature).\n\nUsing the plots/tables from Exercise 1:\n\n\nWhat is one thing you learn about the data from the univariate plots?\nBased on the bivariate plots, do any of the predictors appear to have a significant effect on the id? Briefly explain.\n\n\nBriefly explain why you should use a multinomial logistic regression model to predict id using lineup, weapon and feature.\nFit the multinomial logistic model that only includes main effects. Display the model output.\n\n\nWhat is the baseline category for the response variable?\nInterpret the intercepts for each part of the model in terms of the odds.\nInterpret the coefficients of lineup for each part of the model in terms of the odds.\n\n\nYou want to consider all possible first-order interaction effects (interaction effects between two variables) for the model.\n\n\nUse the appropriate test to determine if there is at least one significant interaction effect.\nBased on your test, is there evidence of any significant interaction effects?\n\nRegardless of your answer to Question 5, use the model that includes the interaction terms for the remainder of the assignment.\n\nAccording to the model,\n\n\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the log-odds of reject vs.Â a correct ID change when there is a simultaneous lineup vs.Â a sequential lineup?\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the odds of reject vs.Â a correct ID change when there is a simultaneous lineup vs.Â a sequential lineup?\nWhich group of participants (i.e., which set of experimental conditions) is described by the intercept?\n\n\nAre the conditions inference met? List of the conditions, and, if relevant, create visualizations to check the conditions and evaluate whether each condition is met. Include an assessment about each condition and a brief explanation about your conclusion.\nUse the model to predict the decision made by each participant. Make a table of the predicted vs.Â the actual decisions.\n\n\nBriefly describe how the predicted decision is determined for each participant.\nWhat is the misclassification rate?"
  },
  {
    "objectID": "hw/hw-4.html#submission",
    "href": "hw/hw-4.html#submission",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember â€“ you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials â¡ï¸ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and youâ€™ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be â€œcheckedâ€).\nSelect the first page of your PDF submission to be associated with the â€œWorkflow & formattingâ€ section."
  },
  {
    "objectID": "hw/hw-4.html#grading",
    "href": "hw/hw-4.html#grading",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nExercises\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-4.html#footnotes",
    "href": "hw/hw-4.html#footnotes",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe â€œWorkflow & formattingâ€ grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.â†©ï¸"
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html",
    "href": "weeks/BSMM_8740_week_1.html",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 1 - Sunday, Sept 15, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html#prepare",
    "href": "weeks/BSMM_8740_week_1.html#prepare",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "Prepare",
    "text": "Prepare\nğŸ“– Read the syllabus\nğŸ“– Read the support resources\nğŸ“– Get familiar with Git by reading Excuse me, do you have a moment to talk about version control?\nğŸ“– Get familiar with Git by reading Excuse me, do you have a moment to talk about version control?\nğŸ“– Read the article What is Tidy Data?\nğŸ“– Read the chapters 2-5 in R for Data Science",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html#participate",
    "href": "weeks/BSMM_8740_week_1.html#participate",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 1 - The tidyverse",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html#practice",
    "href": "weeks/BSMM_8740_week_1.html#practice",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "Practice",
    "text": "Practice\nğŸ“‹ AE 0 - Movies",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_1.html#perform",
    "href": "weeks/BSMM_8740_week_1.html#perform",
    "title": "Week 1 - Tidyverse, EDA & Git",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 1 - Git & the tidyverse\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_2.html",
    "href": "weeks/BSMM_8740_week_2.html",
    "title": "Week 2: The Recipes Package",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 2 - Sunday, Sept 22, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_2.html#prepare",
    "href": "weeks/BSMM_8740_week_2.html#prepare",
    "title": "Week 2: The Recipes Package",
    "section": "Prepare",
    "text": "Prepare\nğŸ“– Read Chapter 8: Feature Engineering with recipes in Tidy Modeling in R\nğŸ“– Read Preprocess your data with recipes - Chapter 2: Preprocessing with Recipes\nğŸ“– Familiarize yourself with the recipes package: Package â€˜recipesâ€™\nğŸ“– Watch this video from Max Kuhn: Cooking Your Data with Recipes in R with Max Kuhn\nğŸ“– Step through Max Kuhnâ€™s slides: Cooking your data with recipes!!!",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_2.html#participate",
    "href": "weeks/BSMM_8740_week_2.html#participate",
    "title": "Week 2: The Recipes Package",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 2 - The Recipes Package",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_2.html#perform",
    "href": "weeks/BSMM_8740_week_2.html#perform",
    "title": "Week 2: The Recipes Package",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 2 - The Recipes Package\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html",
    "href": "weeks/BSMM_8740_week_7.html",
    "title": "Week 7 - Causality: DAGs",
    "section": "",
    "text": "Important\n\n\n\nDue date: Lab 7 - Sunday, Nov 03, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html#prepare",
    "href": "weeks/BSMM_8740_week_7.html#prepare",
    "title": "Week 7 - Causality: DAGs",
    "section": "Prepare",
    "text": "Prepare\nğŸ“– Read Chp 1 in: Statistical Tools for Causal Inference\nğŸ“– Read Tutorial on Directed Acyclic Graphs\nğŸ“– Read ggdag: An R Package for visualizing and analyzing causal directed acyclic graphs\nğŸ“– Read Causal inference & directed acyclic diagrams (DAGs), Chp 7.3-7.4 in (Mostly Clinical) Epidemiology with R\nğŸ“– Read Propensity Score Analysis: A Primer and Tutorial Chp 1-4:\nğŸ“– Read Margin Effects Zoo, Inverse Probability Weighting, for treatment evaluation using the marginaleffects package.",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html#participate",
    "href": "weeks/BSMM_8740_week_7.html#participate",
    "title": "Week 7 - Causality: DAGs",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 7 - Causality: DAGs",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_7.html#perform",
    "href": "weeks/BSMM_8740_week_7.html#perform",
    "title": "Week 7 - Causality: DAGs",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 7 -Causality: DAGs\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_4.html",
    "href": "weeks/BSMM_8740_week_4.html",
    "title": "Week 4 - The TidyModels Package",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 4 - Sunday, Oct 06, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_4.html#prepare",
    "href": "weeks/BSMM_8740_week_4.html#prepare",
    "title": "Week 4 - The TidyModels Package",
    "section": "Prepare",
    "text": "Prepare\nğŸ“– Read A Review of R Modeling Fundamentals\nğŸ“– Read Build a Model\nğŸ“– Read A gentle Introduction to Tidymodels\nğŸ“– Take a look at Tidymodels: tidy machine learning in R\nğŸ“– Check out Tidymodels Cheatsheet: Tidymodels Functions",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_4.html#participate",
    "href": "weeks/BSMM_8740_week_4.html#participate",
    "title": "Week 4 - The TidyModels Package",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 4 -The TidyModels Package",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_4.html#perform",
    "href": "weeks/BSMM_8740_week_4.html#perform",
    "title": "Week 4 - The TidyModels Package",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 4 -The TidyModels Package\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html",
    "href": "weeks/BSMM_8740_week_10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\nDue date: Project proposal due Fri, Mar 18 at 5:00 pm.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html#prepare",
    "href": "weeks/BSMM_8740_week_10.html#prepare",
    "title": "Week 10",
    "section": "Prepare",
    "text": "Prepare\nğŸ“– Read Introduction to Modern Statistics, Chp 9: Logistic regression",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html#participate",
    "href": "weeks/BSMM_8740_week_10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 10 - MonteCarlo methods",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html#practice",
    "href": "weeks/BSMM_8740_week_10.html#practice",
    "title": "Week 10",
    "section": "Practice",
    "text": "Practice\nğŸ“‹ Application Exercise 9 - Odds",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_10.html#perform",
    "href": "weeks/BSMM_8740_week_10.html#perform",
    "title": "Week 10",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 10 - MonteCarlo methods\nâœï¸ HW 3 - Logistic regression and log transformation\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_8.html",
    "href": "weeks/BSMM_8740_week_8.html",
    "title": "Week 8 - Causality: Methods",
    "section": "",
    "text": "Important\n\n\n\nDue date: Lab 8 - Sunday, Nov 10, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_8.html#prepare",
    "href": "weeks/BSMM_8740_week_8.html#prepare",
    "title": "Week 8 - Causality: Methods",
    "section": "Prepare",
    "text": "Prepare\nğŸ“– Read Chp 10-14 in: Causal Inference for the Brave and True\nğŸ“– Read Chp 8-10 in: Causal Inference in R\nğŸ“– Read Chp 8-10 in: Causal Inference in R\nğŸ“– Read Matching and Subclassification, Chp 5 in: Causal Inference the Mixtape",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_8.html#participate",
    "href": "weeks/BSMM_8740_week_8.html#participate",
    "title": "Week 8 - Causality: Methods",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 8 - Causality: Methods",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_8.html#perform",
    "href": "weeks/BSMM_8740_week_8.html#perform",
    "title": "Week 8 - Causality: Methods",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 8 - Causality: Methods\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\nğŸ”— on Duke Container Manager\n\n\nCourse GitHub organization\nğŸ”— on GitHub\n\n\nDiscussion forum\nğŸ”— on Sakai\n\n\nLecture streaming and recordings\nğŸ”— on Panopto\n\n\nGradebook\nğŸ”— on Sakai\n\n\nVirtual meetings\nğŸ”— on Sakai"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html",
    "href": "labs/BSMM_8740_lab_1.html",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "",
    "text": "This lab will go through many of the same operations weâ€™ve demonstrated in class. The main goal is to reinforce our understanding of tidy data, the tidyverse and the pipe, which we will be using throughout the course.\nAs the labs progress, you are encouraged to explore beyond what the labs require; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R and the tidyverse. Today we begin with exercises in the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\n\n\nBy the end of the lab, you willâ€¦\n\nBe familiar with the workflow using R, RStudio, Git, and GitHub\nHave practiced version control using GitHub"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#introduction",
    "href": "labs/BSMM_8740_lab_1.html#introduction",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "",
    "text": "This lab will go through many of the same operations weâ€™ve demonstrated in class. The main goal is to reinforce our understanding of tidy data, the tidyverse and the pipe, which we will be using throughout the course.\nAs the labs progress, you are encouraged to explore beyond what the labs require; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R and the tidyverse. Today we begin with exercises in the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\n\n\nBy the end of the lab, you willâ€¦\n\nBe familiar with the workflow using R, RStudio, Git, and GitHub\nHave practiced version control using GitHub"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#getting-started",
    "href": "labs/BSMM_8740_lab_1.html#getting-started",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-1-[your github username] repository .\nCreate an R project using your 2024-lab-1-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-1.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#packages",
    "href": "labs/BSMM_8740_lab_1.html#packages",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Packages",
    "text": "Packages\n\n\n\n\n\n\nNote\n\n\n\nThe code below will install (if necessary) and load the packages needed in todayâ€™s exercises\n\n\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(tidyverse, Lahman, magrittr, gt, gtExtras, ggplot2, skimr\n                 , dplyr, gt, gtExtras, here, readr, rmarkdown)\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#data-yearly-statistics-and-standings-for-baseball-teams",
    "href": "labs/BSMM_8740_lab_1.html#data-yearly-statistics-and-standings-for-baseball-teams",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Data: Yearly statistics and standings for baseball teams",
    "text": "Data: Yearly statistics and standings for baseball teams\nTodayâ€™s data is all baseball statistics. The data is in the Lahman package.\n\nView the data\nBefore doing any analysis, you may want to get quick view of the data. This is useful when youâ€™ve imported data to see if your data imported correctly. We can use the view() function to see the entire data set in RStudio. Type the code below in the Console to view the entire dataset.\n\ndim(Teams)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#data-dictionary",
    "href": "labs/BSMM_8740_lab_1.html#data-dictionary",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nThe variable definitions are found in the help for Teams\n\n?Teams\n\n\nExercises\nWrite all code and narrative in your R Markdown file where indicated. Write all narrative in complete sentences. Throughout the assignment, you should periodically render your Quarto document to ensure that all code executes and that your document format is intact, save, stage & commit the changes in the Git pane, and push the updated files to your repository. This ensures that your work is saved.\n\n\n\n\n\n\nTip\n\n\n\nMake sure we can read all of the code in your quarto document. This means you will need to break up long lines of code. One way to help avoid long lines of code is is start a new line after every pipe (%&gt;% or |&gt;) and plus sign (+).\n\n\n\n\nExercise 1\nThe view() function helps us get a quick view of the dataset, but letâ€™s get more detail about its structure. Viewing a summary of the data is a useful starting point for data analysis, especially if the dataset has a large number of observations (rows) or variables (columns). Run the code below to use the dplyr::glimpse() function to see a summary of the ikea dataset.\n\ndplyr::glimpse(Teams[1:10,])\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\nHow many observations are in the Teams dataset? How many variables/measurements?\n\ndplyr::glimpse(Teams[1:10,])\n\nHow many character columns/measurements have missing variables?\n\n# run a data exploration here using the skimr package.\nTeams |&gt; skimr::skim()\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn your lab-1.qmd document youâ€™ll see that we already added the code required for most exercises as well as a sentence where you can fill in the blanks to report the answer.\nAlso note that the code chunk as a label: glimpse-data. Itâ€™s not required, but it is good practice and highly encouraged to label your code chunks in this way. If there is an error when you render your document, the code-chunk label will identify where the error is.\n\n\n\n\nExercise 2\nBen BaumerÂ worked for theÂ New York MetsÂ from 2004 to 2012. What was the team W/L record during those years? Use filter() and select() to quickly identify only those pieces of information that we care about.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# fill in the blanks and ensure your code produces the correct result\nmets &lt;- Teams  %&gt;% \n  dplyr::filter(teamID == \"NYN\")\nmy_mets &lt;- mets %&gt;% \n  dplyr::filter(_)\nmy_mets %&gt;% \n  dplyr::select(_,_,_,_)\n\n\n\n\n\nExercise 3\nWeâ€™ve answered the simple question of how the Mets performed during the time that Ben was there, but since we are data scientists, we are interested in deeper questions. For example, some of these seasons were subparâ€”the Mets had more losses than wins. Did the team just get unlucky in those seasons? Or did they actually play as badly as their record indicates?\nIn order to answer this question, we need a model for expected winning percentage. It turns out that one of the most widely used contributions to the field of baseball analytics (courtesy ofÂ Bill James) is exactly that. This model translates the number of runs4Â that a team scores and allows over the course of an entire season into an expectation for how many games they should have won. The simplest version of this model is this:\nWÌ‚pct=11+(RARS)2\n\\hat{\\text{W}}_{\\text{pct}}=\\frac{1}{1+\\left(\\frac{\\text{RA}}{\\text{RS}}\\right)^{2}}\n\nwhere RA\\text{RA} is the number of runs the team allows to be scored, RS\\text{RS} is the number of runs that the team scores, and WÌ‚pct\\hat{\\text{W}}_{\\text{pct}} is the teamâ€™s expected winning percentage. Luckily for us, the runs scored and allowed are present in the Teams table, so letâ€™s grab them and save them in a new data frame.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# fill in the blanks and ensure your code produces the correct result\nmets_ben &lt;- Teams |&gt;\n  dplyr::select(_, _, _, _, _, _) |&gt;\n  dplyr::filter(_ == \"NYN\" & _ %in% 2004:2012)\nmets_ben\n\nFirst, note that the runs-scored variable is calledÂ R in the Teams table, but to stick with our notation we want to rename it RS.\n\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::rename(_ = _)    # new name = old name\nmets_ben\n\n\n\n\nThis is a good place to save, stage, commit, and push changes to your remote lab-1 repository. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message (e.g.,Â â€œCompleted exercises 1 - 3â€), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nNext, we need to compute the teamâ€™s actual winning percentage in each of these seasons. Thus, we need to add a new column to our data frame, and we do this with theÂ mutate()Â command.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# add the calculation using the formula above\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::mutate(WPct = _)\nmets_ben\n\nThe expected number of wins is then equal to the product of the expected winning percentage times the number of games.\n\n# fill in the blanks and ensure your code produces the correct result\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::mutate(W_hat = _)\nmets_ben\n\n\n\n\n\nExercise 5\nIn this case, the Metsâ€™ fortunes were better than expected in three of these seasons, and worse than expected in the other six.\nIn how many seasons were the Mets better than expected? How many were they worse than expected?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# repeat the better_than/ worse_than calculation for all the years in the dataset: show your work.\n\n\n\n\nThis is a good place to save, stage, commit, and push changes to your remote lab-1 repo. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message (e.g.,Â â€œCompleted exercises 4 and 5â€), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 6\nNaturally, the Mets experienced ups and downs during Benâ€™s time with the team. Which seasons were best? To figure this out, we can simply sort the rows of the data frame.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\ndplyr::arrange(mets_ben, _)\n\n\n\n\n\nExercise 7\nIn 2006, the Mets had the best record in baseball during the regular season and nearly made theÂ World Series. How do these seasons rank in terms of the teamâ€™s performance relative to our model?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nmets_ben %&gt;% \n  dplyr::mutate(Diff = _) |&gt;\n  dplyr::arrange(_)\n\nSummarize the Mets performance:\n\n# fill in the blanks and ensure your code produces the correct result\nmets_ben |&gt;\n  dplyr::summarize(\n    num_years = _, \n    total_W = _, \n    total_L = _, \n    total_WPct = _, \n    sum_resid = _\n  )\n\nIn these nine years, the Mets had a combined record of ? wins and ? losses, for an overall winning percentage of _?.\n\n\n\nThis is a good place to save, stage, commit, and push changes to your remote lab-1 repo. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message (e.g.,Â â€œCompleted exercises 6 - 8â€), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 8\nDiscretize the years into three chunks: one for each of the three general managers under whom Ben worked.Â Jim DuquetteÂ was the Metsâ€™Â general managerÂ in 2004,Â Omar MinayaÂ from 2005 to 2010, andÂ Sandy AldersonÂ from 2011 to 2012.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nmets_ben &lt;- mets_ben %&gt;% \n  dplyr::mutate(\n    gm = ifelse(\n      yearID == _, \n      _, \n      ifelse(\n        yearID &gt;= _, \n        _, \n        _)\n    )\n  )\n\nAlternatively, we use the case_when function\n\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::mutate(\n    gm = dplyr::case_when(\n      yearID == _ ~ _, \n      yearID &gt;= _ ~ _, \n      TRUE ~ _\n    )\n  )\n\n\n\n\n\nExercise 9\nThe following dataset is the basis of a model that predicts which businesses are likely to have customer churn at the start of 2015, based on the business type and incorporation_date. This question will give you some practice using the various tidyr:: pivot operations.\n\nset.seed(42)\n\n# read data and drop column 1 (it contains row numbers and doesn't have a column name)\ndf &lt;- readr::read_csv(\"data/monthly_data.csv\", show_col_types = FALSE, col_select = -1)\n\n# Have a glimpse of the data\nglimpse(df)\n\nRows: 902\nColumns: 27\n$ company_id            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, â€¦\n$ `2014-01-01_payments` &lt;dbl&gt; 0, 1, 6, 8, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4, 43, 34â€¦\n$ `2014-02-01_payments` &lt;dbl&gt; 0, 1, 6, 4, 0, 2, 3, 0, 0, 0, 2, 0, 11, 1, 51, 5â€¦\n$ `2014-03-01_payments` &lt;dbl&gt; 0, 3, 6, 7, 39, 1, 1, 6, 0, 0, 0, 8, 0, 1, 44, 4â€¦\n$ `2014-04-01_payments` &lt;dbl&gt; 1, 2, 6, 7, 0, 3, 7, 50, 1, 0, 0, 2, 0, 0, 47, 5â€¦\n$ `2014-05-01_payments` &lt;dbl&gt; 0, 2, 6, 1, 54, 1, 4, 119, 0, 1, 0, 0, 0, 5, 46,â€¦\n$ `2014-06-01_payments` &lt;dbl&gt; 1, 1, 7, 2, 0, 2, 1, 151, 3, 0, 0, 0, 0, 2, 81, â€¦\n$ `2014-07-01_payments` &lt;dbl&gt; 0, 1, 8, 2, 0, 2, 7, 182, 0, 0, 0, 3, 0, 0, 91, â€¦\n$ `2014-08-01_payments` &lt;dbl&gt; 0, 1, 7, 4, 22, 1, 2, 167, 0, 0, 0, 2, 0, 0, 93,â€¦\n$ `2014-09-01_payments` &lt;dbl&gt; 0, 1, 8, 3, 0, 1, 5, 180, 0, 0, 0, 0, 9, 5, 88, â€¦\n$ `2014-10-01_payments` &lt;dbl&gt; 0, 4, 8, 5, 0, 2, 8, 157, 1, 0, 0, 0, 2, 1, 86, â€¦\n$ `2014-11-01_payments` &lt;dbl&gt; 0, 3, 9, 5, 0, 1, 2, 105, 0, 0, 0, 0, 0, 0, 93, â€¦\n$ `2014-12-01_payments` &lt;dbl&gt; 0, 3, 9, 9, 0, 3, 8, 57, 0, 0, 0, 0, 0, 0, 104, â€¦\n$ `2014-01-01_mandates` &lt;dbl&gt; 1, 0, 0, 0, 4, 0, 0, 0, 4, 4, 0, 0, 22, 0, 1, 6,â€¦\n$ `2014-02-01_mandates` &lt;dbl&gt; 2, 0, 0, 0, 31, 1, 0, 2, 3, 8, 0, 0, 20, 12, 0, â€¦\n$ `2014-03-01_mandates` &lt;dbl&gt; 2, 0, 0, 0, 24, 0, 0, 0, 5, 19, 0, 0, 11, 17, 0,â€¦\n$ `2014-04-01_mandates` &lt;dbl&gt; 1, 0, 0, 53, 18, 0, 1, 0, 0, 0, 0, 1, 11, 14, 1,â€¦\n$ `2014-05-01_mandates` &lt;dbl&gt; 0, 0, 2, 0, 8, 0, 0, 0, 0, 0, 0, 1, 15, 0, 0, 3,â€¦\n$ `2014-06-01_mandates` &lt;dbl&gt; 0, 0, 2, 0, 7, 0, 0, 0, 0, 0, 0, 0, 13, 5, 0, 0,â€¦\n$ `2014-07-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 16, 0, 0â€¦\n$ `2014-08-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 8, 0, 0,â€¦\n$ `2014-09-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 2, 1, 1,â€¦\n$ `2014-10-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 16, 1, 0, 2,â€¦\n$ `2014-11-01_mandates` &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 1, 0, â€¦\n$ `2014-12-01_mandates` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 11, 0, 0, 0,â€¦\n$ vertical              &lt;chr&gt; \"gym/fitness\", \"gym/fitness\", \"freelance developâ€¦\n$ incorporation_date    &lt;date&gt; 2013-05-30, 2003-09-25, 2008-10-22, 2005-06-28,â€¦\n\n\nYouâ€™ll notice that the data is not in tidy form: one type of measurement in each column and all measurements that go together in the same row. To put this data into tidy from youâ€™ll need to\n\nTake all the columns with names that start with a date string and use tidyr::pivot_longer to create a column named date containing the original column names and a column named quantity to contain the values.\nIn the date column (which contains strings at this stage), split the date from the remainder of the string, saving the remainder in a column called paymentMandate, and the date string in a column called date. To do this use tidyr::separate_wider_delim with delim = \"_\" and the specified names.\nNext, use tidyr::pivot_wider with names_from = paymentMandate, and values_from = quantity to create payments and mandates columns.\nFinally, use dplyr::mutate to change the type of the data and incorporation_date columns to Date.\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\nWhat is the mean of the sales_per_visit columns/measurement? Are there any grouped observations?\n\n# show your steps and the resulting tibble here:\n\nYour code should produce a tibble that looks like this:\n\n\n# A tibble: 10,824 Ã— 6\n   company_id vertical    incorporation_date date       payments mandates\n        &lt;int&gt; &lt;chr&gt;       &lt;date&gt;             &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1          1 gym/fitness 2013-05-30         2014-01-01        0        1\n 2          1 gym/fitness 2013-05-30         2014-02-01        0        2\n 3          1 gym/fitness 2013-05-30         2014-03-01        0        2\n 4          1 gym/fitness 2013-05-30         2014-04-01        1        1\n 5          1 gym/fitness 2013-05-30         2014-05-01        0        0\n 6          1 gym/fitness 2013-05-30         2014-06-01        1        0\n 7          1 gym/fitness 2013-05-30         2014-07-01        0        0\n 8          1 gym/fitness 2013-05-30         2014-08-01        0        0\n 9          1 gym/fitness 2013-05-30         2014-09-01        0        0\n10          1 gym/fitness 2013-05-30         2014-10-01        0        0\n# â„¹ 10,814 more rows\n\n\n\n\n\n\nExercise 10\n\nThe Business Problem\nThe story begins in a fast paced startup. The company is growing fast and the marketing team is looking for ways to increase the sales from existing customers by making them buy more. The main idea is to unlock the potential of the customer base through incentives, in this case a discount. We of course want to measure the effect of the discount on the customerâ€™s behavior. Still, they do not want to waste money giving discounts to users which are not valuable. As always, it is about return on investment (ROI).\nWithout going into specifics about the nature of the discount, it has been designed to provide a positive return on investment if the customer buys more than $1\\$ 1 as a result of the discount. How can we measure the effect of the discount and make sure our experiment has a positive ROI? The marketing team came up with the following strategy:\n\nSelect a sample of existing customers from the same cohort.\nSet a test window of 1 month.\nLook into the historical data of web visits from the last month. The hypothesis is that web visits are a good proxy for the customerâ€™s interest in the product.\nFor customers with a high number of web visits, send them a discount. There will be a hold out group which will not receive the discount within the potential valuable customers based on the number of web visits. For customers with a low number of web visits, do not send them a discount (the marketing team wants to report a positive ROI, so they do not want to waste money on customers which are not valuable). Still, they want to use them to measure the effect of the discount.\nWe also want to use the results of the test to tag loyal customers. These are customers which got a discount (since they showed potential interest in the product) and customers with exceptional sales numbers even if they did not get a discount. The idea is to use this information to target them in the future if the discount strategy is positive.\n\n\n\nThe Data\nThe team collected data from the experiment above and asked the data science team to analyze it and provide insights. In particular they want to know if they should keep the discount strategy. The data consists of the following fields: - visits: Number of visits to the website during the test window. - discount: Whether the customer received a discount or not. - is_loyal: Whether the customer is loyal or not according to the definition above. - sales: Sales in $\\$ during the test window.\n\nPrepare Notebook\nData scientist A was the one in charge of preparing the environment and collecting the data. As an important best practice, they fixed a global seed which initializes the random number generator in order to make sure every part of the analysis was reproducible. This ensures that the calculations are not affected by pure randomness. In addition all the required packages were listed from the start (reproducible R environment).\n\nset.seed(8740)\n\n\n\nRead Data\nThey pulled the data from a csv file and displayed the first 5 measurements.\n\ndata &lt;- readr::read_csv(\"data/sales_dag.csv\", show_col_types = FALSE)\n\ndata |&gt; dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"sample marketing data\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nsample marketing data\n\n\nvisits\ndiscount\nis_loyal\nsales\nsales_per_visit\n\n\n\n\n12\n0\n0\n13.34830\n1.1123585\n\n\n26\n1\n1\n21.70125\n0.8346635\n\n\n13\n0\n0\n14.70040\n1.1308004\n\n\n24\n0\n0\n20.37734\n0.8490557\n\n\n14\n0\n0\n12.63372\n0.9024089\n\n\n\n\n\n\n\nThey then checked for missing values and whether the measurements were in the correct format.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\nWhat is the mean of the sales_per_visit columns/measurement? Are there any grouped observations?\n\n# run a data exploration here using the skimr package.\ndata |&gt; skimr::skim()\n\n\n\n\n\nExploratory Data Analysis\nAs part of the project scope, the data science team in charge of the analysis was asked to provide a summary of the data. The team was also asked to provide a visualization of the data to help the marketing team understand the data better. Data scientist A took over this task.\nThey started by looking at the share of customers which received a discount:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# calculate the % share of customers receiving a discount vs the % not receiving a discount\n\nSimilarly for the share of customers which are loyal:\n\n# calculate the % share of customers that are 'loyal' vs not 'loyal'\n\nTo understand these features better, they also looked at a cross-tab table:\n\n# build a cross-tab table of 'loyal' customers vs customers getting a discount\n\n\n\nNote that all customers with discount are loyal (as required) and that there are loyal users which did not receive a discount. This is because they had exceptional sales numbers. Verify this:\n\ndata |&gt; dplyr::mutate(id = dplyr::row_number(), .before = 1) |&gt; \n  dplyr::filter(discount == 0) |&gt; \n  dplyr::arrange( desc(sales) ) |&gt; \n  dplyr::slice_head(n=10) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"Sales: loyal customers vs others\") |&gt; \n  gtExtras::gt_theme_espn()\n\nThe loyal customers are the top ones in terms of sales. This is good news. It means that the definition of loyal customers is consistent with the data.\nIn order to have orders of magnitude for the sales, the data scientist provided some summary statistics table:\n\ngtExtras::gt_plt_summary(data)\n\nTo have a better glimpse of the data, the data scientist also provided a histogram of the sales:\n\ndata |&gt; \n  ggplot(aes(x=sales)) +\n  geom_histogram(aes(y = ..density..), bins = 30, colour = 1, fill = \"white\") +\n  geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25) +\n  labs(title = \"Sales Distribution\") +\n  theme_minimal()\n\n\nYouâ€™re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message â€œDone with Lab 1!â€ , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyoneâ€™s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (donâ€™t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#grading",
    "href": "labs/BSMM_8740_lab_1.html#grading",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_1.html#resources-for-additional-practice-optional",
    "href": "labs/BSMM_8740_lab_1.html#resources-for-additional-practice-optional",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Resources for additional practice (optional)",
    "text": "Resources for additional practice (optional)\n\nChapter 2: Get Started Data Visualization by Kieran Healy\nChapter 3: Data visualization in R for Data Science by Hadley Wickham\nRStudio Cloud Primers\n\nVisualization Basics: https://rstudio.cloud/learn/primers/1.1\nWork with Data: https://rstudio.cloud/learn/primers/2\nVisualize Data: https://rstudio.cloud/learn/primers/3"
  },
  {
    "objectID": "labs/BSMM_8740_lab_git.html",
    "href": "labs/BSMM_8740_lab_git.html",
    "title": "Lab Git",
    "section": "",
    "text": "This lab is designed to reinforce your use of git and GitHub, the collaboration and version control system that we will be using throughout the course.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like â€œTrack Changesâ€ features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\nToday we begin with an introductions to GIT followed by exercises in the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\n\n\nBy the end of the lab, you willâ€¦\n\nHave practiced version control using GitHub\nBe familiar with the workflow using R, RStudio, Git, and GitHub\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!"
  },
  {
    "objectID": "labs/BSMM_8740_lab_git.html#introduction",
    "href": "labs/BSMM_8740_lab_git.html#introduction",
    "title": "Lab Git",
    "section": "",
    "text": "This lab is designed to reinforce your use of git and GitHub, the collaboration and version control system that we will be using throughout the course.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like â€œTrack Changesâ€ features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\nToday we begin with an introductions to GIT followed by exercises in the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\n\n\nBy the end of the lab, you willâ€¦\n\nHave practiced version control using GitHub\nBe familiar with the workflow using R, RStudio, Git, and GitHub\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!"
  },
  {
    "objectID": "labs/BSMM_8740_lab_git.html#lets-git-started",
    "href": "labs/BSMM_8740_lab_git.html#lets-git-started",
    "title": "Lab Git",
    "section": "Letâ€™s Git Started",
    "text": "Letâ€™s Git Started\n\n1. Register a Github account\nRegister an account with GitHub. Itâ€™s free!\n\nhttps://github.com\n\n\nUsername advice\n\nIncorporate your actual name! People like to know who theyâ€™re dealing with. Also it makes your username easier for people to guess or remember.\nShorter is better than longer.\nMake it timeless. Donâ€™t highlight your current university, employer, or place of residence, e.g.Â JennyFromTheBlock.\n\nYou can change your username later, but better to get this right the first time.\n\nhttps://help.github.com/articles/changing-your-github-username/\nhttps://help.github.com/articles/what-happens-when-i-change-my-username/\n\n\n\n\n\n\n\nNote\n\n\n\nWe will be switching between the console and the terminal, in this lab and others. The Console is where you can execute R code, while the Terminal is where you can execute system functions like git.\n\n\n\n\n\n\n\n\n\n\n2. Git already installed?\nGo to the Terminal tab in RStudio and enter git --version to see its version:\n\ngit --version\n\nIf this instruction gives an error, itâ€™s possible that git is not installed on your machine. If so, let your instructor know.\n\n\n3. Introduce yourself to Git\n\n\n\n\n\n\nImportant\n\n\n\nmake sure that the package usethis has been installed. You can check under the packages tab in the file & plots viewer (e.g., do a search).\n\n\nYou can set your Git user name and email from within R (i.e.Â go back to the Console tab):\n\nusethis::use_git_config(\n  # user.name does not have to be your GitHub user name\n  user.name = \"Jane Doe\"\n  # user.email MUST be the email associated with your GitHub account.\n  , user.email = \"jane@example.org\"\n)\n\n\n\n\n\n\n\nNote\n\n\n\nYour commits will be labelled with this user name, so make it informative to potential collaborators and future you.\n\n\n\n\n4. Set up personal access tokens for HTTPS\nThe password that you use to login to GitHubâ€™s website is NOT an acceptable credential when talking to GitHub as a Git server. You can learn more in their blog post Token authentication requirements for Git operations.\nThe recommendation to use a personal access token (PAT) is exactly what we cover here. First you need to create your PAT, and you can do this from R (in the Console):\n\nusethis::create_github_token()\n\nThe usethis approach takes you to a pre-filled form with pre-selected some recommended scopes, which you can look over and adjust before clicking â€œGenerate tokenâ€.\nIt is a very good idea to describe the tokenâ€™s purpose in the Note field, because one day you might have multiple PATs. We recommend naming each token after its use case, such as the computer or project you are using it for, e.g.Â â€œpersonal-macbook-airâ€ or â€œlab1-course-8740â€. In the future, you will find yourself staring at this list of tokens, because inevitably youâ€™ll need to re-generate or delete one of them. Make it easy to figure out which token youâ€™ve come here to fiddle with.\n\n\n\n\n\n\nTip\n\n\n\nIf this is your first time generating a PAT, just accept the defaults and scroll to the bottom of the page and click the green Generate token button.\n\n\n\n4.1 Click â€œGenerate tokenâ€.\nYou wonâ€™t be able to see this token again, so donâ€™t close or navigate away from this browser window until you store the PAT. Copy the PAT to the clipboard or a text file in RStudio.\nTreat this PAT like a password! Do not ever hard-wire your PAT into your code! A PAT should always be retrieved implicitly, for example, from the Git credential store, a safe place, where command line Git, RStudio, and R packages can discover it.\n\n\n4.2 Save your PAT\n\nCopy the generated PAT to a secure, long-term system for storing secrets, like 1Password or LastPass.\nemail it to yourself.\ncopy it onto a piece of scrap paper.\n\n\n\n4.3 Store your PAT in the Git credential store\nFinally, we store the PAT in a safe place where command line Git, RStudio, and R packages can discover it. To do this call gitcreds::gitcreds_set(). If you donâ€™t have a PAT stored already, it will prompt you to enter your PAT. Paste!\n\ngitcreds::gitcreds_set()\n\nInstead of saving your PAT you could just re-generate the PAT each lab session and re-store it. If you accept the default 30-day expiration period, this is a workflow youâ€™ll be using often anyway.\nOn github.com, assuming youâ€™re signed in, you can manage your personal access tokens from https://github.com/settings/tokens, also reachable via Settings &gt; Developer settings &gt; Personal access tokens.\n\n\n\n\n\n\nImportant\n\n\n\nGiven that the machines start from the same initial state each lab session, you will follow the above steps to initial your machine at the start of each lab session.\n\n\n\n\n\n5. How Git works\nGit has three storages locally: a Working directory, Staging Area, and a Local repository.\nğŸ­. ğ—ªğ—¼ğ—¿ğ—¸ğ—¶ğ—»ğ—´ ğ——ğ—¶ğ—¿ğ—²ğ—°ğ˜ğ—¼ğ—¿ğ˜† - is where you work, and your files live (â€œuntrackedâ€). GIT is not aware of these files.\nğŸ®. ğ—¦ğ˜ğ—®ğ—´ğ—¶ğ—»ğ—´ ğ—”ğ—¿ğ—²ğ—® - When you stage your changes, GIT will start tracking and saving your changes with files. These changes are stored in the .git directory.\nğŸ¯. ğ—Ÿğ—¼ğ—°ğ—®ğ—¹ ğ—¥ğ—²ğ—½ğ—¼ğ˜€ğ—¶ğ˜ğ—¼ğ—¿ğ˜† - is the area where everything is saved (commits) in the .git directory. So, when you want to move your files from Staging Area to Local Repository, you can use the git commit command. After this, your Staging area will be empty. If you want to see what is in the Local repository, try git log.\nThe workflow looks like this:\n\n\n\nGit workflow\n\n\nYou are now ready interact with GitHub via RStudio!\n\n\nClone the repo & start new RStudio project\n\nFirst make sure you are logged into your own Github account on a web browser.\nNext, in a new browser tab, go to the course organization site at BSMM-8740-Fall-2023 on GitHub. Click on the repo BSMM-lab-1. It contains the starter documents you need to complete the lab.\nClick on the green Use this template button and select Create a new repository. This will make a copy of BSMM-lab-1 in your own github account.\nNext, go back to your Github acount and select your copy of BSMM-lab-1. Click on the green CODE button, select Use HTTPS (this might already be selected by default, and if it is, youâ€™ll see the text Clone with HTTPS). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File â› New Project â›Version Control â› Git.\nCopy and paste the URL of your assignment repo (the clipboard copy you made in step 4) into the dialog box Repository URL. The project directory name should be automatically populated, but make sure you select a directory in Create project as a subdirectory of.\nFinally, click Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick bsmm-lab-1.qmd to open the template R Markdown file. This is where you will write up your code and narrative for the lab.\n\n\n\nR and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\nYAML\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for â€œYAML Ainâ€™t Markup Languageâ€. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (`.qmd`) file in your project, change the author name to your name, and render the document. Examine the rendered HTML document in the Files pane.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your *.qmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf youâ€™re happy with these changes, weâ€™ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, â€œupdated author nameâ€) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou donâ€™t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\n\n\nPush changes\nNow that you have made an update and committed this change, itâ€™s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push.\nNow letâ€™s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and youâ€™re good to go!\nMore on the basic use of git here."
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html",
    "href": "labs/BSMM_8740_lab_6.html",
    "title": "lab 6 - Time Series Methods",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflows with recipes, parsnip models, rsample cross validations, and model comparison in the context of time series data."
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#introduction",
    "href": "labs/BSMM_8740_lab_6.html#introduction",
    "title": "lab 6 - Time Series Methods",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflows with recipes, parsnip models, rsample cross validations, and model comparison in the context of time series data."
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#getting-started",
    "href": "labs/BSMM_8740_lab_6.html#getting-started",
    "title": "lab 6 - Time Series Methods",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-6-[your github username] repository .\nCreate an R project using your 2024-lab-6-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-6.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#packages",
    "href": "labs/BSMM_8740_lab_6.html#packages",
    "title": "lab 6 - Time Series Methods",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#the-data",
    "href": "labs/BSMM_8740_lab_6.html#the-data",
    "title": "lab 6 - Time Series Methods",
    "section": "The Data",
    "text": "The Data\nToday we will be using electricity demand data, based on a paper by James W Taylor:\n\nTaylor, J.W. (2003) Short-term electricity demand forecasting using double seasonal exponential smoothing. Journal of the Operational Research Society, 54, 799-805.\n\nThe data can be found in the timetk package as timetk::taylor_30_min, a tibble with dimensions: 4,032 x 2\n\ndate: A date-time variable in 30-minute increments\nvalue: Electricity demand in Megawatts\n\n\ndata &lt;- timetk::taylor_30_min"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-1-eda",
    "href": "labs/BSMM_8740_lab_6.html#exercise-1-eda",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nPlot the data using the functions timetk::plot_time_series, timetk::plot_acf_diagnostics (using 100 lags), and timetk::plot_seasonal_diagnostics.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# timetk::plot_time_series\n\n\n# timetk::plot_acf_diagnostics\n\n\n# timetk::plot_seasonal_diagnostics"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-2-time-scaling",
    "href": "labs/BSMM_8740_lab_6.html#exercise-2-time-scaling",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 2: Time scaling",
    "text": "Exercise 2: Time scaling\nThe raw data has 30 minute intervals between data points. Downscale the data to 60 minute intervals, using timetk::summarise_by_time, revising the electricity demand (value) variable by adding the two 30-minute intervals in each 60-minute interval. Assign the downscaled data to the variable taylor_60_min.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# downscale the data (down to a lower frequency of measurement)\nset.seed(8740)\ntaylor_60_min &lt;- ?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-3-training-and-test-datasets",
    "href": "labs/BSMM_8740_lab_6.html#exercise-3-training-and-test-datasets",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 3: Training and test datasets",
    "text": "Exercise 3: Training and test datasets\n\nSplit the new (60 min) time series into training and test sets using timetk::time_series_split\n\nset the training period (â€˜initialâ€™) to â€˜2 monthsâ€™ and the assessment period to â€˜1 weeksâ€™\n\nPrepare the data resample specification with timetk::tk_time_series_cv_plan() and plot it with timetk::plot_time_series_cv_plan\nSeparate the training and test data sets using rsample .\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# split\n\n\n# plot splits\n\n\n# separate into train and test data sets"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-4-recipes",
    "href": "labs/BSMM_8740_lab_6.html#exercise-4-recipes",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 4: recipes",
    "text": "Exercise 4: recipes\n\nCreate a base recipe (base_rec) using the formula value ~ date and the training data. This will be used for non-regression models\nCreate a recipe (lm_rec) using the formula value ~ . and the training data. This will be used for regression models. For this recipe:\n\nadd time series signature features using timetk::step_timeseries_signature with the appropriate argument,\nadd a step to select the columns value, date_index.num, date_month.lbl, date_wday.lbl, date_hour ,\nadd a normalization step targeting date_index.num ,\nadd a step to mutate date_hour, changing it to a factor,\nadd a step to one-hot encode nominal predictors.\n\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nbase_rec &lt;- ?\n  \nlm_rec &lt;- ?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-5-models",
    "href": "labs/BSMM_8740_lab_6.html#exercise-5-models",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 5 models",
    "text": "Exercise 5 models\nNow we will create a several models to estimate electricity demand, as follows\n\nCreate a model specification for an exponential smoothing model using engine â€˜etsâ€™\nCreate a model specification for an arima model using engine â€˜auto_arimaâ€™\nCreate a model specification for a linear model using engine â€˜glmnetâ€™ and penalty = 0.02, mixture = 0.5\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nmodel_ets &lt;- ?\n  \nmodel_arima &lt;- ?\n  \nmodel_lm &lt;- ?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-6-model-fitting",
    "href": "labs/BSMM_8740_lab_6.html#exercise-6-model-fitting",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 6 model fitting",
    "text": "Exercise 6 model fitting\nCreate a workflow for each model using workflows::workflow.\n\nAdd a recipe to the workflow\n\nthe linear model uses the lm_rec recipe created above\nthe ets and arima models use the base_rec recipe created above\n\nAdd a model to each workflow\nFit with the training data\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nworkflow_fit_ets &lt;- ?\n  \nworkflow_fit_arima &lt;- ?\n  \nworkflow_fit_lm &lt;- ?\n\n\n\n\nThis is a good place to save, stage, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-7-calibrate",
    "href": "labs/BSMM_8740_lab_6.html#exercise-7-calibrate",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 7: calibrate",
    "text": "Exercise 7: calibrate\nIn this exercise weâ€™ll use the testing data with our fitted models.\n\nCreate a table with the fitted workflows using modeltime::modeltime_table\nUsing the table you just created, run a calibration on the test data with the function modeltime::modeltime_calibrate.\nCompare the accuracy of the models using the modeltime::modeltime_accuracy() on the results of the calibration\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nmodel_tbl &lt;- modeltime::modeltime_table( ? )\n\ncalibration_tbl &lt;- ?\n  \ncalibration_tbl |&gt; ?\n\n\n\n\n\n\n\nImportant\n\n\n\nWhich is the best model by the rmse metric?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-8-forecast---training-data",
    "href": "labs/BSMM_8740_lab_6.html#exercise-8-forecast---training-data",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 8: forecast - training data",
    "text": "Exercise 8: forecast - training data\nUse the calibration table with modeltime::modeltime_forecast to graphically compare the fits to the training data with the observed values.\n\n\n\n\n\n\nYOUR ANSWER:"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#exercise-9-forecast---future",
    "href": "labs/BSMM_8740_lab_6.html#exercise-9-forecast---future",
    "title": "lab 6 - Time Series Methods",
    "section": "Exercise 9: forecast - future",
    "text": "Exercise 9: forecast - future\nNow refit the models using the full data set (using the calibration table and modeltime::modeltime_refit). Save the result in the variable refit_tbl.\n\nUse the refit data in the variable refit_tbl, along with modeltime::modeltime_forecast and argument h = â€˜2 weeksâ€™ (remember to also set the actual_data argument). This will use the models to forecast electricity demand two weeks into the future.\nPlot the forecast with modeltime::plot_modeltime_forecast\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nrefit_tbl &lt;-  ?\n\n\n\n\nYouâ€™re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message â€œDone with Lab 6!â€ , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyoneâ€™s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (donâ€™t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_6.html#grading",
    "href": "labs/BSMM_8740_lab_6.html#grading",
    "title": "lab 6 - Time Series Methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html",
    "href": "labs/BSMM_8740_lab_4.html",
    "title": "Lab 4 - The TidyModels Package",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to build workflows to evaluate different models and feature sets."
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#introduction",
    "href": "labs/BSMM_8740_lab_4.html#introduction",
    "title": "Lab 4 - The TidyModels Package",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to build workflows to evaluate different models and feature sets."
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#getting-started",
    "href": "labs/BSMM_8740_lab_4.html#getting-started",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to your github account and then go to the GitHub organization for the course and find the 2024-lab-4-[your github username] repository to complete the lab.\nCreate an R project using your 2024-lab-4-[your github username] repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the 2024-lab-4.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#packages",
    "href": "labs/BSMM_8740_lab_4.html#packages",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, tidymodels, modeldata, ranger, rsample, broom, recipes, parsnip\n)\n\n# set the efault theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#the-data",
    "href": "labs/BSMM_8740_lab_4.html#the-data",
    "title": "Lab 4 - The TidyModels Package",
    "section": "The Data",
    "text": "The Data\nToday we will be using the Ames Housing Data.\nThis is a data set from De Cock (2011) has 82 fields were recorded for 2,930 properties in Ames Iowa in the US. The version in the modeldata package is copied from the AmesHousing package but does not include a few quality columns that appear to be outcomes rather than predictors.\n\ndat &lt;- modeldata::ames\n\nThe data dictionary can be found on the internet:\n\ncat(readr::read_file(\"http://jse.amstat.org/v19n3/decock/DataDocumentation.txt\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-1-eda",
    "href": "labs/BSMM_8740_lab_4.html#exercise-1-eda",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nWrite and execute the code to perform summary EDA on the Ames Housing data using the package skimr.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-2-train-test-splits",
    "href": "labs/BSMM_8740_lab_4.html#exercise-2-train-test-splits",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 2: Train / Test Splits",
    "text": "Exercise 2: Train / Test Splits\nWrite and execute code to create training and test datasets. Have the training dataset represent 75% of the total data.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nset.seed(8740)\ndata_split &lt;- rsample::__\n\names_train &lt;- rsample::__\names_test  &lt;- rsample::__"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-3-data-preprocessing",
    "href": "labs/BSMM_8740_lab_4.html#exercise-3-data-preprocessing",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 3: Data Preprocessing",
    "text": "Exercise 3: Data Preprocessing\ncreate a recipe based on the formula Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold with the following steps:\n\ntransform the variable Sale_Price to log(Sale_Price)\ncenter and scale all predictors\ncreate dummy variables for all nominal variables\ntransform the variable Neighborhood to pool infrequent values (see recipes::step_other)\n\nFinally prep the recipe.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\nnorm_recipe &lt;- \n  recipes::recipe( ___ ) %&gt;% \n  ...\n  recipes::prep( ___ ) %&gt;% broom::tidy()"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-4-modeling",
    "href": "labs/BSMM_8740_lab_4.html#exercise-4-modeling",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 4 Modeling",
    "text": "Exercise 4 Modeling\nCreate three regression models\n\na base regression model using lm\na regression model using glmnet; set the model parameters penalty and mixture for tuning\na tree model using the ranger engine; set the model parameters min_n and trees for tuning\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\nlm_mod_base &lt;- \n  parsnip::linear_reg( ___ ) %&gt;% ___\n\nlm_mod_glmnet &lt;- \n  parsnip::linear_reg( ___ ) %&gt;% ___\n\nlm_mod_rforest &lt;- \n  parsnip::rand_forest( ___ ) %&gt;% ___"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-5",
    "href": "labs/BSMM_8740_lab_4.html#exercise-5",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse parsnip::translate() on each model to see the code object that is specific to a particular engine\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-6-bootstrap",
    "href": "labs/BSMM_8740_lab_4.html#exercise-6-bootstrap",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 6 Bootstrap",
    "text": "Exercise 6 Bootstrap\nCreate bootstrap samples for the training dataset. You can leave the parameters set to their defaults\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\nset.seed(8740)\ntrain_resamples &lt;- ___ %&gt;% rsample::bootstraps()\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-7",
    "href": "labs/BSMM_8740_lab_4.html#exercise-7",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 7",
    "text": "Exercise 7\nCreate workflows with workflowsets::workflow_set using your recipe and models.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nall_workflows &lt;- \n  workflowsets::workflow_set(\n    preproc = list(base = norm_recipe),\n    models = list(base = lm_mod_base, glmnet = lm_mod_glmnet, forest = lm_mod_rforest)\n  )"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-8",
    "href": "labs/BSMM_8740_lab_4.html#exercise-8",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 8",
    "text": "Exercise 8\nMap the default function (tune::tune_grid()) across the workflows in the workflowset you just created and update the variable all_workflows with the result\n\nall_workflows &lt;- all_workflows %&gt;% \n  workflowsets::workflow_map(\n    verbose = TRUE                # enable logging\n    , resamples = train_resamples # a parameter passed to tune::tune_grid()\n    , grid = 5                    # a parameter passed to tune::tune_grid()\n  )\n\nThe updated variable all_workflows contains a nested column named result, and each cell of the column result is a tibble containing a nested column named .metrics. Write code to\n\nun-nest the metrics in the column .metrics\nfilter out the rows for the metric rmse\ngroup by wflow_id, order the .estimate column from highest to lowest, and pick out the first row of each group\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nall_workflows %&gt;% \n  dplyr::select(wflow_id,__) %&gt;% \n  tidyr::unnest(__) %&gt;% \n  dplyr::select(wflow_id,__) %&gt;% \n  tidyr::unnest(__) %&gt;% \n  dplyr::filter(.metric == '___') %&gt;% \n  dplyr::group_by(wflow_id) %&gt;% \n  dplyr::arrange(desc(__) ) %&gt;% \n  dplyr::slice(1)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-9",
    "href": "labs/BSMM_8740_lab_4.html#exercise-9",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 9",
    "text": "Exercise 9\nRun the code below and compare to your results from exercise 8.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nworkflowsets::rank_results(all_workflows, rank_metric = \"rmse\", select_best = TRUE)\n\n# compare to your results from exercise 8."
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#exercise-10",
    "href": "labs/BSMM_8740_lab_4.html#exercise-10",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Exercise 10",
    "text": "Exercise 10\nSelect the best model per the rsme metric using its id.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nbest_model_workflow &lt;- \n  all_workflows %&gt;% \n  workflowsets::extract_workflow(\"__\")\n\n\n\nFinalize the workflow by setting the parameters for the best model\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\nbest_model_workflow &lt;- \n  best_model_workflow %&gt;% \n  tune::finalize_workflow(\n    tibble::tibble(__ = __, __ = __) # enter the name and value of the best-fit parameters\n  ) \n\n\n\nNow compare the fits\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\ntraining_fit &lt;- best_model_workflow %&gt;% \n  fit(data = ames_train)\n\ntesting_fit &lt;- best_model_workflow %&gt;% \n  fit(data = ames_test)\n\n\n# What is the ratio of the OOB prediction errors (MSE): test/train?\n\n\n\n\nYouâ€™re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message â€œDone with Lab 4!â€, and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyoneâ€™s submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (donâ€™t forget to commit and then push your work by 5:00pm on Sunday!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_4.html#grading",
    "href": "labs/BSMM_8740_lab_4.html#grading",
    "title": "Lab 4 - The TidyModels Package",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html",
    "title": "Lab 2 - The Recipes package",
    "section": "",
    "text": "We will use the following package in this lab.\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2\n)\n\ntheme_set(theme_bw(base_size = 12))\nboston_cocktails &lt;- readr::read_csv('data/boston_cocktails.csv', show_col_types = FALSE)\n\n\n\n\n\n\n\nTip\n\n\n\nI like to use the gt:: and gtExtras:: packages to format my tables. The results can be saved as an image and are especially useful for inserting into PowerPoint decks.\nYouâ€™ll see examples thoughout the course and I encourage you to examine the examples and use gt:: and gtExtras:: in your own work."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html#packages",
    "title": "Lab 2 - The Recipes package",
    "section": "",
    "text": "We will use the following package in this lab.\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2\n)\n\ntheme_set(theme_bw(base_size = 12))\nboston_cocktails &lt;- readr::read_csv('data/boston_cocktails.csv', show_col_types = FALSE)\n\n\n\n\n\n\n\nTip\n\n\n\nI like to use the gt:: and gtExtras:: packages to format my tables. The results can be saved as an image and are especially useful for inserting into PowerPoint decks.\nYouâ€™ll see examples thoughout the course and I encourage you to examine the examples and use gt:: and gtExtras:: in your own work."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html#data-the-boston-cocktail-recipes",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html#data-the-boston-cocktail-recipes",
    "title": "Lab 2 - The Recipes package",
    "section": "Data: The Boston Cocktail Recipes",
    "text": "Data: The Boston Cocktail Recipes\nThe Boston Cocktail Recipes dataset appeared in a TidyTuesday posting. TidyTuesday is a weekly data project in R.\nThe dataset is derived from the Mr.Â Boston Bartenderâ€™s Guide, together with a dataset that was web-scraped as part of a hackathon.\nThis dataset contains the following information for each cocktail:\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of cocktail\n\n\ncategory\ncharacter\nCategory of cocktail\n\n\nrow_id\ninteger\nDrink identifier\n\n\ningredient_number\ninteger\nIngredient number\n\n\ningredient\ncharacter\nIngredient\n\n\nmeasure\ncharacter\nMeasurement/volume of ingredient\n\n\nmeasure_number\nreal\nmeasure as a number"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html#exercises",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html#exercises",
    "title": "Lab 2 - The Recipes package",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nFirst use skimr::skim and DataExplorer::introduce to assess the quality of the data set.\nNext prepare a summary. What is the median measure number across cocktail recipes?\n\n\n\n\n\n\nSOLUTION:\n\n\n\nboston_cocktails %&gt;% skimr::skim()\n\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2542\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n4\n36\n0\n937\n0\n\n\ncategory\n0\n1\n3\n21\n0\n11\n0\n\n\ningredient\n0\n1\n3\n23\n0\n40\n0\n\n\nmeasure\n0\n1\n1\n8\n0\n28\n0\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrow_id\n0\n1\n495.37\n284.75\n1.00\n261.25\n497\n730.75\n990\nâ–‡â–‡â–‡â–‡â–‡\n\n\ningredient_number\n0\n1\n2.54\n1.29\n1.00\n1.00\n2\n3.00\n6\nâ–‡â–ƒâ–‚â–â–\n\n\nmeasure_number\n0\n1\n0.98\n0.72\n0.02\n0.50\n1\n1.50\n16\nâ–‡â–â–â–â–\n\n\n\n\n\n\n\nboston_cocktails %&gt;% summary()\n\n     name             category             row_id      ingredient_number\n Length:2542        Length:2542        Min.   :  1.0   Min.   :1.000    \n Class :character   Class :character   1st Qu.:261.2   1st Qu.:1.000    \n Mode  :character   Mode  :character   Median :497.0   Median :2.000    \n                                       Mean   :495.4   Mean   :2.545    \n                                       3rd Qu.:730.8   3rd Qu.:3.000    \n                                       Max.   :990.0   Max.   :6.000    \n  ingredient          measure          measure_number   \n Length:2542        Length:2542        Min.   : 0.0200  \n Class :character   Class :character   1st Qu.: 0.5000  \n Mode  :character   Mode  :character   Median : 1.0000  \n                                       Mean   : 0.9797  \n                                       3rd Qu.: 1.5000  \n                                       Max.   :16.0000  \n\n\nThe median measure is 1.0. Note that the dimensions are identified as ounces (oz) in the measure column.\n\n\n\n\nExercise 2\nFrom the boston_cocktails dataset select the name, category, ingredient, and measure_number columns and then pivot the table to create a column for each ingredient. Fill any missing values with the number zero.\nSince the names of the new columns may contain spaces, clean them using the janitor::clean_names(). Finally drop any rows with NA values and save this new dataset in a variable.\nHow much gin is in the cocktail called Leap Frog Highball?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ncocktails_df &lt;- boston_cocktails %&gt;%\n  # select the columns (by de-selecting the ones we don't want)\n  dplyr::select(-ingredient_number, -row_id, -measure) %&gt;%\n  # pivot wider (make more columns); use zeros in place of NA values\n  tidyr::pivot_wider(\n    names_from = ingredient\n    , values_from = measure_number\n    , values_fill = 0\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  tidyr::drop_na()\n# show the table in the document\ncocktails_df\n\n# A tibble: 937 Ã— 42\n   name    category light_rum lemon_juice lime_juice sweet_vermouth orange_juice\n   &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1 Gauguin Cocktaiâ€¦      2           1          1               0           0   \n 2 Fort Lâ€¦ Cocktaiâ€¦      1.5         0          0.25            0.5         0.25\n 3 Cuban â€¦ Cocktaiâ€¦      2           0          0.5             0           0   \n 4 Cool Câ€¦ Cocktaiâ€¦      0           0          0               0           1   \n 5 John Câ€¦ Whiskies      0           1          0               0           0   \n 6 Cherryâ€¦ Cocktaiâ€¦      1.25        0          0               0           0   \n 7 Casa Bâ€¦ Cocktaiâ€¦      2           0          1.5             0           0   \n 8 Caribbâ€¦ Cocktaiâ€¦      0.5         0          0               0           0   \n 9 Amber â€¦ Cordialâ€¦      0           0.25       0               0           0   \n10 The Joâ€¦ Whiskies      0           0.5        0               0           0   \n# â„¹ 927 more rows\n# â„¹ 35 more variables: powdered_sugar &lt;dbl&gt;, dark_rum &lt;dbl&gt;,\n#   cranberry_juice &lt;dbl&gt;, pineapple_juice &lt;dbl&gt;, bourbon_whiskey &lt;dbl&gt;,\n#   simple_syrup &lt;dbl&gt;, cherry_flavored_brandy &lt;dbl&gt;, light_cream &lt;dbl&gt;,\n#   triple_sec &lt;dbl&gt;, maraschino &lt;dbl&gt;, amaretto &lt;dbl&gt;, grenadine &lt;dbl&gt;,\n#   apple_brandy &lt;dbl&gt;, brandy &lt;dbl&gt;, gin &lt;dbl&gt;, anisette &lt;dbl&gt;,\n#   dry_vermouth &lt;dbl&gt;, apricot_flavored_brandy &lt;dbl&gt;, bitters &lt;dbl&gt;, â€¦\n\n\n\ncocktails_df %&gt;% \n  # filter for the desired cocktail\n  dplyr::filter(name == 'Leap Frog Highball') %&gt;% \n  dplyr::pull(gin)\n\n[1] 2\n\n\nTwo ounces (oz) of gin are in the Leap Frog Highball.\n\n\n\n\nExercise 3\nPrepare a recipes::recipe object without a target but give name and category as â€˜idâ€™ roles. Add steps to normalize the predictors and perform PCA. Finally prep the data and save it in a variable.\nHow many predictor variables are prepped by the recipe?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create a recipe: y~. with an outcome/target, but here we just use ~.\npca_rec &lt;- recipes::recipe(~., data = cocktails_df) \npca_rec %&gt;% summary()\n\n# A tibble: 42 Ã— 4\n   variable        type      role      source  \n   &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 name            &lt;chr [3]&gt; predictor original\n 2 category        &lt;chr [3]&gt; predictor original\n 3 light_rum       &lt;chr [2]&gt; predictor original\n 4 lemon_juice     &lt;chr [2]&gt; predictor original\n 5 lime_juice      &lt;chr [2]&gt; predictor original\n 6 sweet_vermouth  &lt;chr [2]&gt; predictor original\n 7 orange_juice    &lt;chr [2]&gt; predictor original\n 8 powdered_sugar  &lt;chr [2]&gt; predictor original\n 9 dark_rum        &lt;chr [2]&gt; predictor original\n10 cranberry_juice &lt;chr [2]&gt; predictor original\n# â„¹ 32 more rows\n\n\n\npca_rec &lt;- pca_rec %&gt;% \n  # change the roles of name and category to 'id' from 'predictor'\n  recipes::update_role(name, category, new_role = \"id\") %&gt;%\n  # normalize the remaining predictors\n  recipes::step_normalize(all_predictors()) %&gt;%\n  # convert the predictors to principle components\n  recipes::step_pca(all_predictors())\n\n# note there are 40 predictors, but that nothing has been calculated yet\npca_rec %&gt;% summary()\n\n# A tibble: 42 Ã— 4\n   variable        type      role      source  \n   &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 name            &lt;chr [3]&gt; id        original\n 2 category        &lt;chr [3]&gt; id        original\n 3 light_rum       &lt;chr [2]&gt; predictor original\n 4 lemon_juice     &lt;chr [2]&gt; predictor original\n 5 lime_juice      &lt;chr [2]&gt; predictor original\n 6 sweet_vermouth  &lt;chr [2]&gt; predictor original\n 7 orange_juice    &lt;chr [2]&gt; predictor original\n 8 powdered_sugar  &lt;chr [2]&gt; predictor original\n 9 dark_rum        &lt;chr [2]&gt; predictor original\n10 cranberry_juice &lt;chr [2]&gt; predictor original\n# â„¹ 32 more rows\n\n\n\n# calculate prepare the data per the steps in the recipe\npca_prep &lt;- recipes::prep(pca_rec)\npca_prep %&gt;% summary\n\n# A tibble: 7 Ã— 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 name     &lt;chr [3]&gt; id        original\n2 category &lt;chr [3]&gt; id        original\n3 PC1      &lt;chr [2]&gt; predictor derived \n4 PC2      &lt;chr [2]&gt; predictor derived \n5 PC3      &lt;chr [2]&gt; predictor derived \n6 PC4      &lt;chr [2]&gt; predictor derived \n7 PC5      &lt;chr [2]&gt; predictor derived \n\n\n\nThere are 40 predictors and 2 id variables before the data is prepped.\nOnce prepped, the PCA returns just 5 components by default, so we have (post-prep) 5 predictors and 2 id variables.\n\n\n\n\n\nExercise 4\nApply the recipes::tidy verb to the prepped recipe in the last exercise. The result is a table identifying the information generated and stored by each step in the recipe from the input data.\nTo see the values calculated for normalization, apply the recipes::tidy verb as before, but with second argument = 1.\nWhat ingredient is the most used, on average?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# tidy returns a tibble with the calculations performed by prep\npca_prep %&gt;% recipes::tidy()\n\n# A tibble: 2 Ã— 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_kssgp\n2      2 step      pca       TRUE    FALSE pca_LwyKj      \n\n\n\n# if we select the first (normalization) step we get the values calculated:\n# - the mean and standard deviation for each variable\nfoo &lt;- pca_prep %&gt;% recipes::tidy(1)\nfoo\n\n# A tibble: 80 Ã— 4\n   terms           statistic  value id             \n   &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;          \n 1 light_rum       mean      0.161  normalize_kssgp\n 2 lemon_juice     mean      0.229  normalize_kssgp\n 3 lime_juice      mean      0.138  normalize_kssgp\n 4 sweet_vermouth  mean      0.0691 normalize_kssgp\n 5 orange_juice    mean      0.185  normalize_kssgp\n 6 powdered_sugar  mean      0.0891 normalize_kssgp\n 7 dark_rum        mean      0.0454 normalize_kssgp\n 8 cranberry_juice mean      0.0363 normalize_kssgp\n 9 pineapple_juice mean      0.0608 normalize_kssgp\n10 bourbon_whiskey mean      0.0768 normalize_kssgp\n# â„¹ 70 more rows\n\n\n\n# we can just filter to find the largest mean value\n# first, isolate the mean values\nfoo %&gt;% dplyr::filter(statistic == 'mean') %&gt;% \n  # once we have just the mean values, filter out the row with the max value \n  dplyr::filter(value == max(value))\n\n# A tibble: 1 Ã— 4\n  terms statistic value id             \n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;          \n1 gin   mean      0.252 normalize_kssgp\n\n\nOn average, it is gin that is the largest component of the cocktails, with just over 1/4 oz per cocktail.\n\n\n\n\nExercise 5\nNow look at the result of the PCA, applying the recipes::tidy verb as before, but with second argument = 2. Save the result in a variable and filter for the components PC1 to PC5. Mutate the resulting component column so that the values are factors, ordering them in the order they appear using the forcats::fct_inorder verb.\nPlot this data using ggplot2 and the code below\n\nggplot(aes(value, terms, fill = terms)) +\ngeom_col(show.legend = FALSE) +\nfacet_wrap(~component, nrow = 1) +\nlabs(y = NULL) +\ntheme(axis.text=element_text(size=7),\n      axis.title=element_text(size=14,face=\"bold\"))\n\nHow would you describe the drinks represented by PC1?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# the tidy operation shows the weights of each ingredient, \n# for each principal component.\n# - i.e. PC1 (along with PC2 - PC5) is a weighted sum of ingredients\nbar &lt;- pca_prep %&gt;% recipes::tidy(2)\nbar\n\n# A tibble: 1,600 Ã— 4\n   terms             value component id       \n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 light_rum        0.163  PC1       pca_LwyKj\n 2 lemon_juice     -0.0140 PC1       pca_LwyKj\n 3 lime_juice       0.224  PC1       pca_LwyKj\n 4 sweet_vermouth  -0.0661 PC1       pca_LwyKj\n 5 orange_juice     0.0308 PC1       pca_LwyKj\n 6 powdered_sugar  -0.476  PC1       pca_LwyKj\n 7 dark_rum         0.124  PC1       pca_LwyKj\n 8 cranberry_juice  0.0954 PC1       pca_LwyKj\n 9 pineapple_juice  0.119  PC1       pca_LwyKj\n10 bourbon_whiskey  0.0963 PC1       pca_LwyKj\n# â„¹ 1,590 more rows\n\n\n\n# plot to show the ingredient weights\nbar %&gt;%\n  # since there are only 5 components, this is redundant\n  dplyr::filter(component %in% paste0(\"PC\", 1:5)) %&gt;%\n  # change component from a character to a factor, and give them an order\n  dplyr::mutate(component = forcats::fct_inorder(component)) %&gt;%\n  # plot\n  ggplot(aes(value, terms, fill = terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~component, nrow = 1) +\n  labs(y = NULL) +\n  theme(axis.text=element_text(size=7),\n        axis.title=element_text(size=14,face=\"bold\"))\n\n\n\n\n\n\n\n\nBased on the its largest components (in absolute value) PC1 is a syrupy (not sugary) drink, containing tequila and lime juice but without egg products.\n\n\n\n\nExercise 6\nAs in the last exercise, use the variable with the tidied PCA data and use only PCA components PC1 to PC4. Take/slice the top 8 ingedients by component, ordered by their absolute value using the verb dplyr::slice_max. Next, generate a grouped table using gt::gt, colouring the cell backgrounds (i.e.Â fill) with green for values â‰¥0\\ge0 and red for values &lt;0&lt;0.\nWhat is the characteristic alcoholic beverage of each of the first 4 principle components.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nbar %&gt;%\n  # filter our the rows for PC1 - PC4\n  dplyr::filter(component %in% paste0(\"PC\", 1:4)) %&gt;%\n  # group by component (i.e. principal component) \n  dplyr::group_by(component) %&gt;%\n  # for each group, take the top 8 ingredients by absolute value\n  dplyr::slice_max(n = 8, order_by = abs(value)) %&gt;% \n  # now make a nicely formatted table\n  gt::gt() %&gt;% \n  # make/apply a table style: this one for values &lt; 0\n  gt::tab_style(\n    style = list(\n      gt::cell_fill(color = \"red\"),\n      gt::cell_text(weight = \"bold\")\n      ),\n    locations = gt::cells_body(\n      columns = value,\n      rows = value &lt; 0\n    )\n  ) %&gt;% \n  # make/apply another table style: this one for values &gt;= 0\n    gt::tab_style(\n    style = list(\n      gt::cell_fill(color = \"green\"),\n      gt::cell_text(weight = \"bold\")\n      ),\n    locations = gt::cells_body(\n      columns = value,\n      rows = value &gt;= 0\n    )\n  ) %&gt;% \n  # apply a theme; any theme will do\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nterms\nvalue\nid\n\n\n\n\nPC1\n\n\npowdered_sugar\n-0.4764442\npca_LwyKj\n\n\nsimple_syrup\n0.3125978\npca_LwyKj\n\n\nwhole_egg\n-0.2903794\npca_LwyKj\n\n\negg_white\n-0.2643614\npca_LwyKj\n\n\ngin\n-0.2588481\npca_LwyKj\n\n\nport\n-0.2354010\npca_LwyKj\n\n\nlime_juice\n0.2240295\npca_LwyKj\n\n\nblanco_tequila\n0.2025436\npca_LwyKj\n\n\nPC2\n\n\ndry_vermouth\n0.4332412\npca_LwyKj\n\n\nsweet_vermouth\n0.3754366\npca_LwyKj\n\n\npowdered_sugar\n-0.3273723\npca_LwyKj\n\n\nlemon_juice\n-0.2651836\npca_LwyKj\n\n\nsimple_syrup\n-0.2512150\npca_LwyKj\n\n\ngin\n0.2437374\npca_LwyKj\n\n\nwhole_egg\n-0.2191547\npca_LwyKj\n\n\nport\n-0.1763370\npca_LwyKj\n\n\nPC3\n\n\ngin\n0.3780348\npca_LwyKj\n\n\negg_white\n0.3094631\npca_LwyKj\n\n\nbenedictine\n-0.2864463\npca_LwyKj\n\n\nwhole_egg\n-0.2825812\npca_LwyKj\n\n\nblended_scotch_whiskey\n-0.2352922\npca_LwyKj\n\n\nlemon_juice\n0.2318927\npca_LwyKj\n\n\nvodka\n-0.2167814\npca_LwyKj\n\n\napricot_flavored_brandy\n0.2134899\npca_LwyKj\n\n\nPC4\n\n\ngrenadine\n0.4068737\npca_LwyKj\n\n\norange_juice\n0.3625622\npca_LwyKj\n\n\nvodka\n0.3008406\npca_LwyKj\n\n\nsimple_syrup\n-0.2533007\npca_LwyKj\n\n\nhalf_and_half\n0.2493552\npca_LwyKj\n\n\negg_white\n0.2465323\npca_LwyKj\n\n\ncranberry_juice\n0.2228099\npca_LwyKj\n\n\nwhite_creme_de_cacao\n0.2089063\npca_LwyKj\n\n\n\n\n\n\n\nPrincipal components and similar methods are very useful in reducing the complexity of our models. In this case we reduced the 40 original predictors to just 5 predictors.\nThe challenge with using these methods is attaching meaning to the revised predictors, and this is important when we need to explain our models. The computer can compute the new predictors but they canâ€™t tell us what they represent. For this we need to look at the structure of the new predictors and see if we can attach some meaning to them; often the solution is to give them names that capture the underlying structure.\nIn this case, looking at the ingredients that make up the PCA predictors\n\nPC1 represents a drink with:\n\nlittle or no sugar, egg, gin or port; some or a lot of syrup and citrus juice\nmost often / mainly tequila\n\nPC2 represents a drink with:\n\nlittle or no sugar, syrup, or citrus juice\nmost often / mainly vermouth\n\nPC3 represents a drink with:\n\nlittle or no egg, whiskey or vodka\nmost often / mainly gin\n\nPC4 represents a drink with\n\nlittle or no syrup; some or a lot of juice and dairy product\nmost often / mainly grenadine and vodka\n\n\n\n\n\n\nExercise 7\nFor this exercise, bake the prepped PCA recipe using recipes::bake on the original data and plot each cocktail by its PC1, PC2 component, using\n\nggplot(aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  geom_text(check_overlap = TRUE, hjust = \"inward\") + \n  labs(color = NULL)\n\nCan you create an interpretation of the PCA analysis?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# bake the dataset cocktails_df, creating a new dataset\nrecipes::bake(pca_prep, new_data = cocktails_df) %&gt;%\n  ggplot(aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  geom_text(check_overlap = TRUE, hjust = \"inward\") + \n  labs(color = NULL)\n\n\n\n\n\n\n\n\nIn this exercise we are plotting the cocktails against the first two principal components and trying to interpret the results.\nThe interpretation is more difficult now than it was in the last exercise where we found an interpretation for each principal component. Now we are looking at each cocktail in terms of combinations of PC1 and PC2, both positive and negative.\nIt appears that the lower left quadrant (negative PC1 and PC2 values) are mainly cocktail classics. If you look at the components of PC1 and PC2 from the last exercise and negate them, you can describe the cocktails in this quadrant: they have egg /egg-white, port, sugar, no vermouth or gin or tequila.\nThe lower right quadrant is positive PC1 and negative PC2. PC1 contains juice and syrup, while PC2 is negative juice and syrup - so in this quadrant we have cocktails that are like PC1 (juice and syrup) and unlike PC2 (juice and syrup & sugar). The cocktails in this quadrant have citrus juice, are sweet from the use of syrup & sugar and likely have tequila.\nThe top half of the plot shows cocktails clustered along the PC1=0 axis, so those are mainly tequila cocktails, very much like PC2.\nIs there an opportunity to create a new cocktail in the upper left or upper right quadrants?\n\n\n\n\nExercise 8\nIn the following exercise, weâ€™ll use the recipes package to prepare time series data. The starting dataset contains monthly house price data for each of the four countries/regions in the UK\n\nuk_prices &lt;- readr::read_csv('data/UK_house_prices.csv', show_col_types = FALSE)\n\nWrite code to clean the names in the uk_prices dataset using janitor::clean_names(), and then using skimr::skim confirm that the region names are correct and that there are no missing values. Call the resulting analytic data set df.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# PLEASE SHOW YOUR WORK\ndf &lt;- uk_prices |&gt; janitor::clean_names()\nunique(df$region_name)\n\n[1] \"England\"          \"Northern Ireland\" \"Scotland\"         \"Wales\"           \n\n\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n888\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nregion_name\n0\n1\n5\n16\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2005-01-01\n2023-06-01\n2014-03-16\n222\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsales_volume\n8\n0.99\n20490.35\n29647.56\n665\n2517.5\n4984.5\n18286.5\n145089\nâ–‡â–â–‚â–â–\n\n\n\n\n\n\n\n\n\nExercise 9\nWe want to use the monthly house price data to predict house prices one month ahead for each region. The basic model will be:\nSalesVolume ~ .\nwhere the date and region-name are id variables, not predictor variables. Instead we will use the prior-month lagged prices as the only predictor.\nThe recipe uses the following 5 steps from the recipes package: update_role(region_name, new_role = â€œidâ€) | recipe(sales_volume ~ ., data = df |&gt; janitor::clean_names()) | step_naomit(lag_1_sales_volume, skip=FALSE) | step_lag(sales_volume, lag=1) | step_arrange(region_name, date)\nUse these 5 steps in the proper order to create a recipe to pre-process the time series data for each region. Prep and then bake your recipe using df.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# PLEASE SHOW YOUR WORK\ndf_rec &lt;- df |&gt; recipes::recipe(sales_volume ~ .) |&gt; \n  recipes::update_role(region_name, new_role = \"id\") |&gt; \n  recipes::step_lag(sales_volume, lag=1) |&gt; \n  recipes::step_naomit(lag_1_sales_volume, skip=FALSE) |&gt; \n  recipes::step_arrange(region_name, date)\n\ndf_rec |&gt; \n  recipes::prep() |&gt; \n  recipes::bake(new_data = NULL) |&gt; \n  dplyr::filter(date &lt;= lubridate::ymd(20050301)) \n\n# A tibble: 8 Ã— 4\n  date       region_name      sales_volume lag_1_sales_volume\n  &lt;date&gt;     &lt;fct&gt;                   &lt;dbl&gt;              &lt;dbl&gt;\n1 2005-02-01 England                56044              53464 \n2 2005-03-01 England                67322              56044 \n3 2005-02-01 Northern Ireland         978.               978.\n4 2005-03-01 Northern Ireland         978.               978.\n5 2005-02-01 Scotland                7631               8876 \n6 2005-03-01 Scotland                9661               7631 \n7 2005-02-01 Wales                   2572               2516 \n8 2005-03-01 Wales                   3336               2572 \n\n\n\n\nThe result of the bake step, after filtering for dates â‰¤\\le 2005-03-01, should look like this:\n\nreadRDS(\"data/baked_uk_house_dat.rds\") |&gt; \n  dplyr::filter(date &lt;= lubridate::ymd(20050301)) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_currency(columns = -c(date,region_name), decimals = 0) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\ndate\nregion_name\nsales_volume\nlag_1_sales_volume\n\n\n\n\n2005-02-01\nEngland\n$56,044\n$53,464\n\n\n2005-03-01\nEngland\n$67,322\n$56,044\n\n\n2005-02-01\nNorthern Ireland\n$978\n$978\n\n\n2005-03-01\nNorthern Ireland\n$978\n$978\n\n\n2005-02-01\nScotland\n$7,631\n$8,876\n\n\n2005-03-01\nScotland\n$9,661\n$7,631\n\n\n2005-02-01\nWales\n$2,572\n$2,516\n\n\n2005-03-01\nWales\n$3,336\n$2,572\n\n\n\n\n\n\n\n\n\nExercise 10\n\nRecall The Business Problem\nWeâ€™re at a fast paced startup. The company is growing fast and the marketing team is looking for ways to increase the sales from existing customers by making them buy more. The main idea is to unlock the potential of the customer base through incentives, in this case a discount. We of course want to measure the effect of the discount on the customerâ€™s behavior. Still, they do not want to waste money giving discounts to users which are not valuable. As always, it is about return on investment (ROI).\nWithout going into specifics about the nature of the discount, it has been designed to provide a positive return on investment if the customer buys more than $1\\$ 1 as a result of the discount. How can we measure the effect of the discount and make sure our experiment has a positive ROI? The marketing team came up with the following strategy:\n\nSelect a sample of existing customers from the same cohort.\nSet a test window of 1 month.\nLook into the historical data of web visits from the last month. The hypothesis is that web visits are a good proxy for the customerâ€™s interest in the product.\nFor customers with a high number of web visits, send them a discount. There will be a hold out group which will not receive the discount within the potential valuable customers based on the number of web visits. For customers with a low number of web visits, do not send them a discount (the marketing team wants to report a positive ROI, so they do not want to waste money on customers which are not valuable). Still, they want to use them to measure the effect of the discount.\nWe also want to use the results of the test to tag loyal customers. These are customers which got a discount (since they showed potential interest in the product) and customers with exceptional sales numbers even if they did not get a discount. The idea is to use this information to target them in the future if the discount strategy is positive.\n\nIn the last lab we did some exploratory data analysis. The next step is to prepare some descriptive statistics.\n\nDescriptive Statistics\nThe first thing the data analytics team did was to split the sales distribution by discount group:\n\ndata &lt;- readr::read_csv('data/sales_dag.csv', show_col_types = FALSE)\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x = sales, after_stat(count), fill = discount)) +\n  geom_histogram(alpha = 0.30, position = 'identity', color=\"#e9ecef\", bins = 30)+\n  geom_density(alpha = 0.30) +\n  xlab(\"Sales\") +\n  ylab(\"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks customers with a discount have higher sales. Data scientist A is optimistic with this initial result. To quantify this, they computed the difference in means:\n\n\n\n\n\n\nSOLUTION:\n\n\n\ndifference in means:\n\nmean_sales &lt;- data |&gt; dplyr::group_by(discount) |&gt; \n  dplyr::summarize(\"mean sales\" = mean(sales)) |&gt; \n  dplyr::mutate(\"mean sales difference\" = `mean sales` - lag(`mean sales`))\nmean_sales\n\n# A tibble: 2 Ã— 3\n  discount `mean sales` `mean sales difference`\n     &lt;dbl&gt;        &lt;dbl&gt;                   &lt;dbl&gt;\n1        0         15.8                   NA   \n2        1         20.6                    4.80\n\n\n\n\nOur calculation gives a $4.8\\$ 4.8 mean uplift! This is great news. The discount strategy seems to be working. Data scientist A is happy with the results and decides to get feedback from the rest of the data science team.\nData scientist B is not so happy with the results. They think that the uplift is too good to be true (based on domain knowledge and the sales distributions ğŸ¤”). When thinking about reasons for such a high uplift, they realized the discount assignment was not at random. It was based on the number of web visits (remember the marketing plan?). This means that the discount group is not comparable to the control group completely! They decide to plot sales against web visits per discount group:\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x=visits, y = sales, color = discount)) +\n  geom_point() + \n  facet_grid(cols = vars(discount))\n\n\n\n\n\n\n\n\nIndeed, they realize they should probably adjust for the number of web visits. A natural metric is sales per web visit. Letâ€™s compute it:\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmean_sales_pv &lt;- data |&gt; dplyr::group_by(discount) |&gt; \n  dplyr::summarize(\"sales_per_visit\" = mean(sales_per_visit)) \nmean_sales_pv\n\n# A tibble: 2 Ã— 2\n  discount sales_per_visit\n     &lt;dbl&gt;           &lt;dbl&gt;\n1        0           0.861\n2        1           0.938\n\n\n\n\nThe mean value is higher for the discount group. As always, they also looked at the distributions:\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x = sales_per_visit, after_stat(count), fill = discount)) +\n  geom_histogram(alpha = 0.30, position = 'identity', color=\"#e9ecef\", bins = 30)+\n  # geom_density(alpha = 0.30) +\n  xlab(\"Sales per Visit\") +\n  ylab(\"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor both data scientists A & B the results look much better, but they were unsure about which uplift to report. They thought about the difference in means:\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmean_sales_per_visit &lt;- data |&gt; dplyr::group_by(discount) |&gt; \n  dplyr::summarize(\"mean sales per visit\" = mean(sales_per_visit)) |&gt; \n  dplyr::mutate(\"mean sales difference\" = `mean sales per visit` - dplyr::lag(`mean sales per visit`))\n\nmean_sales_per_visit |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"Mean sales per visit\") |&gt; \n  gt::fmt_number(columns = `mean sales difference`, decimals = 5) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nMean sales per visit\n\n\ndiscount\nmean sales per visit\nmean sales difference\n\n\n\n\n0\n0.8612426\nNA\n\n\n1\n0.9382929\n0.07705\n\n\n\n\n\n\n\n\n\nHowever, how to interpret this value in terms of dollars? To be continued â€¦"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_2_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_2_solutions.html#grading",
    "title": "Lab 2 - The Recipes package",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#introduction",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#packages",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(tidyverse, magrittr, gt, gtExtras, tidymodels, ggplot2)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#the-data",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#the-data",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "The Data",
    "text": "The Data\nToday we will be using customer churn data.\nIn the customer management lifecycle, customer churn refers to a decision made by the customer about ending the business relationship. It is also referred as loss of clients or customers. This dataset contains 20 features related to churn in a telecom context and we will look at how to predict churn and estimate the effect of predictors on the customer churn odds ratio.\n\ndata &lt;- \n  readr::read_csv(\"data/Telco-Customer-Churn.csv\", show_col_types = FALSE) |&gt;\n  dplyr::mutate(churn = as.factor(churn))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-1-eda",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-1-eda",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nWrite and execute the code to perform summary EDA on the data using the package skimr. Plot histograms for monthly charges and tenure. Tenure measures the strength of the customer relationship by measuring the length of time that a person has been a customer.\n\n\n\n\n\n\nSOLUTION:\n\n\n\nskimr::skim(data)\ndata |&gt;\n  ggplot(aes(x=monthly_charges)) + geom_histogram()\ndata |&gt;\n  ggplot(aes(x=tenure)) + geom_histogram()\n\n\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n7032\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n17\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngender\n0\n1\n4\n6\n0\n2\n0\n\n\nsenior_citizen\n0\n1\n2\n3\n0\n2\n0\n\n\npartner\n0\n1\n2\n3\n0\n2\n0\n\n\ndependents\n0\n1\n2\n3\n0\n2\n0\n\n\ntenure_interval\n0\n1\n9\n11\n0\n7\n0\n\n\nphone_service\n0\n1\n2\n3\n0\n2\n0\n\n\nmultiple_lines\n0\n1\n2\n16\n0\n3\n0\n\n\ninternet_service\n0\n1\n2\n11\n0\n3\n0\n\n\nonline_security\n0\n1\n2\n19\n0\n3\n0\n\n\nonline_backup\n0\n1\n2\n19\n0\n3\n0\n\n\ndevice_protection\n0\n1\n2\n19\n0\n3\n0\n\n\ntech_support\n0\n1\n2\n19\n0\n3\n0\n\n\nstreaming_tv\n0\n1\n2\n19\n0\n3\n0\n\n\nstreaming_movies\n0\n1\n2\n19\n0\n3\n0\n\n\ncontract\n0\n1\n8\n14\n0\n3\n0\n\n\npaperless_billing\n0\n1\n2\n3\n0\n2\n0\n\n\npayment_method\n0\n1\n12\n25\n0\n4\n0\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nchurn\n0\n1\nFALSE\n2\nNo: 5163, Yes: 1869\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntenure\n0\n1\n32.42\n24.55\n1.00\n9.00\n29.00\n55.00\n72.00\nâ–‡â–ƒâ–ƒâ–ƒâ–…\n\n\nmonthly_charges\n0\n1\n64.80\n30.09\n18.25\n35.59\n70.35\n89.86\n118.75\nâ–‡â–…â–†â–‡â–…\n\n\ntotal_charges\n0\n1\n2283.30\n2266.77\n18.80\n401.45\n1397.47\n3794.74\n8684.80\nâ–‡â–‚â–‚â–‚â–"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-2-train-test-splits-recipe",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-2-train-test-splits-recipe",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 2: train / test splits & recipe",
    "text": "Exercise 2: train / test splits & recipe\nWrite and execute code to create training and test datasets. Have the training dataset represent 70% of the total data.\nNext create a recipe where churn is related to all the other variables, and\n\nnormalize the numeric variables\ncreate dummy variables for the ordinal predictors\n\nMake sure the steps are in a sequence that preserves the (0,1) dummy variables.\nPrep the data on the training data and show the result.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nset.seed(8740)\n\n# split data\ndata_split    &lt;- rsample::initial_split(data, prop = 0.7)\ndefault_train &lt;- rsample::training(data_split)\ndefault_test  &lt;- rsample::testing(data_split)\n\n# create a recipe\ndefault_recipe &lt;- default_train |&gt;\n  recipes::recipe(formula = churn ~ .) |&gt;\n  recipes::step_normalize(recipes::all_numeric_predictors()) |&gt;\n  recipes::step_dummy(recipes::all_nominal_predictors())\n\ndefault_recipe |&gt; recipes::prep(default_train) |&gt; \n  summary()\n\n# A tibble: 37 Ã— 4\n   variable                     type      role      source  \n   &lt;chr&gt;                        &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 tenure                       &lt;chr [2]&gt; predictor original\n 2 monthly_charges              &lt;chr [2]&gt; predictor original\n 3 total_charges                &lt;chr [2]&gt; predictor original\n 4 churn                        &lt;chr [3]&gt; outcome   original\n 5 gender_Male                  &lt;chr [2]&gt; predictor derived \n 6 senior_citizen_Yes           &lt;chr [2]&gt; predictor derived \n 7 partner_Yes                  &lt;chr [2]&gt; predictor derived \n 8 dependents_Yes               &lt;chr [2]&gt; predictor derived \n 9 tenure_interval_X0.6.Month   &lt;chr [2]&gt; predictor derived \n10 tenure_interval_X12.24.Month &lt;chr [2]&gt; predictor derived \n# â„¹ 27 more rows"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-3-logistic-modeling",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-3-logistic-modeling",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 3: logistic modeling",
    "text": "Exercise 3: logistic modeling\n\nCreate a linear model using logistic regression to predict churn. for the set engine stage use â€œglm,â€ and set the mode to â€œclassification.â€\nCreate a workflow using the recipe of the last exercise and the model if the last step.\nWith the workflow, fit the training data\nCombine the training data and the predictions from step 3 using broom::augment , and assign the result to a variable\nCreate a combined metric function as show in the code below:\nUse the variable from step 4 as the first argument to the function from step 5. The other arguments are truth = churn (from the data) and estimate=.pred_class (from step 4). Make a note of the numerical metrics.\nUse the variable from step 4 as the first argument to the functions below, with arguments truth = churn and estimate =.pred_No.\n\nyardstick::roc_auc\nyardstick::roc_curve followed by ggplot2::autoplot().\n\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create a linear regression model\ndefault_model &lt;- parsnip::logistic_reg() |&gt;\n  parsnip::set_engine(\"glm\") |&gt;\n  parsnip::set_mode(\"classification\")\n\n# create a workflow\ndefault_workflow &lt;- workflows::workflow() |&gt;\n  workflows::add_recipe(default_recipe) |&gt;\n  workflows::add_model(default_model)\n\n# fit the workflow\nlm_fit &lt;-\n  default_workflow |&gt;\n  parsnip::fit(default_train)\n\n# training dataset\ntraining_results &lt;-\n  broom::augment(lm_fit , default_train)\n\n\n# create the metrics function\nm_set_fn &lt;- \n  yardstick::metric_set(\n    yardstick::accuracy\n    , yardstick::precision\n    , yardstick::recall\n    , yardstick::f_meas\n    , yardstick::spec\n    , yardstick::sens\n    , yardstick::ppv\n    , yardstick::npv\n)\ntraining_results |&gt; m_set_fn(truth = churn, estimate = .pred_class)\n\n# A tibble: 8 Ã— 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.807\n2 precision binary         0.842\n3 recall    binary         0.905\n4 f_meas    binary         0.872\n5 spec      binary         0.546\n6 sens      binary         0.905\n7 ppv       binary         0.842\n8 npv       binary         0.683\n\n\n# compute roc_auc and plot the roc_curve\ntraining_results |&gt;\n  yardstick::roc_auc(.pred_No, truth = churn)\ntraining_results |&gt;\n  yardstick::roc_curve(.pred_No, truth = churn) |&gt; autoplot()\n\n\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.854\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild your own\n\n\n\nYou can construct the roc_auc curve directly, as follows:\n\n# use the augmented training results, and find points on the curve for a given threshold\nbuild_roc_aus &lt;- function(threshold, dat = training_results){\n  # threshold on predictions\n  log_preds &lt;- ifelse(dat$.pred_Yes &gt; threshold, 1, 0)\n  # compute predictions and reference/truth\n  log_preds &lt;- factor(log_preds, levels = c(0,1), labels = c('No','Yes'))\n\n  return(\n    tibble::tibble(\n      threshold = threshold\n      , \"1-Specificity\" = \n        1 - yardstick::specificity_vec(truth = dat$churn, estimate = log_preds)\n      , Sensitivity = \n          yardstick::sensitivity_vec(truth = dat$churn, estimate = log_preds)\n    )\n  )\n}\n\n\n#take thesholds in [0,1] and calculate the corresponding x,y points\n((0:50)/50) |&gt; purrr::map(~build_roc_aus(.x)) |&gt; \n  dplyr::bind_rows() |&gt; \n    ggplot(aes(x=`1-Specificity`, y=Sensitivity)) +\n    geom_point(size=1) + coord_fixed() +\n    geom_abline(intercept=0, slope=1)"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-4-effects",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-4-effects",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 4: effects",
    "text": "Exercise 4: effects\nUse broom::tidy() on the fit object from exercise 4 to get the predictor coefficients. Sort them in decreasing order by absolute value.\nWhat is the effect of one additional year of tenure on the churn odds ratio?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nfit0_tbl &lt;- lm_fit |&gt; broom::tidy() |&gt;\n  dplyr::arrange(desc(abs(estimate)))\n\nfit0_tbl\n\n# A tibble: 37 Ã— 5\n   term                         estimate std.error statistic  p.value\n   &lt;chr&gt;                           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 tenure_interval_X6.12.Month     -3.04     0.820    -3.71  2.11e- 4\n 2 tenure_interval_X12.24.Month    -2.78     0.705    -3.94  8.02e- 5\n 3 tenure_interval_X0.6.Month      -2.68     0.905    -2.96  3.09e- 3\n 4 tenure_interval_X24.36.Month    -2.20     0.554    -3.96  7.51e- 5\n 5 tenure                          -2.10     0.386    -5.43  5.67e- 8\n 6 (Intercept)                      1.91     1.62      1.18  2.37e- 1\n 7 contract_Two.year               -1.45     0.217    -6.67  2.60e-11\n 8 tenure_interval_X36.48.Month    -1.28     0.402    -3.19  1.44e- 3\n 9 phone_service_Yes               -1.23     0.782    -1.57  1.17e- 1\n10 monthly_charges                  1.15     1.15      0.997 3.19e- 1\n# â„¹ 27 more rows\n\n\n\n# pull the tenure coefficient and exponentiate it\nfit0_tbl |&gt; dplyr::filter(term == 'tenure') |&gt; \n  dplyr::pull(estimate) |&gt; \n  exp()\n\n[1] 0.1228617"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-5-knn-modeling",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-5-knn-modeling",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 5 knn modeling",
    "text": "Exercise 5 knn modeling\nNow we will create a K-nearest neighbours model to estimate churn. To do this, write the code for the following steps:\n\nCreate a K-nearest neighbours model to predict churn using parsnip::nearest_neighbor with argument neighbors = 3 which will use the three most similar data points from the training set to predict churn. For the set engine stage use â€œkknn,â€ and set the mode to â€œclassification.â€\nTake the workflow from exercise 3 and create a new workflow by updating the original workflow. Use workflows::update_model to swap out the original logistic model for the nearest neighbour model.\nUse the new workflow to fit the training data. Take the fit and use broom::augment to augment the fit with the training data.\nUse the augmented data from step 3 to plot the roc curve, using yardstick::roc_curve(.pred_No, truth = churn) as in exercise 3. How do you interpret his curve?\nTake the fit from step 3 and use broom::augment to augment the fit with the test data.\nRepeat step 4 using the augmented data from step 5.\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create a knn classification model\ndefault_model_knn &lt;- parsnip::nearest_neighbor(neighbors = 3) |&gt;\n  parsnip::set_engine(\"kknn\") |&gt;\n  parsnip::set_mode(\"classification\")\n\n# create a workflow\ndefault_workflow_knn &lt;- default_workflow |&gt;\n  workflows::update_model(default_model_knn)\n\n# fit the workflow\nlm_fit_knn &lt;-\n  default_workflow_knn |&gt;\n  parsnip::fit(default_train)\n\n# augment the training data with the fitted data\ntraining_results_knn &lt;-\n  broom::augment(lm_fit_knn , default_train)\n\n\n# compute the metrics\ntraining_results_knn |&gt; m_set_fn(truth = churn, estimate = .pred_class)\n\n# A tibble: 8 Ã— 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.999\n2 precision binary         0.999\n3 recall    binary         1.00 \n4 f_meas    binary         0.999\n5 spec      binary         0.998\n6 sens      binary         1.00 \n7 ppv       binary         0.999\n8 npv       binary         0.999\n\n\n# compute roc_auc and plot the roc_curve\ntraining_results_knn |&gt;\n  yardstick::roc_auc(.pred_No, truth = churn)\ntraining_results_knn |&gt;\n  yardstick::roc_curve(.pred_No, truth = churn) |&gt; autoplot()\n\n\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.999"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-6-cross-validation",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-6-cross-validation",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 6 cross validation",
    "text": "Exercise 6 cross validation\nFollowing the last exercise, we should have some concerns about over-fitting by the nearest-neighbour model.\nTo address this we will use cross validation to tune the model and evaluate the fits.\n\nCreate a cross-validation dataset based on 5 folds using rsample::vfold_cv.\nUsing the knn workflow from exercise 5, apply tune::fit_resamples with arguments resamples and control where the resamples are the dataset created in step 1 and control is tune::control_resamples(save_pred = TRUE), which will ensure that the predictions are saved.\nUse tune::collect_metrics() on the results from step 2\nUse tune::collect_predictions() on the results from step 2 to plot the roc_auc curve as in exercise 5. Has it changed much from exercise 5?\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create v-fold cross validation data\ndata_vfold_cv &lt;- data |&gt; rsample::vfold_cv(v=5)\n\n# use tune::fit on the cv dat, saving the predictions\nrf_fit_rs &lt;-\n  default_workflow_knn |&gt;\n  tune::fit_resamples(data_vfold_cv, control = tune::control_resamples(save_pred = TRUE))\n\n# collect the metrics\nrf_fit_rs |&gt; tune::collect_metrics()\n\n# A tibble: 3 Ã— 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.729     5 0.00480 Preprocessor1_Model1\n2 brier_class binary     0.207     5 0.00374 Preprocessor1_Model1\n3 roc_auc     binary     0.746     5 0.00790 Preprocessor1_Model1\n\n# compute the roc_curve\nrf_fit_rs |&gt; tune::collect_predictions() |&gt;\n  yardstick::roc_curve(.pred_No, truth = churn) |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-7-tuning-for-k",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-7-tuning-for-k",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 7: tuning for k",
    "text": "Exercise 7: tuning for k\nIn this exercise weâ€™ll tune the number of nearest neighbours in our model to see if we can improve performance.\n\nRedo exercise 5 steps 1 and 2, setting neighbors = tune::tune() for the model, and then updating the workflow with workflows::update_model.\nUse dials::grid_regular(dials::neighbors(), levels = 10) to create a grid for tuning k.\nUse tune::tune_grid with tune::control_grid(save_pred = TRUE) and yardstick::metric_set(yardstick::accuracy, yardstick::roc_auc) to generate tuning results\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n# re-specify the model for tuning\ndefault_model_knn_tuned &lt;- parsnip::nearest_neighbor(neighbors = tune::tune()) |&gt;\n  parsnip::set_engine(\"kknn\") |&gt;\n  parsnip::set_mode(\"classification\")\n\n# update the workflow\ndefault_workflow_knn &lt;- default_workflow |&gt;\n  workflows::update_model(default_model_knn_tuned)\n\n# make a grid for tuning\nclust_num_grid &lt;-\n  dials::grid_regular(dials::neighbors(), levels = 10)\n\n# use the grid to tune the model\ntune_results &lt;- tune::tune_grid(\n  default_workflow_knn,\n  resamples = data_vfold_cv,\n  grid = clust_num_grid,\n  control = tune::control_grid(save_pred = TRUE)\n  , metrics =\n    yardstick::metric_set(yardstick::accuracy, yardstick::roc_auc)\n)\n\n# show the tuning results dataframe\ntune_results\n\n\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 Ã— 5\n  splits              id    .metrics          .notes           .predictions\n  &lt;list&gt;              &lt;chr&gt; &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [5625/1407]&gt; Fold1 &lt;tibble [20 Ã— 5]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n2 &lt;split [5625/1407]&gt; Fold2 &lt;tibble [20 Ã— 5]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n3 &lt;split [5626/1406]&gt; Fold3 &lt;tibble [20 Ã— 5]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n4 &lt;split [5626/1406]&gt; Fold4 &lt;tibble [20 Ã— 5]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n5 &lt;split [5626/1406]&gt; Fold5 &lt;tibble [20 Ã— 5]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-8",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-8",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 8",
    "text": "Exercise 8\nUse tune::collect_metrics() to collect the metrics from the tuning results in exercise 7 and then plot the metrics as a function of k using the code below.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n# collect the metrics\ntune_results |&gt;\n  tune::collect_metrics()\n# plot the collected metrics as a function of K\ntune_results |&gt;\n  tune::collect_metrics() |&gt;\n  ggplot(aes(neighbors,mean)) +\n  geom_line(linewidth = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2)\n\n\n\n# A tibble: 20 Ã— 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1         1 accuracy binary     0.729     5 0.00471 Preprocessor1_Model01\n 2         1 roc_auc  binary     0.656     5 0.00416 Preprocessor1_Model01\n 3         2 accuracy binary     0.729     5 0.00460 Preprocessor1_Model02\n 4         2 roc_auc  binary     0.718     5 0.00752 Preprocessor1_Model02\n 5         3 accuracy binary     0.729     5 0.00480 Preprocessor1_Model03\n 6         3 roc_auc  binary     0.746     5 0.00790 Preprocessor1_Model03\n 7         4 accuracy binary     0.730     5 0.00470 Preprocessor1_Model04\n 8         4 roc_auc  binary     0.760     5 0.00707 Preprocessor1_Model04\n 9         5 accuracy binary     0.729     5 0.00469 Preprocessor1_Model05\n10         5 roc_auc  binary     0.767     5 0.00698 Preprocessor1_Model05\n11         6 accuracy binary     0.757     5 0.00608 Preprocessor1_Model06\n12         6 roc_auc  binary     0.777     5 0.00670 Preprocessor1_Model06\n13         7 accuracy binary     0.763     5 0.00566 Preprocessor1_Model07\n14         7 roc_auc  binary     0.783     5 0.00655 Preprocessor1_Model07\n15         8 accuracy binary     0.768     5 0.00578 Preprocessor1_Model08\n16         8 roc_auc  binary     0.789     5 0.00668 Preprocessor1_Model08\n17         9 accuracy binary     0.770     5 0.00639 Preprocessor1_Model09\n18         9 roc_auc  binary     0.793     5 0.00662 Preprocessor1_Model09\n19        10 accuracy binary     0.771     5 0.00686 Preprocessor1_Model10\n20        10 roc_auc  binary     0.797     5 0.00659 Preprocessor1_Model10"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-9",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-9",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 9",
    "text": "Exercise 9\nUse tune::show_best and tune::select_best with argument â€œroc_aucâ€ to find the best k for the knn classification model. Then\n\nupdate the workflow using tune::finalize_workflow to set the best k value.\nuse tune::last_fit with the updated workflow from step 1, evaluated on the split data from exercise 2 to finalize the fit.\nuse tune::collect_metrics() to get the metrics for the best fit\nuse tune::collect_predictions() to get the predictions and plot the roc_auc as in the prior exercises\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n# show the roc_auc metrics\ntune_results |&gt;\n  tune::show_best(metric = \"roc_auc\")\n# select the best roc_auc metric\nbest_nn &lt;- tune_results |&gt;\n  tune::select_best(metric = \"roc_auc\")\n\n# finalize the workflow with the best nn metric from the last step\nfinal_wf &lt;- default_workflow_knn |&gt;\n  tune::finalize_workflow(best_nn)\n\n# use  tune::last_fit with the finaized workflow on the data_split (ex 2)\nfinal_fit &lt;-\n  final_wf |&gt;\n  tune::last_fit(data_split)\n\n# collect the metrics from the final fit\nfinal_fit |&gt;\n  tune::collect_metrics()\nfinal_fit |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_curve(.pred_No, truth = churn) |&gt;\n  autoplot()\n\n\n\n# A tibble: 5 Ã— 7\n  neighbors .metric .estimator  mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        10 roc_auc binary     0.797     5 0.00659 Preprocessor1_Model10\n2         9 roc_auc binary     0.793     5 0.00662 Preprocessor1_Model09\n3         8 roc_auc binary     0.789     5 0.00668 Preprocessor1_Model08\n4         7 roc_auc binary     0.783     5 0.00655 Preprocessor1_Model07\n5         6 roc_auc binary     0.777     5 0.00670 Preprocessor1_Model06\n\n\n\n\n# A tibble: 3 Ã— 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.773 Preprocessor1_Model1\n2 roc_auc     binary         0.798 Preprocessor1_Model1\n3 brier_class binary         0.157 Preprocessor1_Model1"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-10-clustering",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#exercise-10-clustering",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Exercise 10: clustering",
    "text": "Exercise 10: clustering\nLoad the data for this exercise as below and plot it, and then create an analysis dataset with the cluster labels removed\n\n#\n# read the data\nlabelled_points &lt;- readr::read_csv(\"data/lab_5_clusters.csv\", show_col_types = FALSE)\n\n# plot the clusters\nlabelled_points |&gt; ggplot(aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3) + \n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n# remove cluster labels to make the analysis dataset\npoints &lt;-\n  labelled_points |&gt;\n  select(-cluster)\n\nYou have frequently used broom::augment to combine a model with the data set, and broom::tidy to summarize model components; broom::glance is used to similarly to summarize goodness-of-fit metrics.\nNow perform k-means clustering on the points data for different values of k as follows:\n\nkclusts &lt;-\n  # number of clusters from 1-9\n  tibble(k = 1:9) |&gt;\n  # mutate to add columns\n  mutate(\n    # a list-column with the results of the kmeans function (clustering)\n    kclust = purrr::map(k, ~stats::kmeans(points, .x)),\n    # a list-column with the results broom::tidy applied to the clustering results\n    tidied = purrr::map(kclust, broom::tidy),\n    # a list-column with the results broom::glance applied to the clustering results\n    glanced = purrr::map(kclust, broom::glance),\n    # a list-column with the results broom::augment applied to the clustering results\n    augmented = purrr::map(kclust, broom::augment, points)\n  )\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n(i) Create 3 variables by tidyr::unnesting the appropriate columns of kclusts\n\n# take kclusts and use tidy::unnest() on the appropriate columns\nclusters &lt;-\n  kclusts |&gt;\n  tidyr::unnest(cols = c(tidied))\n\nassignments &lt;-\n  kclusts |&gt;\n  tidyr::unnest(cols = c(augmented))\n\nclusterings &lt;-\n  kclusts |&gt;\n  tidyr::unnest(cols = c(glanced))\n\n(ii) Use the assignment variable to plot the cluster assignments generated by stats::kmeans\n\n# plot the points assigned to each cluster\np &lt;- assignments |&gt; ggplot(aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) +\n  facet_wrap(~ k) + theme(legend.position=\"none\")\np\n\n\n\n\n\n\n\n\n(iii) Use the clusters variable to add the cluster centers to the plot\n\n# on the last plot, mark the cluster centres with an X\np + geom_point(data = clusters, size = 10, shape = \"x\")\n\n\n\n\n\n\n\n\n(iv) Use the clusterings variable to plot the total within sum of squares value by number of clusters.\n\n# make a separate line-and-point plot with the tot-withinss data by cluster number\nclusterings |&gt; ggplot(aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\n(v) Visually and by the â€œelbowâ€ heursistic, we should use k=3, i.e.Â k=3 should give good results: good fit with low model complexity."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#submission",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#submission",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are saved, staged, committed, and pushed to your repository on the course github site.\nRemember â€“ you do not have to turn in an *.html file. I will be pulling your work directly from your repository on the course website."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_5_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_5_solutions.html#grading",
    "title": "Lab 5 - Classification & clustering methods",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html",
    "title": "Lab 3 - Regression",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll explore several data sets and practice building and evaluating regression models.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to use different regression models to predict a response/target/outcome as a function of a set of variates."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#introduction",
    "title": "Lab 3 - Regression",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll explore several data sets and practice building and evaluating regression models.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to use different regression models to predict a response/target/outcome as a function of a set of variates."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#packages",
    "title": "Lab 3 - Regression",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in todayâ€™s lab.\n\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, knitr, ISLR2, stats, xgboost, see\n)\ntheme_set(theme_bw(base_size = 12))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#data-boston-house-values",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#data-boston-house-values",
    "title": "Lab 3 - Regression",
    "section": "Data: Boston House Values",
    "text": "Data: Boston House Values\nThe Boston House Values dataset (usually referred to as the Boston dataset) appears in several R packages in different versions and is based on economic studies published in the late 1970â€™s.\nThis dataset contains the following information for each cocktail:\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ncrim\nper capita crime rate by town.\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus\nproportion of non-retail business acres per town.\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox\nnitrogen oxides concentration (parts per 10 million).\n\n\nrm\naverage number of rooms per dwelling.\n\n\nage\nproportion of owner-occupied units built prior to 1940.\n\n\ndis\nweighted mean of distances to five Boston employment centres.\n\n\nrad\nindex of accessibility to radial highways.\n\n\ntax\nfull-value property-tax rate per $10,000.\n\n\nptratio\npupil-teacher ratio by town.\n\n\nlstat\nlower status of the population (percent).\n\n\nmedv\nmedian value of owner-occupied homes in $1000s.\n\n\n\nUse the code below to load the Boston Cocktail Recipes data set.\n\nboston &lt;- ISLR2::Boston"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#exercises",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#exercises",
    "title": "Lab 3 - Regression",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nPlot the median value of owner-occupied homes (medv) vs the percentage of houses with lower socioeconomic status (lstat) then use lm to model medv ~ lstat and save the result in a variable for use later.\nNext prepare a summary of the model. What is the intercept and the coefficient of lstat in this model?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nboston %&gt;% \n  ggplot(aes(x=lstat, y = medv)) + geom_point()\n\n\n\n\n\n\n\n\nlm_medv_lstat &lt;- lm(medv ~ lstat, data = boston)\nsummary(lm_medv_lstat)\n# alternatively:\nlm_medv_lstat %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::select(1:2)\n\n\n\n\nCall:\nlm(formula = medv ~ lstat, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n# A tibble: 2 Ã— 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)   34.6  \n2 lstat         -0.950\n\n\n\n\n\n\n\nExercise 2\nUsing the result from Exercise 1, and the data below, use the predict function (stats::predict.lm or just predict) with the argument interval = â€œconfidenceâ€ to prepare a summary table with columns lstat, fit, lwr, upr.\n\ntibble(lstat = c(5, 10, 15, 20))\n\nFinally, use your model to plot some performance checks using the performance::check_model function with arguments check=c(\"linearity\",\"qq\",\"homogeneity\", \"outliers\").\nAre there any overly influential observations in this dataset?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ntibble::tibble(lstat = c(5, 10, 15, 20)) %&gt;% \n  # create a nested column\n  dplyr::mutate(\n    results =\n      purrr::map(\n        lstat                         # the data is in column lstat\n        , ~stats::predict.lm(         # the function is based on predict,\n          lm_medv_lstat               # with first argument from the fit \n          , tibble::tibble(lstat = .) # and newdata argument from lstat column \n          , interval = \"confidence\"   # and setting interval argument\n          ) %&gt;% \n          # predict returns a vector; make it a tibble,\n          # with columns fit, lwr, upr\n          # where the last two set lower and upper confidence intervals \n          tibble::as_tibble()\n      )\n  ) %&gt;% \n  tidyr::unnest(results)\n\n# A tibble: 4 Ã— 4\n  lstat   fit   lwr   upr\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     5  29.8  29.0  30.6\n2    10  25.1  24.5  25.6\n3    15  20.3  19.7  20.9\n4    20  15.6  14.8  16.3\n\n\n\n# Alternatively, and more directly\ntibble::tibble(lstat = c(5, 10, 15, 20)) %&gt;% \n  dplyr::bind_cols(\n    stats::predict.lm(         \n          lm_medv_lstat                \n          , tibble::tibble(lstat = c(5, 10, 15, 20))\n          , interval = \"confidence\"   \n          )\n  )\n\n# A tibble: 4 Ã— 4\n  lstat   fit   lwr   upr\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     5  29.8  29.0  30.6\n2    10  25.1  24.5  25.6\n3    15  20.3  19.7  20.9\n4    20  15.6  14.8  16.3\n\n\n\n# Or most directly\ntibble::tibble(lstat = c(5, 10, 15, 20)) %&gt;% \n  broom::augment(\n    lm_medv_lstat, newdata = ., interval = \"confidence\"\n  )\n\n# A tibble: 4 Ã— 4\n  lstat .fitted .lower .upper\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     5    29.8   29.0   30.6\n2    10    25.1   24.5   25.6\n3    15    20.3   19.7   20.9\n4    20    15.6   14.8   16.3\n\n\nCheck the help for stats::predict.lm, dplyr::bind_cols, and broom::augment to more detail on each method.\nFinally\n\nlm_medv_lstat %&gt;% \n  performance::check_model(\n    check = \n      c(\n        \"linearity\"      # linear fit\n        , \"qq\"           # Normal residuals\n        , \"homogeneity\"  # constant variance \n        , \"outliers\"     # any influential observations?\n      ) \n  )\n\n\n\n\n\n\n\n\nSince no points lie outside the dashed lines of the Influential Observations chart, there are no influential observations.\n\n\n\n\nExercise 3\nFit medv to all predictors in the dataset and use the performance::check_collinearity function on the resulting model to check if any predictors are redundant.\nThe variance inflation factor is a measure of the magnitude of multicollinearity of model terms. A VIF less than 5 indicates a low correlation of that predictor with other predictors. A value between 5 and 10 indicates a moderate correlation, while VIF values larger than 10 are a sign for high, not tolerable correlation of model predictors.\nWhich predictors in this dataset might be redundant for predicting medv?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# fit the model\nlm_medv_all &lt;- lm(medv ~ ., data = boston)\n# check for collinearity\nperformance::check_collinearity(lm_medv_all)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n    Term  VIF    VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n    crim 1.77 [1.58,  2.01]         1.33      0.57     [0.50, 0.63]\n      zn 2.30 [2.03,  2.64]         1.52      0.44     [0.38, 0.49]\n   indus 3.99 [3.45,  4.64]         2.00      0.25     [0.22, 0.29]\n    chas 1.07 [1.02,  1.28]         1.03      0.93     [0.78, 0.98]\n     nox 4.37 [3.77,  5.09]         2.09      0.23     [0.20, 0.26]\n      rm 1.91 [1.70,  2.19]         1.38      0.52     [0.46, 0.59]\n     age 3.09 [2.69,  3.57]         1.76      0.32     [0.28, 0.37]\n     dis 3.95 [3.42,  4.60]         1.99      0.25     [0.22, 0.29]\n ptratio 1.80 [1.61,  2.05]         1.34      0.56     [0.49, 0.62]\n   lstat 2.87 [2.51,  3.32]         1.69      0.35     [0.30, 0.40]\n\nModerate Correlation\n\n Term  VIF    VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n  rad 7.45 [6.37,  8.73]         2.73      0.13     [0.11, 0.16]\n  tax 9.00 [7.69, 10.58]         3.00      0.11     [0.09, 0.13]\n\n\nThe variance inflation factor (VIF) is moderate (between 5-10) for rad and tax, suggesting these may be redundant fo predicting the outcome (medv).\n\n\n\n\nExercise 4\nIn this exercise you will compare and interpret the results of linear regression on two similar datasets.\nThe first dataset (dat0 - generated below) has demand0 and price0 variables along with an unobserved variable (unobserved0 - so not in our dataset) that doesnâ€™t change the values of demand0 and price0. Use lm to build a model to predict demand0 from price0 . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset dat0?\n\nN &lt;- 500\nset.seed(1966)\n\ndat0 &lt;- tibble::tibble(\n  price0 = 10+rnorm(500)\n  , demand0 = 30-(price0 + rnorm(500))\n  , unobserved0 = 0.45*price0 + 0.77*demand0 + rnorm(500)\n)\n\nThe second dataset (dat1 - generated below) has demand1 and price1 variables, along with a variable unobserved1 that is completely random and is not observed, so it isnâ€™t in our dataset. Use lm to build a model to predict demand1 from price1 . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset dat1?\n\nset.seed(1966)\n\ndat1 &lt;- tibble::tibble(\n  unobserved1 = rnorm(500)\n  , price1 = 10 + unobserved1 + rnorm(500)\n  , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))\n)\n\nWhich linear model returns the (approximately) correct dependence of demand on price, as given in the data generation process?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# DAT0\n# fit the model\nfit0 &lt;- lm(demand0 ~ price0, data = dat0)\n# get the estimated coefficients for dataset 0\nest0  &lt;- fit0 %&gt;% broom::tidy()\n# plot the data\ndat0 %&gt;% ggplot(aes(x=price0,y=demand0)) +\n  geom_point() + \n  geom_abline(\n    data = est0 %&gt;% \n      dplyr::select(1:2) %&gt;% \n      tidyr::pivot_wider(names_from = term, values_from =estimate)\n    , aes(intercept = `(Intercept)`, slope = price0)\n    , colour = \"red\"\n  )\n\n\n\n\n\n\n\n\n\n#DAT1\n# fit the model\nfit1 &lt;- lm(demand1 ~ price1, data = dat1)\n# get the estimated coefficients for dataset 0\nest1  &lt;- fit1 %&gt;% broom::tidy()\n# plot the data\ndat1 %&gt;% ggplot(aes(x=price1,y=demand1)) +\n  geom_point() + \n  geom_abline(\n    data = est1 %&gt;% dplyr::select(1:2) %&gt;% tidyr::pivot_wider(names_from = term, values_from =estimate)\n    , aes(intercept = `(Intercept)`, slope = price1)\n    , colour = \"red\"\n  )\n\n\n\n\n\n\n\n\nThe linear fit to the two datasets (without the unobserved variables, because they are, well, unobserved) gives very similar intercepts and slopes (dependence on price).\nIf you look at the plots the fits look good, and this is consistent with the estimates.\n### dataset 0\nest0 %&gt;% dplyr::select(1:2) %&gt;% tibble::add_column(data = 'dat0')\n### dataset 1\nest1 %&gt;% dplyr::select(1:2) %&gt;% tibble::add_column(data = 'dat1')\n\n\n\n# A tibble: 2 Ã— 3\n  term        estimate data \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;\n1 (Intercept)    30.5  dat0 \n2 price0         -1.04 dat0 \n\n\n# A tibble: 2 Ã— 3\n  term        estimate data \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;\n1 (Intercept)   27.8   dat1 \n2 price1        -0.989 dat1 \n\n\n\nHowever, now go back and look at the way the datasets were generated.\nIn dat0, the coefficient of price in the equation for demand is -1, consistent with the linear fit.\nBut in dat1, the coefficient of price in the equation for demand is -0.5, yet the linear fit estimates the value as close to 1.\nSo here the model doesnâ€™t give the answers that the knowledge of the data generation process would lead us to expect. What if we can observe the unobservables?\n\n\n\n\nExercise 5\nNow repeat the modeling of exercise 4, but assuming that the formerly unobservable variables are now observable, and so can be included in the linear regression models.\nWhich model returns the (approximately) correct dependence of demand on price, as given in the data generation process?\nWhat can you conclude from these two exercises?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# fit the model with dat0\nlm(demand0 ~ price0 + unobserved0, data = dat0) %&gt;% \n  # pull out the coefficient estimates as a tibble\n  broom::tidy() %&gt;% \n  # combine with another table\n  dplyr::bind_rows(\n    # fit the model with dat1\n    lm(demand1 ~ price1 + unobserved1, data = dat1) %&gt;% \n      broom::tidy()\n  )\n\n# A tibble: 6 Ã— 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   18.9      0.710      26.6  1.33e- 97\n2 price0        -0.878    0.0351    -25.0  9.06e- 90\n3 unobserved0    0.497    0.0267     18.6  4.29e- 59\n4 (Intercept)   22.4      0.443      50.5  1.10e-197\n5 price1        -0.443    0.0444     -9.98 1.74e- 21\n6 unobserved1   -1.08     0.0638    -17.0  3.21e- 51\n\n\nWhen we include the formerly unobserved variables we find that\n\nagain the coefficients of price are similar between the models, but\nnow the coefficient of price1 is correct (should be -0.5) while the coefficient of price0 is not (should be -1.0)\n\nThe conclusion is that whether a covariate is included or not requires thinking about the data generation process. We generally only have the observations, but not the exact process that produced them, but we can hypothesize the process and test the conclusions.\nCould adding more data improve our estimates? It depends.\nIn dataset dat1, the unobserved variable affects the values of both price1 and demand1, which adds a correlation to the relationship between price1 and demand1 that is in addition to their direct relationship (with price1 coefficient -0.5). Because of this, the unobserved variable needs to be included in the regression, to control for it and remove the bias due the extra correlation.\nBy contrast, in dataset dat0, the unobserved variable depends on the values of both price0 and demand0, and there is no correlation when this variable is not included in the regression, which estimates the price0 coefficient correctly as -1.0. But if the unobserved variable is included in the regression, because a regression estimate is â€˜with all other variables held constant,â€™ this induces a new correlation between price0 and demand0, exactly because they both affect the unobserved variable - with a fixed value for the unobserved variable, price0 and demand0 are no longer independent. This additional correlation creates the bias in our estimate of the coefficient of price0.\n\n\n\n\nExercise 6\nFor the next several exercises, weâ€™ll work with a new dataset. This dataset is taken from an EPA site on fuel economy, in particular the fuel economy dataset for 2023.\nUse the code below to load the FE Guide data set.\n\ndat &lt;- \n  readxl::read_xlsx( \"data/2023 FE Guide for DOE-release dates before 7-28-2023.xlsx\")\n\nFrom the raw data in dat, weâ€™ll make a smaller dataset, and weâ€™ll need to do some cleaning to make it useable.\nFirst select the columns â€œComb FE (Guide) - Conventional Fuelâ€, â€œEng Displâ€,â€˜# Cylâ€™, Transmission , â€œ# Gearsâ€, â€œAir Aspiration Method Descâ€, â€œRegen Braking Type Descâ€, â€œBatt Energy Capacity (Amp-hrs)â€ , â€œDrive Descâ€, â€œFuel Usage Desc - Conventional Fuelâ€, â€œCyl Deact?â€, and â€œVar Valve Lift?â€ and then clean the column names using janitor::janitor::clean_names(). Assign the revised data to the variable cars_23.\nPerform a quick check of the data using DataExplorer::introduce() and DataExplorer::plot_missing() and modify the data as follows\n\nmutate the columns comb_fe_guide_conventional_fuel, number_cyl, and number_gears to ensure that they contain integers values, not doubles.\nuse tidyr::replace_na to replace any missing values in batt_energy_capacity_amp_hrs column with zeros, and replace and missing values in regen_braking_type_desc with empty strings (â€œâ€œ).\nfinally, mutate the columns â€˜transmissionâ€™,â€˜air_aspiration_method_descâ€™,â€˜regen_braking_type_descâ€™,â€˜drive_descâ€™ ,â€˜fuel_usage_desc_conventional_fuelâ€™,â€˜cyl_deactâ€™,â€˜var_valve_liftâ€™ so their values are factors.\n\nPrepare a recipe to pre-process cars_23 ahead of modelling, using comb_fe_guide_conventional_fuel as the outcome, with the following steps.\n\nCentering for: recipes::all_numeric()\nScaling for: recipes::all_numeric()\nDummy variables from: recipes::all_factor()\n\nHow many predictor variables are there in cars_23 ?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# CLEANING\n# select the columns\ncars_23 &lt;- dat %&gt;% dplyr::select(\n    \"Comb FE (Guide) - Conventional Fuel\",\n    \"Eng Displ\",'# Cyl',Transmission\n    ,\"# Gears\",\"Air Aspiration Method Desc\"\n    ,\"Regen Braking Type Desc\",\"Batt Energy Capacity (Amp-hrs)\"\n    ,\"Drive Desc\",\"Fuel Usage Desc - Conventional Fuel\"\n    ,\"Cyl Deact?\", \"Var Valve Lift?\"\n  ) %&gt;% \n  # clean the names\n  janitor::clean_names()\n\n# Explore the data\ncars_23 %&gt;% DataExplorer::introduce()\ncars_23 %&gt;% DataExplorer::plot_missing()\n\n\n\n# A tibble: 1 Ã— 9\n   rows columns discrete_columns continuous_columns all_missing_columns\n  &lt;int&gt;   &lt;int&gt;            &lt;int&gt;              &lt;int&gt;               &lt;int&gt;\n1  1119      12                7                  5                   0\n# â„¹ 4 more variables: total_missing_values &lt;int&gt;, complete_rows &lt;int&gt;,\n#   total_observations &lt;int&gt;, memory_usage &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n# convert integer values\ncars_23 %&lt;&gt;% \n  dplyr::mutate( \n    dplyr::across(\n      .cols = all_of(\n        c('comb_fe_guide_conventional_fuel',\"number_cyl\",\"number_gears\"))\n      , .fns = as.integer\n    ) \n  ) \n# replace na values\ncars_23 %&lt;&gt;% \n  tidyr::replace_na(\n    list(\n      batt_energy_capacity_amp_hrs = 0\n      , regen_braking_type_desc = \"\"\n    )\n  ) \n\n\n# convert to factors\ncars_23 %&lt;&gt;% \n  dplyr::mutate( \n    dplyr::across(\n      .cols = all_of(\n        c( 'transmission','air_aspiration_method_desc'\n           ,'regen_braking_type_desc','drive_desc'\n           ,'fuel_usage_desc_conventional_fuel'\n           ,'cyl_deact','var_valve_lift' )\n      )\n      , .fns = as.factor\n    ) \n  ) \n\n\n# create a recipe for this data\ncars_23_rec &lt;- cars_23 %&gt;% \n  recipes::recipe(comb_fe_guide_conventional_fuel~.) %&gt;% \n  recipes::step_center(recipes::all_numeric()) %&gt;%\n  recipes::step_scale(recipes::all_numeric()) %&gt;% \n  recipes::step_dummy(recipes::all_factor())\n\nsummary(cars_23_rec)\n\n# A tibble: 12 Ã— 4\n   variable                          type      role      source  \n   &lt;chr&gt;                             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 eng_displ                         &lt;chr [2]&gt; predictor original\n 2 number_cyl                        &lt;chr [2]&gt; predictor original\n 3 transmission                      &lt;chr [3]&gt; predictor original\n 4 number_gears                      &lt;chr [2]&gt; predictor original\n 5 air_aspiration_method_desc        &lt;chr [3]&gt; predictor original\n 6 regen_braking_type_desc           &lt;chr [3]&gt; predictor original\n 7 batt_energy_capacity_amp_hrs      &lt;chr [2]&gt; predictor original\n 8 drive_desc                        &lt;chr [3]&gt; predictor original\n 9 fuel_usage_desc_conventional_fuel &lt;chr [3]&gt; predictor original\n10 cyl_deact                         &lt;chr [3]&gt; predictor original\n11 var_valve_lift                    &lt;chr [3]&gt; predictor original\n12 comb_fe_guide_conventional_fuel   &lt;chr [2]&gt; outcome   original\n\n\nThere are 11 predictors at this stage in cars_23.\n\n\n\n\nExercise 7\nFor this exercise, set a sample size equal to 75% of the observations of cars_23 and split the data as follows:\n\nset.seed(1966)\n\n# sample 75% of the rows of the cars_23 dataset to make the training set\ntrain &lt;- cars_23 %&gt;% \n  # make an ID column for use as a key\n  tibble::rowid_to_column(\"ID\") %&gt;% \n  # sample the rows\n  dplyr::sample_frac(0.75)\n\n# remove the training dataset from the original dataset to make the training set\ntest  &lt;- \n  dplyr::anti_join(\n    cars_23 %&gt;% tibble::rowid_to_column(\"ID\") # add a key column to the original data\n    , train\n    , by = 'ID'\n  )\n\n# drop the ID column from training and test datasets\ntrain %&lt;&gt;% dplyr::select(-ID); test %&lt;&gt;% dplyr::select(-ID)\n\nNext prep the recipe created in the last exercise using recipes::prep on the training data, and then use the result of the prep step to recipes::bake with the training and test data. Save the baked data in separate variables for use later.\nAfter these two steps how many columns are in the data? Why does this differ from the last step?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# prep the recipe with the training data\ncars_23_prep &lt;- cars_23_rec %&gt;% \n  recipes::prep(\n    training = train    # NOTE: training data\n    , verbose = FALSE\n    , retain = TRUE\n  )\n\ncars_23_train &lt;- cars_23_prep %&gt;% \n  # create a training dataset by baking the training data\n  recipes::bake(new_data=NULL)\n  \ncars_23_test  &lt;- cars_23_prep %&gt;% \n  # create a test dataset by baking the test data\n  recipes::bake(new_data=test)\n\n# columns in the data\ncars_23_train %&gt;% dim()\n# predictors in the recipe\nsummary(cars_23_prep)\n\n\n\n[1] 839  45\n\n\n# A tibble: 45 Ã— 4\n   variable                        type      role      source  \n   &lt;chr&gt;                           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 eng_displ                       &lt;chr [2]&gt; predictor original\n 2 number_cyl                      &lt;chr [2]&gt; predictor original\n 3 number_gears                    &lt;chr [2]&gt; predictor original\n 4 batt_energy_capacity_amp_hrs    &lt;chr [2]&gt; predictor original\n 5 comb_fe_guide_conventional_fuel &lt;chr [2]&gt; outcome   original\n 6 transmission_Auto.A6.           &lt;chr [2]&gt; predictor derived \n 7 transmission_Auto.A8.           &lt;chr [2]&gt; predictor derived \n 8 transmission_Auto.A9.           &lt;chr [2]&gt; predictor derived \n 9 transmission_Auto.AM.S6.        &lt;chr [2]&gt; predictor derived \n10 transmission_Auto.AM.S7.        &lt;chr [2]&gt; predictor derived \n# â„¹ 35 more rows\n\n\n\nThere are now 45 columns in the data and 45 predictors in the recipe. This is because the recipe converted the factors to dummy variables.\n\n\n\n\nExercise 8\nIn this exercise we will run xgboost::xgboost to evaluate the regression.\nFirst run fit the model with default meta-parameters for max_depth and eta, using the training data per the code below:\n\nuntuned_xgb &lt;-\n  xgboost::xgboost(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    max_depth = 6,\n    eta = .25\n    , verbose = FALSE\n  )\n\nNext use the fitted model to predict the outcome using the test data:\n\n# create predictions using the test data and the fitted model\nyhat &lt;- predict(\n  untuned_xgb\n  , cars_23_test %&gt;% \n    dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% \n    as.matrix() \n)\n\nFinally, pull out the comb_fe_guide_conventional_fuel column from the test data, assign it to the variable y and then use caret::postResample with arguments yhat and y to evaluate how well the model fits.\nWhat is the RMSE for the un-tuned model?\n\n\n\n\n\n\nSOLUTION:\n\n\n\nThe RMSE is approximately 0.245.\n\n# select the observations in the test data\ny &lt;- cars_23_test %&gt;% \n    dplyr::pull(comb_fe_guide_conventional_fuel) \n\n# compare observations and predictions\ncaret::postResample(yhat, y)\n\n     RMSE  Rsquared       MAE \n0.2446266 0.9379850 0.1729323 \n\n\n\n\n\n\nExercise 9\nIn this exercise we are going to tune the model using cross validation. First we create a tuning grid for the parameters and then fit the model for all the values in the grid, saving the results.\nFinally, we select the best parameters by least RMSE.\n\n#create hyperparameter grid\nhyper_grid &lt;- expand.grid(max_depth = seq(3, 6, 1), eta = seq(.2, .35, .01))  \n\n# initialize our metric variables\nxgb_train_rmse &lt;- NULL\nxgb_test_rmse  &lt;- NULL\n\nfor (j in 1:nrow(hyper_grid)) {\n  set.seed(123)\n  m_xgb_untuned &lt;- xgboost::xgb.cv(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    nfold = 5,\n    max_depth = hyper_grid$max_depth[j],\n    eta = hyper_grid$eta[j],\n    verbose = FALSE\n  )\n  \n  xgb_train_rmse[j] &lt;- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]\n  xgb_test_rmse[j] &lt;- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]\n}    \n\nbest &lt;- hyper_grid[which(xgb_test_rmse == min(xgb_test_rmse)),]; best # there may be ties\n\n   max_depth  eta\n50         4 0.32\n\n\nre-run the code from the last exercise and evaluate the fit using the best tuning parameters.\nIs the tuned model better than the un-tuned model? If better, how much has the RMSE improved (in %).\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\ntuned_xgb &lt;-\n  xgboost::xgboost(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    max_depth = best[1,1],        # this is the best-fit max_depth\n    eta = best[1,2],              # this is the best-fit eta\n    verbose = FALSE\n  ) \n\nNow the RMSE is 0.2425948/0.2446266 - about 1% better than the untuned model\n\nyhat &lt;- predict(\n  tuned_xgb\n  , cars_23_test %&gt;% \n    dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% \n    as.matrix() \n)\n\ny &lt;- cars_23_test %&gt;% \n    dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% \n    as.matrix() \ncaret::postResample(yhat, y)\n\n     RMSE  Rsquared       MAE \n0.2425948 0.9387286 0.1709782 \n\n\n\n\n\n\nExercise 10\nUsing xgboost::xgb.importance rank the importance of each predictor in the model. Finally, take the top 10 predictors by importance and plot them using xgboost::xgb.plot.importance.\nPer this model, what is the most important feature for predicting fuel efficiency?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# create the importance matrix\nimportance_matrix &lt;- xgboost::xgb.importance(model = tuned_xgb)\n\n# plot the importance measures\nxgboost::xgb.plot.importance(importance_matrix[1:10,], xlab = \"Feature Importance\")\n\n\n\n\n\n\n\n\nPer the model, engine displacement is the most important feature for predicting fuel efficiency."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_3_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_3_solutions.html#grading",
    "title": "Lab 3 - Regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "misc/10-continuous-g-computation-exercises.html",
    "href": "misc/10-continuous-g-computation-exercises.html",
    "title": "Continuous exposures and g-computation",
    "section": "",
    "text": "library(tidyverse)\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(touringplans)\nlibrary(splines)\nFor this set of exercises, weâ€™ll use g-computation to calculate a causal effect for continuous exposures.\nIn the touringplans data set, we have information about the posted waiting times for rides. We also have a limited amount of data on the observed, actual times. The question that we will consider is this: Do posted wait times (avg_spostmin) for the Seven Dwarves Mine Train at 8 am affect actual wait times (avg_sactmin) at 9 am? Hereâ€™s our DAG:\nFirst, letâ€™s wrangle our data to address our question: do posted wait times at 8 affect actual weight times at 9? Weâ€™ll join the baseline data (all covariates and posted wait time at 8) with the outcome (average actual time). We also have a lot of missingness for avg_sactmin, so weâ€™ll drop unobserved values for now.\nYou donâ€™t need to update any code here, so just run this.\neight &lt;- seven_dwarfs_train_2018 |&gt;\n  dplyr::filter(wait_hour == 8) |&gt;\n  dplyr::select(-wait_minutes_actual_avg)\n\nnine &lt;- seven_dwarfs_train_2018 |&gt;\n  dplyr::filter(wait_hour == 9) |&gt;\n  dplyr::select(park_date, wait_minutes_actual_avg)\n\nwait_times &lt;- eight |&gt;\n  dplyr::left_join(nine, by = \"park_date\") |&gt;\n  tidyr::drop_na(wait_minutes_actual_avg)"
  },
  {
    "objectID": "misc/10-continuous-g-computation-exercises.html#stretch-goal-boostrapped-intervals",
    "href": "misc/10-continuous-g-computation-exercises.html#stretch-goal-boostrapped-intervals",
    "title": "Continuous exposures and g-computation",
    "section": "Stretch goal: Boostrapped intervals",
    "text": "Stretch goal: Boostrapped intervals\nLike propensity-based models, we need to do a little more work to get correct standard errors and confidence intervals. In this stretch goal, use rsample to bootstrap the estimates we got from the G-computation model.\nRemember, you need to bootstrap the entire modeling process, including the regression model, cloning the data sets, and calculating the effects.\n\nset.seed(1234)\nlibrary(rsample)\n\nfit_gcomp &lt;- function(split, ...) { \n  .df &lt;- analysis(split) \n  \n  # fit outcome model. remember to model using `.df` instead of `wait_times`\n  standardized_model &lt;-  lm(\n    wait_minutes_actual_avg ~ ns(wait_minutes_posted_avg, df = 2)*park_extra_magic_morning + park_temperature_high + park_ticket_season + park_close, \n    data = .df\n  )\n  \n  # clone datasets. remember to clone `.df` instead of `wait_times`\n  thirty &lt;- .df |&gt;\n    mutate(wait_minutes_posted_avg = 30)\n  \n  sixty &lt;- .df |&gt;\n    mutate(wait_minutes_posted_avg = 60)\n  \n  # predict actual wait time for each cloned dataset\n  predicted_thirty &lt;- standardized_model |&gt;\n    augment(newdata = thirty) |&gt;\n    select(thirty_posted_minutes = .fitted)\n  \n  predicted_sixty &lt;- standardized_model |&gt;\n    augment(newdata = sixty) |&gt;\n    select(sixty_posted_minutes = .fitted)\n  \n  # calculate ATE\n  bind_cols(predicted_thirty, predicted_sixty) |&gt;\n    summarize(\n      mean_thirty = mean(thirty_posted_minutes),\n      mean_sixty = mean(sixty_posted_minutes),\n      difference = mean_sixty - mean_thirty\n    ) |&gt;\n    # rsample expects a `term` and `estimate` column\n    pivot_longer(everything(), names_to = \"term\", values_to = \"estimate\")\n}\n\ngcomp_results &lt;- bootstraps(wait_times, 1000, apparent = TRUE) |&gt;\n  mutate(results = map(splits, fit_gcomp))\n\n# using bias-corrected confidence intervals\nboot_estimate &lt;- int_bca(gcomp_results, results, .fn = fit_gcomp)\n\nboot_estimate\n\n# A tibble: 3 Ã— 6\n  term        .lower .estimate .upper .alpha .method\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n1 difference   -24.8     -1.23   28.6   0.05 BCa    \n2 mean_sixty    16.8     34.6    58.1   0.05 BCa    \n3 mean_thirty   24.2     35.9    49.0   0.05 BCa"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html",
    "href": "misc/in_process/Causal_via_Prediction_R.html",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "",
    "text": "The traditional go-to tool for causal inference is multiple regression: Yi=Î´Di+Xiâ€²Î²+Îµi,\nY_i = \\delta D_i + X_i'\\beta+\\varepsilon_i,\n where DiD_i is the â€œtreatmentâ€ or causal variable whose effects we are interested in, and XiX_i is a vector of controls, conditional on which we are willing to assume DiD_i is as good as randomly assigned.\n\nexample: Suppose we are interested in the magnitude of racial discrimination in the labor market. One way to conceptualize this is the difference in earnings between two workers who are identical in productivity, but differ in their race, or, the â€œeffectâ€ of race. Then DiD_i would be an indicator for, say, a Black worker. YiY_i would be earnings, and XiX_i would be characteristics that capture determinants of productivity, including educational attainment, cognitive ability, and other background characteristics.\n\nWhere does machine learning fit into causal inference? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Donâ€™t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}. Modern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient Î´\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate Î´\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, YÌƒ\\tilde{Y}% _{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where YÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime }\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, DÌƒ\\tilde{D}% _{i}=D_{i}-\\hat{D}_{i}^{OLS}, where DÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime }\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress YÌƒi\\tilde{Y}_{i} on DÌƒi\\tilde{D}_{i}.\n\nSteps 1 and 2 are prediction exercisesâ€“MLâ€™s wheelhouse. When OLS isnâ€™t the right tool for the job, we can replace OLS in those steps with machine learning:\n\nPredict YiY_{i} based on XiX_{i} using ML and compute the residuals, YÌƒ\\tilde{Y}% _{i}=Y_{i}-\\hat{Y}_{i}^{ML}, where YÌ‚iML\\hat{Y}_{i}^{ML} is the prediction from an ML algorithm\nPredict DiD_{i} based on XiX_{i} using ML and compute the residuals, DÌƒ\\tilde{D}% _{i}=D_{i}-\\hat{D}_{i}^{ML}, where DÌ‚iML\\hat{D}_{i}^{ML} is the prediction from an ML algorithm\nRegress YÌƒi\\tilde{Y}_{i} on DÌƒi\\tilde{D}_{i}.\n\nThis is the basis for the two major methods weâ€™ll look at today: The first is â€œPost-Double Selection Lassoâ€ (Belloni, Chernozhukov, Hansen). The second is â€œDouble-Debiased Machine Learningâ€ (Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins)"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#define-outcome-regressor-of-interest",
    "href": "misc/in_process/Causal_via_Prediction_R.html#define-outcome-regressor-of-interest",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Define outcome & regressor of interest",
    "text": "Define outcome & regressor of interest\n\ny: lnw_2016\nd: black"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#simple-regression-with-no-controls",
    "href": "misc/in_process/Causal_via_Prediction_R.html#simple-regression-with-no-controls",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Simple Regression with no Controls",
    "text": "Simple Regression with no Controls\nRegress y on d and print out coefficient\n\nlm(lnw_2016 ~ black, data = nlsy)\n\n\nCall:\nlm(formula = lnw_2016 ~ black, data = nlsy)\n\nCoefficients:\n(Intercept)        black  \n     3.1792      -0.3817  \n\n\n\nâ€¦\nIs this the effect weâ€™re looking for?\nLetâ€™s try a regression where we control for a few things: education (linearly), experience (linearly), and cognitive ability (afqt, linearly).\n\nfit &lt;- lm(lnw_2016 ~ black + educ + exp + afqt, data = nlsy)\n\nbroom::tidy(fit)\n\n# A tibble: 5 Ã— 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   1.15      0.499       2.31 0.0210    \n2 black        -0.262     0.0639     -4.10 0.0000444 \n3 educ          0.0893    0.0195      4.57 0.00000532\n4 exp           0.0362    0.0167      2.17 0.0305    \n5 afqt          0.0371    0.0102      3.63 0.000296  \n\n\n\n\nâ€¦\nHow does it compare to the simple regression?\nBut who is to say the controls we included are sufficient? We have a whole host (hundred!) of other potential controls, not to mention that perhaps the controls we did put in enter linearly. This is a job for ML!\nTo prep, letâ€™s define a matrix X with all of our potential controls:\n\npotential_controls &lt;- setdiff(colnames(nlsy), c(\"lnw_2016\", \"black\"))"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#post-double-selection-lasso",
    "href": "misc/in_process/Causal_via_Prediction_R.html#post-double-selection-lasso",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Post Double Selection Lasso",
    "text": "Post Double Selection Lasso\n\nStep 1: Lasso the outcome on X\nDonâ€™t forget to standard Xs, or choose the normalize=True option\n\nnlsy_rec_y &lt;- nlsy %&gt;% \n  recipes::recipe(lnw_2016 ~ .) %&gt;% \n  recipes::step_rm(black)\n\nnlsy_rec_d &lt;- nlsy %&gt;% \n  recipes::recipe(black ~ .) %&gt;% \n  recipes::step_rm(lnw_2016)\n\n\n# Run cross-validation for y\nlasso_y &lt;- \n  glmnet::cv.glmnet(\n    x = model.matrix(lnw_2016 ~ ., data = nlsy_bake)\n    , y = nlsy_bake$lnw_2016\n  )\n\n\nset.seed(8740)\n\nlr_cv   &lt;- rsample::vfold_cv(nlsy)\nlr_grid &lt;- tibble::tibble(penalty = 10^seq(-4,1, length.out = 200))\n\nlr_mod  &lt;- parsnip::linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  parsnip::set_engine(\"glmnet\") %&gt;% \n  parsnip::set_mode(\"regression\")\n\nlr_wf_y &lt;- workflows::workflow() %&gt;%\n  workflows::add_model(lr_mod) %&gt;%\n  workflows::add_recipe(nlsy_rec_y)\n\nlr_best_y &lt;- lr_wf_y %&gt;%\n  tune::tune_grid(\n    resamples = lr_cv,\n    grid = lr_grid\n  ) %&gt;%\n  tune::select_best(metric = \"rmse\")\n\ntidy_keep_y &lt;- lr_wf_y %&gt;% \n  tune::finalize_workflow(lr_best_y) %&gt;%\n  parsnip::fit(nlsy) %&gt;%\n  workflows::extract_fit_parsnip() %&gt;%\n  broom::tidy() %&gt;% \n  dplyr::filter(estimate != 0 & term != '(Intercept)') %&gt;% \n  dplyr::pull(term)\n\n\nX = as.matrix(nlsy[, potential_controls])\ny = nlsy[[\"lnw_2016\"]]\n\n# Run cross-validation for y\nlasso_y &lt;- glmnet::cv.glmnet(x=X, y=y)\n\nlasso_y_coefs = coef(lasso_y, lasso_y$lambda.1se)\nlasso_y_coefs = as.matrix(lasso_y_coefs)\n\nkeep_y = rownames(lasso_y_coefs)[lasso_y_coefs != 0]\n# Don't need intercept\nkeep_y = setdiff(keep_y, \"(Intercept)\")\n\n\n\nStep 2: Lasso the treatment on d\n\nlr_wf_d &lt;- lr_wf_y %&gt;% workflows::update_recipe(nlsy_rec_d)\n\nlr_best_d &lt;- lr_wf_d %&gt;%\n  tune::tune_grid(\n    resamples = lr_cv,\n    grid = lr_grid\n  ) %&gt;%\n  tune::select_best(metric = \"rmse\")\n\nâ†’ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\ntidy_keep_d &lt;- lr_wf_d %&gt;% \n  tune::finalize_workflow(lr_best_d) %&gt;%\n  parsnip::fit(nlsy) %&gt;%\n  workflows::extract_fit_parsnip() %&gt;%\n  broom::tidy() %&gt;% \n  dplyr::filter(estimate != 0 & term != '(Intercept)') %&gt;% \n  dplyr::pull(term)\n\n\n# Run cross-validation for d\nd = nlsy[[\"black\"]]\nlasso_d &lt;- cv.glmnet(x=X, y=d)\n\nlasso_d_coefs = coef(lasso_d, lasso_d$lambda.1se)\nlasso_d_coefs = as.matrix(lasso_d_coefs)\n\nkeep_d = rownames(lasso_d_coefs)[lasso_d_coefs != 0]\n# Don't need intercept\nkeep_d = setdiff(keep_d, \"(Intercept)\")\n\n\n\nStep 3: Form the union of controls\n\ntidy_keep &lt;- union(tidy_keep_y, tidy_keep_d)\n\n\nkeep = union(keep_y, keep_d)\n\n\n\nConcatenate treatment with union of controls and regress y on that and print out estimate\n\n# Need `` surrounding variables since some variables start with underscore\nformula = paste(\n  \"lnw_2016 ~ black + \", \n  paste0(\"`\", tidy_keep, \"`\", collapse = \" + \")\n)\nformula = as.formula(formula)\n\nlm(formula, data = nlsy) %&gt;% broom::tidy()\n\n# A tibble: 147 Ã— 5\n   term                  estimate std.error statistic       p.value\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 (Intercept)             1.92      0.313       6.15 0.00000000108\n 2 black                  -0.235     0.0912     -2.57 0.0102       \n 3 educ                    0.0506    0.0109      4.62 0.00000425   \n 4 afqt                    0.0310    0.0113      2.74 0.00633      \n 5 youth_bothbio_01_1997   0.205     0.194       1.05 0.292        \n 6 p4_001_1997            -0.0557    0.0316     -1.76 0.0787       \n 7 `_BGypc9_001_3`        -0.181     0.0563     -3.21 0.00134      \n 8 `_BGypc12_02b4`        -0.216     0.0862     -2.51 0.0123       \n 9 `_BGyysaq_29b7`         0.258     0.0839      3.07 0.00219      \n10 `_BGyfp_yhro_21`       -0.372     0.130      -2.87 0.00413      \n# â„¹ 137 more rows\n\n# lr_wf_final &lt;- lr_wf_d %&gt;% workflows::update_formula(as.formula(formula))\n\n\n# Need `` surrounding variables since some variables start with underscore\nformula = paste(\n  \"lnw_2016 ~ black + \", \n  paste0(\"`\", keep, \"`\", collapse = \" + \")\n)\nformula = as.formula(formula)\n\n(fullreg = feols(formula, data = nlsy))\n\nThe variables '`_BGpp4_029__1`' and '`_BGpfp_yfmr_4`' have been removed because of collinearity (see $collin.var).\n\n\nOLS estimation, Dep. Var.: lnw_2016\nObservations: 1,266\nStandard-errors: IID \n                            Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)                 2.341612   0.225027 10.405919  &lt; 2.2e-16 ***\nblack                      -0.141191   0.086111 -1.639644 1.0134e-01    \neduc                        0.054290   0.010662  5.091736 4.1163e-07 ***\nafqt                        0.038466   0.011318  3.398817 6.9899e-04 ***\nhispanic                    0.068641   0.079383  0.864671 3.8739e-01    \nyhea_2200_1997              0.000995   0.000701  1.419437 1.5603e-01    \np4_001_1997                -0.067009   0.026496 -2.528978 1.1567e-02 *  \ncv_bio_mom_age_child1_1997 -0.002313   0.004404 -0.525223 5.9953e-01    \n... 58 coefficients remaining (display them with summary() or use argument n)\n... 2 variables were removed because of collinearity (`_BGpp4_029__1` and `_BGpfp_yfmr_4`)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.802714   Adj. R2: 0.100286"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#double-debiased-machine-learning",
    "href": "misc/in_process/Causal_via_Prediction_R.html#double-debiased-machine-learning",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Double-Debiased Machine Learning",
    "text": "Double-Debiased Machine Learning\nFor simplicity, we will first do it without sample splitting\n\nStep 1: Ridge outcome on Xs, get residuals\n\n# Run cross-validation for y\nridge_y &lt;- cv.glmnet(x=X, y=y, alpha = 0)\n\ny_hat = predict(ridge_y, ridge_y$lambda.1se, newx = X)\nnlsy$y_resid = nlsy$lnw_2016 - as.numeric(y_hat)\n\n\n\nStep 2: Ridge treatment on Xs, get residuals\n\n# Run cross-validation for y\nridge_d &lt;- cv.glmnet(x=X, y=d, alpha = 0)\n\nd_hat = predict(ridge_d, ridge_d$lambda.1se, newx = X)\nnlsy$d_resid = nlsy$black - as.numeric(d_hat)\n\n\n\nStep 3: Regress y resids on d resids and print out estimate\n\nfeols(y_resid ~ d_resid, nlsy)\n\nOLS estimation, Dep. Var.: y_resid\nObservations: 1,266\nStandard-errors: IID \n                 Estimate Std. Error      t value   Pr(&gt;|t|)    \n(Intercept) -1.320000e-15   0.024100 -5.50000e-14 1.00000000    \nd_resid     -3.167224e-01   0.083147 -3.80917e+00 0.00014613 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.856812   Adj. R2: 0.010567\n\n\n\n\nThe real thing: with sample splitting\n\nset.seed(5)\nN_folds = 5\n# 5 folds with equal \nnlsy$fold_id = sample(1:N_folds, size = nrow(nlsy), replace = T)\n\nnlsy$y_resid = 0\nnlsy$d_resid = 0\n\n# Loop through each fold, use other 4 folds to estimate\nfor(i in 1:5) {\n  in_training = (nlsy$fold_id != i)\n  in_test = (nlsy$fold_id == i)\n\n  # Ridge regression for y using training\n  ridge_y = cv.glmnet(\n    x=X[in_training,], y=y[in_training], alpha = 0\n  )\n  # Calculate residuals for testing\n  nlsy[in_test, \"y_resid\"] =\n    y[in_test] - predict(ridge_y, newx = X[in_test, ])\n\n  # Ridge regression for d using training\n  ridge_d = cv.glmnet(\n    x=X[in_training,], y=d[in_training], alpha = 0\n  )\n  # Calculate residuals for testing\n  nlsy[in_test, \"d_resid\"] = \n    d[in_test] - predict(ridge_d, newx = X[in_test, ])\n}\n\n# k-fold cross-validation ensures standard errors are fine\nfeols(\n  y_resid ~ d_resid, data = nlsy, vcov = \"hc1\"\n)\n\nOLS estimation, Dep. Var.: y_resid\nObservations: 1,266\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept) -0.000992   0.024262 -0.040884 0.96739479    \nd_resid     -0.302294   0.077506 -3.900259 0.00010114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.862505   Adj. R2: 0.013436"
  },
  {
    "objectID": "misc/in_process/Causal_via_Prediction_R.html#now-do-dml-using-random-forest",
    "href": "misc/in_process/Causal_via_Prediction_R.html#now-do-dml-using-random-forest",
    "title": "Where ML Fits into Causal Inference (review)",
    "section": "Now do DML using Random Forest!",
    "text": "Now do DML using Random Forest!\n\nset.seed(5)\nN_folds = 5\n# 5 folds with equal \nnlsy$fold_id = sample(1:N_folds, size = nrow(nlsy), replace = T)\n\nnlsy$y_resid = 0\nnlsy$d_resid = 0\n\n# Loop through each fold, use other 4 folds to estimate\nfor(i in 1:5) {\n  in_training = (nlsy$fold_id != i)\n  in_test = (nlsy$fold_id == i)\n\n  # Ridge regression for y using training\n  ridge_y = randomForest(\n    x = X[in_training,], y = y[in_training]\n  )\n  # Calculate residuals for testing\n  nlsy[in_test, \"y_resid\"] =\n    y[in_test] - predict(ridge_y, newdata = X[in_test, ])\n\n  # Ridge regression for d using training\n  ridge_d = randomForest(\n    x = X[in_training,], y = d[in_training]\n  )\n  # Calculate residuals for testing\n  nlsy[in_test, \"d_resid\"] = \n    d[in_test] - predict(ridge_d, newdata = X[in_test, ])\n}\n\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\nWarning in randomForest.default(x = X[in_training, ], y = d[in_training]): The\nresponse has five or fewer unique values.  Are you sure you want to do\nregression?\n\n# k-fold cross-validation ensures standard errors are fine\nfeols(\n  y_resid ~ d_resid, data = nlsy, vcov = \"hc1\"\n)\n\nOLS estimation, Dep. Var.: y_resid\nObservations: 1,266\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept) -0.007765   0.023164 -0.335205  0.73753    \nd_resid     -0.136571   0.074804 -1.825707  0.06813 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.823375   Adj. R2: 0.002127"
  },
  {
    "objectID": "misc/rcts_to_regression.html",
    "href": "misc/rcts_to_regression.html",
    "title": "RCTs to Regression",
    "section": "",
    "text": "Treatment indicator: Diâˆˆ0,1D_i\\in {0,1}\n\nexample: eligibility for expanded Medicaid\n\nOutcome: YiY_{i}\n\nexample: number of doctor visits in past 6 months\n\nPotential outcomes Yi0,Yi1Y_{i}^{0},Y_{i}^{1}\nIndividual-level treatment effect Î”iY=Yi1âˆ’Yi0\\Delta_i^Y=Y_i^1-Y_i^0 (can never know this).\nUnbiased estimate of average treatment effect:\nÎ”Y=ğ”¼[Yi1âˆ’Yi0]\n\\Delta^{Y}=\\mathbb{E}\\left[Y_{i}^{1}-Y_{i}^{0}\\right]\n\nor OLS coefficient on ğ·ğ‘– from this regression:\nYi=Î±+Î´Di+ÏµiY_{i}=\\alpha+\\delta D_{i}+\\epsilon_{i}\n\nurl_str  &lt;- \n  'https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/'\nfile_str &lt;- \n  'oregon_hie_table5.csv?raw=true'\n  \ndat &lt;- readr::read_csv(\n  paste0(url_str, file_str, show_col_types = FALSE)\n) %&gt;% \n  dplyr::select(doc_num, treatment, weight, dplyr::starts_with(\"ddd\")) %&gt;% \n  tidyr::drop_na()\n\n\nfit &lt;- lm(doc_num ~ treatment, data = dat, weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits: {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits: 0.268\n\n\n\n\n\nThe bivariate regression above leans heavily on random assignment of treatment:\nDiâŠ¥âŠ¥Yi0,Yi1D_{i}\\perp\\!\\!\\!\\!\\perp Y_{i}^{0},Y_{i}^{1}\nSometimes, even in an RCT, treatment is assigned randomly only conditional on some set of covariates XiX_i.\n\nexample: in the Oregon HIE, eligibility for Medicaid was granted via lottery, but households with more members could have more lottery entries. So the lottery outcome is random only conditional on household size.\n\nSo what happens if we donâ€™t have random assignment? In terms of our regression model above, it means Ïµi\\epsilon_i may be correlated with DiD_i. For example, perhaps household size, XiX_i, which increases the probability of treatment, is also associated with more doctor visits. If XiX_i is omitted from the model, it is buried in the error term:\nÏµi=Î²Xi+Î·i\n\\epsilon_{i}=\\beta X_{i}+\\eta_{i}\n Weâ€™ll assume for now that everything else related to doctor visits (Î·i\\eta_i) is unrelated to treatment. What does our bivariate regression coefficient deliver in this case?\nÎ´Ì‚OLS=Cov(Yi,Di)Var(Di)=Î´+Î²Cov(Xi,Di)Var(Di)(1)\\hat{\\delta}^{\\text{OLS}}=\\frac{Cov\\left(Y_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}=\\delta+\\beta\\frac{Cov\\left(X_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}\n \\qquad(1)\nSimple regression gives us what we want (Î´\\delta) plus an omitted variables bias term. The form of this term tells us what kinds of XiX_i variables we should take care to control for in our regressions.\nAccording to the ommitted variable bias (OVB) formula (EquationÂ 1), what kinds of variables should you be be sure to control for in regressions?\nCareful investigators will find a set of regressors XiX_i for which they are willing to assume treatment is as good as randomly assigned:\nDiâŠ¥âŠ¥(Yi(0),Yi(1))|Xi.\nD_i\\perp\\!\\!\\!\\!\\perp\\left( Y_{i}\\left( 0\\right) ,Y_{i}\\left( 1\\right) \\right) |X_{i}\n\\text{.}\n This combined with a linear model for the conditional expectation of Yi0Y_{i}^0 and Yi1Y_{i}^1 given XiX_{i} means we can estimate the average treatment via OLS on the following regression equation:\nYi=Î´Di+Xiâ€²Î²+Îµi.\nY_{i}=\\delta D_{i}+X_{i}^{\\prime }\\beta +\\varepsilon _{i}.\n\n\n# Add the household size indicators to our regressor set and run regression:\nfit &lt;- lm(doc_num ~ ., data = dat %&gt;% dplyr::select(-weight), weights = dat$weight)\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (with controls): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (with controls): 0.314\n\n\nHow did the estimate of the effect of Medicaid eligiblity change? What does that tell us about the relationship between the included regressors and the outcome and treatment?\n\n\n\nWhere does machine learning fit into this? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Donâ€™t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}.\nModern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient Î´\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate Î´\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, YÌƒi=Yiâˆ’YÌ‚iOLS\\tilde{Y}_{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where YÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, DÌƒi=Diâˆ’DÌ‚iOLS\\tilde{D}_{i}=D_{i}-\\hat{D}_{i}^{OLS}, where DÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress YÌƒi\\tilde{Y}_{i} on DÌƒi\\tilde{D}_{i}.\n\nLetâ€™s try it!\n\n# Regress outcome on covariates (not treatment)\nyreg &lt;- lm(\n  doc_num ~ .\n  , data = dat %&gt;% dplyr::select(doc_num, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\nytilde = yreg$residuals\n\n# regress treatment on covariates (not outcome)\ndreg &lt;- lm(\n  treatment ~ .\n  , data = dat %&gt;% dplyr::select(treatment, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\ndtilde = dreg$residuals\n\n# regress ytilde on dtilde\nfit &lt;- lm(\n  ytilde ~ dtilde\n  , data = tibble::tibble(ytilde = ytilde, dtilde = dtilde)\n  , weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'dtilde') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (partialled out): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (partialled out): 0.314\n\n\nML enters the picture by providing an alternate way to generate YÌ‚i\\hat{Y}_i and DÌ‚i\\hat{D}_i when OLS is not the best tool for the job. The first two steps are really just prediction exercises, and in principle any supervised machine learning algorithm can step in here.\nBack to the whiteboard!"
  },
  {
    "objectID": "misc/rcts_to_regression.html#rcts-to-regression",
    "href": "misc/rcts_to_regression.html#rcts-to-regression",
    "title": "RCTs to Regression",
    "section": "",
    "text": "Treatment indicator: Diâˆˆ0,1D_i\\in {0,1}\n\nexample: eligibility for expanded Medicaid\n\nOutcome: YiY_{i}\n\nexample: number of doctor visits in past 6 months\n\nPotential outcomes Yi0,Yi1Y_{i}^{0},Y_{i}^{1}\nIndividual-level treatment effect Î”iY=Yi1âˆ’Yi0\\Delta_i^Y=Y_i^1-Y_i^0 (can never know this).\nUnbiased estimate of average treatment effect:\nÎ”Y=ğ”¼[Yi1âˆ’Yi0]\n\\Delta^{Y}=\\mathbb{E}\\left[Y_{i}^{1}-Y_{i}^{0}\\right]\n\nor OLS coefficient on ğ·ğ‘– from this regression:\nYi=Î±+Î´Di+ÏµiY_{i}=\\alpha+\\delta D_{i}+\\epsilon_{i}\n\nurl_str  &lt;- \n  'https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/'\nfile_str &lt;- \n  'oregon_hie_table5.csv?raw=true'\n  \ndat &lt;- readr::read_csv(\n  paste0(url_str, file_str, show_col_types = FALSE)\n) %&gt;% \n  dplyr::select(doc_num, treatment, weight, dplyr::starts_with(\"ddd\")) %&gt;% \n  tidyr::drop_na()\n\n\nfit &lt;- lm(doc_num ~ treatment, data = dat, weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits: {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits: 0.268\n\n\n\n\n\nThe bivariate regression above leans heavily on random assignment of treatment:\nDiâŠ¥âŠ¥Yi0,Yi1D_{i}\\perp\\!\\!\\!\\!\\perp Y_{i}^{0},Y_{i}^{1}\nSometimes, even in an RCT, treatment is assigned randomly only conditional on some set of covariates XiX_i.\n\nexample: in the Oregon HIE, eligibility for Medicaid was granted via lottery, but households with more members could have more lottery entries. So the lottery outcome is random only conditional on household size.\n\nSo what happens if we donâ€™t have random assignment? In terms of our regression model above, it means Ïµi\\epsilon_i may be correlated with DiD_i. For example, perhaps household size, XiX_i, which increases the probability of treatment, is also associated with more doctor visits. If XiX_i is omitted from the model, it is buried in the error term:\nÏµi=Î²Xi+Î·i\n\\epsilon_{i}=\\beta X_{i}+\\eta_{i}\n Weâ€™ll assume for now that everything else related to doctor visits (Î·i\\eta_i) is unrelated to treatment. What does our bivariate regression coefficient deliver in this case?\nÎ´Ì‚OLS=Cov(Yi,Di)Var(Di)=Î´+Î²Cov(Xi,Di)Var(Di)(1)\\hat{\\delta}^{\\text{OLS}}=\\frac{Cov\\left(Y_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}=\\delta+\\beta\\frac{Cov\\left(X_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}\n \\qquad(1)\nSimple regression gives us what we want (Î´\\delta) plus an omitted variables bias term. The form of this term tells us what kinds of XiX_i variables we should take care to control for in our regressions.\nAccording to the ommitted variable bias (OVB) formula (EquationÂ 1), what kinds of variables should you be be sure to control for in regressions?\nCareful investigators will find a set of regressors XiX_i for which they are willing to assume treatment is as good as randomly assigned:\nDiâŠ¥âŠ¥(Yi(0),Yi(1))|Xi.\nD_i\\perp\\!\\!\\!\\!\\perp\\left( Y_{i}\\left( 0\\right) ,Y_{i}\\left( 1\\right) \\right) |X_{i}\n\\text{.}\n This combined with a linear model for the conditional expectation of Yi0Y_{i}^0 and Yi1Y_{i}^1 given XiX_{i} means we can estimate the average treatment via OLS on the following regression equation:\nYi=Î´Di+Xiâ€²Î²+Îµi.\nY_{i}=\\delta D_{i}+X_{i}^{\\prime }\\beta +\\varepsilon _{i}.\n\n\n# Add the household size indicators to our regressor set and run regression:\nfit &lt;- lm(doc_num ~ ., data = dat %&gt;% dplyr::select(-weight), weights = dat$weight)\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (with controls): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (with controls): 0.314\n\n\nHow did the estimate of the effect of Medicaid eligiblity change? What does that tell us about the relationship between the included regressors and the outcome and treatment?\n\n\n\nWhere does machine learning fit into this? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Donâ€™t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}.\nModern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient Î´\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate Î´\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, YÌƒi=Yiâˆ’YÌ‚iOLS\\tilde{Y}_{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where YÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, DÌƒi=Diâˆ’DÌ‚iOLS\\tilde{D}_{i}=D_{i}-\\hat{D}_{i}^{OLS}, where DÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress YÌƒi\\tilde{Y}_{i} on DÌƒi\\tilde{D}_{i}.\n\nLetâ€™s try it!\n\n# Regress outcome on covariates (not treatment)\nyreg &lt;- lm(\n  doc_num ~ .\n  , data = dat %&gt;% dplyr::select(doc_num, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\nytilde = yreg$residuals\n\n# regress treatment on covariates (not outcome)\ndreg &lt;- lm(\n  treatment ~ .\n  , data = dat %&gt;% dplyr::select(treatment, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\ndtilde = dreg$residuals\n\n# regress ytilde on dtilde\nfit &lt;- lm(\n  ytilde ~ dtilde\n  , data = tibble::tibble(ytilde = ytilde, dtilde = dtilde)\n  , weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'dtilde') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (partialled out): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (partialled out): 0.314\n\n\nML enters the picture by providing an alternate way to generate YÌ‚i\\hat{Y}_i and DÌ‚i\\hat{D}_i when OLS is not the best tool for the job. The first two steps are really just prediction exercises, and in principle any supervised machine learning algorithm can step in here.\nBack to the whiteboard!"
  },
  {
    "objectID": "misc/in_process/Kalman.html",
    "href": "misc/in_process/Kalman.html",
    "title": "Kalman",
    "section": "",
    "text": "Sure, here is an example of how you might use the Kalman filter to predict customer churn in a banking context using R and the tidyverse package. Note that this is a simplified illustration and assumes you have a dataset with relevant variables.\n\nSample Data\nLetâ€™s assume we have a dataset customer_data with the following columns: - customer_id: Unique identifier for each customer. - transaction_freq: Frequency of transactions in the current month. - avg_balance: Average account balance in the current month. - cust_service_calls: Number of customer service interactions in the current month. - churn: Whether the customer churned (1) or not (0).\n\n\nSample Code\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(KFAS)\n\n# Generate sample data\nset.seed(123)\ncustomer_data &lt;- tibble(\n  customer_id = 1:100,\n  transaction_freq = rnorm(100, mean = 10, sd = 2),\n  avg_balance = rnorm(100, mean = 5000, sd = 1000),\n  cust_service_calls = rpois(100, lambda = 2),\n  churn = sample(c(0, 1), 100, replace = TRUE)\n)\n\n# Define the state space model\n# For simplicity, we'll use a local level model for each state variable\n# Assume normal distribution for observations\n\n# Build a state space model\nbuild_model &lt;- function(data) {\n  ssmodel &lt;- SSModel(\n    H = matrix(NA, 3, 3),\n    Q = diag(NA, 3),\n    Z = diag(3),\n    T = diag(3),\n    R = diag(3),\n    a1 = c(mean(data$transaction_freq), mean(data$avg_balance), mean(data$cust_service_calls)),\n    P1 = diag(NA, 3)\n  )\n  \n  ssmodel$y &lt;- t(as.matrix(data %&gt;% select(transaction_freq, avg_balance, cust_service_calls)))\n  return(ssmodel)\n}\n\n# Fit the model\nfit_model &lt;- function(ssmodel) {\n  fit &lt;- fitSSM(ssmodel, inits = rep(0, 6), method = \"BFGS\")\n  return(fit$model)\n}\n\n# Kalman filter application\napply_kalman_filter &lt;- function(model) {\n  kf &lt;- KFS(model, filtering = \"mean\", smoothing = \"mean\")\n  return(kf)\n}\n\n# Predict churn probabilities based on filtered states\npredict_churn &lt;- function(kf, threshold = 0.5) {\n  smoothed_states &lt;- kf$a\n  churn_prob &lt;- 1 / (1 + exp(-rowMeans(smoothed_states))) # Logistic function for churn probability\n  churn_pred &lt;- ifelse(churn_prob &gt; threshold, 1, 0)\n  return(churn_pred)\n}\n\n# Build, fit, and apply the Kalman filter model\nssmodel &lt;- build_model(customer_data)\nfitted_model &lt;- fit_model(ssmodel)\nkf_results &lt;- apply_kalman_filter(fitted_model)\n\n# Predict churn\ncustomer_data &lt;- customer_data %&gt;%\n  mutate(predicted_churn = predict_churn(kf_results))\n\n# Display results\nprint(customer_data)\n\n# Evaluate the model\nconfusion_matrix &lt;- table(customer_data$churn, customer_data$predicted_churn)\nprint(confusion_matrix)\naccuracy &lt;- sum(diag(confusion_matrix)) / sum(confusion_matrix)\nprint(paste(\"Accuracy:\", accuracy))\n\n\nExplanation:\n\nData Generation: Simulate customer data for the example.\nState Space Model Definition: Define a simple state space model where each state variable is modeled independently.\nModel Fitting: Fit the state space model using the Kalman filter.\nFiltering and Smoothing: Apply the Kalman filter and smoother to estimate the hidden states.\nChurn Prediction: Use the smoothed states to predict churn probabilities, applying a logistic function.\nEvaluation: Compare the predicted churn with the actual churn to evaluate model performance.\n\nThis code provides a simplified illustration. In a real-world application, you would need to preprocess the data, handle missing values, and potentially use more complex state space models to capture the dynamics of customer behavior more accurately.\n\n\n\n\n\n\nNote\n\n\n\nPlease cite KFAS in publications by using: \n\nJouni Helske (2017). KFAS: Exponential Family State Space Models in R. Journal of Statistical Software, 78(10), 1-39. doi:10.18637/jss.v078.i10.\n\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(KFAS)\n\n# Generate sample data\nset.seed(123)\ncustomer_data &lt;- tibble(\n  customer_id = 1:100,\n  transaction_freq = rnorm(100, mean = 10, sd = 2),\n  avg_balance = rnorm(100, mean = 5000, sd = 1000),\n  cust_service_calls = rpois(100, lambda = 2),\n  churn = sample(c(0, 1), 100, replace = TRUE)\n)\n\ncustomer_data\n\n\n# Define the state space model\n# For simplicity, we'll use a local level model for each state variable\n# Assume normal distribution for observations\n\n# Build a state space model\n# build_model &lt;- function(data) {\n#   ssmodel &lt;- KFAS::SSModel(\n#     H = matrix(NA, 3, 3),\n#     Q = diag(NA, 3),\n#     Z = diag(3),\n#     T = diag(3),\n#     R = diag(3),\n#     a1 = c(mean(data$transaction_freq), mean(data$avg_balance), mean(data$cust_service_calls)),\n#     P1 = diag(NA, 3)\n#   )\n#   \n#   ssmodel$y &lt;- t(as.matrix(data %&gt;% select(transaction_freq, avg_balance, cust_service_calls)))\n#   return(ssmodel)\n# }\n\nbuild_model &lt;- function(data) {\n  # Define the state space model using a formula\n  model &lt;- SSModel(\n    transaction_freq + avg_balance + cust_service_calls ~ \n      SSMtrend(1, Q = list(NA)),  # Local level model for each variable\n    H = matrix(NA, 3, 3),  # Observation noise covariance matrix\n    data = data\n  )\n  \n  # Assign observed data to the model\n  model$y &lt;- t(as.matrix(data %&gt;% select(transaction_freq, avg_balance, cust_service_calls)))\n  \n  return(model)\n}\n\n# Fit the model\nfit_model &lt;- function(ssmodel) {\n  fit &lt;- fitSSM(ssmodel, inits = rep(0, 6), method = \"BFGS\")\n  return(fit$model)\n}\n\n# Kalman filter application\napply_kalman_filter &lt;- function(model) {\n  kf &lt;- KFS(model, filtering = \"mean\", smoothing = \"mean\")\n  return(kf)\n}\n\n# Predict churn probabilities based on filtered states\npredict_churn &lt;- function(kf, threshold = 0.5) {\n  smoothed_states &lt;- kf$a\n  churn_prob &lt;- 1 / (1 + exp(-rowMeans(smoothed_states))) # Logistic function for churn probability\n  churn_pred &lt;- ifelse(churn_prob &gt; threshold, 1, 0)\n  return(churn_pred)\n}\n\n\n# Build, fit, and apply the Kalman filter model\nssmodel &lt;- build_model(customer_data)\nfitted_model &lt;- fit_model(ssmodel)\nkf_results &lt;- apply_kalman_filter(fitted_model)\n\n# Predict churn\ncustomer_data &lt;- customer_data %&gt;%\n  mutate(predicted_churn = predict_churn(kf_results))"
  },
  {
    "objectID": "misc/causal_scratch.html",
    "href": "misc/causal_scratch.html",
    "title": "causal_scratch",
    "section": "",
    "text": "Treatment indicator: Diâˆˆ0,1D_i\\in {0,1}\n\nexample: eligibility for expanded Medicaid\n\nOutcome: YiY_{i}\n\nexample: number of doctor visits in past 6 months\n\nPotential outcomes Yi0,Yi1Y_{i}^{0},Y_{i}^{1}\nIndividual-level treatment effect Î”iY=Yi1âˆ’Yi0\\Delta_i^Y=Y_i^1-Y_i^0 (can never know this).\nUnbiased estimate of average treatment effect:\nÎ”Y=ğ”¼[Yi1âˆ’Yi0]\n\\Delta^{Y}=\\mathbb{E}\\left[Y_{i}^{1}-Y_{i}^{0}\\right]\n\nor OLS coefficient on ğ·ğ‘– from this regression:\nYi=Î±+Î´Di+ÏµiY_{i}=\\alpha+\\delta D_{i}+\\epsilon_{i}\n\nurl_str  &lt;- \n  'https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/'\nfile_str &lt;- \n  'oregon_hie_table5.csv?raw=true'\n  \ndat &lt;- readr::read_csv(\n  paste0(url_str, file_str, show_col_types = FALSE)\n) %&gt;% \n  dplyr::select(doc_num, treatment, weight, dplyr::starts_with(\"ddd\")) %&gt;% \n  tidyr::drop_na()\n\n\nfit &lt;- lm(doc_num ~ treatment, data = dat, weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits: {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits: 0.268\n\n\n\n\n\nThe bivariate regression above leans heavily on random assignment of treatment:\nDiâŠ¥âŠ¥Yi0,Yi1D_{i}\\perp\\!\\!\\!\\!\\perp Y_{i}^{0},Y_{i}^{1}\nSometimes, even in an RCT, treatment is assigned randomly only conditional on some set of covariates XiX_i.\n\nexample: in the Oregon HIE, eligibility for Medicaid was granted via lottery, but households with more members could have more lottery entries. So the lottery outcome is random only conditional on household size.\n\nSo what happens if we donâ€™t have random assignment? In terms of our regression model above, it means Ïµi\\epsilon_i may be correlated with DiD_i. For example, perhaps household size, XiX_i, which increases the probability of treatment, is also associated with more doctor visits. If XiX_i is omitted from the model, it is buried in the error term:\nÏµi=Î²Xi+Î·i\n\\epsilon_{i}=\\beta X_{i}+\\eta_{i}\n Weâ€™ll assume for now that everything else related to doctor visits (Î·i\\eta_i) is unrelated to treatment. What does our bivariate regression coefficient deliver in this case?\nÎ´Ì‚OLS=Cov(Yi,Di)Var(Di)=Î´+Î²Cov(Xi,Di)Var(Di)(1)\\hat{\\delta}^{\\text{OLS}}=\\frac{Cov\\left(Y_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}=\\delta+\\beta\\frac{Cov\\left(X_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}\n \\qquad(1)\nSimple regression gives us what we want (Î´\\delta) plus an omitted variables bias term. The form of this term tells us what kinds of XiX_i variables we should take care to control for in our regressions.\nAccording to the ommitted variable bias (OVB) formula (EquationÂ 1), what kinds of variables should you be be sure to control for in regressions?\nCareful investigators will find a set of regressors XiX_i for which they are willing to assume treatment is as good as randomly assigned:\nDiâŠ¥âŠ¥(Yi(0),Yi(1))|Xi.\nD_i\\perp\\!\\!\\!\\!\\perp\\left( Y_{i}\\left( 0\\right) ,Y_{i}\\left( 1\\right) \\right) |X_{i}\n\\text{.}\n This combined with a linear model for the conditional expectation of Yi0Y_{i}^0 and Yi1Y_{i}^1 given XiX_{i} means we can estimate the average treatment via OLS on the following regression equation:\nYi=Î´Di+Xiâ€²Î²+Îµi.\nY_{i}=\\delta D_{i}+X_{i}^{\\prime }\\beta +\\varepsilon _{i}.\n\n\n# Add the household size indicators to our regressor set and run regression:\nfit &lt;- lm(doc_num ~ ., data = dat %&gt;% dplyr::select(-weight), weights = dat$weight)\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (with controls): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (with controls): 0.314\n\n\nHow did the estimate of the effect of Medicaid eligiblity change? What does that tell us about the relationship between the included regressors and the outcome and treatment?\n\n\n\nWhere does machine learning fit into this? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Donâ€™t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}.\nModern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient Î´\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate Î´\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, YÌƒi=Yiâˆ’YÌ‚iOLS\\tilde{Y}_{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where YÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, DÌƒi=Diâˆ’DÌ‚iOLS\\tilde{D}_{i}=D_{i}-\\hat{D}_{i}^{OLS}, where DÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress YÌƒi\\tilde{Y}_{i} on DÌƒi\\tilde{D}_{i}.\n\nLetâ€™s try it!\n\n# Regress outcome on covariates (not treatment)\nyreg &lt;- lm(\n  doc_num ~ .\n  , data = dat %&gt;% dplyr::select(doc_num, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\nytilde = yreg$residuals\n\n# regress treatment on covariates (not outcome)\ndreg &lt;- lm(\n  treatment ~ .\n  , data = dat %&gt;% dplyr::select(treatment, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\ndtilde = dreg$residuals\n\n# regress ytilde on dtilde\nfit &lt;- lm(\n  ytilde ~ dtilde\n  , data = tibble::tibble(ytilde = ytilde, dtilde = dtilde)\n  , weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'dtilde') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (partialled out): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (partialled out): 0.314\n\n\nML enters the picture by providing an alternate way to generate YÌ‚i\\hat{Y}_i and DÌ‚i\\hat{D}_i when OLS is not the best tool for the job. The first two steps are really just prediction exercises, and in principle any supervised machine learning algorithm can step in here.\nBack to the whiteboard!"
  },
  {
    "objectID": "misc/causal_scratch.html#rcts-to-regression",
    "href": "misc/causal_scratch.html#rcts-to-regression",
    "title": "causal_scratch",
    "section": "",
    "text": "Treatment indicator: Diâˆˆ0,1D_i\\in {0,1}\n\nexample: eligibility for expanded Medicaid\n\nOutcome: YiY_{i}\n\nexample: number of doctor visits in past 6 months\n\nPotential outcomes Yi0,Yi1Y_{i}^{0},Y_{i}^{1}\nIndividual-level treatment effect Î”iY=Yi1âˆ’Yi0\\Delta_i^Y=Y_i^1-Y_i^0 (can never know this).\nUnbiased estimate of average treatment effect:\nÎ”Y=ğ”¼[Yi1âˆ’Yi0]\n\\Delta^{Y}=\\mathbb{E}\\left[Y_{i}^{1}-Y_{i}^{0}\\right]\n\nor OLS coefficient on ğ·ğ‘– from this regression:\nYi=Î±+Î´Di+ÏµiY_{i}=\\alpha+\\delta D_{i}+\\epsilon_{i}\n\nurl_str  &lt;- \n  'https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/'\nfile_str &lt;- \n  'oregon_hie_table5.csv?raw=true'\n  \ndat &lt;- readr::read_csv(\n  paste0(url_str, file_str, show_col_types = FALSE)\n) %&gt;% \n  dplyr::select(doc_num, treatment, weight, dplyr::starts_with(\"ddd\")) %&gt;% \n  tidyr::drop_na()\n\n\nfit &lt;- lm(doc_num ~ treatment, data = dat, weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits: {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits: 0.268\n\n\n\n\n\nThe bivariate regression above leans heavily on random assignment of treatment:\nDiâŠ¥âŠ¥Yi0,Yi1D_{i}\\perp\\!\\!\\!\\!\\perp Y_{i}^{0},Y_{i}^{1}\nSometimes, even in an RCT, treatment is assigned randomly only conditional on some set of covariates XiX_i.\n\nexample: in the Oregon HIE, eligibility for Medicaid was granted via lottery, but households with more members could have more lottery entries. So the lottery outcome is random only conditional on household size.\n\nSo what happens if we donâ€™t have random assignment? In terms of our regression model above, it means Ïµi\\epsilon_i may be correlated with DiD_i. For example, perhaps household size, XiX_i, which increases the probability of treatment, is also associated with more doctor visits. If XiX_i is omitted from the model, it is buried in the error term:\nÏµi=Î²Xi+Î·i\n\\epsilon_{i}=\\beta X_{i}+\\eta_{i}\n Weâ€™ll assume for now that everything else related to doctor visits (Î·i\\eta_i) is unrelated to treatment. What does our bivariate regression coefficient deliver in this case?\nÎ´Ì‚OLS=Cov(Yi,Di)Var(Di)=Î´+Î²Cov(Xi,Di)Var(Di)(1)\\hat{\\delta}^{\\text{OLS}}=\\frac{Cov\\left(Y_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}=\\delta+\\beta\\frac{Cov\\left(X_{i},D_{i}\\right)}{\\text{Var}\\left(D_{i}\\right)}\n \\qquad(1)\nSimple regression gives us what we want (Î´\\delta) plus an omitted variables bias term. The form of this term tells us what kinds of XiX_i variables we should take care to control for in our regressions.\nAccording to the ommitted variable bias (OVB) formula (EquationÂ 1), what kinds of variables should you be be sure to control for in regressions?\nCareful investigators will find a set of regressors XiX_i for which they are willing to assume treatment is as good as randomly assigned:\nDiâŠ¥âŠ¥(Yi(0),Yi(1))|Xi.\nD_i\\perp\\!\\!\\!\\!\\perp\\left( Y_{i}\\left( 0\\right) ,Y_{i}\\left( 1\\right) \\right) |X_{i}\n\\text{.}\n This combined with a linear model for the conditional expectation of Yi0Y_{i}^0 and Yi1Y_{i}^1 given XiX_{i} means we can estimate the average treatment via OLS on the following regression equation:\nYi=Î´Di+Xiâ€²Î²+Îµi.\nY_{i}=\\delta D_{i}+X_{i}^{\\prime }\\beta +\\varepsilon _{i}.\n\n\n# Add the household size indicators to our regressor set and run regression:\nfit &lt;- lm(doc_num ~ ., data = dat %&gt;% dplyr::select(-weight), weights = dat$weight)\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'treatment') %&gt;% \n  dplyr::pull(estimate)\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (with controls): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (with controls): 0.314\n\n\nHow did the estimate of the effect of Medicaid eligiblity change? What does that tell us about the relationship between the included regressors and the outcome and treatment?\n\n\n\nWhere does machine learning fit into this? It might be tempting to treat this regression as a prediction exercise where we are predicting YiY_{i} given DiD_{i} and XiX_{i}. Donâ€™t give in to this temptation. We are not after a prediction for YiY_{i}, we are after a coefficient on DiD_{i}.\nModern machine learning algorithms are finely tuned for producing predictions, but along the way they compromise coefficients. So how can we deploy machine learning in the service of estimating the causal coefficient Î´\\delta?\nTo see where ML fits in, first remember that an equivalent way to estimate Î´\\delta is the following three-step procedure:\n\nRegress YiY_{i} on XiX_{i} and compute the residuals, YÌƒi=Yiâˆ’YÌ‚iOLS\\tilde{Y}_{i}=Y_{i}-\\hat{Y}_{i}^{OLS}, where YÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²Y\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y\nRegress DiD_{i} on XiX_{i} and compute the residuals, DÌƒi=Diâˆ’DÌ‚iOLS\\tilde{D}_{i}=D_{i}-\\hat{D}_{i}^{OLS}, where DÌ‚iOLS=Xiâ€²(Xâ€²X)âˆ’1Xâ€²D\\hat{D}_{i}^{OLS}=X_{i}^{\\prime}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D\nRegress YÌƒi\\tilde{Y}_{i} on DÌƒi\\tilde{D}_{i}.\n\nLetâ€™s try it!\n\n# Regress outcome on covariates (not treatment)\nyreg &lt;- lm(\n  doc_num ~ .\n  , data = dat %&gt;% dplyr::select(doc_num, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\nytilde = yreg$residuals\n\n# regress treatment on covariates (not outcome)\ndreg &lt;- lm(\n  treatment ~ .\n  , data = dat %&gt;% dplyr::select(treatment, dplyr::starts_with(\"ddd\"))\n  , weights = dat$weight\n)\n# Calculate residuals\ndtilde = dreg$residuals\n\n# regress ytilde on dtilde\nfit &lt;- lm(\n  ytilde ~ dtilde\n  , data = tibble::tibble(ytilde = ytilde, dtilde = dtilde)\n  , weights = dat$weight)\n\neffect &lt;- fit %&gt;% \n  broom::tidy() %&gt;% \n  dplyr::filter(term == 'dtilde') %&gt;% \n  dplyr::pull(estimate)\n\nstringr::str_glue(\n  \"Estimated effect of Medicaid eligibility on \n  number of doctor visits (partialled out): {scales::number(effect, accuracy = 0.001)}\"\n)\n\nEstimated effect of Medicaid eligibility on \nnumber of doctor visits (partialled out): 0.314\n\n\nML enters the picture by providing an alternate way to generate YÌ‚i\\hat{Y}_i and DÌ‚i\\hat{D}_i when OLS is not the best tool for the job. The first two steps are really just prediction exercises, and in principle any supervised machine learning algorithm can step in here.\nBack to the whiteboard!"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html",
    "title": "Lab 4 - The Models package",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to build workflows to evaluate different models and featuresets."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#introduction",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#introduction",
    "title": "Lab 4 - The Models package",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to build workflows to evaluate different models and featuresets."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#packages",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#packages",
    "title": "Lab 4 - The Models package",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(tidyverse, magrittr, gt, gtExtras, tidymodels, ggplot2)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#the-data",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#the-data",
    "title": "Lab 4 - The Models package",
    "section": "The Data",
    "text": "The Data\nToday we will be using the Ames Housing Data.\nThis is a data set from De Cock (2011) has 82 fields were recorded for 2,930 properties in Ames Iowa in the US. The version in the modeldata package is copied from the AmesHousing package but does not include a few quality columns that appear to be outcomes rather than predictors.\n\ndat &lt;- modeldata::ames\n\nThe data dictionary can be found on the internet:\n\ncat(readr::read_file(\"http://jse.amstat.org/v19n3/decock/DataDocumentation.txt\"))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-1-eda",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-1-eda",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nWrite and execute the code to perform summary EDA on the Ames Housing data using the package skimr.\n\n\n\n\n\n\nSOLUTION:\n\n\n\ndat |&gt; skimr::skim()\n\n\n\n\nData summary\n\n\nName\ndat\n\n\nNumber of rows\n2930\n\n\nNumber of columns\n74\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n40\n\n\nnumeric\n34\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nMS_SubClass\n0\n1\nFALSE\n16\nOne: 1079, Two: 575, One: 287, One: 192\n\n\nMS_Zoning\n0\n1\nFALSE\n7\nRes: 2273, Res: 462, Flo: 139, Res: 27\n\n\nStreet\n0\n1\nFALSE\n2\nPav: 2918, Grv: 12\n\n\nAlley\n0\n1\nFALSE\n3\nNo_: 2732, Gra: 120, Pav: 78\n\n\nLot_Shape\n0\n1\nFALSE\n4\nReg: 1859, Sli: 979, Mod: 76, Irr: 16\n\n\nLand_Contour\n0\n1\nFALSE\n4\nLvl: 2633, HLS: 120, Bnk: 117, Low: 60\n\n\nUtilities\n0\n1\nFALSE\n3\nAll: 2927, NoS: 2, NoS: 1\n\n\nLot_Config\n0\n1\nFALSE\n5\nIns: 2140, Cor: 511, Cul: 180, FR2: 85\n\n\nLand_Slope\n0\n1\nFALSE\n3\nGtl: 2789, Mod: 125, Sev: 16\n\n\nNeighborhood\n0\n1\nFALSE\n28\nNor: 443, Col: 267, Old: 239, Edw: 194\n\n\nCondition_1\n0\n1\nFALSE\n9\nNor: 2522, Fee: 164, Art: 92, RRA: 50\n\n\nCondition_2\n0\n1\nFALSE\n8\nNor: 2900, Fee: 13, Art: 5, Pos: 4\n\n\nBldg_Type\n0\n1\nFALSE\n5\nOne: 2425, Twn: 233, Dup: 109, Twn: 101\n\n\nHouse_Style\n0\n1\nFALSE\n8\nOne: 1481, Two: 873, One: 314, SLv: 128\n\n\nOverall_Cond\n0\n1\nFALSE\n9\nAve: 1654, Abo: 533, Goo: 390, Ver: 144\n\n\nRoof_Style\n0\n1\nFALSE\n6\nGab: 2321, Hip: 551, Gam: 22, Fla: 20\n\n\nRoof_Matl\n0\n1\nFALSE\n8\nCom: 2887, Tar: 23, WdS: 9, WdS: 7\n\n\nExterior_1st\n0\n1\nFALSE\n16\nVin: 1026, Met: 450, HdB: 442, Wd : 420\n\n\nExterior_2nd\n0\n1\nFALSE\n17\nVin: 1015, Met: 447, HdB: 406, Wd : 397\n\n\nMas_Vnr_Type\n0\n1\nFALSE\n5\nNon: 1775, Brk: 880, Sto: 249, Brk: 25\n\n\nExter_Cond\n0\n1\nFALSE\n5\nTyp: 2549, Goo: 299, Fai: 67, Exc: 12\n\n\nFoundation\n0\n1\nFALSE\n6\nPCo: 1310, CBl: 1244, Brk: 311, Sla: 49\n\n\nBsmt_Cond\n0\n1\nFALSE\n6\nTyp: 2616, Goo: 122, Fai: 104, No_: 80\n\n\nBsmt_Exposure\n0\n1\nFALSE\n5\nNo: 1906, Av: 418, Gd: 284, Mn: 239\n\n\nBsmtFin_Type_1\n0\n1\nFALSE\n7\nGLQ: 859, Unf: 851, ALQ: 429, Rec: 288\n\n\nBsmtFin_Type_2\n0\n1\nFALSE\n7\nUnf: 2499, Rec: 106, LwQ: 89, No_: 81\n\n\nHeating\n0\n1\nFALSE\n6\nGas: 2885, Gas: 27, Gra: 9, Wal: 6\n\n\nHeating_QC\n0\n1\nFALSE\n5\nExc: 1495, Typ: 864, Goo: 476, Fai: 92\n\n\nCentral_Air\n0\n1\nFALSE\n2\nY: 2734, N: 196\n\n\nElectrical\n0\n1\nFALSE\n6\nSBr: 2682, Fus: 188, Fus: 50, Fus: 8\n\n\nFunctional\n0\n1\nFALSE\n8\nTyp: 2728, Min: 70, Min: 65, Mod: 35\n\n\nGarage_Type\n0\n1\nFALSE\n7\nAtt: 1731, Det: 782, Bui: 186, No_: 157\n\n\nGarage_Finish\n0\n1\nFALSE\n4\nUnf: 1231, RFn: 812, Fin: 728, No_: 159\n\n\nGarage_Cond\n0\n1\nFALSE\n6\nTyp: 2665, No_: 159, Fai: 74, Goo: 15\n\n\nPaved_Drive\n0\n1\nFALSE\n3\nPav: 2652, Dir: 216, Par: 62\n\n\nPool_QC\n0\n1\nFALSE\n5\nNo_: 2917, Exc: 4, Goo: 4, Typ: 3\n\n\nFence\n0\n1\nFALSE\n5\nNo_: 2358, Min: 330, Goo: 118, Goo: 112\n\n\nMisc_Feature\n0\n1\nFALSE\n6\nNon: 2824, She: 95, Gar: 5, Oth: 4\n\n\nSale_Type\n0\n1\nFALSE\n10\nWD : 2536, New: 239, COD: 87, Con: 26\n\n\nSale_Condition\n0\n1\nFALSE\n6\nNor: 2413, Par: 245, Abn: 190, Fam: 46\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nLot_Frontage\n0\n1\n57.65\n33.50\n0.00\n43.00\n63.00\n78.00\n313.00\nâ–‡â–‡â–â–â–\n\n\nLot_Area\n0\n1\n10147.92\n7880.02\n1300.00\n7440.25\n9436.50\n11555.25\n215245.00\nâ–‡â–â–â–â–\n\n\nYear_Built\n0\n1\n1971.36\n30.25\n1872.00\n1954.00\n1973.00\n2001.00\n2010.00\nâ–â–‚â–ƒâ–†â–‡\n\n\nYear_Remod_Add\n0\n1\n1984.27\n20.86\n1950.00\n1965.00\n1993.00\n2004.00\n2010.00\nâ–…â–‚â–‚â–ƒâ–‡\n\n\nMas_Vnr_Area\n0\n1\n101.10\n178.63\n0.00\n0.00\n0.00\n162.75\n1600.00\nâ–‡â–â–â–â–\n\n\nBsmtFin_SF_1\n0\n1\n4.18\n2.23\n0.00\n3.00\n3.00\n7.00\n7.00\nâ–ƒâ–‚â–‡â–â–‡\n\n\nBsmtFin_SF_2\n0\n1\n49.71\n169.14\n0.00\n0.00\n0.00\n0.00\n1526.00\nâ–‡â–â–â–â–\n\n\nBsmt_Unf_SF\n0\n1\n559.07\n439.54\n0.00\n219.00\n465.50\n801.75\n2336.00\nâ–‡â–…â–‚â–â–\n\n\nTotal_Bsmt_SF\n0\n1\n1051.26\n440.97\n0.00\n793.00\n990.00\n1301.50\n6110.00\nâ–‡â–ƒâ–â–â–\n\n\nFirst_Flr_SF\n0\n1\n1159.56\n391.89\n334.00\n876.25\n1084.00\n1384.00\n5095.00\nâ–‡â–ƒâ–â–â–\n\n\nSecond_Flr_SF\n0\n1\n335.46\n428.40\n0.00\n0.00\n0.00\n703.75\n2065.00\nâ–‡â–ƒâ–‚â–â–\n\n\nGr_Liv_Area\n0\n1\n1499.69\n505.51\n334.00\n1126.00\n1442.00\n1742.75\n5642.00\nâ–‡â–‡â–â–â–\n\n\nBsmt_Full_Bath\n0\n1\n0.43\n0.52\n0.00\n0.00\n0.00\n1.00\n3.00\nâ–‡â–†â–â–â–\n\n\nBsmt_Half_Bath\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0.00\n2.00\nâ–‡â–â–â–â–\n\n\nFull_Bath\n0\n1\n1.57\n0.55\n0.00\n1.00\n2.00\n2.00\n4.00\nâ–â–‡â–‡â–â–\n\n\nHalf_Bath\n0\n1\n0.38\n0.50\n0.00\n0.00\n0.00\n1.00\n2.00\nâ–‡â–â–…â–â–\n\n\nBedroom_AbvGr\n0\n1\n2.85\n0.83\n0.00\n2.00\n3.00\n3.00\n8.00\nâ–â–‡â–‚â–â–\n\n\nKitchen_AbvGr\n0\n1\n1.04\n0.21\n0.00\n1.00\n1.00\n1.00\n3.00\nâ–â–‡â–â–â–\n\n\nTotRms_AbvGrd\n0\n1\n6.44\n1.57\n2.00\n5.00\n6.00\n7.00\n15.00\nâ–â–‡â–‚â–â–\n\n\nFireplaces\n0\n1\n0.60\n0.65\n0.00\n0.00\n1.00\n1.00\n4.00\nâ–‡â–‡â–â–â–\n\n\nGarage_Cars\n0\n1\n1.77\n0.76\n0.00\n1.00\n2.00\n2.00\n5.00\nâ–…â–‡â–‚â–â–\n\n\nGarage_Area\n0\n1\n472.66\n215.19\n0.00\n320.00\n480.00\n576.00\n1488.00\nâ–ƒâ–‡â–ƒâ–â–\n\n\nWood_Deck_SF\n0\n1\n93.75\n126.36\n0.00\n0.00\n0.00\n168.00\n1424.00\nâ–‡â–â–â–â–\n\n\nOpen_Porch_SF\n0\n1\n47.53\n67.48\n0.00\n0.00\n27.00\n70.00\n742.00\nâ–‡â–â–â–â–\n\n\nEnclosed_Porch\n0\n1\n23.01\n64.14\n0.00\n0.00\n0.00\n0.00\n1012.00\nâ–‡â–â–â–â–\n\n\nThree_season_porch\n0\n1\n2.59\n25.14\n0.00\n0.00\n0.00\n0.00\n508.00\nâ–‡â–â–â–â–\n\n\nScreen_Porch\n0\n1\n16.00\n56.09\n0.00\n0.00\n0.00\n0.00\n576.00\nâ–‡â–â–â–â–\n\n\nPool_Area\n0\n1\n2.24\n35.60\n0.00\n0.00\n0.00\n0.00\n800.00\nâ–‡â–â–â–â–\n\n\nMisc_Val\n0\n1\n50.64\n566.34\n0.00\n0.00\n0.00\n0.00\n17000.00\nâ–‡â–â–â–â–\n\n\nMo_Sold\n0\n1\n6.22\n2.71\n1.00\n4.00\n6.00\n8.00\n12.00\nâ–…â–†â–‡â–ƒâ–ƒ\n\n\nYear_Sold\n0\n1\n2007.79\n1.32\n2006.00\n2007.00\n2008.00\n2009.00\n2010.00\nâ–‡â–‡â–‡â–‡â–ƒ\n\n\nSale_Price\n0\n1\n180796.06\n79886.69\n12789.00\n129500.00\n160000.00\n213500.00\n755000.00\nâ–‡â–‡â–â–â–\n\n\nLongitude\n0\n1\n-93.64\n0.03\n-93.69\n-93.66\n-93.64\n-93.62\n-93.58\nâ–…â–…â–‡â–†â–\n\n\nLatitude\n0\n1\n42.03\n0.02\n41.99\n42.02\n42.03\n42.05\n42.06\nâ–‚â–‚â–‡â–‡â–‡"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-2-train-test-splits",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-2-train-test-splits",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 2: Train / Test Splits",
    "text": "Exercise 2: Train / Test Splits\nWrite and execute code to create training and test datasets. Have the training dataset represent 75% of the total data.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nset.seed(8740)\n# split the data, with 75% in the training set\ndata_split &lt;- rsample::initial_split(dat, strata = \"Sale_Price\", prop = 0.75)\n\n# extract the training set\names_train &lt;- rsample::training(data_split)\n# extract the text set\names_test  &lt;- rsample::testing(data_split)"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-3-data-preprocessing",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-3-data-preprocessing",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 3: Data Preprocessing",
    "text": "Exercise 3: Data Preprocessing\ncreate a recipe based on the formula Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold with the following steps:\n\ntransform the outcome variable Sale_Price to log(Sale_Price) (natural log)\ncenter and scale all numeric predictors\ntransform the categorical variable Neighborhood to pool infrequent values (see recipes::step_other)\ncreate dummy variables for all nominal predictors\n\nFinally prep the recipe.\nMake sure you consider the order of the operations (hint: step_dummy turns factors into multiply integer (numeric) predictor, so consider when to scale numeric predictors relative to creating dummy predictors.\nYou can use broom::tidy() on the recipe to examine whether the prepped data is correct.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nnorm_recipe &lt;- \n  # create a recipe with the specified formula and data\n  recipes::recipe(\n    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, \n    data = ames_train\n  ) |&gt;\n  # center all predictors\n  recipes::step_center(all_numeric_predictors()) |&gt;\n  # scales all predictors\n  recipes::step_scale(all_numeric_predictors()) |&gt;\n  # transformm the outcome using log-base-e (natural log)\n  recipes::step_log(Sale_Price, base = exp(1)) |&gt; \n  # pool categories with few members into a new category - 'other'\n  recipes::step_other(Neighborhood) |&gt; \n  # create dummy variables for all categories\n  recipes::step_dummy(all_nominal_predictors()) \n\n\nnorm_recipe |&gt; \n  # estimate the means and standard deviations by prepping the data\n  recipes::prep(training = ames_train, retain = TRUE) |&gt; broom::tidy()\n\n# A tibble: 5 Ã— 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      center TRUE    FALSE center_8P2vV\n2      2 step      scale  TRUE    FALSE scale_5uUEH \n3      3 step      log    TRUE    FALSE log_Y9hV0   \n4      4 step      other  TRUE    FALSE other_JBWlv \n5      5 step      dummy  TRUE    FALSE dummy_yvcFM"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-4-modeling",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-4-modeling",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 4 Modeling",
    "text": "Exercise 4 Modeling\nCreate three regression models using the parsnip:: package and assign each model to its own variable\n\na base regression model using lm\na regression model using glmnet; set the model parameters penalty and mixture for tuning\na tree model using the ranger engine; set the model parameters min_n and trees for tuning\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# linear model: engine lm, mode regression\nlm_mod_base &lt;- parsnip::linear_reg() |&gt;\n  parsnip::set_engine(\"lm\")  |&gt; \n  parsnip::set_mode(\"regression\")\n\n# tuned linear model: engine glmnet, mode regression; tuning penalty and mixture\nlm_mod_glmnet &lt;- \n  parsnip::linear_reg( penalty = parsnip::tune(), mixture = parsnip::tune() ) |&gt; \n  parsnip::set_engine(\"glmnet\") |&gt; \n  parsnip::set_mode(\"regression\")\n\n# random forest model: engine lm, mode regression\nlm_mod_rforest &lt;- \n  parsnip::rand_forest( min_n = parsnip::tune(), trees = parsnip::tune() ) |&gt; \n  parsnip::set_engine(\"ranger\") |&gt; \n  parsnip::set_mode(\"regression\")\n\n\nlm_mod_base\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nlm_mod_glmnet\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = parsnip::tune()\n  mixture = parsnip::tune()\n\nComputational engine: glmnet \n\n\n\nlm_mod_rforest\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = parsnip::tune()\n  min_n = parsnip::tune()\n\nComputational engine: ranger"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-5",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-5",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse parsnip::translate() on each model to see the model template for each method of fitting.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# lm model\nlm_mod_base |&gt; parsnip::translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\n\n# glmnet model\nlm_mod_glmnet |&gt; parsnip::translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = parsnip::tune()\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    alpha = parsnip::tune(), family = \"gaussian\")\n\n\n\n# rforest model\nlm_mod_rforest |&gt; parsnip::translate()\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = parsnip::tune()\n  min_n = parsnip::tune()\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    num.trees = parsnip::tune(), min.node.size = min_rows(~parsnip::tune(), \n        x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5, \n        1))"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-6-bootstrap",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-6-bootstrap",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 6 Bootstrap",
    "text": "Exercise 6 Bootstrap\nCreate bootstrap samples for the training dataset. You can leave the parameters set to their defaults\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nset.seed(8740)\ntrain_resamples &lt;- rsample::bootstraps(ames_train)"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-7",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-7",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 7",
    "text": "Exercise 7\nCreate workflows with workflowsets::workflow_set using your recipe and models. Show the resulting datastructure, noting the number of columns, and then use tidyr:: to unnest the info column and show its contents.\n\n\n\n\n\n\nSOLUTION:\n\n\n\nall_workflows &lt;- \n  workflowsets::workflow_set(\n    preproc = list(base = norm_recipe),\n    models = list(base = lm_mod_base, glmnet = lm_mod_glmnet, forest = lm_mod_rforest)\n  )\n\nall_workflows # four columns\nall_workflows |&gt; tidyr::unnest(info)\n\n\n\n# A workflow set/tibble: 3 Ã— 4\n  wflow_id    info             option    result    \n  &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 base_base   &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_glmnet &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 base_forest &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\n\n\n# A tibble: 3 Ã— 7\n  wflow_id    workflow   preproc model       comment option    result    \n  &lt;chr&gt;       &lt;list&gt;     &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;   &lt;list&gt;    &lt;list&gt;    \n1 base_base   &lt;workflow&gt; recipe  linear_reg  \"\"      &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_glmnet &lt;workflow&gt; recipe  linear_reg  \"\"      &lt;opts[0]&gt; &lt;list [0]&gt;\n3 base_forest &lt;workflow&gt; recipe  rand_forest \"\"      &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-8",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-8",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 8",
    "text": "Exercise 8\nUse workflowsets::workflow_map to map the default function (tune::tune_grid() - look at the help for workflowsets::workflow_map ) across the workflows in the workflowset you just created and update the variable all_workflows with the result.\n\nall_workflows &lt;- all_workflows |&gt; \n  workflowsets::workflow_map(\n    verbose = TRUE                # enable logging\n    , resamples = train_resamples # a parameter passed to tune::tune_grid()\n    , grid = 5                    # a parameter passed to tune::tune_grid()\n  )\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 1 of 3 resampling: base_base\n\n\nâœ” 1 of 3 resampling: base_base (976ms)\n\n\ni 2 of 3 tuning:     base_glmnet\n\n\nâœ” 2 of 3 tuning:     base_glmnet (3.2s)\n\n\ni 3 of 3 tuning:     base_forest\n\n\nâœ” 3 of 3 tuning:     base_forest (1m 15.9s)\n\n\nThe updated variable all_workflows contains a nested column named result, and each cell of the column result is a tibble containing a nested column named .metrics. Write code to\n\nun-nest the metrics in the column .metrics\nfilter out the rows for the metric rsq\ngroup by wflow_id, order the .estimate column from highest to lowest, and pick out the first row of each group.\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nall_workflows |&gt; \n  # unnest the result column\n  dplyr::select(wflow_id,result) |&gt; \n  tidyr::unnest(result) |&gt; \n  # unnest the .metrics column\n  tidyr::unnest(.metrics) |&gt; \n  # filter out the metric rsq\n  dplyr::filter(.metric == 'rsq') |&gt; \n  # group by wflow_id\n  dplyr::group_by(wflow_id) |&gt; \n  # order by .estimate, starting with the largest value\n  dplyr::arrange(desc(.estimate) ) |&gt; \n  # select the first row for each group (i.e. highest rsq)\n  dplyr::slice(1)\n\n# A tibble: 3 Ã— 12\n# Groups:   wflow_id [3]\n  wflow_id    splits             id         .metric .estimator .estimate .config\n  &lt;chr&gt;       &lt;list&gt;             &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;  \n1 base_base   &lt;split [2197/798]&gt; Bootstrapâ€¦ rsq     standard       0.448 Preproâ€¦\n2 base_forest &lt;split [2197/797]&gt; Bootstrapâ€¦ rsq     standard       0.712 Preproâ€¦\n3 base_glmnet &lt;split [2197/798]&gt; Bootstrapâ€¦ rsq     standard       0.448 Preproâ€¦\n# â„¹ 5 more variables: penalty &lt;dbl&gt;, mixture &lt;dbl&gt;, trees &lt;int&gt;, min_n &lt;int&gt;,\n#   .notes &lt;list&gt;"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-9",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-9",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 9",
    "text": "Exercise 9\nRun the code below and compare to your results from exercise 8.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nworkflowsets::rank_results(\n  all_workflows\n  , rank_metric = 'rsq'\n  , select_best = TRUE\n)\n\n# A tibble: 6 Ã— 9\n  wflow_id    .config       .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 base_forest Preprocessorâ€¦ rmse    0.240 0.00255    25 recipe       randâ€¦     1\n2 base_forest Preprocessorâ€¦ rsq     0.657 0.00471    25 recipe       randâ€¦     1\n3 base_base   Preprocessorâ€¦ rmse    0.321 0.00366    25 recipe       lineâ€¦     2\n4 base_base   Preprocessorâ€¦ rsq     0.376 0.00836    25 recipe       lineâ€¦     2\n5 base_glmnet Preprocessorâ€¦ rmse    0.321 0.00364    25 recipe       lineâ€¦     3\n6 base_glmnet Preprocessorâ€¦ rsq     0.375 0.00836    25 recipe       lineâ€¦     3"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-10",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#exercise-10",
    "title": "Lab 4 - The Models package",
    "section": "Exercise 10",
    "text": "Exercise 10\nSelect the best model per the rsq metric using its id.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# extract the best model (per rsq)\nbest_model_workflow &lt;- \n  all_workflows |&gt; \n  workflowsets::extract_workflow(\"base_forest\")\n\n\n# finalize the workflow\nbest_model_workflow &lt;- \n  best_model_workflow |&gt; \n  tune::finalize_workflow(\n    tibble::tibble(trees = 1971, min_n = 2) # enter the name and value of the best-fit parameters\n  ) \n\n# having trained the model, compare test and training performance\ntraining_fit &lt;- best_model_workflow |&gt; \n  fit(data = ames_train)\ntraining_fit\ntesting_fit &lt;- best_model_workflow |&gt; \n  fit(data = ames_test)\ntesting_fit\n\n\n\nâ•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: rand_forest()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n5 Recipe Steps\n\nâ€¢ step_center()\nâ€¢ step_scale()\nâ€¢ step_log()\nâ€¢ step_other()\nâ€¢ step_dummy()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1971,      min.node.size = min_rows(~2, x), num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1971 \nSample size:                      2197 \nNumber of independent variables:  12 \nMtry:                             3 \nTarget node size:                 2 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.05686626 \nR squared (OOB):                  0.6558831 \n\n\n\n\nâ•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: rand_forest()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n5 Recipe Steps\n\nâ€¢ step_center()\nâ€¢ step_scale()\nâ€¢ step_log()\nâ€¢ step_other()\nâ€¢ step_dummy()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1971,      min.node.size = min_rows(~2, x), num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1971 \nSample size:                      733 \nNumber of independent variables:  12 \nMtry:                             3 \nTarget node size:                 2 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.06798341 \nR squared (OOB):                  0.5976383 \n\n\n\n\n\n\n\n\n\nWhat is the ratio of the OOB prediction errors (MSE): test/train?\nThe ratio is 0.06821716 / 0.05747611 - 1 = 0.1868785, or about 19% higher in the test dataset than in the training dataset."
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_4_solutions.html#grading",
    "href": "labs/solutions/BSMM_8740_lab_4_solutions.html#grading",
    "title": "Lab 4 - The Models package",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_1_solutions.html",
    "href": "labs/solutions/BSMM_8740_lab_1_solutions.html",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "",
    "text": "Todayâ€™s data is all baseball statistics. The data is in the Lahman package.\n\n\nBefore doing any analysis, you will want to get quick view of the data. This is useful as part of the EDA process.\n\ndim(Teams)\n\n[1] 3015   48"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_1_solutions.html#data-yearly-statistics-and-standings-for-baseball-teams",
    "href": "labs/solutions/BSMM_8740_lab_1_solutions.html#data-yearly-statistics-and-standings-for-baseball-teams",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "",
    "text": "Todayâ€™s data is all baseball statistics. The data is in the Lahman package.\n\n\nBefore doing any analysis, you will want to get quick view of the data. This is useful as part of the EDA process.\n\ndim(Teams)\n\n[1] 3015   48"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_1_solutions.html#data-dictionary",
    "href": "labs/solutions/BSMM_8740_lab_1_solutions.html#data-dictionary",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nThe variable definitions are found in the help for Teams, and are listed below.\n\n?Teams\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nyearID\nYear\n\n\nlgID\nLeague; a factor with levels AA AL FL NL PL UA\n\n\nteamID\nTeam; a factor\n\n\nfranchID\nFranchise (links to TeamsFranchises table)\n\n\ndivID\nTeamâ€™s division; a factor with levels C E W\n\n\nRank\nPosition in final standings\n\n\nG\nGames played\n\n\nGhome\nGames played at home\n\n\nW\nWins\n\n\nL\nLosses\n\n\nDivWin\nDivision Winner (Y or N)\n\n\nWCWin\nWild Card Winner (Y or N)\n\n\nLgWin\nLeague Champion(Y or N)\n\n\nWSWin\nWorld Series Winner (Y or N)\n\n\nR\nRuns scored\n\n\nAB\nAt bats\n\n\nH\nHits by batters\n\n\nX2B\nDoubles\n\n\nX3B\nTriples\n\n\nHR\nHomeruns by batters\n\n\nBB\nWalks by batters\n\n\nSO\nStrikeouts by batters\n\n\nSB\nStolen bases\n\n\nCS\nCaught stealing\n\n\nHBP\nBatters hit by pitch\n\n\nSF\nSacrifice flies\n\n\nRA\nOpponents runs scored\n\n\nER\nEarned runs allowed\n\n\nERA\nEarned run average\n\n\nCG\nComplete games\n\n\nSHO\nShutouts\n\n\nSV\nSaves\n\n\nIPouts\nOuts Pitched (innings pitched x 3)\n\n\nHA\nHits allowed\n\n\nHRA\nHomeruns allowed\n\n\nBBA\nWalks allowed\n\n\nSOA\nStrikeouts by pitchers\n\n\nE\nErrors\n\n\nDP\nDouble Plays\n\n\nFP\nFielding percentage\n\n\nname\nTeamâ€™s full name\n\n\npark\nName of teamâ€™s home ballpark\n\n\nattendance\nHome attendance total\n\n\nBPF\nThree-year park factor for batters\n\n\nPPF\nThree-year park factor for pitchers\n\n\nteamIDBR\nTeam ID used by Baseball Reference website\n\n\nteamIDlahman45\nTeam ID used in Lahman database version 4.5\n\n\nteamIDretro\nTeam ID used by Retrosheet\n\n\n\n\n\nExercises\n\n\nExercise 1\nHow many observations are in the Teams dataset? How many variables?\n\n# take the first three rows and glimpse the data\nTeams |&gt; dplyr::slice_head(n=3) |&gt; dplyr::glimpse()\n\nRows: 3\nColumns: 48\n$ yearID         &lt;int&gt; 1871, 1871, 1871\n$ lgID           &lt;fct&gt; NA, NA, NA\n$ teamID         &lt;fct&gt; BS1, CH1, CL1\n$ franchID       &lt;fct&gt; BNA, CNA, CFC\n$ divID          &lt;chr&gt; NA, NA, NA\n$ Rank           &lt;int&gt; 3, 2, 8\n$ G              &lt;int&gt; 31, 28, 29\n$ Ghome          &lt;int&gt; NA, NA, NA\n$ W              &lt;int&gt; 20, 19, 10\n$ L              &lt;int&gt; 10, 9, 19\n$ DivWin         &lt;chr&gt; NA, NA, NA\n$ WCWin          &lt;chr&gt; NA, NA, NA\n$ LgWin          &lt;chr&gt; \"N\", \"N\", \"N\"\n$ WSWin          &lt;chr&gt; NA, NA, NA\n$ R              &lt;int&gt; 401, 302, 249\n$ AB             &lt;int&gt; 1372, 1196, 1186\n$ H              &lt;int&gt; 426, 323, 328\n$ X2B            &lt;int&gt; 70, 52, 35\n$ X3B            &lt;int&gt; 37, 21, 40\n$ HR             &lt;int&gt; 3, 10, 7\n$ BB             &lt;int&gt; 60, 60, 26\n$ SO             &lt;int&gt; 19, 22, 25\n$ SB             &lt;int&gt; 73, 69, 18\n$ CS             &lt;int&gt; 16, 21, 8\n$ HBP            &lt;int&gt; NA, NA, NA\n$ SF             &lt;int&gt; NA, NA, NA\n$ RA             &lt;int&gt; 303, 241, 341\n$ ER             &lt;int&gt; 109, 77, 116\n$ ERA            &lt;dbl&gt; 3.55, 2.76, 4.11\n$ CG             &lt;int&gt; 22, 25, 23\n$ SHO            &lt;int&gt; 1, 0, 0\n$ SV             &lt;int&gt; 3, 1, 0\n$ IPouts         &lt;int&gt; 828, 753, 762\n$ HA             &lt;int&gt; 367, 308, 346\n$ HRA            &lt;int&gt; 2, 6, 13\n$ BBA            &lt;int&gt; 42, 28, 53\n$ SOA            &lt;int&gt; 23, 22, 34\n$ E              &lt;int&gt; 243, 229, 234\n$ DP             &lt;int&gt; 24, 16, 15\n$ FP             &lt;dbl&gt; 0.834, 0.829, 0.818\n$ name           &lt;chr&gt; \"Boston Red Stockings\", \"Chicago White Stockings\", \"Cleâ€¦\n$ park           &lt;chr&gt; \"South End Grounds I\", \"Union Base-Ball Grounds\", \"Natiâ€¦\n$ attendance     &lt;int&gt; NA, NA, NA\n$ BPF            &lt;int&gt; 103, 104, 96\n$ PPF            &lt;int&gt; 98, 102, 100\n$ teamIDBR       &lt;chr&gt; \"BOS\", \"CHI\", \"CLE\"\n$ teamIDlahman45 &lt;chr&gt; \"BS1\", \"CH1\", \"CL1\"\n$ teamIDretro    &lt;chr&gt; \"BS1\", \"CH1\", \"CL1\"\n\n\nHow many character columns/measurements have missing variables?\n\nTeams |&gt; skimr::skim()\n\n\nData summary\n\n\nName\nTeams\n\n\nNumber of rows\n3015\n\n\nNumber of columns\n48\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\nfactor\n3\n\n\nnumeric\n35\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndivID\n1517\n0.50\n1\n1\n0\n3\n0\n\n\nDivWin\n1545\n0.49\n1\n1\n0\n2\n0\n\n\nWCWin\n2181\n0.28\n1\n1\n0\n2\n0\n\n\nLgWin\n28\n0.99\n1\n1\n0\n2\n0\n\n\nWSWin\n357\n0.88\n1\n1\n0\n2\n0\n\n\nname\n0\n1.00\n11\n33\n0\n140\n0\n\n\npark\n34\n0.99\n7\n70\n0\n219\n0\n\n\nteamIDBR\n0\n1.00\n3\n3\n0\n101\n0\n\n\nteamIDlahman45\n0\n1.00\n3\n3\n0\n148\n0\n\n\nteamIDretro\n0\n1.00\n3\n3\n0\n151\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nlgID\n0\n1\nFALSE\n7\nNL: 1534, AL: 1310, AA: 85, NA: 50\n\n\nteamID\n0\n1\nFALSE\n149\nCHN: 147, PHI: 140, PIT: 136, CIN: 133\n\n\nfranchID\n0\n1\nFALSE\n120\nATL: 147, CHC: 147, CIN: 141, PIT: 141\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyearID\n0\n1.00\n1959.49\n43.23\n1871.00\n1923.00\n1968.00\n1997.00\n2022.00\nâ–ƒâ–…â–…â–†â–‡\n\n\nRank\n0\n1.00\n4.03\n2.29\n1.00\n2.00\n4.00\n6.00\n13.00\nâ–‡â–…â–ƒâ–â–\n\n\nG\n0\n1.00\n150.13\n24.33\n6.00\n154.00\n159.00\n162.00\n165.00\nâ–â–â–â–â–‡\n\n\nGhome\n399\n0.87\n78.08\n6.91\n24.00\n77.00\n81.00\n81.00\n84.00\nâ–â–â–â–â–‡\n\n\nW\n0\n1.00\n74.67\n17.97\n0.00\n66.00\n77.00\n87.00\n116.00\nâ–â–â–ƒâ–‡â–‚\n\n\nL\n0\n1.00\n74.67\n17.73\n4.00\n65.00\n76.00\n87.00\n134.00\nâ–â–‚â–‡â–…â–\n\n\nR\n0\n1.00\n681.16\n139.02\n24.00\n614.00\n691.00\n764.00\n1220.00\nâ–â–â–‡â–…â–\n\n\nAB\n0\n1.00\n5132.17\n794.89\n211.00\n5142.00\n5405.00\n5519.00\n5781.00\nâ–â–â–â–â–‡\n\n\nH\n0\n1.00\n1339.25\n229.86\n33.00\n1297.00\n1389.00\n1464.00\n1783.00\nâ–â–â–â–‡â–…\n\n\nX2B\n0\n1.00\n229.03\n59.72\n1.00\n195.00\n234.00\n272.00\n376.00\nâ–â–‚â–†â–‡â–‚\n\n\nX3B\n0\n1.00\n45.43\n22.52\n0.00\n29.00\n40.00\n58.00\n150.00\nâ–…â–‡â–ƒâ–â–\n\n\nHR\n0\n1.00\n106.54\n64.11\n0.00\n47.00\n111.00\n156.00\n307.00\nâ–‡â–†â–‡â–ƒâ–\n\n\nBB\n0\n1.00\n473.61\n132.08\n0.00\n427.00\n494.00\n554.00\n835.00\nâ–â–â–‡â–‡â–\n\n\nSO\n16\n0.99\n768.12\n323.43\n3.00\n517.00\n766.00\n1000.00\n1596.00\nâ–‚â–‡â–‡â–…â–‚\n\n\nSB\n125\n0.96\n109.09\n69.47\n0.00\n62.00\n92.00\n137.00\n581.00\nâ–‡â–ƒâ–â–â–\n\n\nCS\n831\n0.72\n46.26\n21.91\n0.00\n32.75\n43.00\n56.00\n191.00\nâ–†â–‡â–â–â–\n\n\nHBP\n1158\n0.62\n46.18\n18.31\n7.00\n32.00\n44.00\n58.00\n160.00\nâ–†â–‡â–‚â–â–\n\n\nSF\n1541\n0.49\n44.04\n10.16\n7.00\n38.00\n44.00\n50.00\n77.00\nâ–â–‚â–‡â–ƒâ–\n\n\nRA\n0\n1.00\n681.16\n138.81\n34.00\n610.50\n689.00\n766.00\n1252.00\nâ–â–â–‡â–ƒâ–\n\n\nER\n0\n1.00\n573.99\n149.50\n23.00\n504.00\n595.00\n672.00\n1023.00\nâ–â–‚â–‡â–†â–\n\n\nERA\n0\n1.00\n3.84\n0.76\n1.22\n3.37\n3.84\n4.33\n8.00\nâ–â–‡â–‡â–â–\n\n\nCG\n0\n1.00\n47.08\n39.39\n0.00\n9.00\n41.00\n75.00\n148.00\nâ–‡â–…â–ƒâ–‚â–\n\n\nSHO\n0\n1.00\n9.63\n5.03\n0.00\n6.00\n9.00\n13.00\n32.00\nâ–…â–‡â–ƒâ–â–\n\n\nSV\n0\n1.00\n24.59\n16.34\n0.00\n10.00\n25.00\n39.00\n68.00\nâ–‡â–…â–†â–…â–\n\n\nIPouts\n0\n1.00\n4016.16\n660.67\n162.00\n4080.00\n4257.00\n4341.00\n4518.00\nâ–â–â–â–â–‡\n\n\nHA\n0\n1.00\n1339.03\n230.06\n49.00\n1286.00\n1389.00\n1468.00\n1993.00\nâ–â–â–â–‡â–\n\n\nHRA\n0\n1.00\n106.54\n60.99\n0.00\n52.00\n114.00\n154.00\n305.00\nâ–†â–†â–‡â–‚â–\n\n\nBBA\n0\n1.00\n473.96\n131.23\n1.00\n429.50\n496.00\n553.50\n827.00\nâ–â–â–‡â–‡â–\n\n\nSOA\n0\n1.00\n767.54\n324.61\n0.00\n513.00\n767.00\n1002.00\n1687.00\nâ–‚â–‡â–‡â–…â–\n\n\nE\n0\n1.00\n179.84\n108.25\n20.00\n110.00\n141.00\n205.00\n639.00\nâ–‡â–…â–â–â–\n\n\nDP\n0\n1.00\n132.60\n35.76\n0.00\n116.00\n140.00\n156.00\n217.00\nâ–â–â–ƒâ–‡â–\n\n\nFP\n0\n1.00\n0.97\n0.03\n0.76\n0.97\n0.98\n0.98\n0.99\nâ–â–â–â–â–‡\n\n\nattendance\n279\n0.91\n1385099.81\n964039.83\n0.00\n544781.75\n1203014.50\n2080399.25\n4483350.00\nâ–‡â–†â–…â–‚â–\n\n\nBPF\n0\n1.00\n100.19\n4.92\n60.00\n97.00\n100.00\n103.00\n129.00\nâ–â–â–‡â–…â–\n\n\nPPF\n0\n1.00\n100.21\n4.86\n60.00\n97.00\n100.00\n103.00\n141.00\nâ–â–â–‡â–â–\n\n\n\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\nFrom the dim(Teams) statement used after library(Lahman), there are 3015 observations and 48 variables.\nFrom Teams |&gt; skimr::skim() 6 of 10 character variables have missing values\n\n\n\n\nExercise 2\nBen BaumerÂ worked for theÂ New York MetsÂ from 2004 to 2012. What was the team W/L record during those years? Use filter() and select() to quickly identify only those pieces of information that we care about.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# filter to use only rows where teamID equals \"NYN\"\nmets &lt;- Teams  %&gt;% \n  dplyr::filter(teamID == \"NYN\")\n# filter to use only rows where yearID is &gt;= 2004 and &lt;= 2012\n# you could also write dplyr::filter(yearID  %in% 2004:2012)\nmy_mets &lt;- mets %&gt;% \n  dplyr::filter(yearID &gt;= 2004 & yearID &lt;= 2012)\n# the dataset needs to have at least the year and the won (W) loss (L) record for that year\nmy_mets %&gt;% \n  dplyr::select(teamID,yearID,W,L)\n\n  teamID yearID  W  L\n1    NYN   2004 71 91\n2    NYN   2005 83 79\n3    NYN   2006 97 65\n4    NYN   2007 88 74\n5    NYN   2008 89 73\n6    NYN   2009 70 92\n7    NYN   2010 79 83\n8    NYN   2011 77 85\n9    NYN   2012 74 88\n\n\nOverall, the won-loss record was as follows:\n\nmy_mets %&gt;% \n  dplyr::select(teamID,yearID,W,L) %&gt;% \n  dplyr::summarize(\n    \"2004-2012 wins\" = sum(W)\n    , \"2004-2012 losses\" = sum(L)\n  )\n\n  2004-2012 wins 2004-2012 losses\n1            728              730\n\n\n\n\n\n\nExercise 3\nThe model estimates the expected winning percentage as follows:\nWÌ‚pct=11+(RARS)2\n\\hat{\\text{W}}_{\\text{pct}}=\\frac{1}{1+\\left(\\frac{\\text{RA}}{\\text{RS}}\\right)^{2}}\n\nwhere RA\\text{RA} is the number of runs the team allows to be scored, RS\\text{RS} is the number of runs that the team scores, and WÌ‚pct\\hat{\\text{W}}_{\\text{pct}} is the teamâ€™s expected winning percentage. The runs scored and allowed are present in the Teams table, so we start by selecting them.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben &lt;- Teams |&gt;\n  # select to get the columns you want\n  dplyr::select(teamID, yearID, W, L, R, RA) |&gt;\n  # filter to get the rows you want\n  dplyr::filter(teamID == \"NYN\" & yearID %in% 2004:2012)\n\nThe column name can be changed with the dplyr::rename function (Use new_name = old_name to rename selected variables). Alternatively, you can rename the column directly in the select statement above, like this:\ndplyr::select(teamID,yearID,W,L,RS = R,RA)\n\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::rename(RS = R)    # new name = old name\nmets_ben\n\n  teamID yearID  W  L  RS  RA\n1    NYN   2004 71 91 684 731\n2    NYN   2005 83 79 722 648\n3    NYN   2006 97 65 834 731\n4    NYN   2007 88 74 804 750\n5    NYN   2008 89 73 799 715\n6    NYN   2009 70 92 671 757\n7    NYN   2010 79 83 656 652\n8    NYN   2011 77 85 718 742\n9    NYN   2012 74 88 650 709\n\n\n\n\n\n\nExercise 4\nNext, we need to compute the teamâ€™s actual winning percentage in each of these seasons. Thus, we need to add a new column to our data frame, and we do this with theÂ mutate()Â command.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben &lt;- mets_ben |&gt;\n  # once we have the data, we mutate to add a new value (column), using the formula\n  dplyr::mutate( WPct = 1/(1 + (RA/RS)^2 ) )\nmets_ben\n\n  teamID yearID  W  L  RS  RA      WPct\n1    NYN   2004 71 91 684 731 0.4668211\n2    NYN   2005 83 79 722 648 0.5538575\n3    NYN   2006 97 65 834 731 0.5655308\n4    NYN   2007 88 74 804 750 0.5347071\n5    NYN   2008 89 73 799 715 0.5553119\n6    NYN   2009 70 92 671 757 0.4399936\n7    NYN   2010 79 83 656 652 0.5030581\n8    NYN   2011 77 85 718 742 0.4835661\n9    NYN   2012 74 88 650 709 0.4566674\n\n\n\n\nThe expected number of wins is then equal to the product of the expected winning percentage times the number of games.\n\nmets_ben &lt;- mets_ben |&gt;\n  # once we have calculated the expected winning percentage,\n  # the expected number of wins is the percentage times the total number of games played\n  dplyr::mutate( W_hat = WPct * (W+L) )\nmets_ben\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat\n1    NYN   2004 71 91 684 731 0.4668211 75.62501\n2    NYN   2005 83 79 722 648 0.5538575 89.72491\n3    NYN   2006 97 65 834 731 0.5655308 91.61600\n4    NYN   2007 88 74 804 750 0.5347071 86.62255\n5    NYN   2008 89 73 799 715 0.5553119 89.96053\n6    NYN   2009 70 92 671 757 0.4399936 71.27896\n7    NYN   2010 79 83 656 652 0.5030581 81.49541\n8    NYN   2011 77 85 718 742 0.4835661 78.33771\n9    NYN   2012 74 88 650 709 0.4566674 73.98012\n\n\n\n\nExercise 5\nIn this case, the Metsâ€™ fortunes were better than expected in three of these seasons, and worse than expected in the other six.\nWe can confirm this as follows:\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben %&gt;% \n  # first check that the assertion above is correct\n  dplyr::summarize('better then expected' = sum(W &gt;= W_hat), 'worse than expected' = sum(W &lt; W_hat))\n\n  better then expected worse than expected\n1                    3                   6\n\n\nTo see how the Mets did over all seasons we can repeat our calculation\n\nTeams |&gt;\n  # here we repeat our prior calculation (all steps combined) for all the years in the dataset\n  dplyr::select(teamID, yearID, W, L, RS = R, RA) |&gt;\n  dplyr::filter(teamID == \"NYN\") |&gt;\n  dplyr::mutate( \n    WPct = 1/(1 + (RA/RS)^2 )\n    , W_hat = WPct * (W+L)\n  )  |&gt; \ndplyr::summarize( \n  \"better then expected\" = sum(W &gt;= W_hat)\n  , 'worse than expected' = sum(W &lt; W_hat) \n)\n\n  better then expected worse than expected\n1                   22                  39\n\n\n\n\n\n\nExercise 6\nNaturally, the Mets experienced ups and downs during Benâ€™s time with the team. Which seasons were best? To figure this out, we can simply sort the rows of the data frame by number of wins.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# for this we just need to sort the number of wins in descending order\nmets_ben |&gt; dplyr::arrange(desc(W))\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat\n1    NYN   2006 97 65 834 731 0.5655308 91.61600\n2    NYN   2008 89 73 799 715 0.5553119 89.96053\n3    NYN   2007 88 74 804 750 0.5347071 86.62255\n4    NYN   2005 83 79 722 648 0.5538575 89.72491\n5    NYN   2010 79 83 656 652 0.5030581 81.49541\n6    NYN   2011 77 85 718 742 0.4835661 78.33771\n7    NYN   2012 74 88 650 709 0.4566674 73.98012\n8    NYN   2004 71 91 684 731 0.4668211 75.62501\n9    NYN   2009 70 92 671 757 0.4399936 71.27896\n\n\n\n\n\n\nExercise 7\nIn 2006, the Mets had the best record in baseball during the regular season and nearly made the World Series. How do these seasons rank in terms of the teamâ€™s performance relative to our model?\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben %&gt;% \n  # add a column with the difference between wins (W) and expected wins (W_hat)\n  dplyr::mutate(Diff = W - W_hat) |&gt;\n  # then sort the result\n  dplyr::arrange(desc(Diff))\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat        Diff\n1    NYN   2006 97 65 834 731 0.5655308 91.61600  5.38400315\n2    NYN   2007 88 74 804 750 0.5347071 86.62255  1.37744558\n3    NYN   2012 74 88 650 709 0.4566674 73.98012  0.01988152\n4    NYN   2008 89 73 799 715 0.5553119 89.96053 -0.96052803\n5    NYN   2009 70 92 671 757 0.4399936 71.27896 -1.27895513\n6    NYN   2011 77 85 718 742 0.4835661 78.33771 -1.33770571\n7    NYN   2010 79 83 656 652 0.5030581 81.49541 -2.49540821\n8    NYN   2004 71 91 684 731 0.4668211 75.62501 -4.62501135\n9    NYN   2005 83 79 722 648 0.5538575 89.72491 -6.72490937\n\n\nIn the years 2006, 2007 and 2012, the Mets had more wins than expected by the model. In all other seasons they performed worse than predicted by the model.\nWe can summarize the Mets performance as follows:\n\nmets_ben |&gt;\n  dplyr::summarize(\n    num_years = dplyr::n(),  # number of years\n    total_W = sum(W),        # total number of wins\n    total_L = sum(L),        # total number of losses\n    total_WPct = total_W / (total_W + total_L) # win percentage\n  )\n\n  num_years total_W total_L total_WPct\n1         9     728     730  0.4993141\n\n\nIn these nine years, the Mets had a combined record of 728 wins and 730 losses, for an overall winning percentage of 49.93%.\n\n\n\n\nExercise 8\nDiscretize the years into three chunks: one for each of the three general managers under whom Ben worked.Â Jim DuquetteÂ was the Metsâ€™Â general managerÂ in 2004,Â Omar MinayaÂ from 2005 to 2010, andÂ Sandy AldersonÂ from 2011 to 2012.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nmets_ben %&gt;% \n  # this questions requires a logic for deciding \n  # which years each general manager worked\n  dplyr::mutate(\n    # nested ifelse statements are OK for this logic, \n    # but are only practical for about three cases\n    gm = ifelse(\n      yearID == 2004, \n      'Jim Duquette', \n      ifelse(\n        yearID &gt;= 2011, \n        'Sandy Alderson', \n        'Omar Minaya')\n    )\n  )\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat             gm\n1    NYN   2004 71 91 684 731 0.4668211 75.62501   Jim Duquette\n2    NYN   2005 83 79 722 648 0.5538575 89.72491    Omar Minaya\n3    NYN   2006 97 65 834 731 0.5655308 91.61600    Omar Minaya\n4    NYN   2007 88 74 804 750 0.5347071 86.62255    Omar Minaya\n5    NYN   2008 89 73 799 715 0.5553119 89.96053    Omar Minaya\n6    NYN   2009 70 92 671 757 0.4399936 71.27896    Omar Minaya\n7    NYN   2010 79 83 656 652 0.5030581 81.49541    Omar Minaya\n8    NYN   2011 77 85 718 742 0.4835661 78.33771 Sandy Alderson\n9    NYN   2012 74 88 650 709 0.4566674 73.98012 Sandy Alderson\n\n\nAlternatively, we can use the case_when function\n\nmets_ben &lt;- mets_ben |&gt;\n  dplyr::mutate(\n    # same problem, but case_when is easier to work with\n    gm = dplyr::case_when(\n      yearID == 2004 ~ 'Jim Duquette', \n      yearID &gt;= 2011 ~ 'Sandy Alderson', \n      TRUE ~ 'Omar Minaya' # this is the default case\n    )\n  )\nmets_ben\n\n  teamID yearID  W  L  RS  RA      WPct    W_hat             gm\n1    NYN   2004 71 91 684 731 0.4668211 75.62501   Jim Duquette\n2    NYN   2005 83 79 722 648 0.5538575 89.72491    Omar Minaya\n3    NYN   2006 97 65 834 731 0.5655308 91.61600    Omar Minaya\n4    NYN   2007 88 74 804 750 0.5347071 86.62255    Omar Minaya\n5    NYN   2008 89 73 799 715 0.5553119 89.96053    Omar Minaya\n6    NYN   2009 70 92 671 757 0.4399936 71.27896    Omar Minaya\n7    NYN   2010 79 83 656 652 0.5030581 81.49541    Omar Minaya\n8    NYN   2011 77 85 718 742 0.4835661 78.33771 Sandy Alderson\n9    NYN   2012 74 88 650 709 0.4566674 73.98012 Sandy Alderson\n\n\n\n\n\n\nExercise 9\nThe raw churn data can be transformed into a tidy dataset as follows:\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nset.seed(42)\n\n# read data and drop column 1 (it contains row numbers and doesn't have a column name)\ndf &lt;- readr::read_csv(\"data/monthly_data.csv\", show_col_types = FALSE, col_select = -1)\n\ndf |&gt;\n  # take date columns and pivot to longer table\n  tidyr::pivot_longer(starts_with(\"20\"), names_to = \"date\", values_to = \"quantity\") |&gt; \n  # split the 'date' column into two measurements\n  tidyr::separate_wider_delim(cols = date, delim = \"_\", names = c(\"date\",\"paymentMandate\")) |&gt; \n  # pivot the two columns paymentMandate and quantity to two columns called payment and mandate\n  tidyr::pivot_wider(names_from = paymentMandate, values_from = quantity) |&gt; \n  # finally, mutate the date columns from strings to Dates\n  dplyr::mutate(\n    incorporation_date = as.Date(incorporation_date)\n    , date = as.Date(date)\n  )\n\n# A tibble: 10,824 Ã— 6\n   company_id vertical    incorporation_date date       payments mandates\n        &lt;dbl&gt; &lt;chr&gt;       &lt;date&gt;             &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1          1 gym/fitness 2013-05-30         2014-01-01        0        1\n 2          1 gym/fitness 2013-05-30         2014-02-01        0        2\n 3          1 gym/fitness 2013-05-30         2014-03-01        0        2\n 4          1 gym/fitness 2013-05-30         2014-04-01        1        1\n 5          1 gym/fitness 2013-05-30         2014-05-01        0        0\n 6          1 gym/fitness 2013-05-30         2014-06-01        1        0\n 7          1 gym/fitness 2013-05-30         2014-07-01        0        0\n 8          1 gym/fitness 2013-05-30         2014-08-01        0        0\n 9          1 gym/fitness 2013-05-30         2014-09-01        0        0\n10          1 gym/fitness 2013-05-30         2014-10-01        0        0\n# â„¹ 10,814 more rows\n\n\n\n\n\n\nExercise 10\nUse the gm function to define the manager groups with the group_by() operator, and run the summaries again, this time across the manager groups.\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n#| label: read the data\ndata &lt;- readr::read_csv(\"data/sales_dag.csv\", show_col_types = FALSE)\n\ndata |&gt; dplyr::slice_head(n=5) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"sample marketing data\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nsample marketing data\n\n\nvisits\ndiscount\nis_loyal\nsales\nsales_per_visit\n\n\n\n\n12\n0\n0\n13.34830\n1.1123585\n\n\n26\n1\n1\n21.70125\n0.8346635\n\n\n13\n0\n0\n14.70040\n1.1308004\n\n\n24\n0\n0\n20.37734\n0.8490557\n\n\n14\n0\n0\n12.63372\n0.9024089\n\n\n\n\n\n\n\n\ndata |&gt; skimr::skim()\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n700\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nvisits\n0\n1\n21.19\n5.19\n3.00\n17.00\n21.00\n25.00\n40.00\nâ–â–ƒâ–‡â–ƒâ–\n\n\ndiscount\n0\n1\n0.74\n0.44\n0.00\n0.00\n1.00\n1.00\n1.00\nâ–ƒâ–â–â–â–‡\n\n\nis_loyal\n0\n1\n0.75\n0.43\n0.00\n0.75\n1.00\n1.00\n1.00\nâ–‚â–â–â–â–‡\n\n\nsales\n0\n1\n19.40\n4.88\n2.09\n16.44\n19.50\n22.50\n36.14\nâ–â–ƒâ–‡â–ƒâ–\n\n\nsales_per_visit\n0\n1\n0.92\n0.11\n0.42\n0.85\n0.91\n0.98\n1.41\nâ–â–‚â–‡â–‚â–\n\n\n\n\n\nThe mean of the sales_per_visit columns/measurement is 0.9183 and there are no grouped observations.\n\n# calculate the % share of customers receiving a discount vs the % not receiving a discount\ndata$discount |&gt; table() / length(data$discount)\n\n\n   0    1 \n0.26 0.74 \n\n\nSimilarly for the share of customers which are loyal:\n\n# calculate the % share of customers that are 'loyal' vs not 'loyal'\ndata$is_loyal |&gt; table() / length(data$is_loyal)\n\n\n   0    1 \n0.25 0.75 \n\n\nTo understand these features better, they also looked at a cross-tab table:\n\n# build a cross-tab table of 'loyal' customers vs customers getting a discount\ndata |&gt; xtabs(~discount + is_loyal, data = _)\n\n        is_loyal\ndiscount   0   1\n       0 175   7\n       1   0 518\n\n\n\n\nAlternatively:\n\ndata |&gt; \n  dplyr::group_by(discount,is_loyal) |&gt; \n  dplyr::summarize(n = dplyr::n(), .groups='drop') |&gt; \n  tidyr::pivot_wider(\n    names_from = is_loyal\n    , values_from = n\n    , names_prefix = 'is_loyal '\n  ) |&gt; gt::gt() |&gt; \n  gt::tab_header(title = \"Cross-tabs\", subtitle = \"discount vs is_loyal\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nCross-tabs\n\n\ndiscount vs is_loyal\n\n\ndiscount\nis_loyal 0\nis_loyal 1\n\n\n\n\n0\n175\n7\n\n\n1\nNA\n518\n\n\n\n\n\n\ndata |&gt; dplyr::mutate(id = dplyr::row_number(), .before = 1) |&gt; \n  dplyr::filter(discount == 0) |&gt; \n  dplyr::arrange( desc(sales) ) |&gt; \n  dplyr::slice_head(n=10) |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(title = \"Sales: loyal customers vs others\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nSales: loyal customers vs others\n\n\nid\nvisits\ndiscount\nis_loyal\nsales\nsales_per_visit\n\n\n\n\n567\n33\n0\n1\n31.72169\n0.9612633\n\n\n205\n33\n0\n1\n28.31111\n0.8579125\n\n\n366\n33\n0\n1\n28.08163\n0.8509586\n\n\n50\n33\n0\n1\n27.79064\n0.8421407\n\n\n281\n29\n0\n1\n27.58115\n0.9510740\n\n\n546\n27\n0\n1\n26.25533\n0.9724196\n\n\n105\n29\n0\n1\n26.21752\n0.9040526\n\n\n362\n28\n0\n0\n24.09410\n0.8605037\n\n\n652\n28\n0\n0\n24.06459\n0.8594495\n\n\n494\n27\n0\n0\n24.00630\n0.8891220\n\n\n\n\n\n\n\nThe loyal customers are the top ones in terms of sales. This is good news. It means that the definition of loyal customers is consistent with the data.\nIn order to have orders of magnitude for the sales, the data scientist provided some summary statistics table:\n\ngtExtras::gt_plt_summary(data, title = \"Sales data\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales data\n\n\n700 rows x 5 cols\n\n\n\nColumn\nPlot Overview\nMissing\nMean\nMedian\nSD\n\n\n\n\n\n\n\nvisits\n\n\n\n      340\n\n0.0%\n21.2\n21.0\n5.2\n\n\n\n\n\ndiscount\n\n\n\n      01\n\n0.0%\n0.7\n1.0\n0.4\n\n\n\n\n\nis_loyal\n\n\n\n      01\n\n0.0%\n0.8\n1.0\n0.4\n\n\n\n\n\nsales\n\n\n\n      236\n\n0.0%\n19.4\n19.5\n4.9\n\n\n\n\n\nsales_per_visit\n\n\n\n      0.421.41\n\n0.0%\n0.9\n0.9\n0.1\n\n\n\n\n\n\n\nTo have a better glimpse of the data, the data scientist also provided a histogram of the sales:\n\ndata |&gt; \n  ggplot(aes(x=sales)) +\n  geom_histogram(aes(y = ..density..), bins = 30, colour = 1, fill = \"white\") +\n  geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25) +\n  labs(title = \"Sales Distribution\") +\n  theme_minimal()"
  },
  {
    "objectID": "labs/solutions/BSMM_8740_lab_1_solutions.html#resources-for-additional-practice-optional",
    "href": "labs/solutions/BSMM_8740_lab_1_solutions.html#resources-for-additional-practice-optional",
    "title": "Lab 1 - Tidy Data Wrangling",
    "section": "Resources for additional practice (optional)",
    "text": "Resources for additional practice (optional)\n\nChapter 2: Get Started Data Visualization by Kieran Healy\nChapter 3: Data visualization in R for Data Science by Hadley Wickham\nRStudio Cloud Primers\n\nVisualization Basics: https://rstudio.cloud/learn/primers/1.1\nWork with Data: https://rstudio.cloud/learn/primers/2\nVisualize Data: https://rstudio.cloud/learn/primers/3"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html",
    "href": "labs/BSMM_8740_lab_5.html",
    "title": "Lab 5 - Classification and clustering",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to build workflows to fit different classification models.\nBe able to build workflows to evaluate different clustering models."
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#introduction",
    "href": "labs/BSMM_8740_lab_5.html#introduction",
    "title": "Lab 5 - Classification and clustering",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison in the context of classification and clustering.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to build workflows to fit different classification models.\nBe able to build workflows to evaluate different clustering models."
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#getting-started",
    "href": "labs/BSMM_8740_lab_5.html#getting-started",
    "title": "Lab 5 - Classification and clustering",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-5-[your github username] repository .\nCreate an R project using your 2024-lab-5-[your github username] repository (remember to create a PAT, etc.) and add your answers by editing the 2024-lab-5.qmd file in your repository.\nWhen you are done, be sure to: save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#packages",
    "href": "labs/BSMM_8740_lab_5.html#packages",
    "title": "Lab 5 - Classification and clustering",
    "section": "Packages",
    "text": "Packages\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, forcats,\n  broom, yardstick, parsnip, workflows, rsample, tune, dials\n)\n\n# set the default theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#the-data",
    "href": "labs/BSMM_8740_lab_5.html#the-data",
    "title": "Lab 5 - Classification and clustering",
    "section": "The Data",
    "text": "The Data\nToday we will be using customer churn data.\nIn the customer management lifecycle, customer churn refers to a decision made by the customer about ending the business relationship. It is also referred as loss of clients or customers. This dataset contains 20 features related to churn in a telecom context and we will look at how to predict churn and estimate the effect of predictors on the customer churn odds ratio.\n\ndata &lt;- \n  readr::read_csv(\"data/Telco-Customer-Churn.csv\", show_col_types = FALSE) |&gt; \n  dplyr::mutate(churn = as.factor(churn))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-1-eda",
    "href": "labs/BSMM_8740_lab_5.html#exercise-1-eda",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 1: EDA",
    "text": "Exercise 1: EDA\nWrite and execute the code to perform summary EDA on the data using the package skimr. Plot histograms for monthly charges and tenure. Tenure measures the strength of the customer relationship by measuring the length of time that a person has been a customer.\n\n\n\n\n\n\nYOUR ANSWER:"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-2-train-test-splits-recipe",
    "href": "labs/BSMM_8740_lab_5.html#exercise-2-train-test-splits-recipe",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 2: train / test splits & recipe",
    "text": "Exercise 2: train / test splits & recipe\nWrite and execute code to create training and test datasets. Have the training dataset represent 70% of the total data.\nNext create a recipe where churn is related to all the other variables, and\n\nnormalize the numeric variables\ncreate dummy variables for the ordinal predictors\n\nMake sure the steps are in a sequence that preserves the (0,1) dummy variables.\nPrep the data on the training data and show the result.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\nset.seed(8740)\n\n# split data\n\n\n# create a recipe"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-3-logistic-modeling",
    "href": "labs/BSMM_8740_lab_5.html#exercise-3-logistic-modeling",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 3: logistic modeling",
    "text": "Exercise 3: logistic modeling\n\nCreate a linear model using logistic regression to predict churn. for the set engine stage use â€œglm,â€ and set the mode to â€œclassification.â€\nCreate a workflow using the recipe of the last exercise and the model if the last step.\nWith the workflow, fit the training data\nCombine the training data and the predictions from step 3 using broom::augment , and assign the result to a variable\nCreate a combined metric function using yardstick::metric_set as show in the code below:\nUse the variable from step 4 as the first argument to the function from step 5. The other arguments are truth = churn (from the data) and estimate=.pred_class (from step 4). Make a note of the numerical metrics.\nUse the variable from step 4 as the first argument to the functions listed below, with arguments truth = churn and estimate =``.pred_No.\n\nyardstick::roc_auc\nyardstick::roc_curve followed by ggplot2::autoplot().\n\n\n\n\n\n\n\n\nrank-deficiency\n\n\n\nYou can ignore this message. It means that there are a lot of predictors.\n\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# create a linear regression model\n\n# create a workflow\n\n# fit the workflow\n\n# augment the training data with the fitted data\n\n\n# create the metrics function\nm_set_fn &lt;- \n  yardstick::metric_set(\n    yardstick::accuracy\n    , yardstick::precision\n    , yardstick::recall\n    , yardstick::f_meas\n    , yardstick::spec\n    , yardstick::sens\n    , yardstick::ppv\n    , yardstick::npv\n)\n\n\n# compute roc_auc and plot the roc_curve"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-4-effects",
    "href": "labs/BSMM_8740_lab_5.html#exercise-4-effects",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 4: effects",
    "text": "Exercise 4: effects\nUse broom::tidy() on the fit object from exercise 4 to get the predictor coefficients. Sort them in decreasing order by absolute value.\nWhat is the effect of one additional year of tenure on the churn odds ratio?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# \n\nThe effect of one additional year of tenure on the churn odds ratio is __."
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-5-knn-modeling",
    "href": "labs/BSMM_8740_lab_5.html#exercise-5-knn-modeling",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 5 knn modeling",
    "text": "Exercise 5 knn modeling\nNow we will create a K-nearest neighbours model to estimate churn. To do this, write the code for the following steps:\n\nCreate a K-nearest neighbours model to predict churn using parsnip::nearest_neighbor with argument neighbors = 3 which will use the three most similar data points from the training set to predict churn. For the set engine stage use â€œkknn,â€ and set the mode to â€œclassification.â€\nTake the workflow from exercise 3 and create a new workflow by updating the original workflow. Use workflows::update_model to swap out the original logistic model for the nearest neighbour model.\nUse the new workflow to fit the training data. Take the fit and use broom::augment to augment the fit with the training data.\nUse the augmented data from step 3 to plot the roc curve, using yardstick::roc_curve(.pred_No, truth = churn) as in exercise 3. How do you interpret his curve?\nTake the fit from step 3 and use broom::augment to augment the fit with the test data.\nRepeat step 4 using the augmented data from step 5.\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?parsnip::nearest_neighbor\n\n# create a knn classification model model\n\n# create a workflow\n\n# fit the workflow\n\n# augment the training data with the fitted data\n\n\n# compute the metrics\n\n\n?yardstick::roc_curve\n# compute roc_auc and plot the roc_curve"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-6-cross-validation",
    "href": "labs/BSMM_8740_lab_5.html#exercise-6-cross-validation",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 6 cross validation",
    "text": "Exercise 6 cross validation\nFollowing the last exercise, we should have some concerns about over-fitting by the nearest-neighbour model.\nTo address this we will use cross validation to tune the model and evaluate the fits.\n\nCreate a cross-validation dataset based on 5 folds using rsample::vfold_cv.\nUsing the knn workflow from exercise 5, apply tune::fit_resamples with arguments resamples and control where the resamples are the dataset created in step 1 and control is tune::control_resamples(save_pred = TRUE), which will ensure that the predictions are saved.\nUse tune::collect_metrics() on the results from step 2\nUse tune::collect_predictions() on the results from step 2 to plot the roc_auc curve as in exercise 5. Has it changed much from exercise 5?\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?rsample::vfold_cv\n\n# create v-fold cross validation data\n\n# use tune::fit on the cv dat, saving the predictions\n\n\n?tune::fit_resamples\n\n# collect the metrics\n\n# compute the roc_curve\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-7-tuning-for-k",
    "href": "labs/BSMM_8740_lab_5.html#exercise-7-tuning-for-k",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 7: tuning for k",
    "text": "Exercise 7: tuning for k\nIn this exercise weâ€™ll tune the number of nearest neighbours in our model to see if we can improve performance.\n\nRedo exercise 5 steps 1 and 2, setting neighbors = tune::tune() for the model, and then updating the workflow with workflows::update_model.\nUse dials::grid_regular(dials::neighbors(), levels = 10) to create a grid for tuning k.\nUse tune::tune_grid with tune::control_grid(save_pred = TRUE) and yardstick::metric_set(yardstick::accuracy, yardstick::roc_auc) to generate tuning results\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?tune::tune_grid\n\n# re-specify the model for tuning\n\n\n# update the workflow\n\n\n# make a grid for tuning\n\n\n# use the grid to tune the model\n\n\n# show the tuning results dataframe"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-8",
    "href": "labs/BSMM_8740_lab_5.html#exercise-8",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 8",
    "text": "Exercise 8\nUse tune::collect_metrics() to collect the metrics from the tuning results in exercise 7 and then plot the metrics as a function of k using the code below.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# collect the metrics\n\n# plot the collected metrics as a function of K\n_your_metrics_ |&gt;\nggplot(aes(neighbors,mean)) +\n  geom_line(linewidth = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-9",
    "href": "labs/BSMM_8740_lab_5.html#exercise-9",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 9",
    "text": "Exercise 9\nUse tune::show_best and tune::select_best with argument â€œroc_aucâ€ to find the best k for the knn classification model. Then\n\nupdate the workflow using tune::finalize_workflow to set the best k value.\nuse tune::last_fit with the updated workflow from step 1, evaluated on the split data from exercise 2 to finalize the fit.\nuse tune::collect_metrics() to get the metrics for the best fit\nuse tune::collect_predictions() to get the predictions and plot the roc_auc as in the prior exercises\n\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n?tune::finalize_workflow\n# show the roc_auc metrics\n\n# select the best roc_auc metric (using a function from tune::)\n\n# finalize the workflow with the best nn metric from the last step\n\n# use  tune::last_fit with the finaized workflow on the data_split (ex 2)\n\n# collect the metrics from the final fit\n\n# collect the predictions from the final fit and plot the roc_curve"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#exercise-10-clustering",
    "href": "labs/BSMM_8740_lab_5.html#exercise-10-clustering",
    "title": "Lab 5 - Classification and clustering",
    "section": "Exercise 10: clustering",
    "text": "Exercise 10: clustering\nLoad the data for this exercise as below and plot it, and then create an analysis dataset with the cluster labels removed\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# read the data\nlabelled_points &lt;- readr::read_csv(\"data/lab_5_clusters.csv\", show_col_types = FALSE)\n\n# plot the clusters\nlabelled_points |&gt; ggplot(aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3) + \n  theme(legend.position=\"none\")\n\n# remove cluster labels to make the analysis dataset\npoints &lt;-\n  labelled_points |&gt;\n  select(?)\n\nYou have frequently used broom::augment to combine a model with the data set, and broom::tidy to summarize model components; broom::glance is used to similarly to summarize goodness-of-fit metrics.\nNow perform k-means clustering on the points data for different values of k as follows:\n\nkclusts &lt;-\n  # number of clusters from 1-9\n  tibble(k = 1:9) |&gt;\n  # mutate to add columns\n  mutate(\n    # a list-column with the results of the kmeans function (clustering)\n    kclust = purrr::map(k, ~stats::kmeans(points, .x)),\n    # a list-column with the results broom::tidy applied to the clustering results\n    tidied = purrr::map(kclust, broom::tidy),\n    # a list-column with the results broom::glance applied to the clustering results\n    glanced = purrr::map(kclust, broom::glance),\n    # a list-column with the results broom::augment applied to the clustering results\n    augmented = purrr::map(kclust, broom::augment, points)\n  )\n\n(i) Create 3 variables by tidyr::unnesting the appropriate columns of kclusts\n\n# take kclusts and use tidy::unnest() on the appropriate columns\n\nclusters &lt;- ?\n\nassignments &lt;- ?\n\nclusterings &lt;- ?\n\n(ii) Use the assignments variable to plot the cluster assignments generated by stats::kmeans\n\n# plot the points assigned to each cluster\np &lt;- assignments |&gt; ggplot(aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) +\n  facet_wrap(~ k) + theme(legend.position=\"none\")\np\n\n(iii) Use the clusters variable to add the cluster centers to the plot\n\n# on the last plot, mark the cluster centres with an X\np + geom_point(data = clusters, size = 10, shape = \"x\")\n\n(iv) Use the clusterings variable to plot the total within sum of squares value by number of clusters.\n\n# make a separate line-and-point plot with the tot-withinss data by cluster number\nclusterings |&gt; ggplot(aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n\n(v) Using the results of parts (iii) and (iv), the k (number of clusters) that gives the best results is __.\n\n\n\nYouâ€™re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message â€œDone with Lab 5!â€ , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyoneâ€™s repository submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (donâ€™t forget to commit and then push your work!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_5.html#grading",
    "href": "labs/BSMM_8740_lab_5.html#grading",
    "title": "Lab 5 - Classification and clustering",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html",
    "href": "labs/BSMM_8740_lab_2.html",
    "title": "Lab 2 - The Recipes package",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll explore several data sets and practice pre-processing and feature engineering with recipes. The main goal is to increase your understanding of the recipes workflow, including the data structures underlying recipes, what they contain and how to access them.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to use the recipes package to prepare and train & test datasets for analysis/modeling.\nBe able to access recipes data post-processing to both extract & validate the results of your recipes steps."
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#introduction",
    "href": "labs/BSMM_8740_lab_2.html#introduction",
    "title": "Lab 2 - The Recipes package",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll explore several data sets and practice pre-processing and feature engineering with recipes. The main goal is to increase your understanding of the recipes workflow, including the data structures underlying recipes, what they contain and how to access them.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to use the recipes package to prepare and train & test datasets for analysis/modeling.\nBe able to access recipes data post-processing to both extract & validate the results of your recipes steps."
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#getting-started",
    "href": "labs/BSMM_8740_lab_2.html#getting-started",
    "title": "Lab 2 - The Recipes package",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-2-[your github username] repository .\nCreate an R project using your 2024-lab-2-[your github username] repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the 2024-lab-2.qmd file in your personal repository.\nWhen you are done, be sure to save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#packages",
    "href": "labs/BSMM_8740_lab_2.html#packages",
    "title": "Lab 2 - The Recipes package",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in todayâ€™s lab.\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, forcats\n)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#data-the-boston-cocktail-recipes",
    "href": "labs/BSMM_8740_lab_2.html#data-the-boston-cocktail-recipes",
    "title": "Lab 2 - The Recipes package",
    "section": "Data: The Boston Cocktail Recipes",
    "text": "Data: The Boston Cocktail Recipes\nThe Boston Cocktail Recipes dataset appeared in a TidyTuesday posting. TidyTuesday is a weekly data project in R.\nThe dataset is derived from the Mr.Â Boston Bartenderâ€™s Guide, together with a dataset that was web-scraped as part of a hackathon.\nThis dataset contains the following information for each cocktail:\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of cocktail\n\n\ncategory\ncharacter\nCategory of cocktail\n\n\nrow_id\ninteger\nDrink identifier\n\n\ningredient_number\ninteger\nIngredient number\n\n\ningredient\ncharacter\nIngredient\n\n\nmeasure\ncharacter\nMeasurement/volume of ingredient\n\n\nmeasure_number\nreal\nmeasure as a number\n\n\n\nUse the code below to load the Boston Cocktail Recipes data set.\n\nboston_cocktails &lt;- readr::read_csv('data/boston_cocktails.csv', show_col_types = FALSE)\n\n\n\n\n\n\n\nNote\n\n\n\nExercises 1-7 use the recipes package to preprocess data, producing normalized data and principle components whose key parameters can be accessed from the prepped recipe object. As an unsupervised â€˜learningâ€™ method the results of PCA often need to interpreted for management. The majority of exercises 1-7 ask you to do that."
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#exercises",
    "href": "labs/BSMM_8740_lab_2.html#exercises",
    "title": "Lab 2 - The Recipes package",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nFirst use skimr::skim to assess the quality of the data set.\nNext prepare a summary. What is the median measure number across cocktail recipes?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 2\nFrom the boston_cocktails dataset select the name, category, ingredient, and measure_number columns and then pivot the table to create a column for each ingredient. Fill any missing values with the number zero.\nSince the names of the new columns may contain spaces, clean then using the janitor::clean_names(). Finally drop any rows with NA values and save this new dataset in a variable.\nHow much gin is in the cocktail called Leap Frog Highball?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 3\nPrepare a recipes::recipe object without a target but give name and category as â€˜idâ€™ roles. Add steps to normalize the predictors and perform PCA. Finally prep the data and save it in a variable.\nHow many predictor variables are prepped by the recipe?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nApply the recipes::tidy verb to the prepped recipe in the last exercise. The result is a table identifying the information generated and stored by each step in the recipe from the input data.\nTo see the values calculated for normalization, apply the recipes::tidy verb as before, but with second argument = 1.\nWhat ingredient is the most used, on average?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 5\nNow look at the result of the PCA, applying the recipes::tidy verb as before, but with second argument = 2. Save the result in a variable and filter for the components PC1 to PC5. Mutate the resulting component column so that the values are factors, ordering them in the order they appear using the forcats::fct_inorder verb.\nPlot this data using ggplot2 and the code below\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK \n\n\n\n\nggplot(aes(value, terms, fill = terms)) +\ngeom_col(show.legend = FALSE) +\nfacet_wrap(~component, nrow = 1) +\nlabs(y = NULL) +\ntheme(axis.text=element_text(size=7),\n      axis.title=element_text(size=14,face=\"bold\"))\n\nHow would you describe the drinks represented by PC1?\n\n\nExercise 6\nAs in the last exercise, use the variable with the tidied PCA data and use only PCA components PC1 to PC4. Take/slice the top 8 ingedients by component, ordered by their absolute value using the verb dplyr::slice_max. Next, generate a grouped table using gt::gt, colouring the cell backgrounds (i.e.Â fill) with green for values â‰¥0\\ge0 and red for values &lt;0&lt;0.\nWhat is the characteristic alcoholic beverage of each of the first 4 principle components.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 7\nFor this exercise, bake the prepped PCA recipe using recipes::bake on the original data and plot each cocktail by its PC1, PC2 component, using\n\nggplot(aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  geom_text(check_overlap = TRUE, hjust = \"inward\") + \n  labs(color = NULL)\n\nCan you create an interpretation of the PCA analysis?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 8\nIn the following exercise, weâ€™ll use the recipes package to prepare time series data. The starting dataset contains monthly house price data for each of the four countries/regions in the UK\n\nuk_prices &lt;- readr::read_csv('data/UK_house_prices.csv', show_col_types = FALSE)\n\nWrite code to clean the names in the uk_prices dataset using janitor::clean_names(), and then using skimr::skim confirm that the region names are correct and that there are no missing values. Call the resulting analytic data set df.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\n\n\nExercise 9\nWe want to use the monthly house price data to predict house prices one month ahead for each region. The basic model will be:\nSalesVolume ~ .\nwhere the date and region-name are id variables, not predictor variables. Instead we will use the prior-month lagged prices as the only predictor.\nThe recipe uses the following 5 steps from the recipes package: update_role(date, region_name, new_role = â€œidâ€) | recipe(sales_volume ~ ., data = df |&gt; janitor::clean_names()) | step_naomit(lag_1_sales_volume, skip=FALSE) | step_lag(sales_volume, lag=1) | step_arrange(region_name, date)\nUse these 5 steps in the proper order to create a recipe to pre-process the time series data for each region. Prep and then bake your recipe using df.\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n\n\n\nThe result of the bake step, after filtering for dates â‰¤\\le 2005-03-01, should look like this:\n\nreadRDS(\"data/baked_uk_house_dat.rds\") |&gt; \n  dplyr::filter(date &lt;= lubridate::ymd(20050301)) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_currency(columns = -c(date,region_name), decimals = 0) |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\ndate\nregion_name\nsales_volume\nlag_1_sales_volume\n\n\n\n\n2005-02-01\nEngland\n$56,044\n$53,464\n\n\n2005-03-01\nEngland\n$67,322\n$56,044\n\n\n2005-02-01\nNorthern Ireland\n$978\n$978\n\n\n2005-03-01\nNorthern Ireland\n$978\n$978\n\n\n2005-02-01\nScotland\n$7,631\n$8,876\n\n\n2005-03-01\nScotland\n$9,661\n$7,631\n\n\n2005-02-01\nWales\n$2,572\n$2,516\n\n\n2005-03-01\nWales\n$3,336\n$2,572\n\n\n\n\n\n\n\n\n\nExercise 10\n\nRecall The Business Problem\nWeâ€™re at a fast paced startup. The company is growing fast and the marketing team is looking for ways to increase the sales from existing customers by making them buy more. The main idea is to unlock the potential of the customer base through incentives, in this case a discount. We of course want to measure the effect of the discount on the customerâ€™s behavior. Still, they do not want to waste money giving discounts to users which are not valuable. As always, it is about return on investment (ROI).\nWithout going into specifics about the nature of the discount, it has been designed to provide a positive return on investment if the customer buys more than $1\\$ 1 as a result of the discount. How can we measure the effect of the discount and make sure our experiment has a positive ROI? The marketing team came up with the following strategy:\n\nSelect a sample of existing customers from the same cohort.\nSet a test window of 1 month.\nLook into the historical data of web visits from the last month. The hypothesis is that web visits are a good proxy for the customerâ€™s interest in the product.\nFor customers with a high number of web visits, send them a discount. There will be a hold out group which will not receive the discount within the potential valuable customers based on the number of web visits. For customers with a low number of web visits, do not send them a discount (the marketing team wants to report a positive ROI, so they do not want to waste money on customers which are not valuable). Still, they want to use them to measure the effect of the discount.\nWe also want to use the results of the test to tag loyal customers. These are customers which got a discount (since they showed potential interest in the product) and customers with exceptional sales numbers even if they did not get a discount. The idea is to use this information to target them in the future if the discount strategy is positive.\n\nIn the last lab we did some exploratory data analysis. The next step is to prepare some descriptive statistics.\n\nDescriptive Statistics\nThe first thing the data analytics team did was to split the sales distribution by discount group:\n\ndata &lt;- readr::read_csv('data/sales_dag.csv', show_col_types = FALSE)\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x = sales, after_stat(count), fill = discount)) +\n  geom_histogram(alpha = 0.30, position = 'identity', color=\"#e9ecef\", bins = 30)+\n  geom_density(alpha = 0.30) +\n  xlab(\"Sales\") +\n  ylab(\"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks customers with a discount have higher sales. Data scientist A is optimistic with this initial result. To quantify this, compute the difference in means by discount:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\ngroup by discount and find the difference in means:\nWhat is the difference in means?\n\n\nThe discount strategy seems to be working. Data scientist A is happy with the results and decides to get feedback from the rest of the data science team.\nData scientist B is not so happy with the results. They think that the uplift is too good to be true (based on domain knowledge and the sales distributions ğŸ¤”). When thinking about reasons for such a high uplift, they realized the discount assignment was not at random. It was based on the number of web visits (remember the marketing plan?). This means that the discount group is not comparable to the control group completely! They decide to plot sales against web visits per discount group:\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x=visits, y = sales, color = discount)) +\n  geom_point() + \n  facet_grid(cols = vars(discount))\n\n\n\n\n\n\n\n\nIndeed, they realize they should probably adjust for the number of web visits. A natural metric is sales per web visit. Compute the sales per visit for each discount group:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n\n\nThe mean value is higher for the discount group. As always, they also looked at the distributions:\n\ndata |&gt; dplyr::mutate(discount = factor(discount)) |&gt; \n  ggplot(aes(x = sales_per_visit, after_stat(count), fill = discount)) +\n  geom_histogram(alpha = 0.30, position = 'identity', color=\"#e9ecef\", bins = 30)+\n  # geom_density(alpha = 0.30) +\n  xlab(\"Sales per Visit\") +\n  ylab(\"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor both data scientists A & B the results look much better, but they were unsure about which uplift to report. They thought about the difference in means:\nCompute the difference in mean sales_per_visit by discount:\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\nCan you see a way to interpret this in terms of dollars? yes/no\n\n\n\nYouâ€™re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message â€œDone with Lab 2!â€ , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub."
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#submission",
    "href": "labs/BSMM_8740_lab_2.html#submission",
    "title": "Lab 2 - The Recipes package",
    "section": "Submission",
    "text": "Submission\nI will pull (copy) everyoneâ€™s submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (donâ€™t forget to commit and then push your work by 5:00pm on Sunday!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_2.html#grading",
    "href": "labs/BSMM_8740_lab_2.html#grading",
    "title": "Lab 2 - The Recipes package",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html",
    "href": "labs/BSMM_8740_lab_3.html",
    "title": "Lab 3 - Regression",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll explore several data sets and practice building and evaluating regression models.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to use different regression models to predict a response/target/outcome as a function of a set of variates."
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#introduction",
    "href": "labs/BSMM_8740_lab_3.html#introduction",
    "title": "Lab 3 - Regression",
    "section": "",
    "text": "In todayâ€™s lab, youâ€™ll explore several data sets and practice building and evaluating regression models.\n\n\nBy the end of the lab you willâ€¦\n\nBe able to use different regression models to predict a response/target/outcome as a function of a set of variates."
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#getting-started",
    "href": "labs/BSMM_8740_lab_3.html#getting-started",
    "title": "Lab 3 - Regression",
    "section": "Getting started",
    "text": "Getting started\n\nTo complete the lab, log on to your github account and then go to the class GitHub organization and find the 2024-lab-3-[your github username] repository to complete the lab.\nCreate an R project using your 2024-lab-3-[your github username] repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the 2024-lab-3.qmd file in your repository.\nWhen you are done, be sure to save your document, stage, commit and push your work.\n\n\n\n\n\n\n\nImportant\n\n\n\nTo access Github from the lab, you will need to make sure you are logged in as follows:\n\nusername: .\\daladmin\npassword: Business507!\n\nRemember to (create a PAT and set your git credentials)\n\ncreate your PAT using usethis::create_github_token() ,\nstore your PAT with gitcreds::gitcreds_set() ,\nset your username and email with\n\nusethis::use_git_config( user.name = ___, user.email = ___)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#packages",
    "href": "labs/BSMM_8740_lab_3.html#packages",
    "title": "Lab 3 - Regression",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in todayâ€™s lab.\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse, magrittr, gt, gtExtras, tidymodels, DataExplorer, skimr, janitor, ggplot2, knitr,\n  ISLR2, stats, xgboost\n)\ntheme_set(theme_bw(base_size = 12))"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#data-boston-house-values",
    "href": "labs/BSMM_8740_lab_3.html#data-boston-house-values",
    "title": "Lab 3 - Regression",
    "section": "Data: Boston House Values",
    "text": "Data: Boston House Values\nThe Boston House Values dataset (usually referred to as the Boston dataset) appears in several R packages in different versions and is based on economic studies published in the late 1970â€™s.\nThis dataset contains the following information for each cocktail:\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ncrim\nper capita crime rate by town.\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus\nproportion of non-retail business acres per town.\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox\nnitrogen oxides concentration (parts per 10 million).\n\n\nrm\naverage number of rooms per dwelling.\n\n\nage\nproportion of owner-occupied units built prior to 1940.\n\n\ndis\nweighted mean of distances to five Boston employment centres.\n\n\nrad\nindex of accessibility to radial highways.\n\n\ntax\nfull-value property-tax rate per $10,000.\n\n\nptratio\npupil-teacher ratio by town.\n\n\nlstat\nlower status of the population (percent).\n\n\nmedv\nmedian value of owner-occupied homes in $1000s.\n\n\n\nUse the code below to load the Boston Cocktail Recipes data set.\n\nboston &lt;- ISLR2::Boston"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#exercises",
    "href": "labs/BSMM_8740_lab_3.html#exercises",
    "title": "Lab 3 - Regression",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nPlot the median value of owner-occupied homes (medv) vs the percentage of houses with lower socioeconomic status (lstat) then use lm to model medv ~ lstat and save the result in a variable for use later.\nNext prepare a summary of the model. What is the intercept and the coefficient of lstat in this model?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASE SHOW YOUR WORK\n# plot medv vs lstat\n\n\n# create a linear model of medv vs lstat and save the model\n\n\n\n\n\nExercise 2\nUsing the result from Exercise 1, and the data below, use the predict function (stats::predict.lm or just predict) with the argument interval = â€œconfidenceâ€ to prepare a summary table with columns lstat, fit, lwr, upr.\nYou can use stats::predict.lm directly with the data below.\nOr consider creating a nested column using dplyr::mutate along with purrr::map with first argument lstat and second argument a function you create. The last operation is to unnest the nested column with tidyr::unnest(__).\nOr\n\ntibble(lstat = c(5, 10, 15, 20))\n\nFinally, use your model to plot some performance checks using the performance::check_model function with arguments check=c(\"linearity\",\"qq\",\"homogeneity\", \"outliers\").\nAre there any overly influential observations in this dataset?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# Are there any overly influential observations in this dataset?\n\n\n\n\n\nExercise 3\nFit the variable medv (median value of owner-occupied homes) to all predictors in the dataset and use the performance::check_collinearity function on the resulting model to check if any predictors are redundant.\nThe variance inflation factor is a measure of the magnitude of multicollinearity of model terms. A VIF less than 5 indicates a low correlation of that predictor with other predictors. A value between 5 and 10 indicates a moderate correlation, while VIF values larger than 10 are a sign for high, not tolerable correlation of model predictors.\nWhich predictors in this dataset might be redundant for predicting medv?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# Which predictors in this dataset might be redundant for predicting `medv`?\n\n\n\n\nThis is a good place to save, commit, and push changes to your remote lab repo. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nIn this exercise you will compare and interpret the results of linear regression on two similar datasets.\nThe first dataset (dat0 - generated below) has demand0 and price0 variables along with an unobserved variable (unobserved0 - so not in our dataset) that doesnâ€™t change the values of demand0 and price0. Use lm to build a model to predict demand0 from price0 . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset dat0?\n\n\n\n\n\n\nTip\n\n\n\nplot with something like:\ndat0 %&gt;% ggplot(aes(x=price0,y=demand0)) + \n         # plot the points\n         geom_point() +\n         # add a straight line to the plot\n         geom_abline(\n          data = ?? a table with the coefficient estimates ??\n            , aes(intercept = `(Intercept)`, slope = price0)\n            , colour = \"red\"\n         )\n\n\n\nN &lt;- 500\nset.seed(1966)\n\ndat0 &lt;- tibble::tibble(\n  price0 = 10+rnorm(500)\n  , demand0 = 30-(price0 + rnorm(500))\n  , unobserved0 = 0.45*price0 + 0.77*demand0 + rnorm(500)\n)\n\nThe second dataset (dat1 - generated below) has demand1 and price1 variables, along with a variable unobserved1 that is completely random and is not observed, so it isnâ€™t in our dataset. Use lm to build a model to predict demand1 from price1 . Plot the data, including intercept and slope. What is the slope of the demand curve in dataset dat1?\n\nset.seed(1966)\n\ndat1 &lt;- tibble::tibble(\n  unobserved1 = rnorm(500)\n  , price1 = 10 + unobserved1 + rnorm(500)\n  , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))\n)\n\nWhich linear model returns the (approximately) correct dependence of demand on price, as given in the data generation process?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# Which linear model returns the (approximately) correct dependence of demand on price, as given in the data generation process?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#exercise-5",
    "href": "labs/BSMM_8740_lab_3.html#exercise-5",
    "title": "Lab 3 - Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNow repeat the modeling of exercise 4, but assuming that the formerly unobservable variables are now observable, and so can be included in the linear regression models.\nWhich model returns the (approximately) correct dependence of demand on price, as given in the data generation process?\nWhat can you conclude from these two exercises?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# What can you conclude from these two exercises?\n\n\n\n\nExercise 6\nFor the next several exercises, weâ€™ll work with a new dataset. This dataset is taken from an EPA site on fuel economy, in particular the fuel economy dataset for 2023.\nUse the code below to load the FE Guide data set.\n\ndat &lt;- \n  readxl::read_xlsx(\"data/2023 FE Guide for DOE-release dates before 7-28-2023.xlsx\")\n\nFrom the raw data in dat, weâ€™ll make a smaller dataset, and weâ€™ll need to do some cleaning to make it useable.\nFirst select the columns â€œComb FE (Guide) - Conventional Fuelâ€, â€œEng Displâ€,â€˜# Cylâ€™, Transmission , â€œ# Gearsâ€, â€œAir Aspiration Method Descâ€, â€œRegen Braking Type Descâ€, â€œBatt Energy Capacity (Amp-hrs)â€ , â€œDrive Descâ€, â€œFuel Usage Desc - Conventional Fuelâ€, â€œCyl Deact?â€, and â€œVar Valve Lift?â€ and then clean the column names using janitor::janitor::clean_names(). Assign the revised data to the variable cars_23.\nPerform a quick check of the data using DataExplorer::introduce() and DataExplorer::plot_missing() and modify the data as follows\n\nmutate the columns comb_fe_guide_conventional_fuel, number_cyl, and number_gears to ensure that they contain integers values, not doubles.\nuse tidyr::replace_na to replace any missing values in batt_energy_capacity_amp_hrs column with zeros, and replace and missing values in regen_braking_type_desc with empty strings (â€œâ€œ).\nfinally, mutate the columns â€˜transmissionâ€™,â€˜air_aspiration_method_descâ€™,â€˜regen_braking_type_descâ€™,â€˜drive_descâ€™ ,â€˜fuel_usage_desc_conventional_fuelâ€™,â€˜cyl_deactâ€™,â€˜var_valve_liftâ€™ so their values are factors.\n\nPrepare a recipe to pre-process cars_23 ahead of modelling, using comb_fe_guide_conventional_fuel as the outcome, with the following steps.\n\nCentering for: recipes::all_numeric()\nScaling for: recipes::all_numeric()\nDummy variables from: recipes::all_factor()\n\nHow many predictor variables are there in cars_23 ?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# How many predictor variables are there in `cars_23` ?\n\n\n\n\n\nExercise 7\nFor this exercise, set a sample size equal to 75% of the observations of cars_23 and split the data as follows:\n\nset.seed(1966)\n\n# sample 75% of the rows of the cars_23 dataset to make the training set\ntrain &lt;- cars_23 %&gt;% \n  # make an ID column for use as a key\n  tibble::rowid_to_column(\"ID\") %&gt;% \n  # sample the rows\n  dplyr::sample_frac(0.75)\n\n# remove the training dataset from the original dataset to make the training set\ntest  &lt;- \n  dplyr::anti_join(\n    cars_23 %&gt;% tibble::rowid_to_column(\"ID\") # add a key column to the original data\n    , train\n    , by = 'ID'\n  )\n\n# drop the ID column from training and test datasets\ntrain %&lt;&gt;% dplyr::select(-ID); test %&lt;&gt;% dplyr::select(-ID)\n\nNext prep the recipe created in the last exercise using recipes::prep on the training data, and then use the result of the prep step to recipes::bake with the training and test data. Save the baked data in separate variables for use later.\nAfter these two steps how many columns are in the data? Why does this differ from the last step?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# After these two steps how many columns are in the data? Why does this differ from the last step?\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates youâ€™ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 8\nIn this exercise we will run xgboost::xgboost to evaluate the regression.\nFirst run fit the model with default meta-parameters for max_depth and eta, using the training data per the code below:\n\nuntuned_xgb &lt;-\n  xgboost::xgboost(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    max_depth = 6,\n    eta = .25\n    , verbose = FALSE\n  )\n\nNext use the fitted model to predict the outcome using the test data:\n\n# create predictions using the test data and the fitted model\nyhat &lt;- predict(\n  untuned_xgb\n  , cars_23_test %&gt;% \n    dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% \n    as.matrix() \n)\n\nFinally, pull out the comb_fe_guide_conventional_fuel column from the test data, assign it to the variable y and then use caret::postResample with arguments yhat and y to evaluate how well the model fits.\nWhat is the RMSE for the un-tuned model?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# What is the RMSE for the un-tuned model?"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#exercise-9",
    "href": "labs/BSMM_8740_lab_3.html#exercise-9",
    "title": "Lab 3 - Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nIn this exercise we are going to tune the model using cross validation. First we create a tuning grid for the parameters and then fit the model for all the values in the grid, saving the results.\nFinally, we select the best parameters by least RMSE. This code will take a while to run\n\n#create hyperparameter grid\nhyper_grid &lt;- expand.grid(max_depth = seq(3, 6, 1), eta = seq(.2, .35, .01))  \n\n# initialize our metric variables\nxgb_train_rmse &lt;- NULL\nxgb_test_rmse  &lt;- NULL\n\nfor (j in 1:nrow(hyper_grid)) {\n  set.seed(123)\n  m_xgb_untuned &lt;- xgboost::xgb.cv(\n    data = cars_23_train %&gt;% dplyr::select(-comb_fe_guide_conventional_fuel) %&gt;% as.matrix(), \n    label = cars_23_train %&gt;% dplyr::select(comb_fe_guide_conventional_fuel) %&gt;% as.matrix(),\n    nrounds = 1000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 3,\n    nfold = 5,\n    max_depth = hyper_grid$max_depth[j],\n    eta = hyper_grid$eta[j],\n    verbose = FALSE\n  )\n  \n  xgb_train_rmse[j] &lt;- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]\n  xgb_test_rmse[j] &lt;- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]\n}    \n\nbest &lt;- hyper_grid[which(xgb_test_rmse == min(xgb_test_rmse)),]; best # there may be ties\n\nre-run the code from the last exercise and evaluate the fit using one of the best tuning parameters (i.e.Â when re-running the regression, set max_depth and eta to one pair the best-fit parameters [there may be ties]).\nIs the tuned model better than the un-tuned model? If better, how much has the RMSE improved (in %).\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# Is the tuned model better than the un-tuned model? If better, how much has the RMSE improved (in %)."
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#exercise-10",
    "href": "labs/BSMM_8740_lab_3.html#exercise-10",
    "title": "Lab 3 - Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nUsing xgboost::xgb.importance rank the importance of each predictor in the model. Finally, take the top 10 predictors by importance and plot them using xgboost::xgb.plot.importance.\nPer this model, what is the most important feature for predicting fuel efficiency?\n\n\n\n\n\n\nYOUR ANSWER:\n\n\n\n\n# PLEASESHOW YOUR WORK\n\n\n\n# What is the most important feature for predicting fuel efficiency?\n\n\n\n\nYouâ€™re done and ready to submit your work! Save, stage, commit, and push all remaining changes. You can use the commit message â€œDone with Lab 3!â€ , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub.\n\n\n\n\n\n\n\nSubmission\n\n\n\nI will pull (copy) everyoneâ€™s submissions at 5:00pm on the Sunday following class, and I will work only with these copies, so anything submitted after 5:00pm will not be graded. (donâ€™t forget to commit and then push your work by 5:00pm on Sunday!)"
  },
  {
    "objectID": "labs/BSMM_8740_lab_3.html#grading",
    "href": "labs/BSMM_8740_lab_3.html#grading",
    "title": "Lab 3 - Regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 30 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n30"
  },
  {
    "objectID": "GitHub.html",
    "href": "GitHub.html",
    "title": "GitHub",
    "section": "",
    "text": "GitHub data\n\n&lt;p&gt;Loadingâ€¦&lt;/p&gt;",
    "crumbs": [
      "Course information",
      "GitHub username survey"
    ]
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (â€œPublic Licenseâ€). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 â€“ Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapterâ€™s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 â€“ Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor â€“ Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor â€“ Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapterâ€™s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 â€“ License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapterâ€™s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapterâ€™s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapterâ€™s License You apply.\n\n\n\nSection 4 â€“ Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 â€“ Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 â€“ Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 â€“ Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 â€“ Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the â€œLicensor.â€ The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark â€œCreative Commonsâ€ or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html",
    "href": "weeks/BSMM_8740_week_9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\nDue date: Exam 1 released on Fri, Feb 35, due Mon, Feb 28 at 11:59pm",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html#prepare",
    "href": "weeks/BSMM_8740_week_9.html#prepare",
    "title": "Week 9",
    "section": "Prepare",
    "text": "Prepare\nNo readings this week.",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html#participate",
    "href": "weeks/BSMM_8740_week_9.html#participate",
    "title": "Week 9",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 09 - Causality: effects",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html#practice",
    "href": "weeks/BSMM_8740_week_9.html#practice",
    "title": "Week 9",
    "section": "Practice",
    "text": "Practice\nğŸ“‹ Application Exercise 8 - Rail Trail",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_9.html#perform",
    "href": "weeks/BSMM_8740_week_9.html#perform",
    "title": "Week 9",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 9 - Causality: effects\nâœ… Exam 2\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html",
    "href": "weeks/BSMM_8740_week_11.html",
    "title": "Week 11",
    "section": "",
    "text": "No additional readings this week. Catch up with previously assigned readings if youâ€™ve fallen behind.",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html#prepare",
    "href": "weeks/BSMM_8740_week_11.html#prepare",
    "title": "Week 11",
    "section": "",
    "text": "No additional readings this week. Catch up with previously assigned readings if youâ€™ve fallen behind.",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html#participate",
    "href": "weeks/BSMM_8740_week_11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 11 - Bayesian methods",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html#practice",
    "href": "weeks/BSMM_8740_week_11.html#practice",
    "title": "Week 11",
    "section": "Practice",
    "text": "Practice\nğŸ“‹ Application Exercise 10 - Flight delays",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_11.html#perform",
    "href": "weeks/BSMM_8740_week_11.html#perform",
    "title": "Week 11",
    "section": "Perform",
    "text": "Perform\nâœï¸ HW 3 - Logistic regression and log transformation\nâŒ¨ï¸ Lab 11 - Bayesian methods\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html",
    "href": "weeks/BSMM_8740_week_5.html",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 5 - Sunday, Oct 13, 5pm ET\nQuiz 1 will be in class on October 09 (est. 30 minutes)",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#prepare",
    "href": "weeks/BSMM_8740_week_5.html#prepare",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Prepare",
    "text": "Prepare\nğŸ“– Read Introduction to Modern Statistics, Sec 9: Logistic regression\nğŸ“– Read Classification with the Tidymodels Framework in R\nğŸ“– Absorb Understanding Precision, Sensitivity, and Specificity In Classification Modeling and How To Calculate Them With A Confusion Matrix\nğŸ“– Check out Tidymodels Tutorial - Classification\nğŸ“– Look at Classification: ROC and AUC",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#participate",
    "href": "weeks/BSMM_8740_week_5.html#participate",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 5 - Classification & Clustering Methods",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#perform",
    "href": "weeks/BSMM_8740_week_5.html#perform",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 5 - Classification & Clustering Methods",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#podcast",
    "href": "weeks/BSMM_8740_week_5.html#podcast",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Podcast",
    "text": "Podcast\nListen here",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_5.html#study",
    "href": "weeks/BSMM_8740_week_5.html#study",
    "title": "Week 5 - Classification & Clustering Methods",
    "section": "Study",
    "text": "Study\n\nShort Answer Questions\nInstructions: Answer the following questions in 2-3 sentences each.\n\nWhat is the key difference between eager learners and lazy learners in classification?\nProvide an example of a situation where accuracy might be a misleading metric for evaluating a binary classifier.\nExplain how adjusting the classification threshold in a binary classifier affects the trade-off between sensitivity and specificity.\nWhat is the main assumption made by Naive Bayes classification, and why is it considered â€œnaiveâ€?\nHow does the choice of â€˜kâ€™ affect the performance of a k-Nearest Neighbors (k-NN) classifier?\nWhat is the concept of a â€œmarginâ€ in Support Vector Machine (SVM) classification, and why is maximizing it desirable?\nDescribe two common kernel functions used in SVM classification and their applications.\nWhat is the curse of dimensionality, and how does it impact k-NN classification?\nWhat is the fundamental difference between classification and clustering in machine learning?\nBriefly explain the steps involved in the k-means clustering algorithm.\n\nShort Answer Key\n\nEager learners (e.g., logistic regression, decision trees) learn a model from the training data before making predictions, while lazy learners (e.g., k-NN) memorize the training data and classify new instances based on similarity to stored instances.\nIn a highly imbalanced dataset (e.g., fraud detection), a model that always predicts the majority class will have high accuracy but fail to identify the minority class, which is often of greater interest.\nLowering the classification threshold increases sensitivity (recall) but decreases specificity, leading to more true positives but also more false positives. Raising the threshold has the opposite effect.\nNaive Bayes assumes that all features are conditionally independent, given the class label. This assumption is often unrealistic in real-world data, hence the term â€œnaive.â€\nA small â€˜kâ€™ can make k-NN sensitive to noise (overfitting), while a large â€˜kâ€™ can oversmooth the decision boundary (underfitting). The optimal â€˜kâ€™ depends on the dataset and is typically found through cross-validation.\nThe margin in SVM is the distance between the separating hyperplane and the nearest data points (support vectors) of each class. Maximizing the margin aims to improve the classifierâ€™s generalization ability and reduce overfitting.\nThe Radial Basis Function (RBF) kernel is commonly used for non-linear classification problems. The polynomial kernel maps data into a higher-dimensional space and can model non-linear relationships between features.\nThe curse of dimensionality refers to the phenomenon where distances between points become less meaningful as the number of dimensions increases. In high-dimensional spaces, k-NN can struggle to find meaningful nearest neighbors.\nClassification is a supervised learning task where the goal is to predict predefined labels for data points. Clustering is an unsupervised learning task where the goal is to discover natural groupings or structures in data without predefined labels.\nThe k-means algorithm initializes â€˜kâ€™ centroids randomly. Then, it iteratively assigns each data point to the nearest centroid and recalculates the centroids until convergence (centroids no longer change significantly).\n\nEssay Questions\n\nCompare and contrast eager learning and lazy learning algorithms for classification. Discuss their strengths, weaknesses, and situations where one might be preferred over the other.\nExplain the concepts of precision and recall in the context of binary classification. Discuss how these metrics are affected by changes in the classification threshold and provide examples of applications where each metric might be prioritized.\nDescribe the Naive Bayes classification algorithm in detail, including its underlying assumptions and the calculation of conditional probabilities. Discuss its advantages, limitations, and potential applications.\nExplain the workings of Support Vector Machines (SVMs) for classification. Describe the concepts of support vectors, margins, and kernel functions. Compare and contrast linear, polynomial, and radial basis function (RBF) kernels in SVM.\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html",
    "href": "weeks/BSMM_8740_week_6.html",
    "title": "Week 6 - Time Series Methods",
    "section": "",
    "text": "Important\n\n\n\nDue date: Lab 6 - Sunday, Oct 27, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html#prepare",
    "href": "weeks/BSMM_8740_week_6.html#prepare",
    "title": "Week 6 - Time Series Methods",
    "section": "Prepare",
    "text": "Prepare\nğŸ“– Read Time Series Analysis with R, Chapter 7: Structural Decomposition\nğŸ“– Read Modeling time series with tidy resampling\nğŸ“– Read Introducing Modeltime: Tidy Time Series Forecasting using Tidymodels\nğŸ“– Read Forecasting: Principles and Practice, Chapter 7: Time series regression models\nğŸ“– Read Forecasting: Principles and Practice, Chapter 9: ARIMA Models",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html#participate",
    "href": "weeks/BSMM_8740_week_6.html#participate",
    "title": "Week 6 - Time Series Methods",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 6 - Time Series Methods",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html#practice",
    "href": "weeks/BSMM_8740_week_6.html#practice",
    "title": "Week 6 - Time Series Methods",
    "section": "Practice",
    "text": "Practice\nğŸ“‹ Quiz 1 Solutions",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_6.html#perform",
    "href": "weeks/BSMM_8740_week_6.html#perform",
    "title": "Week 6 - Time Series Methods",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 6 - Time Series Methods\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_3.html",
    "href": "weeks/BSMM_8740_week_3.html",
    "title": "Week 3 - Regression Methods",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 3 - Sunday, Sept 29, 5pm ET",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_3.html#prepare",
    "href": "weeks/BSMM_8740_week_3.html#prepare",
    "title": "Week 3 - Regression Methods",
    "section": "Prepare",
    "text": "Prepare\nğŸ“– Read Chapter 2 - General Aspects of Fitting Regression Models in: Regression Modeling Strategies\nğŸ“– Read Chapter (8.1-8.5) - Regression Models in: Modern Statistics in R\nğŸ“– Follow along with the R code in Linear Regression in R: Linear Regression Hands on Tutorial\nğŸ“– Follow along with the R code from R code for Regression Analysis in An R companion\nğŸ“– Check out Regression and Other Stories - Examples: Regression and Other Stories Examples",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_3.html#participate",
    "href": "weeks/BSMM_8740_week_3.html#participate",
    "title": "Week 3 - Regression Methods",
    "section": "Participate",
    "text": "Participate\nğŸ–¥ï¸ Lecture 3 - Regression Methods",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/BSMM_8740_week_3.html#perform",
    "href": "weeks/BSMM_8740_week_3.html#perform",
    "title": "Week 3 - Regression Methods",
    "section": "Perform",
    "text": "Perform\nâŒ¨ï¸ Lab 3 - Regression Methods\nâŒ¨ï¸ Example 1: grouped data & weighted regression\nFrom Section 10.8 of Regression and Other Stories:\n\nThree models leading to weighted regression\nWeighted least squares can be derived from three different models:\n\nUsing observed data to represent a larger population. This is the most common way that regression weights are used in practice. A weighted regression is fit to sample data in order to estimate the (unweighted) linear model that would be obtained if it could be fit to the entire population. For example, suppose our data come from a survey that oversamples older white women, and we are interested in estimating the population regression. Then we would assign to survey respondent a weight that is proportional to the number of people of that type in the population represented by that person in the sample. In this example, men, younger people, and members of ethnic minorities would have higher weights. Including these weights in the regression is a way to approximately minimize the sum of squared errors with respect to the population rather than the sample.\nDuplicate observations. More directly, suppose each data point can represent one or more actual observations, so that i represents a collection of w_i data points, all of which happen to have x_i as their vector of predictors, and where y_i is the average of the corresponding wi outcome variables. Then weighted regression on the compressed dataset, (x, y, w), is equivalent to unweighted regression on the original data.\nUnequal variances. From a completely different direction, weighted least squares is the maximum likelihood estimate for the regression model with independent normally distributed errors with unequal variances, where sd(Îµ_i) is proportional to 1/âˆšw_i . That is, measurements with higher variance get lower weight when fitting the model. As discussed further in Section 11.1, unequal variances are not typically a major issue for the goal of estimating regression coefficients, but they become more important when making predictions about individual cases.\n\nWe will use weighted regression later in the course (Lectures 7 & 8), using observed data to represent a larger population - case 1 above.\nHereâ€™s an example of the second case:\n\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(dplyr, broom)\n\n\n  The 'cran_repo' argument in shelf() was not set, so it will use\n  cran_repo = 'https://cran.r-project.org' by default.\n\n  To avoid this message, set the 'cran_repo' argument to a CRAN\n  mirror URL (see https://cran.r-project.org/mirrors.html) or set\n  'quiet = TRUE'.\n\nset.seed(1024)\n\n# individual (true) dataset, with 100,000 rows\nx &lt;- round(rnorm(1e5))\ny &lt;- round(x + x^2 + rnorm(1e5))\nind &lt;- data.frame(x, y)\n\n# aggregated dataset: grouped\nagg &lt;- ind %&gt;%\n  dplyr::group_by(x, y) |&gt; \n  dplyr::summarize(freq = dplyr::n(), .groups = 'drop') \n\nmodels &lt;- list( \n  \"True\"                = lm(y ~ x, data = ind),\n  \"Aggregated\"          = lm(y ~ x, data = agg),\n  \"Aggregated & W\"      = lm(y ~ x, data = agg, weights=freq)\n)\n\nmodels[['True']] |&gt; broom::tidy(conf.int = TRUE)\n\n# A tibble: 2 Ã— 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1.08   0.00580      187.       0    1.07       1.10\n2 x               1.01   0.00558      181.       0    0.998      1.02\n\nmodels[['Aggregated']] |&gt; broom::tidy(conf.int = TRUE)\n\n# A tibble: 2 Ã— 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    5.51      0.717      7.69 8.74e-11    4.08       6.95\n2 x              0.910     0.302      3.01 3.69e- 3    0.306      1.51\n\nmodels[['Aggregated & W']] |&gt; broom::tidy(conf.int = TRUE)\n\n# A tibble: 2 Ã— 7\n  term        estimate std.error statistic    p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1.08     0.224      4.84 0.00000795    0.637      1.53\n2 x               1.01     0.216      4.68 0.0000145     0.579      1.44\n\n\nNote the differences in the coefficient estimate for xx and the corresponding standard errors.\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "hw/hw-5.html",
    "href": "hw/hw-5.html",
    "title": "HW 5 - Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1ï¸âƒ£ Have a statistics experience\n2ï¸âƒ£ Make a slide summarizing on your experience\nYou must complete both parts to receive credit."
  },
  {
    "objectID": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity youâ€™d like to do but youâ€™re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::global 2021 talks\nrstudio::conf 2020 talks\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team. One option is DataFest, which will take place over the April 1-3, 2022 weekend. More information to follow here.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isnâ€™t on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy Oâ€™Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some donâ€™t by Nate Silver\nHow Charts Lie by Alberto Cairo\nList of books about data science ethics\n\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\nâœ… Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\nâœ… The visualization should include features or customization that are beyond what weâ€™ve done in class .\nâœ… Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: Coding out loud\nWatch an episode of Coding out loud (either live or pre-recorded) and work through the project.\nA few guidelines:\nâœ… Create a GitHub repo for your Coding out loud submission. Your repo should include\n\nThe Quarto file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\nâœ… The final product (visualuzation, table, etc.) should include features or customization that are beyond what was achieved in the Coding out loud episode.\nâœ… Include the link to your GitHub repo in the slide summarizing your experience."
  },
  {
    "objectID": "hw/hw-5.html#part-2-summarize-your-experience",
    "href": "hw/hw-5.html#part-2-summarize-your-experience",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something weâ€™ve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e.Â use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "hw/hw-5.html#submission",
    "href": "hw/hw-5.html#submission",
    "title": "HW 5 - Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the HW 5 - Statistics Experience assignment on Gradescope by Fri, Apr 15 at 5 pm ET. It must be submitted by the deadline on Gradescope to be considered for grading."
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "",
    "text": "In this assignment, youâ€™ll get to put into practice the logistic regression skills youâ€™ve developed.\n\n\nIn this assignment, you willâ€¦\n\nFit and interpret logistic regression models.\nFit and interpret multiple linear regression models with log transformed outcomes.\nReason around log transformations of various types.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-3. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-3.html#introduction",
    "href": "hw/hw-3.html#introduction",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "",
    "text": "In this assignment, youâ€™ll get to put into practice the logistic regression skills youâ€™ve developed.\n\n\nIn this assignment, you willâ€¦\n\nFit and interpret logistic regression models.\nFit and interpret multiple linear regression models with log transformed outcomes.\nReason around log transformations of various types.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-3. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-3.html#part-1---palmer-penguins",
    "href": "hw/hw-3.html#part-1---palmer-penguins",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 1 - Palmer penguins",
    "text": "Part 1 - Palmer penguins\nIn this part weâ€™ll go back to the Palmer penguins dataset from HW 2.\nWe will use the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ninteger\nPenguin species (Adelie, Gentoo, Chinstrap)\n\n\nisland\ninteger\nIsland where recorded (Biscoe, Dream, Torgersen)\n\n\nflipper_length_mm\ninteger\nFlipper length in mm\n\n\n\nThe goal of this analysis is to use logistic regression to understand the relationship between flipper length, island, and whether a penguin is from the Adelie species. First, we need to create a new response variable to identify whether a penguin is from the Adelie species.\n\npenguins &lt;- penguins %&gt;%\n  mutate(adelie = factor(if_else(species == \"Adelie\", 1, 0)))\n\nAnd letâ€™s check to make sure the new variable looks right before we continue with the analysis.\n\npenguins %&gt;%\n  count(adelie, species)\n\n# A tibble: 3 Ã— 3\n  adelie species       n\n  &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt;\n1 0      Chinstrap    68\n2 0      Gentoo      124\n3 1      Adelie      152\n\n\nLetâ€™s start by looking at the relationship between island and whether a penguin is from the Adelie species.\n\nWhat does the values_fill argument do in the following chunk? The documentation for the function will be helpful in answering this question.\n\npenguins %&gt;%\n  count(island, adelie) %&gt;%\n  pivot_wider(names_from = adelie, values_from = n, values_fill = 0)\n\n# A tibble: 3 Ã— 3\n  island      `0`   `1`\n  &lt;fct&gt;     &lt;int&gt; &lt;int&gt;\n1 Biscoe      124    44\n2 Dream        68    56\n3 Torgersen     0    52\n\n\nCalculate the odds ratio of a penguin being from the Adelie species for those recorded on Dream compared to those recorded on Biscoe.\nYou want to fit a model using island to predict the odds of being from the Adelie species. Let Ï€\\pi be the probability a penguin is from the Adelie species. The model has the following form. What do you expect the value of Î²Ì‚1\\hat{\\beta}_1, the estimated coefficient for Dream, to be? Explain your reasoning.\n\nlog(Ï€1âˆ’Ï€)=Î²0+Î²1Dream+Î²2Torgersen\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1 ~ Dream + \\beta_2 ~ Torgersen\n\n\nFit a model predicting adelie from island and display the model output. For the following exercise, use this model.\nBased on this model, what are the odds of a penguin being from the Adelie species if it was recorded on Biscoe island? on Dream island?\nNext, add flipper length to the model so that there are two predictors. Display the model output. For the following exercises, use this model.\nWrite the regression equation for the model.\nInterpret the coefficient of flipper_length_mm in terms of the log-odds of being from the Adelie species.\nInterpret the coefficient of flipper_length_mm in terms of the odds of being from the Adelie species.\nInterpret the coefficient of Dream in terms of the odds of being from the Adelie species.\nHow do you expect the log-odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island.\nHow do you expect the odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island."
  },
  {
    "objectID": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "href": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 2 - GDP and Urban population",
    "text": "Part 2 - GDP and Urban population\nData on countriesâ€™ Gross Domestic Product (GDP) and percentage of urban population was collected and made available by The World Bank in 2020. A description of the variables as defined by The World Bank are provided below.\n\nGDP: â€œGDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.â€\nUrban Population (% of total): â€œUrban population refers to people living in urban areas as defined by national statistical offices. It is calculated using World Bank population estimates and urban ratios from the United Nations World Urbanization Prospects.â€\n\nThe data can be found in the data folder of your repository. Read the data and name it gdp_2020.\n\nFit a model predicting GDP from urban population. Then make a plot of residuals vs.Â fitted for this model. Does the linear model seem appropriate for modeling this relationship? Explain your reasoning.\nAdd a new column to the gdp_2020 dataset called gdp_log which is the (natural) log of gdp.\nFit a new model, predicting the log of GDP from urban population. Then make a plot of residuals vs.Â fitted for this model. Does the model predicting logged GDP or original GDP appear to be a better fit? Explain your reasoning.\n\nThe model output for predicting logged GDP.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.107\n0.202\n30.291\n0\n\n\nurban\n0.042\n0.003\n13.769\n0\n\n\n\n\n\nThe linear model for predicting log of GDP can be expressed as follows:\nlog(GDP)Ì‚=6.11+0.042Ã—urban\n\\widehat{\\log(GDP)} = 6.11 + 0.042 \\times urban\n\nTherefore, the coefficient of urban (0.042) can be interpreted as the change in logged GDP associated with 1 percentage point increase in urban population. The problem is, logged GDP is not a very informative value to talk about. So we need to undo the transformation weâ€™ve done.\nTo do so, letâ€™s do a quick review of some properties of logs.\n\nSubtraction and logs: log(a)âˆ’log(b)=log(ab)log(a) âˆ’ log(b) = log(\\frac{a}{b})\nNatural logarithm: elog(x)=xe^{log(x)} = x\n\nBased on the interpretation of the slope above, the difference between the predicted values of logged GDP for a given value of urban and a value that is 1 percentage point higher is 0.0425. Letâ€™s write this out mathematically, and then use the properties weâ€™ve listed above to work through the equation.\nlog(GDP for urban x+1)âˆ’log(GDP for urban x)=0.042log(GDP for urban x+1GDP for urban x)=0.042elog(GDP for urban x+1GDP for urban x)=e0.042GDP for urban x+1GDP for urban x=e0.042\n\\begin{aligned}\nlog(\\text{GDP for urban } x + 1) - log(\\text{GDP for urban } x) &= 0.042 \\\\\nlog\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big) &= 0.042 \\\\\ne^{log\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big)} &= e^{0.042}\\\\\n\\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} &= e^{0.042}\n\\end{aligned}\n\n\nBased on the derivation above, fill in the blanks in the following sentence for an alternative (and more useful interpretation) of the slope of urban.\n\nFor each additional percentage point the urban population is higher, the GDP of a country is expected to be ___, on average, by a factor of ___."
  },
  {
    "objectID": "hw/hw-3.html#submission",
    "href": "hw/hw-3.html#submission",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember â€“ you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials â¡ï¸ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and youâ€™ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be â€œcheckedâ€).\nSelect the first page of your PDF submission to be associated with the â€œWorkflow & formattingâ€ section."
  },
  {
    "objectID": "hw/hw-3.html#grading",
    "href": "hw/hw-3.html#grading",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-3.html#footnotes",
    "href": "hw/hw-3.html#footnotes",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe â€œWorkflow & formattingâ€ grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.â†©ï¸"
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "HW 1 - In-person voting trends",
    "section": "",
    "text": "In this assignment, youâ€™ll use simple linear regression to explore the percent of votes cast in-person in the 2020 U.S. election based on the countyâ€™s political leanings.\n\n\nIn this assignment, you willâ€¦\n\nFit and interpret simple linear regression models\nAssess the conditions for simple linear regression.\nCreate and interpret spatial data visualizations using R.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-1.html#introduction",
    "href": "hw/hw-1.html#introduction",
    "title": "HW 1 - In-person voting trends",
    "section": "",
    "text": "In this assignment, youâ€™ll use simple linear regression to explore the percent of votes cast in-person in the 2020 U.S. election based on the countyâ€™s political leanings.\n\n\nIn this assignment, you willâ€¦\n\nFit and interpret simple linear regression models\nAssess the conditions for simple linear regression.\nCreate and interpret spatial data visualizations using R.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-1.html#getting-started",
    "href": "hw/hw-1.html#getting-started",
    "title": "HW 1 - In-person voting trends",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\n\nGo to https://vm-manage.oit.duke.edu/containers and login with your Duke NetID and Password.\nClick STA210 to log into the Docker container. You should now see the RStudio environment.\n\n\n\nClone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-s22 organization on GitHub. Click on the repo with the prefix hw-1. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, youâ€™ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File â› New Project â›Version Control â› Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick hw-1-voting.qmd to open the template R Markdown file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "hw/hw-1.html#packages",
    "href": "hw/hw-1.html#packages",
    "title": "HW 1 - In-person voting trends",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(scales)"
  },
  {
    "objectID": "hw/hw-1.html#data-2020-election",
    "href": "hw/hw-1.html#data-2020-election",
    "title": "HW 1 - In-person voting trends",
    "section": "Data: 2020 Election",
    "text": "Data: 2020 Election\nThere are multiple data sets for this assignment. Use the code below to load the data.\n\nelection_nc &lt;- read_csv(\"data/nc-election-2020.csv\") %&gt;%\n  mutate(fips = as.integer(FIPS))\ncounty_map_data &lt;-  read_csv(\"data/nc-county-map-data.csv\")\nelection_sample &lt;- read_csv(\"data/us-election-2020-sample.csv\")\n\nThe county-level election data in election_nc and election_sample are from The Economist GitHub repo. The data were originally analyzed in the July 2021 article In-person voting really did accelerate covid-19â€™s spread in America. For this analysis, we will focus on the following variables:\n\ninperson_pct: The proportion of a countyâ€™s votes cast in-person in the 2020 election\npctTrump_2016: The proportion of a countyâ€™s votes cast for Donald Trump in the 2016 election\n\nThe data in county_map_data were obtained from the maps package in R. We will not analyze any of the variables in this data set but will use it to help create maps in the assignment. Click here to see the documentation for the maps package. Click here for code examples."
  },
  {
    "objectID": "hw/hw-1.html#exercises",
    "href": "hw/hw-1.html#exercises",
    "title": "HW 1 - In-person voting trends",
    "section": "Exercises",
    "text": "Exercises\nDue to COVID-19 pandemic, many states made alternatives in-person voting, such as voting by mail, more widely available for the 2020 U.S. election. The general consensus was that voters who were more Democratic leaning would be more likely to vote by mail, while more Republican leaning voters would largely vote in-person. This was supported by multiple surveys, including this survey conducted by Pew Research.\nThe goal of this analysis is to use regression analysis to explore the relationship between a countyâ€™s political leanings and the proportion of votes cast in-person in 2020. The ultimate question we want to answer is â€œDid counties with more Republican leanings have a larger proportion of votes cast in-person in the 2020 election?â€\nWe will use the proportion of votes cast for Donald Trump in 2016 (pctTrump_2016) as a measure of a countyâ€™s political leaning. Counties with a higher proportion of votes for Trump in 2016 are considered to have more Republican leanings.\n\n\n\n\n\n\nNote\n\n\n\nAll narrative should be written in complete sentences, and all visualizations should have informative titles and axis labels.\n\n\n\nPart 1: Counties in North Carolina\nFor this part of the analysis, we will focus on counties in North Carolina. We will use the data sets election_nc and county_map_data.\n\nVisualize the distribution of the response variable inperson_pct and calculate appropriate summary statistics. Use the visualization and summary statistics to describe the distribution. Include an informative title and axis labels on the plot.\nLetâ€™s view the data in another way. Use the code below to make a map of North Carolina with the color of each county filled in based on the percentage of votes cast in-person in the 2020 election. Fill in title and axis labels.\nThen use the plot answer the following:\n\nWhat are 2 - 3 observations you have from the plot?\nWhat is a feature that is apparent in the map that wasnâ€™t apparent from the histogram in the previous exercise? What is a feature that is apparent in the histogram that is not apparent in the map?\n\n\n\nelection_map_data &lt;- left_join(election_nc, county_map_data)\n\nggplot() +\n  geom_polygon(data = county_map_data,\n    mapping = aes(x = long, y = lat, group = group),\n    fill = \"lightgray\", color = \"white\"\n    ) +\n  geom_polygon(data = election_map_data, \n    mapping = aes(x = long, y = lat, group = group,\n    fill = inperson_pct)\n    ) +\n  labs(\n    x = \"___\",\n    y = \"___\",\n    fill = \"___\",\n    title = \"___\"\n  ) +\n  scale_fill_viridis_c(labels = label_percent(scale = 1)) +\n  coord_quickmap()\n\n\nCreate a visualization of the relationship between inperson_pct and pctTrump_2016. Use the visualization to describe the relationship between the two variables.\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you havenâ€™t yet done so, now is a good time to render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe can use a linear regression model to better quantify the relationship between the variables.\n\nFit the linear model to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nWrite the regression equation using mathematical notation.\n\nNow letâ€™s use the model coefficients to describe the relationship.\n\nInterpret the slope. The interpretation should be written in a way that is meaningful in the context of the data.\nDoes it make sense to interpret the intercept? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\n\nIf the linear model is a good fit to these data, there should be no structure left in the residuals and the residuals should have constant variance. Augment the data with the model to obtain the residuals and predicted values for each observation, and call the augmented data frame nc_election_aug (You will use this name in Exercise 8). Then, make a plot of the residuals vs.Â the fitted values, and based on this plot, and provide a brief explanation for whether these two conditions are met. Hint: Zoom out on the plot by extending the limits of the y-axis.\n\n\n\n\n\n\n\nWarning\n\n\n\nNow is a good time to render your document again if you havenâ€™t done so recently and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe might also be interested in our observations being independent, particularly if we are to use these data for inference. To evaluate whether the independence condition is met, we will examine a map of the counties in North Carolina with the color filled based on the value of the residuals.\n\nBriefly explain why we may want to view the residuals on a map to assess independence.\nBriefly explain what pattern (if any) we would expect to observe on the map if the independence condition is satisfied.\n\nFill in the name of your model in the code below to calculate the residuals and add them to election_map_data. Then, a map with the color of each county filled in based on the value of the residual. Hint: Start with the code from Exercise 2.\nIs the independence condition satisfied? Briefly explain based on what you observe from the plot.\n\nnc_election_aug &lt;- nc_election_aug %&gt;% \n  bind_cols(fips = election_nc$fips)\n\nelection_map_data &lt;- left_join(election_map_data, nc_election_aug)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore moving on to the next part, make sure you render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\n\nPart 2: Inference for the U.S.\nTo get a better understanding of the trend across the entire United States, we analyze data from a random sample of 200 counties. This data is in the election_sample data frame. Because these counties were randomly selected out of the 3,006 counties in the United States, we can reasonably treat the counties as independent observations.\n\nFit the linear model to these sample data to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nConduct a hypothesis test for the slope using a permutation test. In your response, state the null and alternative hypotheses in words, and state the conclusion in the context of the data.\nNext, construct a 95% confidence interval for the slope using bootstrapping. Interpret the confidence interval in the context of the data.\nComment on whether the hypothesis test and confidence interval support the general consensus that Republican voters were more likely to vote in-person in the 2020 election? A brief explanation is sufficient but it should be based on your conclusions from Exercises 10 and 11.\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-1.html#submission",
    "href": "hw/hw-1.html#submission",
    "title": "HW 1 - In-person voting trends",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember â€“ you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials â¡ï¸ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and youâ€™ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be â€œcheckedâ€).\nSelect the first page of your PDF submission to be associated with the â€œWorkflow & formattingâ€ section."
  },
  {
    "objectID": "hw/hw-1.html#grading",
    "href": "hw/hw-1.html#grading",
    "title": "HW 1 - In-person voting trends",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-1.html#footnotes",
    "href": "hw/hw-1.html#footnotes",
    "title": "HW 1 - In-person voting trends",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe â€œWorkflow & formattingâ€ grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.â†©ï¸"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html",
    "href": "ae/ae-11-volcanoes.html",
    "title": "AE 11: Multinomial classification",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-11-volcanoes-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#packages",
    "href": "ae/ae-11-volcanoes.html#packages",
    "title": "AE 11: Multinomial classification",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#data",
    "href": "ae/ae-11-volcanoes.html#data",
    "title": "AE 11: Multinomial classification",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of on volcanoes. The data come from The Smithsonian Institution via TidyTuesday.\n\nvolcano &lt;- read_csv(here::here(\"ae\", \"data/volcano.csv\"))\n\nRows: 958 Columns: 26\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (18): volcano_name, primary_volcano_type, last_eruption_year, country, r...\ndbl  (8): volcano_number, latitude, longitude, elevation, population_within_...\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst, a bit of data prep:\n\nvolcano &lt;- volcano %&gt;%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  ) %&gt;%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %&gt;%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "href": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "title": "AE 11: Multinomial classification",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nCreate a map of volcanoes that is faceted by volcano_type.\n\n\nworld &lt;- map_data(\"world\")\n\nworld_map &lt;- ggplot() +\n  geom_polygon(\n    data = world, \n    aes(\n      x = long, y = lat, group = group),\n      color = \"white\", fill = \"gray50\", \n      size = 0.05, alpha = 0.2\n    ) +\n  theme_minimal() +\n  coord_quickmap() +\n  labs(x = NULL, y = NULL)\n\nworld_map +\n  geom_point(\n    data = volcano,\n    aes(x = longitude, y = latitude,\n        color = volcano_type, \n        shape = volcano_type),\n    alpha = 0.5\n  ) +\n  facet_wrap(~volcano_type) +\n  scale_color_OkabeIto()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#build-a-new-model",
    "href": "ae/ae-11-volcanoes.html#build-a-new-model",
    "title": "AE 11: Multinomial classification",
    "section": "Build a new model",
    "text": "Build a new model\n\nBuild a new model that uses a recipe that includes geographic information (latitude and longitude). How does this model compare to the original? Note:\nUse the same test/train split as well as same cross validation folds. Code for these is provided below.\n\n\n# test/train split\nset.seed(1234)\n\nvolcano_split &lt;- initial_split(volcano)\nvolcano_train &lt;- training(volcano_split)\nvolcano_test  &lt;- testing(volcano_split)\n\n# cv folds\nset.seed(9876)\n\nvolcano_folds &lt;- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 Ã— 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [574/144]&gt; Fold1\n2 &lt;split [574/144]&gt; Fold2\n3 &lt;split [574/144]&gt; Fold3\n4 &lt;split [575/143]&gt; Fold4\n5 &lt;split [575/143]&gt; Fold5\n\n\nNew recipe, including geographic information:\n\nvolcano_rec2 &lt;- recipe(volcano_type ~ ., data = volcano_train) %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())\n\nOriginal model specification and new workflow:\n\nvolcano_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\")\n\nvolcano_wflow2 &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec2) %&gt;%\n  add_model(volcano_spec)\n\nvolcano_wflow2\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: multinom_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n5 Recipe Steps\n\nâ€¢ step_other()\nâ€¢ step_other()\nâ€¢ step_dummy()\nâ€¢ step_zv()\nâ€¢ step_center()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\n\nFit resamples:\n\nvolcano_fit_rs2 &lt;- volcano_wflow2 %&gt;%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs2\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 Ã— 5\n  splits            id    .metrics         .notes           .predictions      \n  &lt;list&gt;            &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n1 &lt;split [574/144]&gt; Fold1 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 1]&gt; &lt;tibble [144 Ã— 7]&gt;\n2 &lt;split [574/144]&gt; Fold2 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 1]&gt; &lt;tibble [144 Ã— 7]&gt;\n3 &lt;split [574/144]&gt; Fold3 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 1]&gt; &lt;tibble [144 Ã— 7]&gt;\n4 &lt;split [575/143]&gt; Fold4 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 1]&gt; &lt;tibble [143 Ã— 7]&gt;\n5 &lt;split [575/143]&gt; Fold5 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 1]&gt; &lt;tibble [143 Ã— 7]&gt;\n\n\nCollect metrics:\n\ncollect_metrics(volcano_fit_rs2)\n\n# A tibble: 2 Ã— 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.606     5  0.0138 Preprocessor1_Model1\n2 roc_auc  hand_till  0.695     5  0.0245 Preprocessor1_Model1\n\n\nROC curves:\n\nvolcano_fit_rs2 %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#roc-curves",
    "href": "ae/ae-11-volcanoes.html#roc-curves",
    "title": "AE 11: Multinomial classification",
    "section": "ROC curves",
    "text": "ROC curves\n\nRecreate the ROC curve from the slides.\n\n\nfinal_fit &lt;- last_fit(\n  volcano_wflow2, \n  split = volcano_split\n  )\n\ncollect_predictions(final_fit) %&gt;%\n  roc_curve(truth = volcano_type, .pred_Stratovolcano:.pred_Other) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +\n  geom_path(size = 1) +\n  scale_color_OkabeIto() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  theme_minimal() +\n  labs(color = NULL)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#acknowledgement",
    "href": "ae/ae-11-volcanoes.html#acknowledgement",
    "title": "AE 11: Multinomial classification",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://juliasilge.com/blog/multinomial-volcano-eruptions."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html",
    "href": "ae/ae-6-the-office-cv.html",
    "title": "AE 6: The Office",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-6-the-office-cv-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#packages",
    "href": "ae/ae-6-the-office-cv.html#packages",
    "title": "AE 6: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#load-data",
    "href": "ae/ae-6-the-office-cv.html#load-data",
    "title": "AE 6: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_episodes &lt;- read_csv(\"data/office_episodes.csv\")"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "href": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "title": "AE 6: The Office",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#specify-model",
    "href": "ae/ae-6-the-office-cv.html#specify-model",
    "title": "AE 6: The Office",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-recipe",
    "href": "ae/ae-6-the-office-cv.html#create-recipe",
    "title": "AE 6: The Office",
    "section": "Create recipe",
    "text": "Create recipe\nCreate the recipe from class. Call it office_rec1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-workflow",
    "href": "ae/ae-6-the-office-cv.html#create-workflow",
    "title": "AE 6: The Office",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe. Call it office_wflow1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#cross-validation",
    "href": "ae/ae-6-the-office-cv.html#cross-validation",
    "title": "AE 6: The Office",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "href": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "title": "AE 6: The Office",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "href": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "title": "AE 6: The Office",
    "section": "Another model - Model 2",
    "text": "Another model - Model 2\nCreate a different (simpler, involving fewer variables) recipe and call it office_rec2. Conduct 10-fold cross validation and summarize metrics. Describe how the two models compare to each other based on cross validation metrics."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html",
    "href": "ae/ae-1-dcbikeshare.html",
    "title": "AE 02: Bike rentals in DC",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-1-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 02: Bike rentals in DC",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#data",
    "href": "ae/ae-1-dcbikeshare.html#data",
    "title": "AE 02: Bike rentals in DC",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- readr::read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts and temperature",
    "text": "Daily counts and temperature\n\nExercise 1\nVisualize the distribution of daily bike rentals and temperature as well as the relationship between these two variables.\n\nggplot(bikeshare, aes(x = count)) +\n  geom_histogram(binwidth = 250)\n\n\n\n\n\n\n\nggplot(bikeshare, aes(y = count, x = temp_orig)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nExercise 2\nDescribe the distribution of daily bike rentals and the distribution of temperature based on the visualizations created in Exercise 1. Include the shape, center, spread, and presence of any potential outliers.\n[Add your answer here]\n\n\nExercise 3\nThere appears to be one day with a very small number of bike rentals. What was the day? Why were the number of bike rentals so low on that day? Hint: You can Google the date to figure out what was going on that day.\n[Add your answer here]\n\n\nExercise 4\nDescribe the relationship between daily bike rentals and temperature based on the visualization created in Exercise 1. Comment on how we expect the number of bike rentals to change as the temperature increases.\n[Add your answer here]\n\n\nExercise 5\nSuppose you want to fit a model so you can use the temperature to predict the number of bike rentals. Would a model of the form\ncount=Î²0+Î²1temp_orig+Ïµ\\text{count} = \\beta_0 + \\beta_1 ~ \\text{temp_orig} + \\epsilon\nbe the best fit for the data? Why or why not?\nNo."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 6\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 7\nNext, letâ€™s look at how the daily bike rentals differ by season. Letâ€™s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a â€œsmoothed out histogramâ€. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 8\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs.Â temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 9\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#modeling",
    "href": "ae/ae-1-dcbikeshare.html#modeling",
    "title": "AE 02: Bike rentals in DC",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 10\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 11\nUsing the data you filtered in Exercise 10, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here\n\n\n\nExercise 12\nUse the output to write out the estimated regression equation.\n[Add your answer here]\n\n\nExercise 13\nInterpret the slope in the context of the data.\n[Add your answer here]\n\n\nExercise 14\nInterpret the intercept in the context of the data.\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#synthesis",
    "href": "ae/ae-1-dcbikeshare.html#synthesis",
    "title": "AE 02: Bike rentals in DC",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 15\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]\n\nThe following exercises will be completed only if time permits.\n\n\nExercise 16\nPick another season. Based on the visualization in Exercise 8, would you expect the slope of the relationship between temperature and daily bike rentals to be smaller or larger than the slope of the model youâ€™ve been working with so far? Explain your reasoning.\n[Add your answer here]\n\n\nExercise 17\nFor this season you picked in Exercise 16, fit a linear model for predicting daily bike rentals from temperature. Note, you will need to filter your data for this season first. Use the output to write out the estimated regression equation and interpret the slope and the intercept of this model.\n\n# add your code here\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-9-odds.html",
    "href": "ae/ae-9-odds.html",
    "title": "AE 9: Odds",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-9-odds-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-9-odds.html#packages",
    "href": "ae/ae-9-odds.html#packages",
    "title": "AE 9: Odds",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(here::here(\"ae\", \"data/framingham.csv\")) %&gt;%\n  select(totChol, TenYearCHD) %&gt;%\n  drop_na() %&gt;%\n  mutate(high_risk = as.factor(TenYearCHD)) %&gt;%\n  select(totChol, high_risk)"
  },
  {
    "objectID": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "href": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "title": "AE 9: Odds",
    "section": "Linear regression vs.Â logistic regression",
    "text": "Linear regression vs.Â logistic regression\nState whether a linear regression model or logistic regression model is more appropriate for each scenario:\n\nUse age and education to predict if a randomly selected person will vote in the next election.\nUse budget and run time (in minutes) to predict a movieâ€™s total revenue.\nUse age and sex to calculate the probability a randomly selected adult will visit Duke Health in the next year."
  },
  {
    "objectID": "ae/ae-9-odds.html#heart-disease",
    "href": "ae/ae-9-odds.html#heart-disease",
    "title": "AE 9: Odds",
    "section": "Heart disease",
    "text": "Heart disease\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the total cholesterol to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\ntotChol: total cholesterol (mg/dL)\n\n\n\nOutcome: high_risk\n\nggplot(data = heart_disease, aes(x = high_risk)) + \n  geom_bar() + \n  scale_x_discrete(labels = c(\"1\" = \"High risk\", \"0\" = \"Low risk\")) +\n  labs(\n    title = \"Distribution of 10-year risk of heart disease\", \n    x = NULL)\n\n\n\n\n\n\n\n\n\nheart_disease %&gt;%\n  count(high_risk)\n\n# A tibble: 2 Ã— 2\n  high_risk     n\n  &lt;fct&gt;     &lt;int&gt;\n1 0          3555\n2 1           635\n\n\n\n\nCalculating probability and odds\n\nWhat is the probability a randomly selected person in the study is not high risk for heart disease?\nWhat are the odds a randomly selected person in the study is not high risk for heart disease?\n\n\n\nLogistic regression model\nFit a logistic regression model to understand the relationship between total cholesterol and risk for heart disease.\nLet pipi be the probability an adult is high risk. The statistical model is\nlog(Ï€i1âˆ’Ï€i)=Î²0+Î²1TotCholi\\log\\Big(\\frac{\\pi_i}{1-\\pi_i}\\Big) = \\beta_0 + \\beta_1 TotChol_i\n\nheart_disease_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ totChol, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) %&gt;% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.894\n0.230\n-12.607\n0\n\n\ntotChol\n0.005\n0.001\n5.268\n0\n\n\n\n\n\n\nWrite the regression equation. Round to 3 digits.\n\n\n\nCalculating log-odds, odds and probabilities\nBased on the model, if a randomly selected person has a total cholesterol of 250 mg/dL,\n\nWhat are the log-odds they are high risk for heart disease?\nWhat are the odds they are high risk for heart disease?\nWhat is the probability they are high risk for heart disease? Use the odds to calculate your answer.\n\n\n\nComparing observations\nSuppose a personâ€™s cholesterol changes from 250 mg/dL to 200 mg/dL.\n\nHow do you expect the log-odds that this person is high risk for heart disease to change?\nHow do you expect the odds that this person is high risk for heart disease to change?"
  },
  {
    "objectID": "ae/ae-0-movies.html",
    "href": "ae/ae-0-movies.html",
    "title": "Movie budgets and revenues",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is a demo only. You do not have a corresponding repository for it and youâ€™re not expected to turn in anything for it.\nWe will look at the relationship between budget and revenue for movies made in the United States in 1986 to 2020. The dataset is created based on data from the Internet Movie Database (IMDB).\n# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  tidyverse  # for data analysis and visualisation\n  , magrittr # for piping data between operations\n  , scales   # for pretty axis labels\n  , DT       # for interactive table\n)"
  },
  {
    "objectID": "ae/ae-0-movies.html#data",
    "href": "ae/ae-0-movies.html#data",
    "title": "Movie budgets and revenues",
    "section": "Data",
    "text": "Data\nThe movies data set includes basic information about each movie including budget, genre, movie studio, director, etc. A full list of the variables may be found here.\n\nmovies &lt;- \n  readr::read_csv(\n    \"https://raw.githubusercontent.com/danielgrijalva/movie-stats/master/movies.csv\"\n    , show_col_types = FALSE\n    )\n\nView the first 10 rows of data.\n\nmovies\n\n# A tibble: 7,668 Ã— 15\n   name   rating genre  year released score  votes director writer star  country\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n 1 The Sâ€¦ R      Drama  1980 June 13â€¦   8.4 9.27e5 Stanleyâ€¦ Stephâ€¦ Jackâ€¦ Unitedâ€¦\n 2 The Bâ€¦ R      Adveâ€¦  1980 July 2,â€¦   5.8 6.5 e4 Randal â€¦ Henryâ€¦ Brooâ€¦ Unitedâ€¦\n 3 Star â€¦ PG     Actiâ€¦  1980 June 20â€¦   8.7 1.20e6 Irvin Kâ€¦ Leighâ€¦ Markâ€¦ Unitedâ€¦\n 4 Airplâ€¦ PG     Comeâ€¦  1980 July 2,â€¦   7.7 2.21e5 Jim Abrâ€¦ Jim Aâ€¦ Robeâ€¦ Unitedâ€¦\n 5 Caddyâ€¦ R      Comeâ€¦  1980 July 25â€¦   7.3 1.08e5 Harold â€¦ Brianâ€¦ Chevâ€¦ Unitedâ€¦\n 6 Fridaâ€¦ R      Horrâ€¦  1980 May 9, â€¦   6.4 1.23e5 Sean S.â€¦ Victoâ€¦ Betsâ€¦ Unitedâ€¦\n 7 The Bâ€¦ R      Actiâ€¦  1980 June 20â€¦   7.9 1.88e5 John Laâ€¦ Dan Aâ€¦ Johnâ€¦ Unitedâ€¦\n 8 Raginâ€¦ R      Biogâ€¦  1980 Decembeâ€¦   8.2 3.30e5 Martin â€¦ Jake â€¦ Robeâ€¦ Unitedâ€¦\n 9 Superâ€¦ PG     Actiâ€¦  1980 June 19â€¦   6.8 1.01e5 Richardâ€¦ Jerryâ€¦ Geneâ€¦ Unitedâ€¦\n10 The Lâ€¦ R      Biogâ€¦  1980 May 16,â€¦   7   1   e4 Walter â€¦ Bill â€¦ Daviâ€¦ Unitedâ€¦\n# â„¹ 7,658 more rows\n# â„¹ 4 more variables: budget &lt;dbl&gt;, gross &lt;dbl&gt;, company &lt;chr&gt;, runtime &lt;dbl&gt;\n\n\nThe ___ dataset has ___ observations and ___ variables."
  },
  {
    "objectID": "ae/ae-0-movies.html#analysis",
    "href": "ae/ae-0-movies.html#analysis",
    "title": "Movie budgets and revenues",
    "section": "Analysis",
    "text": "Analysis\n\nGross over time\nWe begin by looking at how the average gross revenue (gross) has changed over time. Since we want to visualize the results, we will choose a few genres of interest for the analysis.\n\ngenre_list &lt;- c(\"Comedy\", \"Action\", \"Animation\", \"Horror\")\n\nThen, we will filter for these genres and visualize the average gross revenue over time.\n\nmovies %&gt;%\n  dplyr::filter(genre %in% genre_list) %&gt;% \n  dplyr::group_by(genre,year) %&gt;%\n  dplyr::summarise(avg_gross = mean(gross), .groups = \"keep\") %&gt;%\n  ggplot(mapping = aes(x = year, y = avg_gross, color= genre)) +\n    geom_point() + \n    geom_line() +\n    scale_color_viridis_d() +\n    scale_y_continuous(labels = label_dollar()) +\n    labs(\n      x = \"Year\",\n      y = \"Average Gross Revenue (US Dollars)\",\n      color = \"Genre\",\n      title = \"Gross Revenue Over Time\"\n    )\n\n\n\n\n\n\n\n\nThe plot suggests â€¦\n\n\nBudget and gross\nNext, letâ€™s see the relationship between a movieâ€™s budget and its gross revenue.\n\nmovies %&gt;%\n  dplyr::filter(genre %in% genre_list, budget &gt; 0) %&gt;% \n  ggplot(mapping = aes(x=log(budget), y = log(gross), color=genre)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ genre) + \n  scale_color_viridis_d() +\n  labs(\n    x = \"Log-transformed Budget\",\n    y = \"Log-transformed Gross Revenue\"\n  )"
  },
  {
    "objectID": "ae/ae-0-movies.html#exercises",
    "href": "ae/ae-0-movies.html#exercises",
    "title": "Movie budgets and revenues",
    "section": "Exercises",
    "text": "Exercises\n\nSuppose we fit a regression model for each genre that uses budget to predict gross revenue. What are the signs of the correlation between budget and gross and the slope in each regression equation?\nSuppose we fit the regression model from the previous question. Which genre would you expect to have the smallest residuals, on average (residual = observed revenue - predicted revenue)?\nIn the remaining time, discuss the following: Notice in the graph above that budget and gross are log-transformed. Why are the log-transformed values of the variables displayed rather than the original values (in U.S. dollars)?"
  },
  {
    "objectID": "ae/ae-0-movies.html#appendix",
    "href": "ae/ae-0-movies.html#appendix",
    "title": "Movie budgets and revenues",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of genres in the data set:\n\nmovies %&gt;% \n  distinct(genre) %&gt;%\n  arrange(genre) %&gt;% \n  datatable()"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html",
    "href": "ae/ae-2-dcbikeshare.html",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-2-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#data",
    "href": "ae/ae-2-dcbikeshare.html#data",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- readr::read_csv(\"data/dcbikeshare.csv\")\n\nSee AE 1 for the first part of this analysis."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 1\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 2\nNext, letâ€™s look at how the daily bike rentals differ by season. Letâ€™s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a â€œsmoothed out histogramâ€. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 3\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs.Â temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 4\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#modeling",
    "href": "ae/ae-2-dcbikeshare.html#modeling",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 5\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 6\nUsing the data you filtered in Exercise 5, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#synthesis",
    "href": "ae/ae-2-dcbikeshare.html#synthesis",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 7\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]"
  },
  {
    "objectID": "old_index.html",
    "href": "old_index.html",
    "title": "BSMM-8740: Data Analytic Methods & Algorithms",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\nAE\nLab\nLab Solution\nExam\nProject\n\n\n\n\n1\nMon, Sep 11\nTidyverse & Git\nğŸ“–\nğŸ–¥ï¸\nğŸ“‹\n\n\n\n\n\n\n\n\nLab 1\n\n\n\n\n\n\n\n\n\n2\nMon, Sep 18\nEDA and Feature engineering\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 2\n\n\n\n\n\n\n\n\n\n3\nMon, Sep 25\nThe Recipes package\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 3\n\n\n\n\n\n\n\n\n\n4\nMon, Oct 02\nRegression methods\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 4\n\n\n\n\n\n\n\n\n\n5\nMon, Oct 16\nThe Models Package [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 5\n\n\n\n\n\n\n\n\n\n6\nMon, Oct 23\nClassification methods [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 6\n\n\n\n\n\n\n\n\n\n6\nMon, Oct 30\nTime Series methods [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 7\n\n\n\n\n\n\n\n\n\n6\nMon, Oct 06\nCausality: DAGs [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 8\n\n\n\n\n\n\n\n\n\n6\nMon, Nov 13\nCausality: Effects [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 9\n\n\n\n\n\n\n\n\n\n6\nMon, Nov 20\nMonte Carlo methods [DRAFT STAGE]\n\n\n\n\n\n\n\n\n\n\n\nLab 10\n\n\n\nğŸ’»\n\n\n\n\n\n6\nMon, Nov 27\nBayesian methods [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 11\n\n\n\nğŸ’»\n\n\n\n\n\n6\nMon, Dec 04\nAdvanced Topics [DRAFT STAGE]\nğŸ“–\nğŸ–¥ï¸\n\n\n\n\n\n\n\n\n\nLab 12\n\n\n\nğŸ’»"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "BSMM-8740: Data Analytic Methods & Algorithms",
    "section": "",
    "text": "This course is the exploration of an analytical framework for method selection and model building to help students develop professional capability in data-based techniques of data analytics. A focus will be placed on comparing and selecting appropriate methodology to conduct advanced statistical analysis and on building predictive modeling in order to create a competitive advantage in business operations with efficient analytical methods and data modeling.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Dr.Â Louis (Lou) L. Odette was most recently Head of Advanced Analytics at Greenery Retail Canada. Prior to joining Greenery, Lou was Vice President of Quantitative Modeling & Research at AIG in Boston with responsibilities for review and validation of existing risk models and risk metrics, in addition to modeling and assessment of alternative asset-class risks. He holds a PhD in Electrical Engineering from MIT and an MSc in Mathematics from Oxford University. Spoiler alert: the picture to the right was taken many years ago.\n\n\n\n\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesdays 2:30pm - 3:30 pm\n(on Teams by request)\nMeeting ID: 230 892 400 109\nPasscode: eXFjwj\n\n\nOr call in (audio only)\n+1 226-782-3511,,488681491# Windsor\n(866) 603-5721,,488681491# (Toll-free)\nPhone Conference ID: 488 681 491#\n\n\n\nIf this time doesnâ€™t work for you or youâ€™d like to schedule a one-on-one meeting, you can email me at lodette@uwindsor.ca.",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Dr.Â Louis (Lou) L. Odette was most recently Head of Advanced Analytics at Greenery Retail Canada. Prior to joining Greenery, Lou was Vice President of Quantitative Modeling & Research at AIG in Boston with responsibilities for review and validation of existing risk models and risk metrics, in addition to modeling and assessment of alternative asset-class risks. He holds a PhD in Electrical Engineering from MIT and an MSc in Mathematics from Oxford University. Spoiler alert: the picture to the right was taken many years ago.\n\n\n\n\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesdays 2:30pm - 3:30 pm\n(on Teams by request)\nMeeting ID: 230 892 400 109\nPasscode: eXFjwj\n\n\nOr call in (audio only)\n+1 226-782-3511,,488681491# Windsor\n(866) 603-5721,,488681491# (Toll-free)\nPhone Conference ID: 488 681 491#\n\n\n\nIf this time doesnâ€™t work for you or youâ€™d like to schedule a one-on-one meeting, you can email me at lodette@uwindsor.ca.",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-team.html#teaching-assistants",
    "href": "course-team.html#teaching-assistants",
    "title": "Teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\n\n\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\n\nBinny Kaur\nTBD: starting Sept 18, 2024\nTBD",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_05.html#recap-of-last-week",
    "title": "Classification & clustering methods",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast time we introduced the Tidymodels framework in R\nWe showed how we can use the Tidymodels framework to create a workflow for data prep, feature engineering, model fitting and model evaluation.\nToday we look at the using the Tidymodels package to build classification and clustering models."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#classification",
    "href": "slides/BSMM_8740_lec_05.html#classification",
    "title": "Classification & clustering methods",
    "section": "Classification",
    "text": "Classification\n\nClassification is a supervised machine learning method where the model tries to predict a categorical outcome for given input data.\nThese models are essential in various business applications, such as credit scoring, customer segmentation, fraud detection, and more. Classification methods can broadly be categorized into two types: eager learners and lazy (instance-based) learners."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#eager-learners",
    "href": "slides/BSMM_8740_lec_05.html#eager-learners",
    "title": "Classification & clustering methods",
    "section": "Eager Learners",
    "text": "Eager Learners\nEager learners are machine learning algorithms that first build a model from the training dataset before making any prediction on future datasets. They spend more time on the training process to better generalize from the data.\nThey usually require less time to make predictions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#eager-learners-1",
    "href": "slides/BSMM_8740_lec_05.html#eager-learners-1",
    "title": "Classification & clustering methods",
    "section": "Eager Learners",
    "text": "Eager Learners\nExample eager learners are:\n\nLRDTRFSVMANNGMB\n\n\n\nLogistic Regression:\n\nOverview: A statistical method for binary classification that models the probability of a binary outcome based on one or more predictor variables.\nAdvantages: Simple, interpretable, and works well with linear decision boundaries.\nDisadvantages: Assumes linearity between predictors and the log-odds of the outcome.\n\n\n\n\n\nDecision Trees:\n\nOverview: A model that splits the data into subsets based on feature values, resulting in a tree structure where each leaf node represents a class label.\nAdvantages: Easy to interpret and visualize, handles both numerical and categorical data.\nDisadvantages: Prone to overfitting, especially with deep trees.\n\n\n\n\n\nRandom Forests:\n\nOverview: An ensemble method that builds multiple decision trees and aggregates their predictions to improve accuracy and reduce overfitting.\nAdvantages: Robust to overfitting, handles large datasets well.\nDisadvantages: Less interpretable than single decision trees.\n\n\n\n\n\nSupport Vector Machines (SVM):\n\nOverview: A model that finds the hyperplane that best separates the classes in the feature space.\nAdvantages: Effective in high-dimensional spaces, works well with a clear margin of separation.\nDisadvantages: Computationally intensive, less effective with noisy data or overlapping classes.\n\n\n\n\n\nNeural Networks:\n\nOverview: Models inspired by the human brain, consisting of layers of interconnected neurons that learn to map inputs to outputs.\nAdvantages: Capable of capturing complex patterns and relationships.\nDisadvantages: Requires large amounts of data and computational power, less interpretable.\n\n\n\n\n\nGradient Boosting Machines (GBM):\n\nOverview: An ensemble technique that builds trees sequentially, where each new tree corrects errors made by the previous ones.\nAdvantages: High accuracy, effective at handling various data types.\nDisadvantages: Computationally intensive, requires careful tuning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#lazy-learners",
    "href": "slides/BSMM_8740_lec_05.html#lazy-learners",
    "title": "Classification & clustering methods",
    "section": "Lazy Learners",
    "text": "Lazy Learners\nLazy learners or instance-based learners, do not create any model immediately from the training data, and this where the lazy aspect comes from. They just memorize the training data, and each time there is a need to make a prediction, they predict based on similarity between the query instance and stored instances."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#lazy-learners-1",
    "href": "slides/BSMM_8740_lec_05.html#lazy-learners-1",
    "title": "Classification & clustering methods",
    "section": "Lazy Learners",
    "text": "Lazy Learners\nExample lazy learners are:\n\nKNNCase-based\n\n\n\nk-Nearest Neighbors (k-NN):\n\nOverview: Predicts the class of a query instance based on the majority class among its k nearest neighbors in the training data.\nAdvantages: Simple, intuitive, and effective for small datasets.\nDisadvantages: Computationally intensive during prediction, performance degrades with high-dimensional data.\n\n\n\n\n\nCase-based Learning:\n\nOverview: Makes predictions by fitting simple models (like linear regression) to localized regions of the data.\nAdvantages: Flexible, can adapt to complex patterns locally.\nDisadvantages: Computationally expensive, sensitive to the choice of kernel and bandwidth parameters."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#comparing-eager-and-lazy-learners",
    "href": "slides/BSMM_8740_lec_05.html#comparing-eager-and-lazy-learners",
    "title": "Classification & clustering methods",
    "section": "Comparing Eager and Lazy Learners",
    "text": "Comparing Eager and Lazy Learners\n\n\nTraining vs.Â Prediction Time:\n\nEager learners invest time in building the model during training, resulting in faster predictions.\nLazy learners have negligible training time but are computationally intensive during prediction.\n\nModel Interpretability:\n\nEager learners like decision trees and logistic regression are generally more interpretable.\nLazy learners like k-NN are less interpretable as they rely on instance-based comparisons."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#comparing-eager-and-lazy-learners-1",
    "href": "slides/BSMM_8740_lec_05.html#comparing-eager-and-lazy-learners-1",
    "title": "Classification & clustering methods",
    "section": "Comparing Eager and Lazy Learners",
    "text": "Comparing Eager and Lazy Learners\n\n\nHandling of High-Dimensional Data:\n\nEager learners like SVMs and neural networks can handle high-dimensional data effectively.\nLazy learners like k-NN can struggle with the curse of dimensionality.\n\nFlexibility and Complexity:\n\nEager learners can capture complex relationships and interactions through models like neural networks and gradient boosting.\nLazy learners are simpler but can be flexible in capturing local patterns."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#types-of-classification",
    "href": "slides/BSMM_8740_lec_05.html#types-of-classification",
    "title": "Classification & clustering methods",
    "section": "Types of classification",
    "text": "Types of classification\n\nBinary classification\nMulti-Class Classification (mutually exclusive)\n\nmulticlass\n\nMulti-Label Classification (not mutually exclusive)\n\nmultilabel\n\nImbalanced Classification\n\nclass imbalance"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-logistic-regression",
    "href": "slides/BSMM_8740_lec_05.html#binary-logistic-regression",
    "title": "Classification & clustering methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\nLogistic regression is a Generalized Linear Model where the dependent (categorical) variable \\(y\\) is binary, i.e.Â takes values in \\(\\{0,1\\}\\) (e.g., yes/no, success/failure).\nThis can be interpreted as identifying two classes, and logistic regression provides a prediction for class membership based on a linear combination of the explanatory variables.\nLogistic regression is an example of supervised learning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-1",
    "href": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-1",
    "title": "Classification & clustering methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\nFor the logistic GLM:\n\nthe distribution of the observations is Binomial with parameter \\(\\pi\\equiv\\mathbb{P}(\\left.Y=1\\right|\\eta)\\)\nthe explanatory variables are linear in the parameters: \\(\\eta=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_2 x_2\\ldots+\\beta_n x_n\\)\nthe link function is the logit: \\(\\eta=\\text{logit}(\\pi) = \\log(\\frac{\\pi}{1-\\pi})\\)\n\nIt follows that \\(\\pi = \\frac{e^\\eta}{1+e^\\eta} = \\frac{1}{1+e^{-\\eta}}\\), which is a sigmoid function in the explanatory variables. The equation \\(\\eta=0\\) defines a linear decision boundary or classification threshold."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-2",
    "href": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-2",
    "title": "Classification & clustering methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\n\n\n\n&gt; tibble::tibble(eta = seq(-5,5,0.2)) %&gt;% dplyr::mutate(pi = 1/(1+exp(-eta))) %&gt;% \n+ ggplot(aes(x=eta, y=pi)) + geom_point() + theme_bw(base_size = 38)\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\n\\pi & =\\frac{1}{1+e^{-\\eta}}\\;\\text{(logistic function)}\\\\\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) & =\\log\\left(\\frac{\\frac{1}{1+e^{-\\eta}}}{1-\\frac{1}{1+e^{-\\eta}}}\\right)\\\\\n& =\\log\\left(\\frac{\\frac{1}{1+e^{-\\eta}}}{\\frac{e^{-\\eta}}{1+e^{-\\eta}}}\\right)=\\log\\left(e^{\\eta}\\right)=\\eta\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-3",
    "href": "slides/BSMM_8740_lec_05.html#binary-logistic-regression-3",
    "title": "Classification & clustering methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\nThe term \\(\\frac{\\pi}{1-\\pi}\\) is called the the odds-ratio. By its definition:\n\\[\n\\frac{\\pi}{1-\\pi}=e^{\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_2 x_2\\ldots+\\beta_n x_n}\n\\]\nSo if \\(x_1\\) changes by one unit (\\(x_1\\rightarrow x_1+1\\)), then the odds ratio changes by \\(e^{\\beta_1}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nConfusion matrix\nThe confusion matrix is a 2x2 table summarizing the number of correct predictions of the model (a function of the decision boundary): It is the foundation for understanding other evaluation metrics.\n\n\n\n\npredict 1\npredict 0\n\n\n\n\ndata = 1\ntrue positives (TP)\nfalse negatives (FN)1\n\n\ndata = 0\nfalse positives (FP)2\ntrue negatives (TN)\n\n\n\nType II errorType I error"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-1",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-1",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nAccuracy - the simplest metric:\nAccuracy measures the percent of correct predictions:\n\\[\n\\begin{align*}\n\\frac{\\text{TP}+\\text{TN}}{\\text{observation count}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-2",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-2",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nAccuracy:\n\nUsefulnessLimitations\n\n\n\nAccuracy is a useful metric in evaluating classification models under certain conditions:\n\nBalanced Datasets: When the classes in the dataset are roughly equal in number, accuracy can be a reliable indicator of model performance. For example, if you have a dataset where 50% of the samples are class A and 50% are class B, accuracy is a good measure of whether the model correctly predicts the classes.\nGeneral Performance: For an overall sense of a modelâ€™s performance, accuracy provides a straightforward, easy-to-understand measure, giving the proportion of correct predictions out of all predictions made.\n\n\n\n\n\nAccuracy has several limitations, especially in the context of imbalanced datasets:\n\n\nImbalanceImportanceBehaviour\n\n\n\nImbalanced Datasets:\n\nFalse Sense of Performance: When one class significantly outnumbers the other, accuracy can be misleading: if 95% of the samples belong to class A and only 5% to class B, always predicting class A will have a high accuracy (95%) but may fail to correctly identify any instances of class B.\n\n\n\n\n\nIgnoring Class Importance:\n\nPrecision and Recall: Accuracy does not take into account the importance of different classes or the costs of different types of errors (false positives and false negatives). In many business contexts, these costs are not equal, e.g., in medical diagnosis, a false -ve might be much more serious than a false +ve.\n\n\n\n\n\nLack of Insight into Model Behavior:\n\nDetailed Performance: Accuracy give no insight into the types of errors the model is making. Precision and recall, on the other hand, offer more detailed information about the performance on individual classes - crucial for understanding and improving the model."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-3",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-3",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nAccuracy: examples of limitation\n\nFraud Detection:\n\nScenario: Suppose you have a dataset with 1,000 transactions, of which 990 are legitimate and 10 are fraudulent. A model that always predicts â€œlegitimateâ€ will have an accuracy of 99% but fails to identify fraudulent transactions, making it useless for fraud detection purposes.\n\nSpam Detection:\n\nScenario: Consider an email classification problem where 95% of emails are legitimate and 5% are spam. A model that always predicts â€œlegitimateâ€ will have high accuracy but will not catch any spam emails, which is the primary goal."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-4",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-4",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nPrecision\nPrecision measures the percent of positive predictions that are correct (true positives / all positives predicted):\n\\[\n\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-5",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-5",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nRecall / Sensitivity\n\nMeasures the success at predicting the first class (true positives predicted / actual positives):\n\n\\[\n\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\\qquad\\text{(True Positive Rate - TPR)}\n\\]\nRecall / Specificity\n\nMeasures the success at predicting the second class (true negatives predicted / actual negative):\n\n\\[\n\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}\\qquad\\text{(True Negative Rate - TNR)}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-6",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-6",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nReceiver Operating Characteristic (ROC) curve & the Area Under the Curve (AUC)\n\n\nROC Curve: Plot of the true positive rate (Recall) against the false positive rate (1 - Specificity) at various threshold settings.\nAUC: The area under the ROC curve, representing the probability that the model ranks a randomly chosen positive instance higher than a randomly chosen negative instance.\n\nAUC values range from 0.5 (no discrimination) to 1 (perfect discrimination)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-7",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-metrics-7",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier metrics",
    "text": "Binary Classifier metrics\nROC Curves\nConsider plotting the TPR against the FPR (1-TNR) at different classification thresholds.\n\nthe diagonal (TPR = 1-TNR) describes a process equivalent to tossing a fair coin (i.e.Â no predictive power)\nour method should have a curve above the diagonal; which shape is better depends on the purpose of our classifier.\n\nSo, how to compute?"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#example-create-the-workflow",
    "href": "slides/BSMM_8740_lec_05.html#example-create-the-workflow",
    "title": "Classification & clustering methods",
    "section": "Example: create the workflow",
    "text": "Example: create the workflow\nWorkflow to model credit card default\n\n\nCode\n&gt; data &lt;- ISLR::Default %&gt;% tibble::as_tibble()\n&gt; set.seed(8740)\n&gt; \n&gt; # split data\n&gt; data_split &lt;- rsample::initial_split(data)\n&gt; default_train &lt;- rsample::training(data_split)\n&gt; \n&gt; # create a recipe\n&gt; default_recipe &lt;- default_train %&gt;% \n+   recipes::recipe(formula = default ~ student + balance + income) %&gt;% \n+   recipes::step_dummy(recipes::all_nominal_predictors())\n&gt; \n&gt; # create a linear regression model\n&gt; default_model &lt;- parsnip::logistic_reg() %&gt;% \n+   parsnip::set_engine(\"glm\") %&gt;% \n+   parsnip::set_mode(\"classification\")\n&gt; \n&gt; # create a workflow\n&gt; default_workflow &lt;- workflows::workflow() %&gt;%\n+   workflows::add_recipe(default_recipe) %&gt;%\n+   workflows::add_model(default_model)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#example-fit-the-model-using-the-data",
    "href": "slides/BSMM_8740_lec_05.html#example-fit-the-model-using-the-data",
    "title": "Classification & clustering methods",
    "section": "Example: fit the model using the data",
    "text": "Example: fit the model using the data\n\n&gt; # fit the model\n&gt; lm_fit &lt;- \n+   default_workflow %&gt;% \n+   parsnip::fit(default_train)\n&gt; \n&gt; # augment the data with the predictions using the model fit\n&gt; training_results &lt;- \n+   broom::augment(lm_fit , default_train) \n&gt; \n&gt; training_results %&gt;% dplyr::slice_head(n=6)\n\n# A tibble: 6 Ã— 7\n  .pred_class .pred_No .pred_Yes default student balance income\n  &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 No             0.998  0.00164  No      No         759. 45774.\n2 No             1.00   0.000145 No      Yes        452. 19923.\n3 No             0.770  0.230    Yes     No        1666. 30070.\n4 No             1.00   0.000256 No      No         434. 57146.\n5 No             1.00   0.000449 No      No         536. 32994.\n6 No             0.973  0.0269   No      No        1252. 32721."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#example-compute-the-auc",
    "href": "slides/BSMM_8740_lec_05.html#example-compute-the-auc",
    "title": "Classification & clustering methods",
    "section": "Example: compute the AUC",
    "text": "Example: compute the AUC\n\n\n\n&gt; training_results %&gt;% \n+   yardstick::roc_curve(\n+     truth = default\n+     , .pred_No\n+   ) %&gt;% \n+   autoplot() + \n+   theme_bw(base_size = 38) \n\n\n\n\n\n\n\n\n\n\n&gt; training_results %&gt;% \n+    yardstick::roc_auc(\n+      truth = default\n+      , .pred_No\n+     )\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.949"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier",
    "text": "Binary Classifier\nClassification Threshold\nRecall:\n\nIn binary classification, the model predicts the probability of an instance belonging to the positive class. The classification threshold is the probability value above which an instance is classified as positive.\nCommonly, this threshold is set at 0.5, meaning any instance with a predicted probability above 0.5 is classified as positive, and below 0.5 as negative."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-1",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-1",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier",
    "text": "Binary Classifier\nClassification Threshold Impact\n\nTrue Positives (TP) and False Positives (FP):\n\nLower Threshold: A lower threshold increases the number of instances classified as positive, which increases both true positives and false positives.\nHigher Threshold: A higher threshold decreases the number of instances classified as positive, which decreases both true positives and false positives.\n\nTrue Negatives (TN) and False Negatives (FN):\n\nLower Threshold: A lower threshold decreases the number of instances classified as negative, which decreases both true negatives and increases false negatives.\nHigher Threshold: A higher threshold increases the number of instances classified as negative, which increases true negatives and decreases false negatives.\n\nTrade-offs:\n\nAdjusting the threshold affects the trade-off between sensitivity (recall) and specificity. A lower threshold improves sensitivity but reduces specificity, and vice versa."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-2",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-2",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier",
    "text": "Binary Classifier\nROC Curve and AUC\n\n\nROC Curve:\n\nThe ROC curve plots the true positive rate (TPR, or recall) against the false positive rate (FPR) at various threshold settings.\nEach point on the ROC curve represents a TPR/FPR pair corresponding to a specific threshold.\n\nAUC (Area Under the Curve):\n\nThe AUC represents the modelâ€™s ability to discriminate between positive and negative classes across all threshold values.\nA higher AUC indicates better overall performance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-optimal-threshold",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-optimal-threshold",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier: optimal threshold",
    "text": "Binary Classifier: optimal threshold\n\n\nBusiness Context:\n\nThe optimal threshold depends on the specific costs of false positives and false negatives in the business context. For example, in fraud prediction, the cost of missing a positive case (false negative) might be much higher than a false alarm (false positive).\n\nMaximize Specific Metrics:\n\nYou can choose a threshold that maximizes a specific metric such as F1 score, which balances precision and recall.\nAlternatively, you might want to maximize precision, recall, or minimize a cost function that accounts for both false positives and false negatives.\n\nYoudenâ€™s Index:\n\nOne method to select an optimal threshold is to maximize Youdenâ€™s Index (\\(J\\)), which is defined as: \\(J=\\mathrm{Sensitivity}+\\mathrm{Specificity}-1\\)\nThis index helps to find a threshold that maximizes the difference between true positive rate and false positive rate.\n\nCost-Benefit Analysis:\n\nPerform a cost-benefit analysis by assigning costs to false positives and false negatives and choosing the threshold that minimizes the total expected cost."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#binary-classifier-threshold-examples",
    "href": "slides/BSMM_8740_lec_05.html#binary-classifier-threshold-examples",
    "title": "Classification & clustering methods",
    "section": "Binary Classifier: threshold examples",
    "text": "Binary Classifier: threshold examples\n\nExample Scenario\nFraud Detection: - In fraud detection, missing a fraudulent transaction (false negative) might be more costly than flagging a legitimate transaction as fraud (false positive). - You might choose a lower threshold to ensure higher sensitivity (recall), even if it means a higher false positive rate, thereby catching more fraudulent transactions.\nConclusion\nThe choice of classification threshold in computing the ROC curve is crucial for balancing the trade-offs between sensitivity and specificity, and ultimately for optimizing the modelâ€™s performance in a way that aligns with business goals and context. Understanding and carefully selecting the appropriate threshold ensures that the modelâ€™s predictions are most useful and cost-effective for the specific application."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nBayes Rule - using the rules of conditional probability:\n\\[\n\\mathbb{P}(A,B) = \\mathbb{P}(A|B)\\mathbb{P}(B) = \\mathbb{P}(B|A)\\mathbb{P}(A)\n\\] We can write:\n\\[\n\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A|B)\\mathbb{P}(B)}{\\mathbb{P}(A)}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-1",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-1",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nThis method starts with Bayes rule:\nfor \\(K\\) classes and an observation \\(x\\) consisting of \\(N\\) features \\(\\{x_1,\\ldots,x_N\\}\\), since \\(\\mathbb{P}\\left[\\left.C_{k}\\right|x_{1},\\ldots,x_{N}\\right]\\times\\mathbb{P}\\left[x_{1},\\ldots,x_{N}\\right]\\) is equal to \\(\\mathbb{P}\\left[\\left.x_{1},\\ldots,x_{N}\\right|C_{k}\\right]\\times\\mathbb{P}\\left[C_{k}\\right]\\), we can write\n\\[\n\\mathbb{P}\\left[\\left.C_{k}\\right|x_{1},\\ldots,x_{N}\\right]=\\frac{\\mathbb{P}\\left[\\left.x_{1},\\ldots,x_{N}\\right|C_{k}\\right]\\times\\mathbb{P}\\left[C_{k}\\right]}{\\mathbb{P}\\left[x_{1},\\ldots,x_{N}\\right]}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-2",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-2",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nIf we assume that the features are all independent we can write Bayes rule as\n\\[\n\\mathbb{P}\\left[\\left.C_{k}\\right|x_{1},\\ldots,x_{N}\\right]=\\frac{\\mathbb{P}\\left[C_{k}\\right]\\times\\prod_{n=1}^{N}\\mathbb{P}\\left[\\left.x_{n}\\right|C_{k}\\right]}{\\prod_{n=1}^{N}\\mathbb{P}\\left[x_{n}\\right]}\n\\]\nand since the denominator is independent of \\(C_{k}\\), our classifier is\n\\[\nC_{k}=\\arg\\max_{C_{k}}\\mathbb{P}\\left[C_{k}\\right]\\prod_{n=1}^{N}\\mathbb{P}\\left[\\left.x_{n}\\right|C_{k}\\right]\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-3",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-3",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nSo it remains to calculate the class probability \\(\\mathbb{P}\\left[C_{k}\\right]\\) and the conditional probabilities \\(\\mathbb{P}\\left[\\left.x_{n}\\right|C_{k}\\right]\\)\nThe different naive Bayes classifiers differ mainly by the assumptions they make regarding the conditional probabilities."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-4",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-4",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nIf our features are all ordinal, then\n\nThe class probabilities1 are simply the frequency of observations that belong to each class divided by the total number of observations.\nThe conditional probabilities are the frequency of each feature value for a given class value divided by the frequency of measurements with that class value.\n\ni.e.Â empirical probabilities"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-5",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-5",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nIf any features are numeric, we can estimate conditional probabilities by assuming that the numeric features have a Gaussian distribution for each class"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-6",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-6",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\n\nCode\n&gt; library(discrim)\n&gt; # create a naive bayes classifier\n&gt; default_model_nb &lt;- parsnip::naive_Bayes() %&gt;% \n+   parsnip::set_engine(\"klaR\") %&gt;% \n+   parsnip::set_mode(\"classification\")\n&gt; \n&gt; # create a workflow\n&gt; default_workflow_nb &lt;- workflows::workflow() %&gt;%\n+   workflows::add_recipe(default_recipe) %&gt;%\n+   workflows::add_model(default_model_nb)\n&gt; \n&gt; # fit the model\n&gt; lm_fit_nb &lt;- \n+   default_workflow_nb %&gt;% \n+   parsnip::fit(\n+     default_train\n+   , control = \n+     workflows::control_workflow(parsnip::control_parsnip(verbosity = 1L))\n+   )\n&gt; \n&gt; # augment the data with the predictions using the model fit\n&gt; training_results_nb &lt;- \n+   broom::augment(lm_fit_nb , default_train)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-7",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-7",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\nAUCROC\n\n\n\n&gt; training_results_nb %&gt;% \n+   yardstick::roc_auc(.pred_No, truth = default)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.942"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-uses",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-uses",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification: uses",
    "text": "Naive Bayes Classification: uses\n\n\nText Classification:\n\nSentiment Analysis: It can classify text data into categories such as positive, negative, or neutral sentiment.\n\nDocument Categorization:\n\nUseful in classifying news articles, blog posts, or any document into predefined categories based on content.\n\nMedical Diagnosis:\n\nCan be used to predict the likelihood of a disease based on patient symptoms and medical history, assuming independence between symptoms.\n\nRecommender Systems:\n\nHelps in predicting user preferences and recommending items such as movies, books, or products based on previous user behavior.\n\nReal-time Prediction:\n\nDue to its simplicity and speed, Naive Bayes is suitable for real-time prediction tasks where quick decisions are essential."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-abuses",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-classification-abuses",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Classification: abuses",
    "text": "Naive Bayes Classification: abuses\n\n\nAssumption of Feature Independence:\n\nMisuse: The model assumes that all features are independent given the class.\nImpact: Poor performance if features are highly correlated.\n\nImbalanced Data:\n\nMisuse: Naive Bayes can struggle with imbalanced class datasets.\nImpact: Bias towards the majority class, leads to high accuracy but poor minority recall.\n\nZero Probability Problem:\n\nMisuse: If a feature \\(x_j\\) missing for class \\(k\\), then \\(\\mathbb{P}\\left[\\left.x_{j}\\right|C_{k}\\right]=0\\), which can skew results.\nSolution: Use techniques like Laplace Smoothing to handle zero probabilities.\n\nOverfitting on Small Datasets:\n\nMisuse: Naive Bayes may overfit if trained on a small dataset with noise or outliers.\nImpact: This can result in poor generalization to new data.\n\nIgnoring Feature Scaling:\n\nMisuse: The model does not inherently handle features with different scales or units.\nImpact: Features with larger scales can disproportionately influence the model"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#naive-bayes-best-practices",
    "href": "slides/BSMM_8740_lec_05.html#naive-bayes-best-practices",
    "title": "Classification & clustering methods",
    "section": "Naive Bayes Best Practices",
    "text": "Naive Bayes Best Practices\n\n\nFeature Engineering:\n\nProper feature selection and engineering can help mitigate some of the independence assumption issues. For instance, combining related features can improve performance.\n\nHandling Correlated Features:\n\nWhile Naive Bayes assumes independence, it can still perform well with moderately correlated features. In cases of strong correlation, consider using other models.\n\nEvaluation Metrics:\n\nUse appropriate metrics such as precision, recall, and ROC-AUC, especially for imbalanced datasets, to get a comprehensive understanding of model performance.\n\nCross-Validation:\n\nEmploy cross-validation techniques to ensure that the model generalizes well to unseen data and to avoid overfitting.\n\nComparative Analysis:\n\nCompare Naive Bayes with other classifiers (e.g., logistic regression, SVM, random forest) to ensure that it is the best choice for your specific problem."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification",
    "href": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification",
    "title": "Classification & clustering methods",
    "section": "Nearest Neighbour Classification",
    "text": "Nearest Neighbour Classification\nThe k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.\nIt is typically used as a classification algorithm, working off the assumption that similar class predictions can be made by predictors near one another."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification-1",
    "href": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification-1",
    "title": "Classification & clustering methods",
    "section": "Nearest Neighbour Classification",
    "text": "Nearest Neighbour Classification\nFor classification problems, a class label is assigned on the basis of a majority voteâ€”i.e.Â the label that is most frequently represented around a given data point is used.\nBefore a classification can be made, the distance between points must be defined. Euclidean distance is most commonly used."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification-2",
    "href": "slides/BSMM_8740_lec_05.html#nearest-neighbour-classification-2",
    "title": "Classification & clustering methods",
    "section": "Nearest Neighbour Classification",
    "text": "Nearest Neighbour Classification\nNote that the KNN algorithm is also part of a family of â€œlazy learningâ€ models, meaning that it only stores a training dataset versus undergoing a training stage. This also means that all the computation occurs when a classification or prediction is being made.\nThe k value in the k-NN algorithm determines how many neighbors will be checked to determine the classification of a specific query point."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-distance-measures",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-distance-measures",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: distance measures",
    "text": "KNN Classification: distance measures\n\nEuclidean: \\(\\text{d}(x,y)=\\sqrt{\\sum_i(y_i- x_i)^2}\\)\nManhattan: \\(\\text{d}(x,y)=\\sum_{i}\\left|y_{i}-x_{i}\\right|\\)\nMinkowski: \\(\\text{d}(x,y;p)=\\left(\\sum_{i}\\left|y_{i}-x_{i}\\right|\\right)^{1/p}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-algorithm",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-algorithm",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: algorithm",
    "text": "KNN Classification: algorithm\n\nChoose the value of K, which is the number of nearest neighbors that will be used to make the prediction.\nCalculate the distance between the observation you want to classify and all the observations in the training set.\nSelect the K nearest neighbors based on the distances calculated.\nAssign the label of the majority class to the new data point.\nRepeat steps 2 to 4 for all the data points in the test set."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-model",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-model",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: model",
    "text": "KNN Classification: model\nOur classification workflow only differs by the model, e.g.:\n\n&gt; default_model_knn &lt;- parsnip::nearest_neighbor(neighbors = 4) %&gt;% \n+   parsnip::set_engine(\"kknn\") %&gt;% \n+   parsnip::set_mode(\"classification\")\n\n\n\n\n\n\n\nk-NN regression\n\n\nTo use k-NN for a regression problem, calculate the mean or median (or another aggregate measure) of the dependent variable among the k neighbors."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-metrics",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-metrics",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: metrics",
    "text": "KNN Classification: metrics\nIn the context of k-Nearest Neighbors (kNN) classification, while the general evaluation metrics like accuracy, precision, recall, F1 score, and others are commonly used, there are no unique metrics that are exclusively specific to kNN. However, there are certain considerations and additional analyses that are particularly relevant when evaluating a kNN model."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-metrics-1",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-metrics-1",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: metrics",
    "text": "KNN Classification: metrics\n\n\nChoice of k (Number of Neighbors): The value of k affects the performance of a kNN model. Testing the model with various values of k and evaluating the performance using standard metrics (like accuracy, F1 score) can to select the best k.\nFeature Scaling Sensitivity: kNN is sensitive to the scale of the features because it relies on calculating distances. Evaluate the modelâ€™s performance before and after feature scaling (like Min-Max scaling or Z-score normalization)\nCurse of Dimensionality: kNN can perform poorly with high-dimensional data (many features). Evaluating the modelâ€™s performance in relation to the number of features (dimensionality) can be important. Dimensionality reduction techniques like PCA might be used with kNN."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-uses",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-uses",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: uses",
    "text": "KNN Classification: uses\n\n\nPattern Recognition:\n\nHandwriting Recognition: kNN can classify handwritten digits or letters using their features.\nImage Classification: Categorize by comparing pixels/features with known images.\n\nRecommender Systems:\n\nContent-Based Filtering: kNN can recommend items using similarities between user preferences and the attributes of items.\nCollaborative Filtering: Can also recommend items based on preferences of similar users.\n\nMedical Diagnosis:\n\nDisease Prediction: kNN can predict likelihood of diseases by comparing patient data to historical patient data with known diagnoses.\n\nAnomaly Detection:\n\nFraud Detection: kNN can identify unusual transactions by comparing to known legitimate and fraudulent transactions.\n\nCustomer Segmentation:\n\nMarket Analysis: It can segment customers based on purchasing behavior, demographics, or other attributes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-abuses",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-abuses",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: abuses",
    "text": "KNN Classification: abuses\n\n\nHigh Dimensionality:\n\nMisuse: kNN can struggle with high-dimensional data as distances become less meaningful (curse of dimensionality).\nImpact: Performance degrades as irrelevant or noisy features overshadow important ones.\nSolution: Use dimensionality reduction (e.g., PCA, t-SNE) or feature selection before using kNN.\n\nLarge Datasets:\n\nMisuse: kNN stores all training data and calculates all distances during prediction, & can be computationally expensive.\nImpact: It becomes slow and impractical for large datasets.\nSolution: Approximate kNN techniques or other algorithms suited for large datasets.\n\nImbalanced Datasets:\n\nMisuse: kNN can be biased towards the majority class in imbalanced datasets because the majority class neighbors dominate.\nImpact: Low recall for minority class, & to poor performance in e.g.Â fraud detection or rare disease diagnosis.\nSolution: Use resampling, synthetic data generation (SMOTE), or adjusting the decision rule to account for class imbalance."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-abuses-1",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-abuses-1",
    "title": "Classification & clustering methods",
    "section": "KNN Classification: abuses",
    "text": "KNN Classification: abuses\n\n\nChoice of k and Distance Metric:\n\nMisuse: An inappropriate choice of ( k ) (too small or too large) or an unsuitable distance metric can lead to poor classification performance.\nImpact: A small ( k ) can make the model sensitive to noise (overfitting), while a large ( k ) can oversmooth the decision boundary (underfitting).\nSolution: Use cross-validation to choose the optimal ( k ) and experiment with different distance metrics (e.g., Euclidean, Manhattan).\n\nScalability:\n\nMisuse: without optimization, kNN does not scale well to datasets with a large number of features or samples.\nImpact: Slow predictions and high memory usage.\nSolution: Use k-d trees, ball trees, or locality-sensitive hashing to speed up nearest neighbor searches."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#knn-classification-best-practices",
    "href": "slides/BSMM_8740_lec_05.html#knn-classification-best-practices",
    "title": "Classification & clustering methods",
    "section": "KNN Classification Best Practices",
    "text": "KNN Classification Best Practices\n\n\nNormalization and Scaling:\n\nImportance: Features should be on a similar scale for kNN to perform well, as distance calculations are sensitive to feature scales.\nPractice: Apply normalization (e.g., Min-Max scaling) or standardization (mean=0, variance=1) to the features.\n\nHandling Missing Data:\n\nImportance: kNN cannot handle missing values directly.\nPractice: Impute missing values before applying kNN, using mean/mode imputation or kNN-based imputation.\n\nChoosing k:\n\nImportance: The choice of \\(k\\) can significantly affect model performance.\nPractice: Use cross-validation to determine the optimal value of \\(k\\).\n\nEvaluating Model Performance:\n\nImportance: Use appropriate metrics to evaluate the model, especially with imbalanced data.\nPractice: Evaluate using precision, recall, and ROC-AUC, not just accuracy."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#support-vector-machine-classification",
    "href": "slides/BSMM_8740_lec_05.html#support-vector-machine-classification",
    "title": "Classification & clustering methods",
    "section": "Support Vector Machine Classification",
    "text": "Support Vector Machine Classification\nThe SVM assumes a training set of the form \\((x_1,y_1),\\ldots,(x_n,y_n)\\) where the \\(y_i\\) are either \\(-1\\) or \\(1\\), indicating the class to which each \\(x_i\\) belongs.\nThe SVM algorithm looks to find the maximum-margin hyperplane that divides the group of points \\(x_i\\) for which \\(y_1=-1\\) from the group for which \\(Y_1=1\\), such that the distance between the hyperplane and the nearest point \\(x_i\\) from either group is maximized."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification",
    "title": "Classification & clustering methods",
    "section": "SVM Classification",
    "text": "SVM Classification\nBasic Concepts\n\n\nSeparating Hyperplane: The core idea of SVM is to find a hyperplane (in two-dimensional space, this would be a line) that best separates the classes in the feature space.\nSupport Vectors: Support vectors are the data points that are closest to the separating hyperplane. These points are critical in defining the position and orientation of the hyperplane.\nMargin: The algorithm aims to maximize the margin, which is the distance between the hyperplane and the nearest points from both classes. A larger margin is considered better as it may lead to lower generalization error of the classifier."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-large-margin",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-large-margin",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: large margin",
    "text": "SVM Classification: large margin\nIllustration of SVM large-margin principle"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-1",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-1",
    "title": "Classification & clustering methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\n\n\n\nLet our decision boundary be given by \\(f\\left(x\\right)=w^{\\top}x+w_{0}=0\\), for a vector \\(w\\) perpendicular to the boundary1.\nWe can express any point as \\(x=x_{\\bot}+r\\frac{w}{\\left\\Vert w\\right\\Vert }\\)\nNote that \\(f\\left(x\\right)=\\left(w^{\\top}x_{\\bot}+w_{0}\\right)+r\\left\\Vert w\\right\\Vert\\)\n\n\n\n\n\n\n\n\nany point \\(x\\) on the red line satisfies \\(w^\\top x+w_0=0\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-2",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-2",
    "title": "Classification & clustering methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\n\n\n\nSince \\(f\\left(x_{\\bot}\\right)=w^{\\top}x_{\\bot}+w_{0}=0\\), we have \\(f\\left(x\\right)=r\\left\\Vert w\\right\\Vert\\).\nWe also require \\(f\\left(x_{n}\\right)\\tilde{y}_{n}&gt;0\\)\nTo maximize the distance to the closest point, the objective is \\(\\max_{w,w_{0}}\\min_{n}\\left[\\tilde{y}_{n}\\left(w^{\\top}x_{n}+w_{0}\\right)\\right]\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-3",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-3",
    "title": "Classification & clustering methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\nIt is common to scale the vector \\(w\\) and the offset \\(w_0\\) such that \\(f_n\\hat{y}_n=1\\) for the point nearest the decision boundary, such that \\(f_n\\hat{y}_n\\ge1\\) for all \\(n\\).\nIn addition, since minimizing \\(1/ \\left\\Vert w\\right\\Vert\\) is equivalent to minimizing \\(\\left\\Vert w\\right\\Vert^2\\), we can state the objective as\n\\[\n\\min_{w,w_{0}}\\frac{1}{2}\\left\\Vert w\\right\\Vert ^{2}\\quad\\text{s.t.}\\quad\\tilde{y}_{n}\\left(w^{\\top}x_{n}+w_{0}\\right)\\ge 1, \\forall n\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-4",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-4",
    "title": "Classification & clustering methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\n\n\nIf there is no solution to the objective we can add slack variables \\(\\xi_n\\ge0\\) to replace the hard constraints that \\(f_n\\hat{y}_n\\ge1\\) with the soft margin constraints that \\(f_n\\hat{y}_n\\ge1-\\xi_n\\).\nThe new objective is\n\\[\n\\min_{w,w_{0},\\xi}\\frac{{1}}{2}\\left\\Vert w\\right\\Vert ^{2} + C\\sum_n \\xi_n \\\\\n\\text{s.t.}\\xi_n\\ge0, \\quad\\tilde{y}_{n}\\left(w^{\\top}x_{n}+w_{0}\\right)\\ge 1, \\forall n\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-data-not-separable",
    "href": "slides/BSMM_8740_lec_05.html#svm-data-not-separable",
    "title": "Classification & clustering methods",
    "section": "SVM: data not separable",
    "text": "SVM: data not separable\n\n\nIf the data is not separable:\n\na transformation of data may make them separable\nan embedding in a higher dimensional space might make them separable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-support-vectors",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-support-vectors",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: Support Vectors",
    "text": "SVM Classification: Support Vectors\n\nSupport vectors are the data points that lie closest to the decision surface (or hyperplane)\nThey are the data points most difficult to classify\nThey have direct bearing on the optimum location of the decision surface\nSupport vectors are the elements of the training set that would change the position of the dividing hyperplane if\nremoved"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-example",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-example",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: example",
    "text": "SVM Classification: example\n\n\nCode\n&gt; # show_engines(\"svm_linear\")\n&gt; default_model_svm &lt;- parsnip::svm_linear() %&gt;% \n+   parsnip::set_engine(\"svm_linear\") %&gt;% \n+   parsnip::set_mode(\"classification\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-variants",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-variants",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: variants",
    "text": "SVM Classification: variants\nThe linear SVM constructs a linear decision boundary (hyperplane) to separate classes in the feature space. It aims to find the hyperplane that maximizes the margin between the closest points (support vectors) of the classes. The decision function is \\(f(x) = w \\cdot x + b\\), where \\(w\\) is the weight vector and \\(b\\) is the bias term.\nThere are similar SVM methods that are adapted to more complex boundaries."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-polynomial",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-polynomial",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: polynomial",
    "text": "SVM Classification: polynomial\nPolynomial SVM uses a polynomial kernel to create a non-linear decision boundary. It transforms the input features into higher-dimensional space where a linear separation is possible.\nThe polynomial kernel is \\(K(x, x') = (w \\cdot x + b)^d\\), where \\(d\\) is the degree of the polynomial. This kernel is implemented though parsnip::svm_poly and engine kernlab."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-radial-basis",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-radial-basis",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: radial basis",
    "text": "SVM Classification: radial basis\nRBF SVM uses the Radial Basis Function (Gaussian) kernel to handle non-linear classification problems. It maps the input space into an infinite-dimensional space where a linear separation is possible.\nThe RBF kernel is \\(K(x, x') = \\exp\\left(-\\gamma \\left\\Vert x - x'\\right\\Vert^2\\right)\\), where \\(\\gamma\\) controls the width of the Gaussian function. This kernel is implemented though parsnip::svm_rbf and engine kernlab."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-comparisons",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-comparisons",
    "title": "Classification & clustering methods",
    "section": "SVM Classification: Comparisons",
    "text": "SVM Classification: Comparisons\n\n\n\n\n\n\n\n\n\n\nFeature\nLinear SVM\nPolynomial SVM\nRBF SVM\n\n\n\n\nKernel Function\nLinear\nPolynomial\nRadial Basis Function\n\n\nEquation\n\\(w \\cdot x + b\\)\n\\((w \\cdot x + b)^d\\)\n\\(\\exp\\left(-\\gamma \\left\\Vert x - x'\\right\\Vert^2\\right)\\)\n\n\nComplexity\nLow\nMedium to High (depending on \\(d\\))\nHigh\n\n\nInterpretability\nHigh\nMedium\nLow\n\n\nComputational Cost\nLow\nMedium to High (higher with increasing \\(d\\))\nHigh\n\n\nFlexibility\nLow\nMedium to High\nHigh\n\n\nRisk of Overfitting\nLow\nMedium to High (higher with increasing \\(d\\))\nMedium to High (depends on \\(\\gamma\\) and \\(C\\))\n\n\nTypical Use Cases\nLinearly separable, high-dimensional spaces (e.g., text)\nData with polynomial relationships\nHighly non-linear data, complex patterns"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices",
    "title": "Classification & clustering methods",
    "section": "SVM Classification Best Practices",
    "text": "SVM Classification Best Practices\n\n1. Data Preparation\na. Feature Scaling\n\nImportance: SVM is sensitive to the scale of the features. Features with larger ranges can dominate the distance calculations, leading to suboptimal boundaries.\nBest Practice: Standardize or normalize the features so that they have similar scales. Common techniques include Min-Max scaling (to a [0, 1] range) and StandardScaler (to zero mean and unit variance).\n\nb. Handling Missing Data\n\nImportance: SVM cannot handle missing values directly.\nBest Practice: Impute missing values using methods like mean/mode/median imputation or more sophisticated techniques such as k-NN imputation or using models to predict missing values.\n\n2. Choosing the Kernel\na. Linear Kernel\n\nWhen to Use: If the data is approximately linearly separable or when the number of features is very large compared to the number of samples.\nBest Practice: Start with a linear kernel as a baseline, especially for high-dimensional data like text classification.\n\nb. Non-linear Kernels (Polynomial, RBF)\n\nWhen to Use: If the data is not linearly separable. The RBF kernel is generally a good first choice for non-linear problems.\nBest Practice: Experiment with different kernels. Use cross-validation to compare performance and select the most appropriate kernel."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices-1",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices-1",
    "title": "Classification & clustering methods",
    "section": "SVM Classification Best Practices",
    "text": "SVM Classification Best Practices\n\n3. Hyperparameter Tuning\na. Regularization Parameter (cost \\(C\\))\n\nImportance: Controls the trade-off between achieving a low training error and a low testing error.\nBest Practice: Use cross-validation to find the optimal value of cost \\(C\\). Start with a wide range and then narrow down.\n\nb. Kernel-specific Parameters\n\nFor RBF Kernel: Tune the \\(\\sigma\\) parameter, which defines the influence of individual data points.\nFor Polynomial Kernel: Tune the degree \\(d\\), the margin coefficient \\(r\\), and the scale_factor.\nBest Practice: Use grid search or random search for hyperparameter tuning. Cross-validation is crucial to avoid overfitting and to find the best combination of parameters.\n\n4. Handling Imbalanced Data\na. Class Weights\n\nImportance: Imbalanced classes can bias the SVM towards the majority class.\nBest Practice: Adjust the class weights to give more importance to the minority class. Most SVM implementations allow setting the class_weight parameter to â€˜balancedâ€™ or manually specifying weights.\n\nb. Resampling Techniques\n\nImportance: Can further help in dealing with imbalanced datasets.\nBest Practice: Use oversampling (e.g., themis::step_smote()) or undersampling techniques to balance the class distribution before training the model."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices-2",
    "href": "slides/BSMM_8740_lec_05.html#svm-classification-best-practices-2",
    "title": "Classification & clustering methods",
    "section": "SVM Classification Best Practices",
    "text": "SVM Classification Best Practices\n\n5. Model Evaluation\na. Performance Metrics\n\nImportance: Accuracy alone may not be sufficient, especially for imbalanced datasets.\nBest Practice: Evaluate using metrics like precision, recall,and ROC-AUC to get a comprehensive understanding of model performance.\n\nb. Cross-validation\n\nImportance: Ensures that the model generalizes well to unseen data.\nBest Practice: Use k-fold cross-validation to assess the modelâ€™s performance and robustness. This helps in reducing the variance in performance estimates.\n\n6. Implementation Considerations\na. Choosing the Right Software/Library\n\nImportance: Efficient and reliable implementation is crucial for performance.\nBest Practice: Use well-established libraries like those in parsnip.\n\nb. Handling Large Datasets\n\nImportance: SVM can be computationally intensive for large datasets.\nBest Practice: Use linear SVMs for very large datasets or use dimensionality reduction techniques (e.g., PCA) to reduce the feature space."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#clustering",
    "href": "slides/BSMM_8740_lec_05.html#clustering",
    "title": "Classification & clustering methods",
    "section": "Clustering",
    "text": "Clustering\nClassification and clustering serve different purposes in machine learning. Classification is a supervised learning technique used for predicting predefined labels, requiring labeled data and focusing on accuracy and interpretability.\nClustering, on the other hand, is an unsupervised learning technique used for discovering natural groupings in data, requiring no labeled data and focusing on exploratory data analysis and pattern discovery. Understanding the strengths and limitations of each method is crucial for applying them effectively to solve real-world problems."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#clustering-1",
    "href": "slides/BSMM_8740_lec_05.html#clustering-1",
    "title": "Classification & clustering methods",
    "section": "Clustering",
    "text": "Clustering\nCluster analysis refers to algorithms that group similar objects into groups called clusters. The endpoint of cluster analysis is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\nThe purpose of cluster analysis is to help reveal patterns and structures within a dataset that may provide insights into underlying relationships and associations."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#clustering-applications",
    "href": "slides/BSMM_8740_lec_05.html#clustering-applications",
    "title": "Classification & clustering methods",
    "section": "Clustering Applications",
    "text": "Clustering Applications\n\n\nMarket Segmentation:Â Cluster analysis is often used in marketing to segment customers into groups based on their buying behavior, demographics, or other characteristics.\nImage Processing:Â In image processing, cluster analysis is used to group pixels with similar properties together, allowing for the identification of objects and patterns in images.\nBiology and Medicine:Â Cluster analysis is used in biology and medicine to identify genes associated with specific diseases or to group patients with similar clinical characteristics together.\nSocial Network Analysis:Â In social network analysis, cluster analysis is used to group individuals with similar social connections and characteristics together, allowing for the identification of subgroups within a larger network.\nAnomaly Detection:Â Cluster analysis can be used to detect anomalies in data, such as fraudulent financial transactions, unusual patterns in network traffic, or outliers in medical data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-clustering",
    "href": "slides/BSMM_8740_lec_05.html#k-means-clustering",
    "title": "Classification & clustering methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nk-means is a method of unsupervised learning that produces a partitioning of observations into k unique clusters.\nThe goal of k-means is to minimize the sum of squared Euclidian distances between observations in a cluster and the centroid, or geometric mean, of that cluster."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-clustering-1",
    "href": "slides/BSMM_8740_lec_05.html#k-means-clustering-1",
    "title": "Classification & clustering methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nIn k-means clustering, observed variables (columns) are considered to be locations on axes in multidimensional space.\n\nThe basic k-means algorithm has the following steps.\n\npick the number of clusters k\nChoose k observations in the dataset. These locations in space are declared to be the initial centroids.\nAssign each observation to the nearest centroid.\nCompute the new centroids of each cluster (the mean of each measurement over all observations in the cluster).\nRepeat steps 3 and 4 until the centroids do not change."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-clustering-2",
    "href": "slides/BSMM_8740_lec_05.html#k-means-clustering-2",
    "title": "Classification & clustering methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nThere are three common methods for selecting initial centers:\n\nRandom observations: Chosing random observations to act as our initial centers is the most commonly used approach, implemented in the Forgy, Lloyd, and MacQueen methods.\nRandom partition: The observations are assigned to a cluster uniformly at random. The centroid of each cluster is computed, and these are used as the initial centers. This approach is implemented in the Hartigan-Wong method.\nk-means++: Beginning with one random set of the observations, further observations are sampled via probability-weighted sampling until \\(k\\) clusters are formed. The centroids of these clusters are used as the initial centers."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-clustering-3",
    "href": "slides/BSMM_8740_lec_05.html#k-means-clustering-3",
    "title": "Classification & clustering methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nBecause the initial conditions are based on random selection in both approaches, the k-means algorithm is not deterministic.\nRunning the clustering twice on the same data may not result in the same cluster assignments."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-example",
    "href": "slides/BSMM_8740_lec_05.html#k-means-example",
    "title": "Classification & clustering methods",
    "section": "K-means Example",
    "text": "K-means Example\n\n\nCode\n&gt; # create recipe for 2-D clustering\n&gt; cluster_recipe &lt;- data |&gt; \n+   recipes::recipe(~ x1 + x2, data = _)\n&gt; \n&gt; # specify the workflows\n&gt; all_workflows &lt;- \n+   workflowsets::workflow_set(\n+     preproc = list(base = cluster_recipe),\n+     models = list(tidyclust::k_means( num_clusters = parsnip::tune() ) )\n+   )\n&gt; # create bootstrap samples\n&gt; dat_resamples &lt;- data |&gt; rsample::bootstraps(apparent = TRUE)\n&gt; \n&gt; tuned_results &lt;-\n+    all_workflows |&gt; \n+    workflow_map(\n+       fn = \"tune_cluster\"\n+       , resamples = dat_resamples\n+       , grid = dials::grid_regular(dials::num_clusters(), levels = 10)\n+       , metrics = tidyclust::cluster_metric_set(sse_within_total, sse_total, sse_ratio)\n+       , control = tune::control_grid(save_pred = TRUE, extract = identity)\n+    )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-example-1",
    "href": "slides/BSMM_8740_lec_05.html#k-means-example-1",
    "title": "Classification & clustering methods",
    "section": "K-means Example",
    "text": "K-means Example\n\n\nCode\n&gt; set.seed(8740)\n&gt; \n&gt; centers &lt;- tibble::tibble(\n+   cluster = factor(1:4), \n+   num_points = c(100, 150, 50, 90),  # number points in each cluster\n+   x1 = c(5, 0, -3, -4),              # x1 coordinate of cluster center\n+   x2 = c(-1, 1, -2, 1.5),               # x2 coordinate of cluster center\n+ )\n&gt; \n&gt; labelled_points &lt;- \n+   centers |&gt;\n+   dplyr::mutate(\n+     x1 = purrr::map2(num_points, x1, rnorm),\n+     x2 = purrr::map2(num_points, x2, rnorm)\n+   ) |&gt; \n+   dplyr::select(-num_points) |&gt; \n+   tidyr::unnest(cols = c(x1, x2))\n&gt; \n&gt; p &lt;- ggplot(labelled_points, aes(x1, x2, color = cluster)) +\n+   geom_point(alpha = 0.3) + \n+   geom_point(data = centers, size = 10, shape = \"o\")\n&gt; p"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#k-means-example-2",
    "href": "slides/BSMM_8740_lec_05.html#k-means-example-2",
    "title": "Classification & clustering methods",
    "section": "K-means Example",
    "text": "K-means Example\n\n\nCode\n&gt; # create recipe\n&gt; labelled_points_recipe &lt;- labelled_points |&gt; \n+   recipes::recipe(~ x1 + x2, data = _)\n&gt; # create model spec\n&gt; kmeans_spec &lt;- tidyclust::k_means( num_clusters = 4 )\n&gt; # create workflow\n&gt; wflow &lt;- workflows::workflow() |&gt;\n+   workflows::add_model(kmeans_spec) |&gt;\n+   workflows::add_recipe(labelled_points_recipe)\n&gt; # fit workflow & extract centroids\n&gt; cluster_centers &lt;- wflow |&gt;\n+   parsnip::fit(labelled_points) %&gt;% tidyclust::extract_centroids() |&gt; \n+   dplyr::mutate( cluster = stringr::str_extract(.cluster,\"\\\\d\") )\n&gt; # plot\n&gt; p + geom_point(data = cluster_centers, size = 10, shape = \"x\")\n\n\n\n\n&gt; # all_workflows &lt;- all_workflows %&gt;% \n&gt; #   workflowsets::workflow_map(\n&gt; #     resamples = train_resamples, grid = 5, verbose = TRUE\n&gt; #   )\n\n\n&gt; # all_workflows &lt;- \n&gt; #   workflowsets::workflow_set(\n&gt; #     preproc = list(base = labelled_points_recipe),\n&gt; #     models = \n&gt; #       3:20 %&gt;% purrr::map( ~tidyclust::k_means( num_clusters = .x ) )\n&gt; #   )\n\n\n&gt; all_workflows &lt;- \n+   workflowsets::workflow_set(\n+     preproc = list(base = labelled_points_recipe),\n+     models = list(tidyclust::k_means( num_clusters = parsnip::tune() ) )\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#hierarchical-clustering",
    "href": "slides/BSMM_8740_lec_05.html#hierarchical-clustering",
    "title": "Classification & clustering methods",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical Clustering, sometimes called Agglomerative Clustering, is a method of unsupervised learning that produces a dendrogram, which can be used to partition observations into clusters (see tidyclust)\nFor other clustering algorithms, see\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nMean Shift Clustering\nSpectral Clustering"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#clustering-general-considerations",
    "href": "slides/BSMM_8740_lec_05.html#clustering-general-considerations",
    "title": "Classification & clustering methods",
    "section": "Clustering: General Considerations",
    "text": "Clustering: General Considerations\n\nFeature Scaling: Most clustering algorithms benefit from feature scaling.\nChoosing the Right Algorithm: Depends on the size, dimensionality of data, and the nature of the clusters.\nEvaluation: Since clustering is unsupervised, evaluating the results can be subjective and is often based on domain knowledge."
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#more",
    "href": "slides/BSMM_8740_lec_05.html#more",
    "title": "Classification & clustering methods",
    "section": "More",
    "text": "More\n\nRead An Idiotâ€™s Guide to Support Vector Machines"
  },
  {
    "objectID": "slides/BSMM_8740_lec_05.html#recap",
    "href": "slides/BSMM_8740_lec_05.html#recap",
    "title": "Classification & clustering methods",
    "section": "Recap",
    "text": "Recap\n\nWe have looked at several classification algorithms in the context of tidymodels workflows.\nWe also looked at clustering and several algorithms in the tidyclust package.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_06.html#recap-of-last-week",
    "title": "Time series methods",
    "section": "Recap of last week:",
    "text": "Recap of last week:\n\nLast week we introduced classification and clustering methods within the Tidymodels framework in R.\nToday we look at methods for analysing time series, and we use the timetk and modeltime packages in conjunction with Tidymodels to create and evaluate time series models."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#this-week",
    "href": "slides/BSMM_8740_lec_06.html#this-week",
    "title": "Time series methods",
    "section": "This week:",
    "text": "This week:\n\nToday we will explore time series - data where each observation includes a time measurement and the time measurements are ordered.\nWeâ€™ll look at how to manipulate our time values, create time-based features, plot our time series, and decompose time series into components.\nFinally we will use our time series for forecasting, using regression, exponential smoothing and ARIMA1 models\n\nAuto Regressive Integrated Moving Average"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series",
    "href": "slides/BSMM_8740_lec_06.html#time-series",
    "title": "Time series methods",
    "section": "Time series",
    "text": "Time series\nA time series data is a data frame (tibble) with an ordered temporal measurement.\nWhy is this a separate area of study? Consider the simple linear regression model\n\\[\ny_t=x_t^\\top\\beta + \\epsilon_t;\\;t=1,\\ldots,R\n\\]\nerrors should not be serially correlated for OLS estimates:\n\n\\(\\mathbb{E}[\\epsilon_t]=\\mathbb{E}[\\epsilon_t|\\epsilon_{t-1},\\epsilon_{t-1},\\ldots]\\), and\n\\(\\mathbb{E}[\\epsilon_t\\epsilon_{t-j}]=0,\\forall j\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series-characteristics",
    "href": "slides/BSMM_8740_lec_06.html#time-series-characteristics",
    "title": "Time series methods",
    "section": "Time series: characteristics",
    "text": "Time series: characteristics\n\n\nMost economic and financial time series exhibit some form of serial correlation\n\nIf economic output is large during the previous quarter then there is a good chance that it is going to be large in the current quarter\n\nA change that arises in the current period may only affect other variables in the distant future\nA particular shock may affect variables over successive quarters\n\nHence, we need to start thinking about the dynamic structure of the system that we are investigating"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series-dynamics",
    "href": "slides/BSMM_8740_lec_06.html#time-series-dynamics",
    "title": "Time series methods",
    "section": "Time series: dynamics",
    "text": "Time series: dynamics\nWhether time series data is used for forecasting or for testing various theories/hypotheses, we always need to identify the dynamic evolution of the variables, e.g.\n\\[\n\\begin{align*}\n\\text{(trend)}\\qquad T_{t} & =1+0.05t\\qquad\\\\\n\\text{(seasonality)}\\qquad S_{t} & =1.5\\cos(t\\pi\\times0.166)\\\\\n\\text{(noise)}\\qquad I_{t} & =0.5I_{t-1}+\\epsilon_{t}\\quad\\epsilon_t\\sim\\mathscr{N}(0,\\sigma^2)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series-generation",
    "href": "slides/BSMM_8740_lec_06.html#time-series-generation",
    "title": "Time series methods",
    "section": "Time series: generation",
    "text": "Time series: generation\n\n\nCode\n&gt; set.seed(8740)\n&gt; \n&gt; dat &lt;- tibble::tibble(\n+   date = seq(as.Date('2015-04-7'),as.Date('2020-03-22'),'2 weeks')\n+ ) %&gt;% \n+   tibble::rowid_to_column(\"t\") %&gt;% \n+   dplyr::mutate(\n+     trend = 1 + 0.05 * t\n+     , seasonality = 1.5 * cos(pi * t * 0.166)\n+     , noise = rnorm(length(t))\n+     , temp = dplyr::lag(noise)\n+   ) %&gt;% \n+   tidyr::replace_na(list(temp = 0)) %&gt;% \n+   dplyr::mutate(\n+     noise =\n+       purrr::map2_dbl(\n+         noise\n+         , temp\n+         , ~ .x + 0.5 * .y\n+       )\n+   ) %&gt;% \n+   dplyr::select(-temp)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series-generation-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series-generation-1",
    "title": "Time series methods",
    "section": "Time series: generation",
    "text": "Time series: generation\n\ncomponentscombined\n\n\n\n\nCode\n&gt; dat %&gt;% \n+   tidyr::pivot_longer(-c(t, date)) %&gt;% \n+   ggplot(aes(x=date, y=value, color=name)) + geom_line(linewidth=1) + \n+   labs(\n+     title = 'trend, seasonality, and noise'\n+     , subtitle = \"deterministic: trend, seasonality | stochastic: noise\"\n+     , color=NULL) + theme_minimal() + theme(plot.margin = margin(0,0,0,0, \"cm\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n&gt; dat %&gt;% \n+   dplyr::mutate(y = trend + seasonality + noise) %&gt;% \n+   ggplot(aes(x=date, y=y)) + geom_line(linewidth=1) +\n+   labs(\n+     title = 'trend + seasonality + noise'\n+     , subtitle = \"deterministic: trend, seasonality | stochastic: noise\") + theme_minimal() +\n+   theme(plot.margin = margin(0,0,0,0, \"cm\"))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\n\nTime series is a collection of observations indexed by the date of each realisation\nUsing notation that starts at time, \\(t=1\\), and using the end point, \\(t=T\\)\n\n\\[\\{y_1,y_2,y_3,â€¦,y_T\\}\\]\n\nTime index can be of any frequency (e.g.Â daily, quarterly, etc.)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-1",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nDeterministic and Stochastic Processes\n\ndeterministic processes always produce the same output from a given starting point or state\nstochastic processes have indeterminacy\n\nUsually described by some form of statistical distribution\nExamples include: white noise processes, random walks, Brownian motions, Markov chains, martingale difference sequences, etc."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-2",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-2",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - White noise\n\n\nA white noise series is made of serially uncorrelated random variables with zero mean and finite variance\nFor example, errors may be characterised by a Gaussian white noise process, where such a variable has a normal distribution\nSlightly stronger condition is that they are independent from one another\n\n\\[\\epsilon_{t}\\sim\\text{{i.i.d.}}\\mathscr{N}\\left(0,\\sigma_{\\epsilon_{t}}^{2}\\right)\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-3",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-3",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - White noise\n\nimplications:\n\n\\(\\mathbb{E}\\left[\\epsilon_{t}\\right]=\\mathbb{E}\\left[\\left.\\epsilon_{t}\\right|\\epsilon_{t-1},\\epsilon_{t-2},\\ldots\\right]=0\\)\n\\(\\mathbb{E}\\left[\\epsilon_{t}\\epsilon_{t-j}\\right]=\\text{cov}\\left(\\epsilon_{t},\\epsilon_{t-j}\\right)=0\\)\n\\(\\text{var}\\left(\\epsilon_{t}\\right)=\\text{cov}\\left(\\epsilon_{t},\\epsilon_{t}\\right)=\\sigma_{\\epsilon_{t}}^{2}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-4",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-4",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Random walk\n\n\nRandom walk would imply that the effect of a shock is permanent. \\[y_t=y_{t-1}+\\epsilon_t\\]\nGiven the starting value \\(y_0\\), and using recursive substitution, this process could be represented as \\[y_t=y_0+\\sum_{j=1}^t\\epsilon_t\\]\nSince the effect of past shocks do not dissipate we say it has an infinite memory"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-5",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-5",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Random walk + drift\n\n\nRandom walk plus a constant term. \\[y_t=\\mu+y_{t-1}+\\epsilon_t\\]\nGiven the starting value \\(y_0=0\\), and using recursive substitution, this process could be represented as \\[y_t=\\mu t+\\sum_{j=1}^t\\epsilon_t\\]\nShocks have permanent effects and are influenced by drift"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-6",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-6",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Random walks\n\nCharacteristics:\n\nIndependence:\n\nThe steps in a random walk are independent of each other. The future position depends only on the current position and a random step.\n\nTypes:\n\nSimple Random Walk: The step sizes are often \\(\\pm 1\\), with equal probability.\nGeneral Random Walk: The step sizes can follow any distribution.\n\nApplications:\n\nStock price movements, particle diffusion, and population genetics."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-7",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-7",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Markov chains\n\nCharacteristics:\n\nState Space: - A Markov chain consists of a set of states and transition probabilities between these states.\nMarkov Property:\n\nThe probability of transitioning to the next state depends only on the current state: \\(\\mathbb{P}(X_{t+1} = s' | X_t = s, X_{t-1}, ..., X_0) = \\mathbb{P}(X_{t+1} = s' | X_t = s)\\).\n\nTransition Matrix:\n\nThe transitions are governed by a matrix of probabilities, where each entry \\(\\mathbb{P}_{i,j}\\) represents the probability of moving from state \\(i\\) to state \\(j\\).\n\nTypes:\n\nDiscrete-Time Markov Chain: The process is observed at discrete time intervals.\nContinuous-Time Markov Chain: The process evolves continuously over time.\n\nApplications:\n\nWeather modeling, queueing theory, board games (like Monopoly), and speech recognition."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-8",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-8",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStochastic Processes - Markov chains\n\nIn the discrete case, given a state transition matrix \\(A\\) and an initial state \\(\\pi_0\\), then\n\\[\n\\begin{align*}\n\\pi_1 & = A\\pi_0\\\\\n\\pi_2 & = A\\pi_1 = A^2\\pi_0\\\\\n&\\vdots\\\\\n\\pi_n & = A^n\\pi_0\n\\end{align*}\n\\]\nIf the markov chain has a stationary distribution \\(\\pi^{s}\\), then \\(A\\pi^{s}=\\pi^{s}\\) by definition and \\((I-A)\\pi^{s}=0\\) determines \\(\\pi^{s}\\) up to a constant."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-9",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-9",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\n\nIn an AR(1) process the current value is a linear function of the previous value.\n\n\\[y_t=\\phi_1 y_{t-1}+\\epsilon_t\\] - Fixing the starting value at \\(y_0=0\\), and with repeated substitution, this process could be represented as \\[y_t=\\sum_{j=1}^t\\phi_1^{t-j}\\epsilon_j\\] - The distribution of each error term is \\(\\epsilon_t=\\mathscr{N}(0,\\sigma^2)\\) with \\(\\mathbb{E}[\\epsilon_i\\epsilon_j]=0,\\,\\forall i\\ne j\\), and we can generalize to several lags (i.e.Â an AR(p) model)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-10",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-10",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nMoments of distribution\n\n\nThe first moment of a stochastic process is the average of \\(y_t\\) over all possible realisations \\[\\hat{y}=\\mathbb{E}[y_t];\\;\\;t=1,\\ldots,T\\]\nThe second moment is defined as the variance \\[\\text{var}(y_t)=\\mathbb{E}[y_t\\times y_t]=\\mathbb{E}[y_t- \\mathbb{E}[y_t]^2];\\;\\;t=1,\\ldots,T\\]\nThe covariance, for \\(j\\) \\[\\text{cov}(y_t,y_{t-j})=\\mathbb{E}[y_t\\times y_{t-j}]=\\mathbb{E}[(y_t- \\mathbb{E}[y_t])(y_{t-j}- \\mathbb{E}[y_{t-j}])];\\;\\;t=1,\\ldots,T\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-11",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-11",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nConditional moments\n\n\nConditional distribution is based on past realisations of a random variable\nFor the AR(1) model (where \\(\\epsilon_t\\) are iid Gausian and \\(|\\phi|&lt;0\\)) \\[y_t=\\phi y_{t-1}+\\epsilon_t\\]\nThe conditional moments are \\[\n\\begin{align*}\n\\mathbb{E}\\left[\\left.y_{t}\\right|y_{t-1}\\right] & =\\phi y_{t-1}\\\\\n\\text{var}\\left(\\left.y_{t}\\right|y_{t-1}\\right) & =\\mathbb{E}\\left[\\phi y_{t-1}+\\epsilon_{t}-\\phi y_{t-1}\\right]^{2}=\\mathbb{E}\\left[\\epsilon\\right]^{2}=\\sigma^{2}\\\\\n\\text{cov}\\left(\\left.y_{t}\\right|y_{t-1},\\left.y_{t-j}\\right|y_{t-j-1}\\right) & =0;\\;j&gt;1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-12",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-12",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nConditional moments\n\n\nConditioning on \\(y_{t-2}\\) for \\(y_t\\) \\[\n\\begin{align*}\n\\mathbb{E}\\left[\\left.y_{t}\\right|y_{t-2}\\right] & =\\phi^{2}y_{t-2}\\\\\n\\text{var}\\left(\\left.y_{t}\\right|y_{t-2}\\right) & =\\left(1+\\phi^{2}\\right)\\sigma^{2}\\\\\n\\text{cov}\\left(\\left.y_{t}\\right|y_{t-2},\\left.y_{t-j}\\right|y_{t-j-2}\\right) & =\\phi\\sigma^{2};\\;j=1\\\\\n& =0;\\;j&gt;1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-13",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-13",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nUnconditional moments\n\n\nFor the AR(1) model (where \\(\\epsilon_t\\) are iid Gausian and \\(|\\phi|&lt;1\\)) \\[y_t=\\phi y_{t-1}+\\epsilon_t\\]\nThe unconditional moments are (assuming stationarity and \\(\\phi&lt;1\\)) \\[\n\\begin{align*}\n\\mathbb{E}\\left[y_{t}\\right] & =0\\\\\n\\text{var}\\left(y_{t}\\right) & =\\text{var}\\left(\\phi y_{t-1}+\\epsilon_t\\right)\\\\\n& = \\phi^2 \\text{var}\\left(y_{t-1}\\right) + \\text{var}\\left(\\epsilon_{t}\\right)\\\\\n& = \\frac{\\sigma^2}{1-\\phi^2}\\\\\n\\text{cov}\\left(y_{t},y_{t-k}\\right) & =\\phi^k\\frac{\\sigma^2}{1-\\phi^2}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-14",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-14",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nStationarity\n\n\nA time series is strictly stationary if for any \\(\\{j_1,j_2,\\ldots,j_n\\}\\)\nthe joint distribution of \\(\\{y_t,y_{t+j_1},y_{t+j_2},\\ldots,y_{t+j_n}\\}\\)\ndepends only on the intervals separating the dates \\(\\{j_1,j_2,\\ldots,j_n\\}\\)\nand not on the date \\(t\\) itself"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-15",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-15",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nCovariance stationary\n\n\nIf neither the mean \\(\\hat{y}\\) nor the covariance \\(\\text{cov}(y_t,y_{t-j})\\) depend on \\(t\\)\nthe the process for \\(y_t\\) is said to be covariance (weakly) stationary, where \\(\\forall t,j\\) \\[\n\\begin{align*}\n\\mathbb{E}\\left[y_{t}\\right] & =\\bar{y}\\\\\n\\mathbb{E}\\left[\\left(y_{t}-\\bar{y}\\right)\\left(y_{t-j}-\\bar{y}\\right)\\right] & =\\text{cov}\\left(y_{t},y_{t-j}\\right)\n\\end{align*}\n\\]\nNote that the process \\(y_t=\\alpha t+\\epsilon_t\\) would not be stationary, as the mean clearly depends on \\(t\\)\nWe saw that the unconditional moments of the AR(1) with \\(|Ï•|&lt;1\\) had a mean and covariance that did not depend on time"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-16",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-16",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutocorrelation function (ACF)\n\n\nFor a stationary process we can plot the standardised covariance of the process over successive lags\nThe autocovariance function is denoted by \\(\\gamma (j)\\equiv\\text{cov}(y_t,y_{t-j})\\) for \\(t=1,\\ldots,T\\)\nTh autocovariance function is standardized by dividing each function by the variance, giving the ACF for successive values of \\(j\\)\n\\[\\rho(j)\\equiv\\frac{\\gamma(j)}{\\gamma(0)}\\]\nTo display the results of the ACF we usually plot \\(Ï(j)\\) against (non-negative) \\(j\\) to illustrate the degree of persistence in a variable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-17",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-17",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nPartial autocorrelation function (PACF)\n\n\nWith an AR(1) process \\(y_t=\\phi y_{t-1}+\\epsilon_t\\), the ACF would suggest \\(y_t\\) and \\(y_{t-2}\\) are correlated, even though \\(y_{t-2}\\) does not appear in the model.\nThis is due to the pass through, where we noted that \\(y_t=\\phi^2y_{t-2}\\) when performing recursive substitution\nPACF eliminates the effects of passthrough and puts the focus on the independent relationship between \\(y_t\\) and \\(y_{t_2}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-18",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-18",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nPartial autocorrelation function (PACF)\n\nCode\n&gt; library(ggfortify)\n&gt; # Parameters for the AR(1) process\n&gt; phi = 0.8  # Autoregressive coefficient (should be less than 1 in absolute value)\n&gt; n = 100    # Number of observations\n&gt; \n&gt; # Simulate AR(1) process\n&gt; set.seed(123)  # For reproducibility\n&gt; epsilon = rnorm(n)  # White noise\n&gt; X = rep(0, n)  # Initialize the series\n&gt; \n&gt; # Generate the AR(1) series\n&gt; for (t in 2:n) {\n+   X[t] = phi * X[t-1] + epsilon[t]\n+ }\n&gt; \n&gt; # Plot the AR(1) series\n&gt; # plot(X, type = \"l\", main = \"AR(1) Process\", xlab = \"Time\", ylab = \"Value\")\n&gt; tibble::tibble(y=X, x=1:length(X)) %&gt;% ggplot(aes(x=x, y=y)) + geom_line() + \n+   labs(title=\"AR(1) Process\", x = \"Time\", y = \"Value\")\n&gt; # Plot the autocorrelation function (ACF)\n&gt; #stats::acf(X, main = \"Autocorrelation of AR(1) Process\")\n&gt; autoplot(stats::acf(X, plot = FALSE)) + labs(title = \"Autocorrelation of AR(1) Process\")\n&gt; # Plot the autocorrelation function (ACF)\n&gt; #stats::pacf(X, main = \"Partial Autocorrelation of AR(1) Process\")\n&gt; autoplot(stats::pacf(X, plot = FALSE)) + labs(title = \"Partial Autocorrelation of AR(1) Process\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-19",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-19",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\n\nThe characteristic equation for an AR(p) process is derived from the autoregressive parameters \\((\\phi_1, \\phi_2, \\ldots, \\phi_p)\\).\nThe characteristic equation is: \\(1-\\phi_1 z-\\phi_2 z^2-\\cdots-\\phi_p z^p = 0\\)\nA process \\({y_t}\\) is strictly stationary if for each \\(k\\) and \\(t\\), and \\(n\\), the distribution of \\({y_t,\\ldots,y_{t+k}}\\) is the same as the distribution of \\({y_{t+n},\\ldots,y_{t+k+n}}\\)\nFor an AR process to be stationary (its statistical properties do not change over time), the parameters \\(\\phi_1, \\phi_2, \\ldots, \\phi_p\\) must satisfy certain conditions (typically related to the roots of the characteristic equation lying outside the unit circle)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-20",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-20",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\n\nThe AR(p) model is sometimes expressed in terms of the Lag operator \\(\\mathrm{L}\\), where \\(\\mathrm{L}y_t=y_{t-1}\\).\nThe lag operator can be raised to powers, e.g.Â \\(\\mathrm{L}^2y_t=y_{t-2}\\) and its powers can be combined into polynomials to form a new operator: \\(a(\\mathrm{L})=a_0+a_1\\mathrm{L}+a_2\\mathrm{L}^2+\\ldots+a_p\\mathrm{L}^p\\) such that \\(a(\\mathrm{L})y_t=a_0y_t+a_1y_{t-1}+a_2y_{t-2}+\\ldots+a_py_{t-p}\\)\nLag polymonials can be multiplied and multiplication commutes: \\(a(\\mathrm{L})b(\\mathrm{L})=b(\\mathrm{L})a(\\mathrm{L})\\).\nWe define \\((1-\\rho\\mathrm{L})^{-1}\\) by \\((1-\\rho\\mathrm{L})(1-\\rho\\mathrm{L})^{-1}\\equiv1\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-21",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-21",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\n\nper the formula for the geometric series, if \\(|\\rho|&lt;1, then\\)\n\n\\[\n(1-\\rho\\mathrm{L})^{-1} = \\sum_{i=0}^\\infty\\rho^i\\mathrm{L}^i\n\\]\n\nEXERCISE: check that \\((1-\\rho\\mathrm{L})(1-\\rho\\mathrm{L})^{-1}\\equiv1\\) holds on both sides of the equation above.\nIf the sum is to converge, then we need \\(|\\rho|&lt;1\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-22",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-22",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\nFor the AR(1) process, using the lag operator we can write:\n\\[\n\\begin{align*}\ny_t & = \\phi_1 y_{t-1}+\\epsilon_t\\\\\n(1-\\phi L)y_t & =  \\epsilon_t\\\\\ny_t & = (1-\\phi L)^{-1} \\epsilon_t\\\\\n  & = (\\sum_{i=0}^\\infty\\phi^i\\mathrm{L}^i)\\epsilon_t\\\\\n  & = \\sum_{j=1}^t\\phi_1^{t-j}\\epsilon_j\n\\end{align*}\n\\] - EXERCISE: check that the last equality holds."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-23",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-23",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\nAR(p) processes can be written as AR(1) vector processes, e.g.Â for AR(2)\n\\[\n\\left(\\begin{array}{c}\ny_{t}\\\\\ny_{t-1}\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n\\phi_{1} & \\phi_{2}\\\\\n1 & 0\n\\end{array}\\right)\\left(\\begin{array}{c}\ny_{t-1}\\\\\ny_{t-2}\n\\end{array}\\right)+\\left(\\begin{array}{c}\n\\epsilon_{t}\\\\\n0\n\\end{array}\\right)\n\\] where the matrix \\(A\\) here is (i.e.Â AR(2)):\n\\[A=\\left(\\begin{array}{cc}\n\\phi_{1} & \\phi_{2}\\\\\n1 & 0\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-24",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-24",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\nRepeating the argument for AR(1) processes\n\\[\n\\begin{align*}\n\\vec{y}_t & = (1-A L)^{-1} \\vec{\\epsilon}_t\\\\\n  & = (\\sum_{i=0}^\\infty A^i\\mathrm{L}^i)\\vec{\\epsilon}_t\\\\\n\\end{align*}\n\\]\nAnd we only converge if \\(A^i\\rightarrow0 \\;\\mathrm{as}\\;i\\rightarrow\\infty\\), i.e.Â all eigenvalues are &lt; 1."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-25",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-25",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nAutoregressive Processes\n\nIn general if an \\(n\\times n\\) matrix \\(A\\) has \\(n\\) distinct eigenvalues, then all eigenvalues must have magnitude \\(&lt;1\\) for \\(A^i\\rightarrow0 \\;\\mathrm{as}\\;i\\rightarrow\\infty\\).\nIn this case we have \\(A=X\\Lambda X^{-1}\\) where the columns of \\(x\\) are the eigenvectors of \\(A\\) and \\(\\Lambda\\) is a diagonal matrix with the eigenvalues on the diagonal.\n\\(A^i=X\\Lambda^i X^{-1}\\) so \\(A^i\\rightarrow0 \\;\\mathrm{as}\\;i\\rightarrow\\infty\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-26",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-26",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nComputing determinants\n\nDefinitions\n\nMinor:\n\nThe minor \\(M_{ij}\\) of an element \\(a_{ij}\\) in an \\(n \\times n\\) matrix \\(A\\) is the determinant of the \\((n-1) \\times (n-1)\\) matrix that results from removing the \\(i\\)-th row and \\(j\\)-th column from \\(A\\).\n\nCofactor:\n\nThe cofactor \\(C_{ij}\\) of an element \\(a_{ij}\\) is given by: \\(C_{ij} = (-1)^{i+j} M_{ij}\\)\nThe sign factor \\((-1)^{i+j}\\) alternates according to the position of the element in the matrix.\n\n\nCofactor Expansion\nThe determinant of an \\(n \\times n\\) matrix \\(A\\) can be computed by expanding along any row or column. The cofactor expansion along the \\(i\\)-th row is given by:\n\\[\n\\det(A) = \\sum_{j=1}^n a_{ij} C_{ij}\n\\] Similarly, the cofactor expansion along the \\(j\\)-th column is: \\(\\det(A) = \\sum_{i=1}^n a_{ij} C_{ij}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-27",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-27",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nComputing determinants\nThe structure of the AR(1) vector process makes it easy to compute the polynomial for the determinant in therms of the coefficients of the process.\nThe roots of the polynomial can be computed using polyroot, passing in a vector of polynomial coefficients in increasing order. The magnitudes of the eigenvalues can using the base function Mod.\npolyroot(c(1,.2,3,.6)) |&gt; Mod()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-28",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-28",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nMoving Average Processes\n\n\nIn an MA(q) process the present value is a weighted sum on the current and previous errors. \\[y_t= \\epsilon_t +\\theta_1\\epsilon_{t-1};\\;\\text(MA(1))\\]\nMA(q) models describe processes where it takes a bit of time for the error (or â€œshockâ€) to dissipate\nThis type of expression may be used to describe a wide variety of stationary time series processes"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-29",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-29",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nMoving Average Processes\n\n\nThere is a duality of AR() and MA() processes.\nWe have already seen how AR processes can be expressed as MA() processes:\n\n\\[\n\\vec{y}_t = (1-A L)^{-1} \\vec{\\epsilon}_t = (\\sum_{i=0}^\\infty A^i\\mathrm{L}^i)\\vec{\\epsilon}_t\\\\\n\\]\nand MA() processes can be expressed as AR processes by a similar argument."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-30",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-30",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nARMA Processes\n\n\nA combination of an AR(1) and a MA(1) process is termed an ARMA(1,1) observation. \\[y_t=\\phi y_{t-1}+\\epsilon_t+\\theta\\epsilon_{t-1}\\]\nAN ARMA(p,q) process takes the form \\[y_t=\\sum_{j=1}^p\\phi_jy_{t-j}+\\epsilon_t+\\sum_{i=1}^q\\theta_i\\epsilon_{t-i}\\]\nThis model was popularized by Box & Jenkins, who developed a methodology that may be used to identify the terms that should be included in the model"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-31",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-31",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nLong memory & fractional differencing\n\n\n\nMost AR(p), MA(q) and ARMA(p,q) processes are termed short-memory process because the coefficients in the representation are dominated by exponential decay\nLong-memory (or persistent) time series are considered intermediate compromises between the short-memory models and integrated nonstationary processes"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---processes-32",
    "href": "slides/BSMM_8740_lec_06.html#time-series---processes-32",
    "title": "Time series methods",
    "section": "Time series - processes",
    "text": "Time series - processes\nARIMA\n\n\nAutoRegressive Integrated Moving Average combines three key concepts: autoregression (AR), differencing (I - for Integrated), and moving average (MA).\nAR (AutoRegressive): captures the relationship between an observation and lagged observations.\nI (Integrated): Differencing is used to make the time series stationary.\nMA (Moving Average): captures the relationship between an observation and a residual error from a moving average model.\nAn ARIMA model is denoted as ARIMA(p, d, q), where: - p is the number of lag observations in the model (AR part), - d is the number of times that the raw observations are differenced (I part), - q is the size of the moving average window (MA part)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing",
    "href": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing",
    "title": "Time series methods",
    "section": "Time series - exponential smoothing",
    "text": "Time series - exponential smoothing\nExponential smoothing or exponential moving average (EMA) is a rule of thumb technique for smoothing time series data using the exponential window function.\nFor a raw data series \\(\\{x_t\\}\\) the output of the exponential smoothing process is \\(\\{y_t\\}\\), where, for \\(0\\le\\alpha\\le1\\) and \\(t&gt;0\\)\n\\[\n\\begin{align*}\ny_0 & = x_0 \\\\\ny_t & = \\alpha x_t + (1-\\alpha)y_{t-1}\n\\end{align*}\n\\]\n\\(\\alpha\\) is called the smoothing factor."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-1",
    "title": "Time series methods",
    "section": "Time series - exponential smoothing",
    "text": "Time series - exponential smoothing\nThe time constant \\(\\tau\\) is the amount of time for the smoothed response of a unit step function to reach \\(1-1/e\\approx 63.2\\%\\) of the original signal.\n\\[\n\\begin{align*}\n\\alpha & = 1-e^{-\\Delta T/\\tau}\\\\\n\\tau & = -\\frac{\\Delta T}{ln(1-\\alpha)}\n\\end{align*}\n\\]\nwhere \\(\\Delta T\\) is the time interval."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-2",
    "href": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-2",
    "title": "Time series methods",
    "section": "Time series - exponential smoothing",
    "text": "Time series - exponential smoothing\nWe can also include a trend term\n\\[\n\\begin{align*}\ny_0 & = x_0 \\\\\nb_0 & = x_1 - x_0\\\\\n\\end{align*}\n\\]\nand for \\(t&gt;0\\), \\(0\\le\\alpha\\le1\\) and \\(0\\le\\beta\\le1\\)\n\\[\n\\begin{align*}\ny_t & = \\alpha x_t + (1-\\alpha)(y_{t-1} + b_{t-1})\\\\\nb_t & = \\beta(y_t-y_{t-1}) + (1-\\beta)b_{t-1}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-3",
    "href": "slides/BSMM_8740_lec_06.html#time-series---exponential-smoothing-3",
    "title": "Time series methods",
    "section": "Time series - exponential smoothing",
    "text": "Time series - exponential smoothing\nFinally we can also include a seasonality term, with a cycle length of \\(K\\) time intervals.\nand for \\(t&gt;0\\), \\(0\\le\\alpha\\le1\\), \\(0\\le\\beta\\le1\\) and \\(0\\le\\gamma\\le1\\)\n\\[\n\\begin{align*}\ny_t & = \\alpha \\frac{x_t}{c_{t-L}} + (1-\\alpha)(y_{t-1} + b_{t-1})\\\\\nb_t & = \\beta(y_t-y_{t-1}) + (1-\\beta)b_{t-1}\\\\\nc_t & = \\gamma\\frac{x_t}{y_t} + (1-\\gamma)c_{t-L}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---ets-models",
    "href": "slides/BSMM_8740_lec_06.html#time-series---ets-models",
    "title": "Time series methods",
    "section": "Time series - ETS models",
    "text": "Time series - ETS models\n\nETS stands for Error-Trend-Seasonality, and the exponential smoothing model is clearly in this class, with each component additive. The more general taxonomy is:\n\nError: â€œAdditiveâ€ (A), or â€œMultiplicativeâ€ (M);\nTrend: â€œNoneâ€ (N), â€œAdditiveâ€ (A), â€œAdditive dampedâ€ (Ad), â€œMultiplicativeâ€ (M), or â€œMultiplicative dampedâ€ (Md);\nSeasonality: â€œNoneâ€ (N), or â€œAdditiveâ€ (A), or â€œMultiplicativeâ€ (M).\n\nIn this taxonomy, the exponential smoothing model is denoted as ETS(A,A,A)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---ets-models-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---ets-models-1",
    "title": "Time series methods",
    "section": "Time series - ETS models",
    "text": "Time series - ETS models\nThe additive ETS models are shown below, and more detailed discussion can be found here"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---plotting",
    "href": "slides/BSMM_8740_lec_06.html#time-series---plotting",
    "title": "Time series methods",
    "section": "Time series - plotting",
    "text": "Time series - plotting\n\n&gt; timetk::bike_sharing_daily %&gt;% \n+   dplyr::slice_head(n=5) %&gt;% \n+   dplyr::glimpse()\n\nRows: 5\nColumns: 16\n$ instant    &lt;dbl&gt; 1, 2, 3, 4, 5\n$ dteday     &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-05\n$ season     &lt;dbl&gt; 1, 1, 1, 1, 1\n$ yr         &lt;dbl&gt; 0, 0, 0, 0, 0\n$ mnth       &lt;dbl&gt; 1, 1, 1, 1, 1\n$ holiday    &lt;dbl&gt; 0, 0, 0, 0, 0\n$ weekday    &lt;dbl&gt; 6, 0, 1, 2, 3\n$ workingday &lt;dbl&gt; 0, 0, 1, 1, 1\n$ weathersit &lt;dbl&gt; 2, 2, 1, 1, 1\n$ temp       &lt;dbl&gt; 0.344167, 0.363478, 0.196364, 0.200000, 0.226957\n$ atemp      &lt;dbl&gt; 0.363625, 0.353739, 0.189405, 0.212122, 0.229270\n$ hum        &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957\n$ windspeed  &lt;dbl&gt; 0.160446, 0.248539, 0.248309, 0.160296, 0.186900\n$ casual     &lt;dbl&gt; 331, 131, 120, 108, 82\n$ registered &lt;dbl&gt; 654, 670, 1229, 1454, 1518\n$ cnt        &lt;dbl&gt; 985, 801, 1349, 1562, 1600"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---plotting-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---plotting-1",
    "title": "Time series methods",
    "section": "Time series - plotting",
    "text": "Time series - plotting\nThe timetk::plot_time_series() function is a good way to to get a quick timeseries plot. From a tidy table we\n\nselect the time value and the columns we want to plot\npivot (longer) the columns we want to plot\nplot\n\nThe timetk::plot_time_series() function has many options that can be changed."
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---plotting-2",
    "href": "slides/BSMM_8740_lec_06.html#time-series---plotting-2",
    "title": "Time series methods",
    "section": "Time series - plotting",
    "text": "Time series - plotting\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;% \n+   dplyr::select(dteday, casual, registered) %&gt;% \n+   tidyr::pivot_longer(-dteday) %&gt;% \n+   timetk::plot_time_series(\n+     .date_var = dteday\n+     , .value = value\n+     , .color_var = name\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime downscaling\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;% \n+   timetk::summarise_by_time(\n+     .date_var = dteday\n+     , .by = \"week\"\n+     , .week_start = 7\n+     , causal = sum(casual)\n+     , registered = mean(registered)\n+     , max_cnt = max(cnt)\n+   )\n\n\n# A tibble: 106 Ã— 4\n   dteday     causal registered max_cnt\n   &lt;date&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 2010-12-26    331       654      985\n 2 2011-01-02    745      1235.    1606\n 3 2011-01-09    477      1167.    1421\n 4 2011-01-16    706      1183.    1927\n 5 2011-01-23    632       994.    1985\n 6 2011-01-30    550      1314.    1708\n 7 2011-02-06   1075      1450.    1746\n 8 2011-02-13   2333      1734.    2927\n 9 2011-02-20   1691      1405.    1969\n10 2011-02-27   2120      1631.    2402\n# â„¹ 96 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-1",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-1",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime upscaling\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;% \n+   dplyr::select(dteday, casual) %&gt;% \n+   timetk::pad_by_time(.date_var = dteday, .by = \"hour\") %&gt;% \n+   timetk::mutate_by_time(\n+     .date_var = dteday\n+     , .by = \"day\"\n+     , casual = sum(casual,na.rm=T)/24\n+   )\n\n\n# A tibble: 17,521 Ã— 2\n   dteday              casual\n   &lt;dttm&gt;               &lt;dbl&gt;\n 1 2011-01-01 00:00:00   13.8\n 2 2011-01-01 01:00:00   13.8\n 3 2011-01-01 02:00:00   13.8\n 4 2011-01-01 03:00:00   13.8\n 5 2011-01-01 04:00:00   13.8\n 6 2011-01-01 05:00:00   13.8\n 7 2011-01-01 06:00:00   13.8\n 8 2011-01-01 07:00:00   13.8\n 9 2011-01-01 08:00:00   13.8\n10 2011-01-01 09:00:00   13.8\n# â„¹ 17,511 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-2",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-2",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime filtering\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;%\n+   timetk::filter_by_time(\n+     .date_var = dteday\n+     , .start_date=\"2012-01-15\"\n+     , .end_date = \"2012-07-01\"\n+   ) %&gt;% \n+   timetk::plot_time_series(.date_var = dteday, casual)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-3",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-3",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime offsets\n\n\nCode\n&gt; require(timetk, quietly = FALSE)\n&gt; timetk::bike_sharing_daily %&gt;%\n+   timetk::filter_by_time(\n+     .date_var = dteday\n+     , .start_date=\"2012-01-15\"\n+     , .end_date = \"2012-01-15\" %+time% \"12 weeks\"\n+   ) %&gt;% \n+   timetk::plot_time_series(.date_var = dteday, casual)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-4",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-4",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\nmutate by period\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;%\n+   dplyr::select(dteday, casual) %&gt;% \n+   timetk::mutate_by_time(\n+     .date_var = dteday\n+     , .by = \"7 days\"\n+     , casual_mean = mean(casual)\n+     , casual_median = median(casual)\n+     , casual_max = max(casual)\n+     , casual_min = min(casual)\n+   )\n\n\n# A tibble: 731 Ã— 6\n   dteday     casual casual_mean casual_median casual_max casual_min\n   &lt;date&gt;      &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 2011-01-01    331       144             120        331         82\n 2 2011-01-02    131       144             120        331         82\n 3 2011-01-03    120       144             120        331         82\n 4 2011-01-04    108       144             120        331         82\n 5 2011-01-05     82       144             120        331         82\n 6 2011-01-06     88       144             120        331         82\n 7 2011-01-07    148       144             120        331         82\n 8 2011-01-08     68        46.1            43         68         25\n 9 2011-01-09     54        46.1            43         68         25\n10 2011-01-10     41        46.1            43         68         25\n# â„¹ 721 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-5",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-5",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\nsummarize by period\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;%\n+   timetk::summarize_by_time(\n+     .date_var = dteday\n+     , .by = \"7 days\"\n+     , casual_mean = mean(casual)\n+     , registered_mean = mean(registered)\n+     , windspeed_max = max(windspeed)\n+   )\n\n\n# A tibble: 119 Ã— 4\n   dteday     casual_mean registered_mean windspeed_max\n   &lt;date&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1 2011-01-01       144             1201.         0.249\n 2 2011-01-08        46.1           1147.         0.362\n 3 2011-01-15       119.            1203.         0.353\n 4 2011-01-22        86              981.         0.294\n 5 2011-01-29       102.            1130          0.187\n 6 2011-02-01       120.            1377.         0.278\n 7 2011-02-08       172.            1455.         0.418\n 8 2011-02-15       366             1618.         0.507\n 9 2011-02-22       233.            1546.         0.347\n10 2011-03-01       243.            1495          0.343\n# â„¹ 109 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-6",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-6",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ncreate a timeseries\n\n\nCode\n&gt; tibble::tibble(\n+   date = \n+     timetk::tk_make_timeseries(\n+       start_date = \"2024\"\n+       , length_out = 100\n+       , by = \"month\"\n+     )\n+   , values=1:100\n+ )\n\n\n# A tibble: 100 Ã— 2\n   date       values\n   &lt;date&gt;      &lt;int&gt;\n 1 2024-01-01      1\n 2 2024-02-01      2\n 3 2024-03-01      3\n 4 2024-04-01      4\n 5 2024-05-01      5\n 6 2024-06-01      6\n 7 2024-07-01      7\n 8 2024-08-01      8\n 9 2024-09-01      9\n10 2024-10-01     10\n# â„¹ 90 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-7",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-7",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ncreate a timeseries\n\n\nCode\n&gt; timetk::tk_make_holiday_sequence(\n+   start_date = \"2024\"\n+   , end_date = \"2026\"\n+   , calendar = \"TSX\"\n+ ) %&gt;% \n+   timetk::tk_get_holiday_signature(holiday_pattern = \"Thanksgiving\",locale_set = \"CA\", exchange = \"TSX\") %&gt;% \n+   dplyr::slice_head(n = 6) %&gt;% \n+   dplyr::glimpse()\n\n\nRows: 6\nColumns: 6\n$ index              &lt;date&gt; 2024-01-01, 2024-02-19, 2024-02-19, 2024-02-19, 2024-03-29, 2024-â€¦\n$ exch_TSX           &lt;dbl&gt; 1, 1, 1, 1, 1, 1\n$ locale_CA          &lt;dbl&gt; 0, 1, 1, 1, 0, 1\n$ CA_ThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0\n$ JP_ThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0\n$ US_ThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-8",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-8",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntimeseries transformations\n\nCode\n&gt; # plot wind speed\n&gt; timetk::bike_sharing_daily %&gt;% \n+   timetk::plot_time_series(dteday, windspeed, .title = \"Time Series - Raw\")\n&gt; # plot transformed speed\n&gt; timetk::bike_sharing_daily %&gt;% \n+   timetk::plot_time_series(\n+     dteday\n+     , timetk::box_cox_vec(windspeed, lambda=\"auto\",  silent = T)\n+     , .title = \"Time Series - Box Cox Tranformed\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---timetk-9",
    "href": "slides/BSMM_8740_lec_06.html#time-series---timetk-9",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntimeseries transformations\nSee Also\n\n\nLag Transformation: lag_vec()\nDifferencing Transformation: diff_vec()\nRolling Window Transformation: slidify_vec()\nLoess Smoothing Transformation: smooth_vec()\nFourier Series: fourier_vec()\nMissing Value Imputation for Time Series: ts_impute_vec(), ts_clean_vec()\n\nOther common transformations to reduce variance: log(), log1p() and sqrt()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#time-series---feature-engineering",
    "href": "slides/BSMM_8740_lec_06.html#time-series---feature-engineering",
    "title": "Time series methods",
    "section": "Time series - Feature engineering",
    "text": "Time series - Feature engineering\n\nCode\n&gt; subscribers_tbl   &lt;- readRDS(\"data/00_data/mailchimp_users.rds\")\n&gt; \n&gt; data_prepared_tbl &lt;- subscribers_tbl %&gt;% \n+   timetk::summarize_by_time(optin_time, .by=\"day\", optins=dplyr::n()) %&gt;% \n+   timetk::pad_by_time(.pad_value=0) %&gt;% \n+   # preprocessing\n+   dplyr::mutate(optins_trans=timetk::log_interval_vec(optins, limit_lower=0, offset=1)) %&gt;% \n+   dplyr::mutate(optins_trans=timetk::standardize_vec(optins_trans)) %&gt;% \n+   # fix missing vals at start\n+   timetk::filter_by_time(.start_date = \"2018-07-03\") %&gt;% \n+   # outliers clean\n+   dplyr::mutate(optins_trans_cleaned = timetk::ts_clean_vec(optins_trans, period=7)) %&gt;% \n+   dplyr::mutate(optins_trans=ifelse(optin_time %&gt;% timetk::between_time(\"2018-11-18\",\"2018-11-20\")\n+                              , optins_trans_cleaned\n+                              , optins_trans\n+                              )) %&gt;% \n+   dplyr::select(-optins, -optins_trans_cleaned)\n&gt;   \n&gt; data_prepared_tbl     # show the dt\n&gt; data_prepared_tbl %&gt;% # plot the table\n+   tidyr::pivot_longer(contains(\"trans\")) %&gt;% \n+   timetk::plot_time_series(optin_time,value,name) \n\n\n\n\n# A tibble: 609 Ã— 2\n   optin_time optins_trans\n   &lt;date&gt;            &lt;dbl&gt;\n 1 2018-07-03      -0.492 \n 2 2018-07-04      -0.153 \n 3 2018-07-05      -0.578 \n 4 2018-07-06      -0.413 \n 5 2018-07-07      -1.20  \n 6 2018-07-08      -1.66  \n 7 2018-07-09      -0.274 \n 8 2018-07-10      -0.212 \n 9 2018-07-11      -0.0986\n10 2018-07-12      -0.274 \n# â„¹ 599 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#date-features",
    "href": "slides/BSMM_8740_lec_06.html#date-features",
    "title": "Time series methods",
    "section": "Date Features",
    "text": "Date Features\n\n\nCode\n&gt; data_prep_signature_tbl &lt;- \n+   data_prepared_tbl %&gt;% \n+   timetk::tk_augment_timeseries_signature(\n+     .date_var = optin_time\n+   ) \n\n\n\n\nRows: 4\nColumns: 30\n$ optin_time   &lt;date&gt; 2018-07-03, 2018-07-04, 2018-07-05, 2018-07-06\n$ optins_trans &lt;dbl&gt; -0.4919060, -0.1534053, -0.5779424, -0.4133393\n$ index.num    &lt;dbl&gt; 1530576000, 1530662400, 1530748800, 1530835200\n$ diff         &lt;dbl&gt; NA, 86400, 86400, 86400\n$ year         &lt;int&gt; 2018, 2018, 2018, 2018\n$ year.iso     &lt;int&gt; 2018, 2018, 2018, 2018\n$ half         &lt;int&gt; 2, 2, 2, 2\n$ quarter      &lt;int&gt; 3, 3, 3, 3\n$ month        &lt;int&gt; 7, 7, 7, 7\n$ month.xts    &lt;int&gt; 6, 6, 6, 6\n$ month.lbl    &lt;ord&gt; July, July, July, July\n$ day          &lt;int&gt; 3, 4, 5, 6\n$ hour         &lt;int&gt; 0, 0, 0, 0\n$ minute       &lt;int&gt; 0, 0, 0, 0\n$ second       &lt;int&gt; 0, 0, 0, 0\n$ hour12       &lt;int&gt; 0, 0, 0, 0\n$ am.pm        &lt;int&gt; 1, 1, 1, 1\n$ wday         &lt;int&gt; 3, 4, 5, 6\n$ wday.xts     &lt;int&gt; 2, 3, 4, 5\n$ wday.lbl     &lt;ord&gt; Tuesday, Wednesday, Thursday, Friday\n$ mday         &lt;int&gt; 3, 4, 5, 6\n$ qday         &lt;int&gt; 3, 4, 5, 6\n$ yday         &lt;int&gt; 184, 185, 186, 187\n$ mweek        &lt;int&gt; 1, 1, 1, 1\n$ week         &lt;int&gt; 27, 27, 27, 27\n$ week.iso     &lt;int&gt; 27, 27, 27, 27\n$ week2        &lt;int&gt; 1, 1, 1, 1\n$ week3        &lt;int&gt; 0, 0, 0, 0\n$ week4        &lt;int&gt; 3, 3, 3, 3\n$ mday7        &lt;int&gt; 1, 1, 1, 1"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#trend",
    "href": "slides/BSMM_8740_lec_06.html#trend",
    "title": "Time series methods",
    "section": "Trend",
    "text": "Trend\n\n\nCode\n&gt; data_prep_signature_tbl %&gt;% \n+   timetk::plot_time_series_regression(\n+     .date_var = optin_time\n+     , .formula = optins_trans ~ index.num\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#non-linear-trends",
    "href": "slides/BSMM_8740_lec_06.html#non-linear-trends",
    "title": "Time series methods",
    "section": "Non-linear trends",
    "text": "Non-linear trends\n\nCode\n&gt; data_prep_signature_tbl %&gt;% \n+   timetk::plot_time_series_regression(\n+     .date_var = optin_time\n+     , .formula = optins_trans ~ splines::bs(index.num, degree=3)\n+     , .show_summary = FALSE\n+     , .title = \"B-spline, degree 3\"\n+   )\n&gt; data_prep_signature_tbl %&gt;% \n+   timetk::plot_time_series_regression(\n+     .date_var=optin_time\n+     , .formula=\n+       optins_trans ~ splines::ns(\n+         index.num\n+         , knots=quantile(index.num, probs=c(0.25, 0.5, 0.75)))\n+     , .show_summary = FALSE\n+     , .title = \"Cubic spline, 3 knots\"\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#seasonality",
    "href": "slides/BSMM_8740_lec_06.html#seasonality",
    "title": "Time series methods",
    "section": "Seasonality",
    "text": "Seasonality\n\nCode\n&gt; # Weekly Seasonality\n&gt; data_prep_signature_tbl %&gt;% \n+   timetk::plot_time_series_regression(\n+     .date_var=optin_time\n+     , .formula=optins_trans ~ wday.lbl + splines::bs(index.num, degree=3)\n+     , .show_summary = FALSE\n+     , .title = \"Weekday seasonality\"\n+   )\n&gt; # ** Monthly Seasonality\n&gt; data_prep_signature_tbl %&gt;% \n+   timetk::plot_time_series_regression(\n+     .date_var=optin_time\n+     , .formula=optins_trans ~ month.lbl + splines::bs(index.num, degree=3)\n+     , .show_summary = FALSE\n+     , .title = \"Monthly seasonality\"\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#seasonality-1",
    "href": "slides/BSMM_8740_lec_06.html#seasonality-1",
    "title": "Time series methods",
    "section": "Seasonality",
    "text": "Seasonality\n\n\nCode\n&gt; # ** Together with Trend\n&gt; model_formula_seasonality &lt;- as.formula(\n+   optins_trans ~ wday.lbl + month.lbl +\n+     splines::ns(index.num\n+                 , knots=quantile(index.num, probs=c(0.25, 0.5, 0.75))) + .\n+ )\n&gt; data_prep_signature_tbl %&gt;% \n+   timetk::plot_time_series_regression(\n+     .date_var=optin_time\n+     , .formula = model_formula_seasonality\n+     , .show_summary = FALSE\n+     , .title = \"Day and Month seasonality + Cubic spline, 3 knots\"\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#fourier-series",
    "href": "slides/BSMM_8740_lec_06.html#fourier-series",
    "title": "Time series methods",
    "section": "Fourier series",
    "text": "Fourier series\n\n\nCode\n&gt; data_prep_signature_tbl %&gt;% \n+   timetk::plot_acf_diagnostics(optin_time,optins_trans)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#fourier-series-1",
    "href": "slides/BSMM_8740_lec_06.html#fourier-series-1",
    "title": "Time series methods",
    "section": "Fourier series",
    "text": "Fourier series\n\n\nCode\n&gt; data_prep_fourier_tbl &lt;- \n+   data_prep_signature_tbl %&gt;% \n+   timetk::tk_augment_fourier(optin_time, .periods=c(7,14,30,90,365), .K=2)\n&gt; \n&gt; data_prep_fourier_tbl %&gt;% dplyr::slice_head(n=3) %&gt;% dplyr::glimpse()\n\n\nRows: 3\nColumns: 50\n$ optin_time           &lt;date&gt; 2018-07-03, 2018-07-04, 2018-07-05\n$ optins_trans         &lt;dbl&gt; -0.4919060, -0.1534053, -0.5779424\n$ index.num            &lt;dbl&gt; 1530576000, 1530662400, 1530748800\n$ diff                 &lt;dbl&gt; NA, 86400, 86400\n$ year                 &lt;int&gt; 2018, 2018, 2018\n$ year.iso             &lt;int&gt; 2018, 2018, 2018\n$ half                 &lt;int&gt; 2, 2, 2\n$ quarter              &lt;int&gt; 3, 3, 3\n$ month                &lt;int&gt; 7, 7, 7\n$ month.xts            &lt;int&gt; 6, 6, 6\n$ month.lbl            &lt;ord&gt; July, July, July\n$ day                  &lt;int&gt; 3, 4, 5\n$ hour                 &lt;int&gt; 0, 0, 0\n$ minute               &lt;int&gt; 0, 0, 0\n$ second               &lt;int&gt; 0, 0, 0\n$ hour12               &lt;int&gt; 0, 0, 0\n$ am.pm                &lt;int&gt; 1, 1, 1\n$ wday                 &lt;int&gt; 3, 4, 5\n$ wday.xts             &lt;int&gt; 2, 3, 4\n$ wday.lbl             &lt;ord&gt; Tuesday, Wednesday, Thursday\n$ mday                 &lt;int&gt; 3, 4, 5\n$ qday                 &lt;int&gt; 3, 4, 5\n$ yday                 &lt;int&gt; 184, 185, 186\n$ mweek                &lt;int&gt; 1, 1, 1\n$ week                 &lt;int&gt; 27, 27, 27\n$ week.iso             &lt;int&gt; 27, 27, 27\n$ week2                &lt;int&gt; 1, 1, 1\n$ week3                &lt;int&gt; 0, 0, 0\n$ week4                &lt;int&gt; 3, 3, 3\n$ mday7                &lt;int&gt; 1, 1, 1\n$ optin_time_sin7_K1   &lt;dbl&gt; -9.749279e-01, -7.818315e-01, 2.256296e-13\n$ optin_time_cos7_K1   &lt;dbl&gt; -0.2225209, 0.6234898, 1.0000000\n$ optin_time_sin7_K2   &lt;dbl&gt; 4.338837e-01, -9.749279e-01, 4.512593e-13\n$ optin_time_cos7_K2   &lt;dbl&gt; -0.9009689, -0.2225209, 1.0000000\n$ optin_time_sin14_K1  &lt;dbl&gt; 7.818315e-01, 4.338837e-01, -1.128148e-13\n$ optin_time_cos14_K1  &lt;dbl&gt; -0.6234898, -0.9009689, -1.0000000\n$ optin_time_sin14_K2  &lt;dbl&gt; -9.749279e-01, -7.818315e-01, 2.256296e-13\n$ optin_time_cos14_K2  &lt;dbl&gt; -0.2225209, 0.6234898, 1.0000000\n$ optin_time_sin30_K1  &lt;dbl&gt; 1.126564e-13, -2.079117e-01, -4.067366e-01\n$ optin_time_cos30_K1  &lt;dbl&gt; -1.0000000, -0.9781476, -0.9135455\n$ optin_time_sin30_K2  &lt;dbl&gt; -2.253127e-13, 4.067366e-01, 7.431448e-01\n$ optin_time_cos30_K2  &lt;dbl&gt; 1.0000000, 0.9135455, 0.6691306\n$ optin_time_sin90_K1  &lt;dbl&gt; -0.8660254, -0.8290376, -0.7880108\n$ optin_time_cos90_K1  &lt;dbl&gt; 0.5000000, 0.5591929, 0.6156615\n$ optin_time_sin90_K2  &lt;dbl&gt; -0.8660254, -0.9271839, -0.9702957\n$ optin_time_cos90_K2  &lt;dbl&gt; -0.5000000, -0.3746066, -0.2419219\n$ optin_time_sin365_K1 &lt;dbl&gt; -0.2135209, -0.2303057, -0.2470222\n$ optin_time_cos365_K1 &lt;dbl&gt; -0.9769385, -0.9731183, -0.9690098\n$ optin_time_sin365_K2 &lt;dbl&gt; 0.4171936, 0.4482293, 0.4787338\n$ optin_time_cos365_K2 &lt;dbl&gt; 0.9088176, 0.8939186, 0.8779601"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#visualization",
    "href": "slides/BSMM_8740_lec_06.html#visualization",
    "title": "Time series methods",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\n&gt; # Model\n&gt; model_formula_fourier &lt;- \n+   as.formula(\n+     optins_trans ~ . +\n+       splines::ns(index.num\n+                   , knots=quantile(index.num, probs=c(0.25, 0.5, 0.75)))\n+   )\n&gt; \n&gt; # Visualize\n&gt; data_prep_fourier_tbl %&gt;% \n+   timetk::filter_by_time(.start_date=\"2018-09-13\") %&gt;% \n+   timetk::plot_time_series_regression(\n+     .date_var = optin_time\n+     , .formula = model_formula_fourier\n+     , .show_summary = FALSE\n+     , .title = \"Fourier + Cubic spline, 3 knots\"\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#test-train-splits",
    "href": "slides/BSMM_8740_lec_06.html#test-train-splits",
    "title": "Time series methods",
    "section": "Test-train splits",
    "text": "Test-train splits\n\n\nCode\n&gt; dat &lt;- subscribers_tbl %&gt;% \n+   timetk::summarize_by_time(optin_time, .by=\"day\", optins=dplyr::n()) %&gt;% \n+   timetk::pad_by_time(.pad_value=0) %&gt;% \n+   timetk::filter_by_time(.start_date = \"2018-12-15\")\n&gt; \n&gt; # Split Data 80/20\n&gt; splits &lt;- \n+   timetk::time_series_split(\n+     data = dat\n+     , initial = \"12 months\"\n+     , assess = \"1 months\"\n+   )\n&gt; \n&gt; splits %&gt;%\n+   timetk::tk_time_series_cv_plan() %&gt;%\n+   timetk::plot_time_series_cv_plan(.date_var = optin_time, .value = optins)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#feature-engineering-w-recipes",
    "href": "slides/BSMM_8740_lec_06.html#feature-engineering-w-recipes",
    "title": "Time series methods",
    "section": "Feature engineering w/ recipes",
    "text": "Feature engineering w/ recipes\n\n\nCode\n&gt; time_rec &lt;- dat %&gt;% \n+   recipes::recipe(optins ~ ., data = rsample::training(splits)) %&gt;% \n+   timetk::step_log_interval(optins, limit_lower = 0, offset = 1) %&gt;% \n+   recipes::step_normalize(recipes::all_outcomes()) %&gt;% \n+   timetk::step_timeseries_signature(optin_time) %&gt;% \n+   timetk::step_fourier(optin_time, period = c(7,14,30,90,365), K=2)\n&gt; \n&gt; time_rec %&gt;% recipes::prep(training = rsample::training(splits)) %&gt;% \n+   recipes::bake(new_data = NULL) %&gt;% \n+   timetk::plot_time_series_regression(\n+     .date_var = optin_time\n+     , .formula = optins ~ .\n+     , .show_summary = FALSE\n+   )\n\n\n$optins\n[1] 0\n\n$optins\n[1] 400"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#workflows",
    "href": "slides/BSMM_8740_lec_06.html#workflows",
    "title": "Time series methods",
    "section": "Workflows",
    "text": "Workflows\n\n&gt; model_spec_arima &lt;- modeltime::arima_reg() %&gt;%\n+     parsnip::set_engine(\"auto_arima\")\n&gt; \n&gt; recipe_spec_fourier &lt;- \n+   recipes::recipe(\n+     optins ~ optin_time\n+     , data = rsample::training(splits)\n+   ) %&gt;%\n+     timetk::step_fourier(optin_time, period = c(7, 14, 30, 90), K = 1) \n&gt; \n&gt; workflow_fit_arima &lt;- workflows::workflow() %&gt;%\n+   workflows::add_recipe(recipe_spec_fourier) %&gt;%\n+   workflows::add_model(model_spec_arima) %&gt;%\n+   parsnip::fit(rsample::training(splits))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#workflows-1",
    "href": "slides/BSMM_8740_lec_06.html#workflows-1",
    "title": "Time series methods",
    "section": "Workflows",
    "text": "Workflows\n\n&gt; model_spec_lm &lt;- parsnip::linear_reg() %&gt;%\n+   parsnip::set_engine(\"lm\") \n&gt; \n&gt; recipe_spec_linear &lt;- \n+   recipes::recipe(\n+     optins ~ optin_time\n+     , data = rsample::training(splits)\n+   ) %&gt;%\n+     timetk::step_fourier(optin_time, period = c(7, 14, 30, 90), K = 1) \n&gt; \n&gt; workflow_fit_linear &lt;- workflows::workflow() %&gt;%\n+   workflows::add_recipe(recipe_spec_linear) %&gt;%\n+   workflows::add_model(model_spec_lm) %&gt;%\n+   parsnip::fit(rsample::training(splits))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#predict",
    "href": "slides/BSMM_8740_lec_06.html#predict",
    "title": "Time series methods",
    "section": "Predict",
    "text": "Predict\n\n\nCode\n&gt; models_tbl &lt;- \n+   modeltime::modeltime_table(workflow_fit_arima, workflow_fit_linear)\n&gt; \n&gt; calibration_tbl &lt;- models_tbl %&gt;%\n+   modeltime::modeltime_calibrate(new_data = rsample::testing(splits))\n&gt; \n&gt; calibration_tbl %&gt;%\n+   modeltime::modeltime_forecast(\n+     new_data    = rsample::testing(splits),\n+     actual_data = dat\n+   ) %&gt;%\n+   modeltime::plot_modeltime_forecast(\n+     .legend_max_width = 25, # For mobile screens\n+     .interactive      = TRUE\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#evaluate",
    "href": "slides/BSMM_8740_lec_06.html#evaluate",
    "title": "Time series methods",
    "section": "Evaluate",
    "text": "Evaluate\n\n\nCode\n&gt; calibration_tbl %&gt;%\n+   modeltime::modeltime_accuracy() %&gt;%\n+   modeltime::table_modeltime_accuracy(\n+     .interactive = FALSE\n+   )\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nREGRESSION WITH ARIMA(2,0,1)(2,0,0)[7] ERRORS\nTest\n60.82\n65.18\n0.6\n72.87\n124.54\n0.08\n\n\n2\nLM\nTest\n60.80\n65.60\n0.6\n68.92\n126.34\n0.02"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#re-fit",
    "href": "slides/BSMM_8740_lec_06.html#re-fit",
    "title": "Time series methods",
    "section": "Re-fit",
    "text": "Re-fit\n\n\nCode\n&gt; refit_tbl &lt;- calibration_tbl %&gt;%\n+   modeltime::modeltime_refit(data = dat)\n&gt; \n&gt; refit_tbl %&gt;%\n+   modeltime::modeltime_forecast(h = \"3 months\", actual_data = dat) %&gt;%\n+   modeltime::plot_modeltime_forecast(\n+     .legend_max_width = 12, # For mobile screens\n+     .interactive      = TRUE\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_06.html#recap",
    "href": "slides/BSMM_8740_lec_06.html#recap",
    "title": "Time series methods",
    "section": "Recap",
    "text": "Recap\n\nIn this section we have worked with the tidymodels and timetk packages to build a workflow that facilitates building and evaluating multiple models.\nCombined with the recipes package we now have a complete data modeling framework.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recap-of-last-lecture",
    "href": "slides/BSMM_8740_lec_02.html#recap-of-last-lecture",
    "title": "The recipes package",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\nLast week we introduced the tidyverse verbs used to manipulate (tidy) data\nWe also used the tidyverse verbs for exploratory data analysis and to engineer features into our datasets."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#todays-outline",
    "href": "slides/BSMM_8740_lec_02.html#todays-outline",
    "title": "The recipes package",
    "section": "Todayâ€™s Outline",
    "text": "Todayâ€™s Outline\n\nWeâ€™ll review the R model formula approach to model specification,\nWeâ€™ll introduce data pre-processing and design/model matrix generation with the recipes package, and\nWeâ€™ll show how using recipes will facilitate developing feature engineering workflows that can be applied to multiple datasets (e.g.Â train and test as well as cross-validation1 datasets)\n\nCross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#r-model-formulas",
    "href": "slides/BSMM_8740_lec_02.html#r-model-formulas",
    "title": "The recipes package",
    "section": "R Model Formulas",
    "text": "R Model Formulas\nHereâ€™s a simple formula used in a linear model to predict house prices (using the dataset Sacremento from the package modeldata):\n\n\nCode\n&gt; Sacramento &lt;- modeldata::Sacramento\n&gt; mod1 &lt;- stats::lm(\n+   log(price) ~ type + sqft\n+   , data = Sacramento\n+   , subset = beds &gt; 2\n+   )\n\n\n\nThe purpose of this code chunk:\n\nsubset some of the observations (using the subset argument)\ncreate a design matrix for 2 predictor variables (but 3 model terms)\nlog transform the outcome variables\nfit a linear regression model\n\nThe first two steps create the design- or model- matrix."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example",
    "href": "slides/BSMM_8740_lec_02.html#example",
    "title": "The recipes package",
    "section": "Example",
    "text": "Example\nThe dataset Sacramento has three categorical variables:\n\n\nCode\n&gt; skimr::skim(Sacramento) %&gt;% \n+   dplyr::select(c(skim_variable,contains('factor')) ) %&gt;% \n+   tidyr::drop_na() %&gt;% \n+   gt::gt() %&gt;% \n+   gtExtras:::gt_theme_espn() %&gt;% \n+   gt::tab_options( table.font.size = gt::px(20) ) %&gt;% \n+   gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nskim_variable\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\n\n\n\n\ncity\nFALSE\n37\nSAC: 438, ELK: 114, ROS: 48, CIT: 35\n\n\nzip\nFALSE\n68\nz95: 61, z95: 45, z95: 44, z95: 37\n\n\ntype\nFALSE\n3\nRes: 866, Con: 53, Mul: 13"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-design-matrix",
    "href": "slides/BSMM_8740_lec_02.html#example-design-matrix",
    "title": "The recipes package",
    "section": "Example design matrix",
    "text": "Example design matrix\n\n&gt;  mm &lt;- model.matrix(\n+    log(price) ~ type + sqft\n+    , data = Sacramento\n+ )\n\n\n\n   (Intercept) typeMulti_Family typeResidential sqft\n1            1                0               1  836\n2            1                0               1 1167\n3            1                0               1  796\n4            1                0               1  852\n5            1                0               1  797\n6            1                0               0 1122\n7            1                0               1 1104\n8            1                0               1 1177\n9            1                0               0  941\n10           1                0               1 1146\n11           1                0               1  909\n12           1                0               1 1289\n13           1                0               1  871\n14           1                0               1 1020"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#summary-model-formula-method",
    "href": "slides/BSMM_8740_lec_02.html#summary-model-formula-method",
    "title": "The recipes package",
    "section": "Summary: Model Formula Method",
    "text": "Summary: Model Formula Method\n\nModel formulas are very expressive in that they can represent model terms easily\nThe formula/terms framework does some elegant functional programming\nFunctions can be embedded inline to do fairly complex things (on single variables) and these can be applied to new data sets."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#model-formula-examples",
    "href": "slides/BSMM_8740_lec_02.html#model-formula-examples",
    "title": "The recipes package",
    "section": "Model formula examples",
    "text": "Model formula examples\n\nleave outinteractioncrossingnestingas-is\n\n\n\n&gt; model.matrix(log(price) ~ -1 + type + sqft, data = Sacramento) %&gt;% head()\n\n  typeCondo typeMulti_Family typeResidential sqft\n1         0                0               1  836\n2         0                0               1 1167\n3         0                0               1  796\n4         0                0               1  852\n5         0                0               1  797\n6         1                0               0 1122\n\n\n\n\n\n&gt; model.matrix(log(price) ~ type : sqft, data = Sacramento) %&gt;% head()\n\n  (Intercept) typeCondo:sqft typeMulti_Family:sqft typeResidential:sqft\n1           1              0                     0                  836\n2           1              0                     0                 1167\n3           1              0                     0                  796\n4           1              0                     0                  852\n5           1              0                     0                  797\n6           1           1122                     0                    0\n\n\n\n\n\n&gt; model.matrix(log(price) ~ type * sqft, data = Sacramento) %&gt;% head()\n\n  (Intercept) typeMulti_Family typeResidential sqft typeMulti_Family:sqft\n1           1                0               1  836                     0\n2           1                0               1 1167                     0\n3           1                0               1  796                     0\n4           1                0               1  852                     0\n5           1                0               1  797                     0\n6           1                0               0 1122                     0\n  typeResidential:sqft\n1                  836\n2                 1167\n3                  796\n4                  852\n5                  797\n6                    0\n\n\n\n\n\n&gt; model.matrix(log(price) ~ type %in% sqft, data = Sacramento) %&gt;% head()\n\n  (Intercept) typeCondo:sqft typeMulti_Family:sqft typeResidential:sqft\n1           1              0                     0                  836\n2           1              0                     0                 1167\n3           1              0                     0                  796\n4           1              0                     0                  852\n5           1              0                     0                  797\n6           1           1122                     0                    0\n\n\n\n\n\n&gt; model.matrix(log(price) ~ type + sqft + I(sqft^2), data = Sacramento) %&gt;% head()\n\n  (Intercept) typeMulti_Family typeResidential sqft I(sqft^2)\n1           1                0               1  836    698896\n2           1                0               1 1167   1361889\n3           1                0               1  796    633616\n4           1                0               1  852    725904\n5           1                0               1  797    635209\n6           1                0               0 1122   1258884\n\n\ncontrast with log(price) ~ type + sqft + sqft^2"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#summary-model-formula-method-1",
    "href": "slides/BSMM_8740_lec_02.html#summary-model-formula-method-1",
    "title": "The recipes package",
    "section": "Summary: Model Formula Method",
    "text": "Summary: Model Formula Method\nThere are significant limitations to what this framework can do and, in some cases, it can be very inefficient.\nThis is mostly due to being written well before large scale modeling and machine learning were commonplace."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#limitations-of-the-current-system",
    "href": "slides/BSMM_8740_lec_02.html#limitations-of-the-current-system",
    "title": "The recipes package",
    "section": "Limitations of the Current System",
    "text": "Limitations of the Current System\n\nFormulas are not very extensible especially with nested or sequential operations (e.g.Â y ~ scale(center(knn_impute(x)))).\nWhen used in modeling functions, you cannot recycle the previous computations.\nFor wide data sets with lots of columns, the formula method can be very inefficient and consume a significant proportion of the total execution time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#limitations-of-the-current-system-1",
    "href": "slides/BSMM_8740_lec_02.html#limitations-of-the-current-system-1",
    "title": "The recipes package",
    "section": "Limitations of the Current System",
    "text": "Limitations of the Current System\n\nMultivariate outcomes are kludgy by requiring cbind .\nFormulas have a limited set of roles for measurements (just predictor and outcome). Weâ€™ll look further at roles in the next two slides.\n\nA more in-depth discussion of these issues can be found in this blog post (recommended to read)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#variable-roles",
    "href": "slides/BSMM_8740_lec_02.html#variable-roles",
    "title": "The recipes package",
    "section": "Variable Roles",
    "text": "Variable Roles\nFormulas have been re-implemented in different packages for a variety of different reasons:\n\n&gt; # ?lme4::lmer\n&gt; # Subjects need to be in the data but are not part of the model\n&gt; lme4::lmer(Reaction ~ Days + (Days | Subject), data = lme4::sleepstudy)\n&gt; \n&gt; # BradleyTerry2\n&gt; # We want to make the outcomes to be a function of a \n&gt; # competitor-specific function of reach \n&gt; BradleyTerry2::BTm(outcome = 1, player1 = winner, player2 = loser,\n+     formula = ~ reach[..] + (1|..), \n+     data = boxers)\n&gt; \n&gt; # modeltools::ModelEnvFormula (using the modeltools package for formulas)\n&gt; # mob\n&gt; data(PimaIndiansDiabetes, package = 'mlbench')\n&gt; modeltools::ModelEnvFormula(diabetes ~ glucose | pregnant + mass +  age,\n+     data = PimaIndiansDiabetes)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#variable-roles-1",
    "href": "slides/BSMM_8740_lec_02.html#variable-roles-1",
    "title": "The recipes package",
    "section": "Variable Roles",
    "text": "Variable Roles\nA general list of possible variable roles could be:\n\n\noutcomes\npredictors\nstratification\nmodel performance data (e.g.Â loan amount to compute expected loss)\nconditioning or faceting variables (e.g.Â lattice or ggplot2)\nrandom effects or hierarchical model ID variables\ncase weights (*)\noffsets (*)\nerror terms (limited to Error in the aov function)(*)\n\n(*) Can be handled in formulas but are hard-coded into functions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recipes",
    "href": "slides/BSMM_8740_lec_02.html#recipes",
    "title": "The recipes package",
    "section": "Recipes",
    "text": "Recipes\nWe can approach the design matrix and preprocessing steps by first specifying a sequence of steps.\n\nprice is an outcome\ntype and sqft are predictors\nlog transform price\nconvert type to dummy variables"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recipes-1",
    "href": "slides/BSMM_8740_lec_02.html#recipes-1",
    "title": "The recipes package",
    "section": "Recipes",
    "text": "Recipes\nA recipe is a specification of intent.\nOne issue with the formula method is that it couples the specification for your predictors along with the model implementation.\nRecipes separate the planning from the doing.\n\n\n\n\n\n\nNote\n\n\nThe Recipes website is found at: https://topepo.github.io/recipes"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recipes-2",
    "href": "slides/BSMM_8740_lec_02.html#recipes-2",
    "title": "The recipes package",
    "section": "Recipes",
    "text": "Recipes\n\n\nrecipes workflow\n&gt; ## Create an initial recipe with only predictors and outcome\n&gt; rec &lt;- recipes::recipe(price ~ type + sqft, data = Sacramento)\n&gt; \n&gt; rec &lt;- rec %&gt;% \n+   recipes::step_log(price) %&gt;%\n+   recipes::step_dummy(type)\n&gt; \n&gt; # estimate any parameters\n&gt; rec_trained &lt;- recipes::prep(rec, training = Sacramento, retain = TRUE)\n&gt; # apply the computations to new_data\n&gt; design_mat  &lt;- recipes::bake(rec_trained, new_data = Sacramento)\n\n\nOnce created, a recipe can be prepped on training data then baked with any other data.\n\n\nthe prep step calculates and stores variables related to the steps (e.g.Â (min,max) for scaling), using the training data\nthe bake step applies the steps to new data"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#selecting-variables",
    "href": "slides/BSMM_8740_lec_02.html#selecting-variables",
    "title": "The recipes package",
    "section": "Selecting Variables",
    "text": "Selecting Variables\nIn the last slide, we used dplyr-like syntax for selecting variables such as step_dummy(type).\nIn some cases, the names of the predictors may not be known at the time when you construct a recipe (or model formula). For example:\n\ndummy variable columns\nPCA feature extraction when you keep components that capture \\(\\mathrm{X}\\%\\) of the variability.\ndiscretized predictors with dynamic bins"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-1",
    "href": "slides/BSMM_8740_lec_02.html#example-1",
    "title": "The recipes package",
    "section": "Example",
    "text": "Example\nUsing the airquality dataset in the datasets package\n\n&gt; dat &lt;- datasets::airquality\n&gt; \n&gt; dat %&gt;% skimr::skim() %&gt;% \n+   dplyr::select(skim_variable:numeric.sd) %&gt;% \n+   gt::gt() %&gt;% \n+   gtExtras:::gt_theme_espn() %&gt;% \n+   gt::tab_options( table.font.size = gt::px(20) ) %&gt;% \n+   gt::as_raw_html()\n\n\n  \n  \n\n\n\nskim_variable\nn_missing\ncomplete_rate\nnumeric.mean\nnumeric.sd\n\n\n\n\nOzone\n37\n0.7581699\n42.129310\n32.987885\n\n\nSolar.R\n7\n0.9542484\n185.931507\n90.058422\n\n\nWind\n0\n1.0000000\n9.957516\n3.523001\n\n\nTemp\n0\n1.0000000\n77.882353\n9.465270\n\n\nMonth\n0\n1.0000000\n6.993464\n1.416522\n\n\nDay\n0\n1.0000000\n15.803922\n8.864520"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-create-basic-recipe",
    "href": "slides/BSMM_8740_lec_02.html#example-create-basic-recipe",
    "title": "The recipes package",
    "section": "Example: create basic recipe",
    "text": "Example: create basic recipe\n\n&gt; # create recipe\n&gt; aq_recipe &lt;- recipes::recipe(Ozone ~ ., data = aq_df_train)\n&gt; \n&gt; summary(aq_recipe)\n\n# A tibble: 6 Ã— 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 Solar.R  &lt;chr [2]&gt; predictor original\n2 Wind     &lt;chr [2]&gt; predictor original\n3 Temp     &lt;chr [2]&gt; predictor original\n4 Month    &lt;chr [2]&gt; predictor original\n5 Day      &lt;chr [2]&gt; predictor original\n6 Ozone    &lt;chr [2]&gt; outcome   original"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-summary-of-basic-recipe",
    "href": "slides/BSMM_8740_lec_02.html#example-summary-of-basic-recipe",
    "title": "The recipes package",
    "section": "Example: summary of basic recipe",
    "text": "Example: summary of basic recipe\n\n&gt; # update roles for variables with missing data\n&gt; aq_recipe &lt;- aq_recipe %&gt;% \n+   recipes::update_role(Ozone, Solar.R, new_role = 'NA_Variable')\n&gt; \n&gt; summary(aq_recipe)\n\n# A tibble: 6 Ã— 4\n  variable type      role        source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;       &lt;chr&gt;   \n1 Solar.R  &lt;chr [2]&gt; NA_Variable original\n2 Wind     &lt;chr [2]&gt; predictor   original\n3 Temp     &lt;chr [2]&gt; predictor   original\n4 Month    &lt;chr [2]&gt; predictor   original\n5 Day      &lt;chr [2]&gt; predictor   original\n6 Ozone    &lt;chr [2]&gt; NA_Variable original"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-add-recipe-steps",
    "href": "slides/BSMM_8740_lec_02.html#example-add-recipe-steps",
    "title": "The recipes package",
    "section": "Example: add recipe steps",
    "text": "Example: add recipe steps\n\n&gt; aq_recipe &lt;- aq_recipe %&gt;% \n+   # impute Ozone missing values using the mean\n+   step_impute_mean(has_role('NA_Variable'), -Solar.R) %&gt;%\n+   # impute Solar.R missing values using knn\n+   step_impute_knn(contains('.R'), neighbors = 3) %&gt;%\n+   # center all variable except the NA_Variable\n+   step_center(all_numeric(), -has_role('NA_Variable')) %&gt;%\n+   # scale all variable except the NA_Variable\n+   step_scale(all_numeric(), -has_role('NA_Variable'))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-recipe-prep-and-bake",
    "href": "slides/BSMM_8740_lec_02.html#example-recipe-prep-and-bake",
    "title": "The recipes package",
    "section": "Example: recipe prep and bake",
    "text": "Example: recipe prep and bake\n\n&gt; # prep with training data\n&gt; aq_prep_train &lt;- aq_recipe %&gt;% prep(aq_df_train)\n&gt; \n&gt; # bake with testing data\n&gt; aq_bake &lt;- aq_prep_train %&gt;% bake(aq_df_test)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-recipe-prep-values",
    "href": "slides/BSMM_8740_lec_02.html#example-recipe-prep-values",
    "title": "The recipes package",
    "section": "Example: recipe prep values",
    "text": "Example: recipe prep values\nThe prepped recipe is a data structure that contains any computed values.\n\n&gt; aq_prep_train |&gt; recipes::tidy()\n\n# A tibble: 4 Ã— 6\n  number operation type        trained skip  id               \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;            \n1      1 step      impute_mean TRUE    FALSE impute_mean_iJ0Ps\n2      2 step      impute_knn  TRUE    FALSE impute_knn_OUyEi \n3      3 step      center      TRUE    FALSE center_7iQB6     \n4      4 step      scale       TRUE    FALSE scale_QDUIq"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-recipe-prep-values-1",
    "href": "slides/BSMM_8740_lec_02.html#example-recipe-prep-values-1",
    "title": "The recipes package",
    "section": "Example: recipe prep values",
    "text": "Example: recipe prep values\nWe can examine any computed values by using the step number as an argument to recipes::tidy.\n\n&gt; aq_prep_train |&gt; recipes::tidy(3)\n\n# A tibble: 4 Ã— 3\n  terms value id          \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 Wind   9.66 center_7iQB6\n2 Temp  78.7  center_7iQB6\n3 Month  7.06 center_7iQB6\n4 Day   15.9  center_7iQB6"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#example-change-roles-again",
    "href": "slides/BSMM_8740_lec_02.html#example-change-roles-again",
    "title": "The recipes package",
    "section": "Example: change roles again",
    "text": "Example: change roles again\nHere we update the original recipe to set the required roles.\n\n&gt; aq_recipe &lt;- aq_recipe %&gt;% \n+   recipes::update_role(Ozone, new_role = 'outcome') %&gt;% \n+   recipes::update_role(Solar.R, new_role = 'predictor')"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recommended-baseline-steps",
    "href": "slides/BSMM_8740_lec_02.html#recommended-baseline-steps",
    "title": "The recipes package",
    "section": "Recommended Baseline Steps",
    "text": "Recommended Baseline Steps\nBaseline preprocessing methods can be categorized as:\n\n\ndummy: Do qualitative predictors require a numeric encoding (e.g., via dummy variables or other methods)?\nzv: Should columns with a single unique value be removed?\nimpute: If some predictors are missing, should they be estimated via imputation?\ndecorrelate: If there are correlated predictors, should this correlation be mitigated? This might mean filtering out predictors, using principal component analysis, or a model-based technique (e.g., regularization).\nnormalize: Should predictors be centered and scaled?\ntransform: Is it helpful to transform predictors to be more symmetric?\n\nSee recommended preprocessing for recipe steps."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#available-steps",
    "href": "slides/BSMM_8740_lec_02.html#available-steps",
    "title": "The recipes package",
    "section": "Available Steps",
    "text": "Available Steps\n\nBasic: logs, roots, polynomials, logits, hyperbolics\nEncodings: dummy variables, â€œotherâ€ factor level collapsing, discretization\nDate Features: Encodings for day/doy/month etc, holiday indicators\nFilters: correlation, near-zero variables, linear dependencies\nImputation: K-nearest neighbors, bagged trees, mean/mode imputation"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#available-steps-1",
    "href": "slides/BSMM_8740_lec_02.html#available-steps-1",
    "title": "The recipes package",
    "section": "Available Steps",
    "text": "Available Steps\n\nNormalization/Transformations: center, scale, range, Box-Cox, Yeo-Johnson\nDimension Reduction: PCA, kernel PCA, ICA, Isomap, data depth features, class distances\nOthers: spline basis functions, interactions, spatial sign\n\nMore on the way (i.e.Â autoencoders, more imputation methods, etc.)\nOne of the package vignettes shows how to write your own step functions."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#extending",
    "href": "slides/BSMM_8740_lec_02.html#extending",
    "title": "The recipes package",
    "section": "Extending",
    "text": "Extending\nNeed to add more pre-processing or other operations?\n\n&gt; standardized &lt;- rec_trained %&gt;%\n+   recipes::step_center(recipes::all_numeric()) %&gt;%\n+   recipes::step_scale(recipes::all_numeric()) %&gt;%\n+   recipes::step_pca(recipes::all_numeric())\n&gt;           \n&gt; ## Only estimate the new parts:\n&gt; standardized &lt;- recipes::prep(standardized)\n\nIf an initial step is computationally expensive, you donâ€™t have to redo those operations to add more."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#extending-1",
    "href": "slides/BSMM_8740_lec_02.html#extending-1",
    "title": "The recipes package",
    "section": "Extending",
    "text": "Extending\nRecipes can also be created with different roles manually (note: no formula)\n\n&gt; rec &lt;- \n+   recipes::recipe(data = Sacramento) %&gt;%\n+   recipes::update_role(price, new_role = \"outcome\") %&gt;%\n+   recipes::update_role(type, sqft, new_role = \"predictor\") %&gt;%\n+   recipes::update_role(zip, new_role = \"strata\")\n\nAlso, the sequential nature of steps means that steps donâ€™t have to be R operations and could call other compute engines (e.g.Â Weka, scikit-learn, Tensorflow, etc. )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#extending-2",
    "href": "slides/BSMM_8740_lec_02.html#extending-2",
    "title": "The recipes package",
    "section": "Extending",
    "text": "Extending\nWe can create wrappers to work with recipes too:\n\n&gt; lin_reg.recipe &lt;- function(rec, data, ...) {\n+   trained &lt;- recipes::prep(rec, training = data)\n+   lm.fit(\n+     x = recipes::bake(trained, newdata = data, all_predictors())\n+     , y = recipes::bake(trained, newdata = data, all_outcomes())\n+     , ...\n+   )\n+ }"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#an-example",
    "href": "slides/BSMM_8740_lec_02.html#an-example",
    "title": "The recipes package",
    "section": "An Example",
    "text": "An Example\nKuhn and Johnson (2013) analyze a data set where thousands of cells are determined to be well-segmented (WS) or poorly segmented (PS) based on 58 image features. We would like to make predictions of the segmentation quality based on these features.\n\n\n\n\n\n\n\nNote\n\n\nThe dataset segmentationData is in the package caret and represents the results of automated microscopy to collect images of cultured cells. The images are subjected to segmentation algorithms to identify cellular structures and quantitate their morphology, for hundreds to millions of individual cells."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#an-example-1",
    "href": "slides/BSMM_8740_lec_02.html#an-example-1",
    "title": "The recipes package",
    "section": "An Example",
    "text": "An Example\nThe segmentationData dataset has 61 columns\n\n&gt; data(segmentationData, package = \"caret\")\n&gt; \n&gt; seg_train &lt;- segmentationData %&gt;% \n+   dplyr::filter(Case == \"Train\") %&gt;% \n+   dplyr::select(-Case)\n&gt; \n&gt; seg_test  &lt;- segmentationData %&gt;% \n+   dplyr::filter(Case == \"Test\")  %&gt;% \n+   dplyr::select(-Case)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#a-simple-recipe",
    "href": "slides/BSMM_8740_lec_02.html#a-simple-recipe",
    "title": "The recipes package",
    "section": "A Simple Recipe",
    "text": "A Simple Recipe\n\n\nCode\n&gt; rec &lt;- recipes::recipe(Class  ~ ., data = seg_train)\n&gt; \n&gt; basic &lt;- rec %&gt;%\n+   # the column Cell contains identifiers\n+   recipes::update_role(Cell, new_role = 'ID') %&gt;%\n+   # Correct some predictors for skewness\n+   recipes::step_YeoJohnson(recipes::all_predictors()) %&gt;%\n+   # Standardize the values\n+   recipes::step_center(recipes::all_predictors()) %&gt;%\n+   recipes::step_scale(recipes::all_predictors())\n&gt; \n&gt; # Estimate the transformation and standardization parameters \n&gt; basic &lt;- \n+   recipes::prep(\n+     basic\n+     , training = seg_train\n+     , verbose = FALSE\n+     , retain = TRUE\n+   )  \n\n\n\n\n\n\n\n\n\nNote\n\n\nThe Yeo-Johnson is similar to the Box-Cox method, however it allows for the transformation of nonpositive data as well. A Box Cox transformation is a transformation of non-normal dependent variables into a normal shape. Both transformations have a single parameter \\(\\lambda\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#a-simple-recipe-1",
    "href": "slides/BSMM_8740_lec_02.html#a-simple-recipe-1",
    "title": "The recipes package",
    "section": "A Simple Recipe",
    "text": "A Simple Recipe\nWe can examine the center, scale, and Yeo Johnson parameters computed for each continuous measurement.\n\nYeo Johnsonmeansstd deviations\n\n\n\n&gt; basic |&gt; recipes::tidy(1) |&gt; dplyr::slice_head(n=8)\n\n# A tibble: 8 Ã— 3\n  terms                value id              \n  &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;           \n1 AngleCh1             0.806 YeoJohnson_5ALzj\n2 AreaCh1             -0.861 YeoJohnson_5ALzj\n3 AvgIntenCh1         -0.340 YeoJohnson_5ALzj\n4 AvgIntenCh2          0.434 YeoJohnson_5ALzj\n5 AvgIntenCh3          0.219 YeoJohnson_5ALzj\n6 AvgIntenCh4          0.213 YeoJohnson_5ALzj\n7 DiffIntenDensityCh1 -0.929 YeoJohnson_5ALzj\n8 DiffIntenDensityCh3  0.116 YeoJohnson_5ALzj\n\n\n\n\n\n&gt; basic |&gt; recipes::tidy(2) |&gt; dplyr::slice_head(n=8)\n\n# A tibble: 8 Ã— 3\n  terms                    value id          \n  &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;       \n1 AngleCh1                45.0   center_HytOY\n2 AreaCh1                  1.15  center_HytOY\n3 AvgIntenCh1              2.24  center_HytOY\n4 AvgIntenCh2             17.4   center_HytOY\n5 AvgIntenCh3              6.99  center_HytOY\n6 AvgIntenCh4              7.56  center_HytOY\n7 ConvexHullAreaRatioCh1   1.21  center_HytOY\n8 ConvexHullPerimRatioCh1  0.893 center_HytOY\n\n\n\n\n\n&gt; basic |&gt; recipes::tidy(3) |&gt; dplyr::slice_head(n=8)\n\n# A tibble: 8 Ã— 3\n  terms                      value id         \n  &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;      \n1 AngleCh1                21.1     scale_1WyUk\n2 AreaCh1                  0.00334 scale_1WyUk\n3 AvgIntenCh1              0.204   scale_1WyUk\n4 AvgIntenCh2              9.42    scale_1WyUk\n5 AvgIntenCh3              2.50    scale_1WyUk\n6 AvgIntenCh4              3.04    scale_1WyUk\n7 ConvexHullAreaRatioCh1   0.210   scale_1WyUk\n8 ConvexHullPerimRatioCh1  0.0777  scale_1WyUk"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca",
    "href": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca",
    "title": "The recipes package",
    "section": "More sophisticated steps: PCA",
    "text": "More sophisticated steps: PCA\nPrincipal Component Analysis (PCA) is a technique used in data analysis to simplify a large dataset (many measurements/columns) by reducing its number of dimensions (columns) while still retaining as much important information as possible.\nA pretty good description of PCA can be found here."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-1",
    "href": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-1",
    "title": "The recipes package",
    "section": "More sophisticated steps: PCA",
    "text": "More sophisticated steps: PCA\nPCA step-by-step:\n\n\nStandardize the Data:\n\nBefore applying PCA, we often standardize the data, which means adjusting all variables to have the same scale. This is important because PCA is sensitive to the scale of the data.\n\nFind the Principal Components:\n\nPCA identifies the directions (principal components) in which the data varies the most. Think of these as new axes in a new coordinate system that best capture the variation in the data.\nThe first principal component captures the most variation. The second principal component is orthogonal (at a right angle) to the first and captures the next most variation, and so on.\n\nTransform the Data:\n\nWe then transform the original data into this new set of principal components. Each data point can now be represented in terms of these principal components rather than the original variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-2",
    "href": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-2",
    "title": "The recipes package",
    "section": "More sophisticated steps: PCA",
    "text": "More sophisticated steps: PCA\n\nYou have test scores in Math, Science, and English. You want to find a way to understand overall performance without looking at all three subjects separately.\n\nOriginal Data: You have three scores for each student: Math, Science, and English.\nStandardize the Data: You adjust the scores so that Math, Science, and English scores are on the same scale.\nFind Principal Components:\n\nPCA finds that the first principal component might be a combination of Math and Science scores, capturing the overall academic ability in quantitative subjects.\nThe second principal component might represent the difference between scores in English and the average of Math and Science scores, capturing a different aspect of performance.\n\nTransform the Data:\n\nEach studentâ€™s performance can now be described using these new principal components instead of the original scores. For example, a student might have a high score on the first principal component (strong in Math and Science) but a lower score on the second (relatively weaker in English)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-3",
    "href": "slides/BSMM_8740_lec_02.html#more-sophisticated-steps-pca-3",
    "title": "The recipes package",
    "section": "More sophisticated steps: PCA",
    "text": "More sophisticated steps: PCA\n\nBenefits of PCA\n\nSimplification: Reduces the number of variables, making it easier to analyze and visualize the data.\nNoise Reduction: Helps to remove noise from the data by focusing on the main components that capture the most variation.\nFeature Extraction: Identifies the most important variables (principal components) that explain the majority of the variance in the data.\n\nPCA is like finding the most important â€œdirectionsâ€ in your data, where most of the interesting stuff happens. It helps you see the big picture by reducing complexity while keeping the essential information. This makes it a powerful tool for data analysis, especially when dealing with high-dimensional datasets."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#principal-component-analysis",
    "href": "slides/BSMM_8740_lec_02.html#principal-component-analysis",
    "title": "The recipes package",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n\nCode\n&gt; pca &lt;- basic %&gt;% \n+   recipes::step_pca(\n+     recipes::all_predictors()\n+     , num_comp = 5\n+   )\n\n\n\n\n# A tibble: 60 Ã— 4\n   variable                type      role      source  \n   &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Cell                    &lt;chr [2]&gt; ID        original\n 2 AngleCh1                &lt;chr [2]&gt; predictor original\n 3 AreaCh1                 &lt;chr [2]&gt; predictor original\n 4 AvgIntenCh1             &lt;chr [2]&gt; predictor original\n 5 AvgIntenCh2             &lt;chr [2]&gt; predictor original\n 6 AvgIntenCh3             &lt;chr [2]&gt; predictor original\n 7 AvgIntenCh4             &lt;chr [2]&gt; predictor original\n 8 ConvexHullAreaRatioCh1  &lt;chr [2]&gt; predictor original\n 9 ConvexHullPerimRatioCh1 &lt;chr [2]&gt; predictor original\n10 DiffIntenDensityCh1     &lt;chr [2]&gt; predictor original\n# â„¹ 50 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#principal-component-analysis-1",
    "href": "slides/BSMM_8740_lec_02.html#principal-component-analysis-1",
    "title": "The recipes package",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca %&lt;&gt;% recipes::prep() \n\n\nsummarycomponents\n\n\n\n&gt; pca %&gt;% summary()\n\n# A tibble: 7 Ã— 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 Cell     &lt;chr [2]&gt; ID        original\n2 Class    &lt;chr [3]&gt; outcome   original\n3 PC1      &lt;chr [2]&gt; predictor derived \n4 PC2      &lt;chr [2]&gt; predictor derived \n5 PC3      &lt;chr [2]&gt; predictor derived \n6 PC4      &lt;chr [2]&gt; predictor derived \n7 PC5      &lt;chr [2]&gt; predictor derived \n\n\n\n\n\n&gt; pca %&gt;% recipes::tidy(4)\n\n# A tibble: 3,364 Ã— 4\n   terms                      value component id       \n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 AngleCh1                 0.00521 PC1       pca_Nl9ky\n 2 AreaCh1                  0.0823  PC1       pca_Nl9ky\n 3 AvgIntenCh1             -0.204   PC1       pca_Nl9ky\n 4 AvgIntenCh2             -0.209   PC1       pca_Nl9ky\n 5 AvgIntenCh3             -0.0873  PC1       pca_Nl9ky\n 6 AvgIntenCh4             -0.203   PC1       pca_Nl9ky\n 7 ConvexHullAreaRatioCh1   0.191   PC1       pca_Nl9ky\n 8 ConvexHullPerimRatioCh1 -0.181   PC1       pca_Nl9ky\n 9 DiffIntenDensityCh1     -0.185   PC1       pca_Nl9ky\n10 DiffIntenDensityCh3     -0.0760  PC1       pca_Nl9ky\n# â„¹ 3,354 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#principal-component-analysis-2",
    "href": "slides/BSMM_8740_lec_02.html#principal-component-analysis-2",
    "title": "The recipes package",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n&gt; pca %&lt;&gt;% \n+   recipes::bake(\n+     new_data = seg_test\n+     , everything()\n+   )\n&gt; pca[1:8, 1:7]\n\n# A tibble: 8 Ã— 7\n       Cell Class   PC1    PC2    PC3    PC4   PC5\n      &lt;int&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 207827637 PS     4.86 -5.85  -0.891 -4.13  1.84 \n2 207932455 PS     3.28 -1.51   0.353 -2.24  0.441\n3 207827656 WS    -7.03 -1.77  -2.42  -0.652 3.22 \n4 207827659 WS    -6.96 -2.08  -2.89  -1.79  3.20 \n5 207827661 PS     6.52 -3.77  -0.924 -2.61  2.49 \n6 207932479 WS     2.87  1.66   1.75  -5.41  0.324\n7 207932480 WS     2.72  0.433 -1.05  -5.45  1.18 \n8 207827711 WS    -3.01  1.94   2.68  -0.409 3.55"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#principal-component-analysis-3",
    "href": "slides/BSMM_8740_lec_02.html#principal-component-analysis-3",
    "title": "The recipes package",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\n\nPCs are predictive\n&gt; pca %&gt;% ggplot(aes(x = PC1, y = PC2, color = Class)) + \n+   geom_point(alpha = .4) +\n+   theme_bw(base_size = 25)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recap",
    "href": "slides/BSMM_8740_lec_02.html#recap",
    "title": "The recipes package",
    "section": "Recap",
    "text": "Recap\n\nWeâ€™ve used the recipes package to create a workflow for data pre-processing and feature engineering\nThe recipe verbs define the pre-processing and feature engineering steps\nusing the recipe object, the verb prep prepares the data on a training set, storing the meta-parameters.\nthe verb bake applies the prepped recipe to new data, using the meta-parameters."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02.html#recap-1",
    "href": "slides/BSMM_8740_lec_02.html#recap-1",
    "title": "The recipes package",
    "section": "Recap",
    "text": "Recap\n\nWhen first created, the recipe object contains the the steps defined for pre-processing\nOnce the recipe has been prepped, usually on training data, it contains the calculations required to perform pre-processing of data in the context of the training set, e.g.Â mean and stdev of normalized columns, PCA components, etc. ( the meta-parameters).\nthe prepped recipe is used to preprocess datasets in context of the training data. E.g. to normalize a column in the test data we subtract the corresponding training set mean and divide by the corresponding training set stdev.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#announcements",
    "href": "slides/BSMM_8740_lec_01.html#announcements",
    "title": "Tidyverse, EDA & git",
    "section": "Announcements",
    "text": "Announcements\n\n\nPlease go to the course website to review the weekly slides, access the labs, read the syllabus, etc.\nMy intent is to have assigned lab exercises each week, which will be started during class.\n\nLab assignments will be due at 5:00pm sharp on the Sunday following the lecture.\n\nLab 1 is due Sunday September 15 at 5pm.\nMy regular office hour will be on Wednesday from 2:30pm - 3:30pm or via MS Teams as requested. Please email ahead of time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#expected-course-topics",
    "href": "slides/BSMM_8740_lec_01.html#expected-course-topics",
    "title": "Tidyverse, EDA & git",
    "section": "Expected Course Topics",
    "text": "Expected Course Topics\n\n\n\n\n  \n  \n\n\n\nweek\ntopic\n\n\n\n\n1\nThe Tidyverse, EDA & Git\n\n\n2\nThe Recipes Package\n\n\n3\nRegression Methods\n\n\n4\nThe Tidymodels Packages\n\n\n5\nClassification & Clustering Methods\n\n\n6\nTime Series Methods\n\n\n7\nCausality: DAGs\n\n\n8\nCausality: Methods\n\n\n9\nMonte-Carlo Methods\n\n\n10\nBayesian Methods"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#todays-outline",
    "href": "slides/BSMM_8740_lec_01.html#todays-outline",
    "title": "Tidyverse, EDA & git",
    "section": "Todayâ€™s Outline",
    "text": "Todayâ€™s Outline\n\nIntroduction to Tidy data & Tidyverse syntax in R.\nIntroduction to EDA and feature egineering.\nIntroduction to Git workflows and version control."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidy-data",
    "href": "slides/BSMM_8740_lec_01.html#tidy-data",
    "title": "Tidyverse, EDA & git",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nA dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative).\nValues are organised in two ways. Every value belongs to a variable and an observation.\nA variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidy-data-1",
    "href": "slides/BSMM_8740_lec_01.html#tidy-data-1",
    "title": "Tidyverse, EDA & git",
    "section": "Tidy Data",
    "text": "Tidy Data\nTidy data in practice:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidy-data-examples",
    "href": "slides/BSMM_8740_lec_01.html#tidy-data-examples",
    "title": "Tidyverse, EDA & git",
    "section": "Tidy Data Examples",
    "text": "Tidy Data Examples\n\ntable 1table 2table 3\n\n\n\n\n# A tibble: 6 Ã— 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\n\n\n# A tibble: 12 Ã— 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\n\n# A tibble: 6 Ã— 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidyverse-principles",
    "href": "slides/BSMM_8740_lec_01.html#tidyverse-principles",
    "title": "Tidyverse, EDA & git",
    "section": "Tidyverse principles",
    "text": "Tidyverse principles\n\nDesign for humans\nReuse existing data structures\nDesign for the pipe and functional programming"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidyverse-packages",
    "href": "slides/BSMM_8740_lec_01.html#tidyverse-packages",
    "title": "Tidyverse, EDA & git",
    "section": "Tidyverse packages",
    "text": "Tidyverse packages\nSome packages in the tidyverse."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#a-grammar-for-data-wrangling",
    "href": "slides/BSMM_8740_lec_01.html#a-grammar-for-data-wrangling",
    "title": "Tidyverse, EDA & git",
    "section": "A grammar for data wrangling",
    "text": "A grammar for data wrangling\nThe dplyr package gives a grammar for data wrangling, including these 5 verbs for working with data frames.\n\n\nselect(): take a subset of the columns (i.e., features, variables)\nfilter(): take a subset of the rows (i.e., observations)\nmutate(): add or modify existing columns\narrange(): sort the rows\nsummarize(): aggregate the data across rows (e.g., group it according to some criteria)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#a-grammar-for-data-wrangling-1",
    "href": "slides/BSMM_8740_lec_01.html#a-grammar-for-data-wrangling-1",
    "title": "Tidyverse, EDA & git",
    "section": "A grammar for data wrangling",
    "text": "A grammar for data wrangling\nEach of these functions takes a data frame as its first argument, and returns a data frame.\nBeing able to combine these verbs with nouns (i.e., data frames) and adverbs (i.e., arguments) creates a flexible and powerful way to wrangle data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#filter",
    "href": "slides/BSMM_8740_lec_01.html#filter",
    "title": "Tidyverse, EDA & git",
    "section": "filter()",
    "text": "filter()\nThe two simplest of the five verbs areÂ filter() and select(), which return a subset of the rows or columns of a data frame, respectively.\n\nThe filter() function. At left, a data frame that contains matching entries in a certain column for only a subset of the rows. At right, the resulting data frame after filtering."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#select",
    "href": "slides/BSMM_8740_lec_01.html#select",
    "title": "Tidyverse, EDA & git",
    "section": "select()",
    "text": "select()\n\nThe select() function. At left, a data frame, from which we retrieve only a few of the columns. At right, the resulting data frame after selecting those columns."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example",
    "href": "slides/BSMM_8740_lec_01.html#example",
    "title": "Tidyverse, EDA & git",
    "section": "Example",
    "text": "Example\n\nggplot2::presidential\n\n# A tibble: 12 Ã— 4\n   name       start      end        party     \n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;     \n 1 Eisenhower 1953-01-20 1961-01-20 Republican\n 2 Kennedy    1961-01-20 1963-11-22 Democratic\n 3 Johnson    1963-11-22 1969-01-20 Democratic\n 4 Nixon      1969-01-20 1974-08-09 Republican\n 5 Ford       1974-08-09 1977-01-20 Republican\n 6 Carter     1977-01-20 1981-01-20 Democratic\n 7 Reagan     1981-01-20 1989-01-20 Republican\n 8 Bush       1989-01-20 1993-01-20 Republican\n 9 Clinton    1993-01-20 2001-01-20 Democratic\n10 Bush       2001-01-20 2009-01-20 Republican\n11 Obama      2009-01-20 2017-01-20 Democratic\n12 Trump      2017-01-20 2021-01-20 Republican"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-select",
    "href": "slides/BSMM_8740_lec_01.html#example-select",
    "title": "Tidyverse, EDA & git",
    "section": "Example: select",
    "text": "Example: select\nTo get just the names and parties of these presidents, use select(). The firstÂ argument is the data frame, followed by the column names.\n\ndplyr::select(presidential, name, party)\n\n# A tibble: 12 Ã— 2\n   name       party     \n   &lt;chr&gt;      &lt;chr&gt;     \n 1 Eisenhower Republican\n 2 Kennedy    Democratic\n 3 Johnson    Democratic\n 4 Nixon      Republican\n 5 Ford       Republican\n 6 Carter     Democratic\n 7 Reagan     Republican\n 8 Bush       Republican\n 9 Clinton    Democratic\n10 Bush       Republican\n11 Obama      Democratic\n12 Trump      Republican"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-filter",
    "href": "slides/BSMM_8740_lec_01.html#example-filter",
    "title": "Tidyverse, EDA & git",
    "section": "Example: filter",
    "text": "Example: filter\nSimilarly, the first argument toÂ filter()Â is a data frame, and subsequent arguments are logical conditions that are evaluated on any involved columns.Â \n\ndplyr::filter(presidential, party == \"Republican\")\n\n# A tibble: 7 Ã— 4\n  name       start      end        party     \n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;     \n1 Eisenhower 1953-01-20 1961-01-20 Republican\n2 Nixon      1969-01-20 1974-08-09 Republican\n3 Ford       1974-08-09 1977-01-20 Republican\n4 Reagan     1981-01-20 1989-01-20 Republican\n5 Bush       1989-01-20 1993-01-20 Republican\n6 Bush       2001-01-20 2009-01-20 Republican\n7 Trump      2017-01-20 2021-01-20 Republican"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-combined-operations",
    "href": "slides/BSMM_8740_lec_01.html#example-combined-operations",
    "title": "Tidyverse, EDA & git",
    "section": "Example: combined operations",
    "text": "Example: combined operations\nCombining theÂ filter()Â andÂ select()Â commands enables one to drill down to very specific pieces of information.\n\ndplyr::select(\n  dplyr::filter(\n    presidential\n    , lubridate::year(start) &gt; 1973 & party == \"Democratic\"\n  )\n  , name\n)\n\n# A tibble: 3 Ã— 1\n  name   \n  &lt;chr&gt;  \n1 Carter \n2 Clinton\n3 Obama"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-pipe",
    "href": "slides/BSMM_8740_lec_01.html#example-pipe",
    "title": "Tidyverse, EDA & git",
    "section": "Example: pipe",
    "text": "Example: pipe\nAs written the filter() operation is nested inside the select() operation.\nWith the pipe (%&gt;%), we can write the same expression as above in a more readable syntax.\n\npresidential %&gt;% \n  dplyr::filter(lubridate::year(start) &gt; 1973 & party == \"Democratic\") %&gt;%\n  dplyr::select(name)\n\n# A tibble: 3 Ã— 1\n  name   \n  &lt;chr&gt;  \n1 Carter \n2 Clinton\n3 Obama"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#mutate",
    "href": "slides/BSMM_8740_lec_01.html#mutate",
    "title": "Tidyverse, EDA & git",
    "section": "mutate()",
    "text": "mutate()\nWe might want to create, re-define, or rename some of our variables. A graphical illustration of the mutate() operation is shown below\n\nThe mutate() function creating a column. At right, the resulting data frame after adding a new column."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mutate-new-column",
    "href": "slides/BSMM_8740_lec_01.html#example-mutate-new-column",
    "title": "Tidyverse, EDA & git",
    "section": "Example: mutate, new column",
    "text": "Example: mutate, new column\n\nmy_presidents &lt;- presidential %&gt;% \n  dplyr::mutate( \n    term.length = lubridate::interval(start, end) / lubridate::dyears(1) \n  )\nmy_presidents\n\n# A tibble: 12 Ã— 5\n   name       start      end        party      term.length\n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;\n 1 Eisenhower 1953-01-20 1961-01-20 Republican        8   \n 2 Kennedy    1961-01-20 1963-11-22 Democratic        2.84\n 3 Johnson    1963-11-22 1969-01-20 Democratic        5.16\n 4 Nixon      1969-01-20 1974-08-09 Republican        5.55\n 5 Ford       1974-08-09 1977-01-20 Republican        2.45\n 6 Carter     1977-01-20 1981-01-20 Democratic        4   \n 7 Reagan     1981-01-20 1989-01-20 Republican        8   \n 8 Bush       1989-01-20 1993-01-20 Republican        4   \n 9 Clinton    1993-01-20 2001-01-20 Democratic        8   \n10 Bush       2001-01-20 2009-01-20 Republican        8   \n11 Obama      2009-01-20 2017-01-20 Democratic        8   \n12 Trump      2017-01-20 2021-01-20 Republican        4"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mutate-existing-column",
    "href": "slides/BSMM_8740_lec_01.html#example-mutate-existing-column",
    "title": "Tidyverse, EDA & git",
    "section": "Example: mutate, existing column",
    "text": "Example: mutate, existing column\nTheÂ mutate()Â function can be used to modify existing columns. Below we add a variable containing the year in which each president was elected assuming that every president was elected in the year before he took office.\n\n\nCode\nmy_presidents %&lt;&gt;% \n  dplyr::mutate(elected = year(start) - 1)\n\n\n\n\n# A tibble: 4 Ã— 6\n  name       start      end        party      term.length elected\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n2 Kennedy    1961-01-20 1963-11-22 Democratic        2.84    1960\n3 Johnson    1963-11-22 1969-01-20 Democratic        5.16    1962\n4 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-new-column",
    "href": "slides/BSMM_8740_lec_01.html#example-new-column",
    "title": "Tidyverse, EDA & git",
    "section": "Example: new column",
    "text": "Example: new column\nSome entries in this data set are wrong, because presidential elections are only held every four years, and some presidents are not elected (e.g.Â Johnson and Ford).\n\n\nCode\nmy_presidents  %&lt;&gt;% \n  dplyr::mutate(elected = ifelse(elected %in% c(1962, 1973), NA, elected))\n\n\n\n\n# A tibble: 6 Ã— 6\n  name       start      end        party      term.length elected\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n2 Kennedy    1961-01-20 1963-11-22 Democratic        2.84    1960\n3 Johnson    1963-11-22 1969-01-20 Democratic        5.16      NA\n4 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968\n5 Ford       1974-08-09 1977-01-20 Republican        2.45      NA\n6 Carter     1977-01-20 1981-01-20 Democratic        4       1976"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#rename",
    "href": "slides/BSMM_8740_lec_01.html#rename",
    "title": "Tidyverse, EDA & git",
    "section": "rename()",
    "text": "rename()\nIt is considered bad practice to use a period in names (functions, variables, columns) - we should change the name of theÂ term.lengthÂ column that we created earlier.Â \n\nmy_presidents %&lt;&gt;% \n  dplyr::rename(term_length = term.length)\n\n\n\n# A tibble: 9 Ã— 6\n  name       start      end        party      term_length elected\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n2 Kennedy    1961-01-20 1963-11-22 Democratic        2.84    1960\n3 Johnson    1963-11-22 1969-01-20 Democratic        5.16      NA\n4 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968\n5 Ford       1974-08-09 1977-01-20 Republican        2.45      NA\n6 Carter     1977-01-20 1981-01-20 Democratic        4       1976\n7 Reagan     1981-01-20 1989-01-20 Republican        8       1980\n8 Bush       1989-01-20 1993-01-20 Republican        4       1988\n9 Clinton    1993-01-20 2001-01-20 Democratic        8       1992"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#arrange",
    "href": "slides/BSMM_8740_lec_01.html#arrange",
    "title": "Tidyverse, EDA & git",
    "section": "arrange()",
    "text": "arrange()\nThe functionÂ sort()Â will sort a vector but not a data frame. TheÂ arrange()function sorts a data frame:Â \n\nThe arrange() function. At left, a data frame with an ordinal variable. At right, the resulting data frame after sorting the rows in descending order of that variable."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-arrange---sort-column",
    "href": "slides/BSMM_8740_lec_01.html#example-arrange---sort-column",
    "title": "Tidyverse, EDA & git",
    "section": "Example: arrange - sort column",
    "text": "Example: arrange - sort column\nTo use arrange you have to specify the data frame, and the column by which you want it to be sorted. You also have to specify the direction in which you want it to be sorted.\n\nmy_presidents %&gt;% \n  dplyr::arrange(desc(term_length))\n\n\n\n# A tibble: 9 Ã— 6\n  name       start      end        party      term_length elected\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n2 Reagan     1981-01-20 1989-01-20 Republican        8       1980\n3 Clinton    1993-01-20 2001-01-20 Democratic        8       1992\n4 Bush       2001-01-20 2009-01-20 Republican        8       2000\n5 Obama      2009-01-20 2017-01-20 Democratic        8       2008\n6 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968\n7 Johnson    1963-11-22 1969-01-20 Democratic        5.16      NA\n8 Carter     1977-01-20 1981-01-20 Democratic        4       1976\n9 Bush       1989-01-20 1993-01-20 Republican        4       1988"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-arrange---multiple-columns",
    "href": "slides/BSMM_8740_lec_01.html#example-arrange---multiple-columns",
    "title": "Tidyverse, EDA & git",
    "section": "Example, arrange - multiple columns",
    "text": "Example, arrange - multiple columns\nTo break ties, we can further sort by other variables\n\nmy_presidents %&gt;% \n  dplyr::arrange(desc(term_length), party, elected)\n\n# A tibble: 12 Ã— 6\n   name       start      end        party      term_length elected\n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1 Clinton    1993-01-20 2001-01-20 Democratic        8       1992\n 2 Obama      2009-01-20 2017-01-20 Democratic        8       2008\n 3 Eisenhower 1953-01-20 1961-01-20 Republican        8       1952\n 4 Reagan     1981-01-20 1989-01-20 Republican        8       1980\n 5 Bush       2001-01-20 2009-01-20 Republican        8       2000\n 6 Nixon      1969-01-20 1974-08-09 Republican        5.55    1968\n 7 Johnson    1963-11-22 1969-01-20 Democratic        5.16      NA\n 8 Carter     1977-01-20 1981-01-20 Democratic        4       1976\n 9 Bush       1989-01-20 1993-01-20 Republican        4       1988\n10 Trump      2017-01-20 2021-01-20 Republican        4       2016\n11 Kennedy    1961-01-20 1963-11-22 Democratic        2.84    1960\n12 Ford       1974-08-09 1977-01-20 Republican        2.45      NA"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#summarize-with-group_by",
    "href": "slides/BSMM_8740_lec_01.html#summarize-with-group_by",
    "title": "Tidyverse, EDA & git",
    "section": "summarize() with group_by()",
    "text": "summarize() with group_by()\nThe summarize verb is often used with group_by\n\nThe summarize() function. At left, a data frame. At right, the resulting data frame after aggregating four of the columns."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-summarize---no-groups",
    "href": "slides/BSMM_8740_lec_01.html#example-summarize---no-groups",
    "title": "Tidyverse, EDA & git",
    "section": "Example: summarize - no groups",
    "text": "Example: summarize - no groups\nWhen used without grouping, summarize() collapses a data frame into a single row.\n\nmy_presidents %&gt;% \n  dplyr::summarize(\n    N = n(), \n    first_year = min(year(start)), \n    last_year = max(year(end)), \n    num_dems = sum(party == \"Democratic\"), \n    years = sum(term_length), \n    avg_term_length = mean(term_length)\n  )\n\n# A tibble: 1 Ã— 6\n      N first_year last_year num_dems years avg_term_length\n  &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1    12       1953      2021        5    68            5.67"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-pipe---groups",
    "href": "slides/BSMM_8740_lec_01.html#example-pipe---groups",
    "title": "Tidyverse, EDA & git",
    "section": "Example: pipe - groups",
    "text": "Example: pipe - groups\nTo make comparisons, we can first group then summarize, giving us one summary row for each group.\n\nmy_presidents %&gt;% \n  dplyr::group_by(party) %&gt;% \n  dplyr::summarize(\n    N = n(), \n    first_year = min(year(start)), \n    last_year = max(year(end)), \n    num_dems = sum(party == \"Democratic\"), \n    years = sum(term_length), \n    avg_term_length = mean(term_length)\n  )\n\n# A tibble: 2 Ã— 7\n  party          N first_year last_year num_dems years avg_term_length\n  &lt;chr&gt;      &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1 Democratic     5       1961      2017        5    28            5.6 \n2 Republican     7       1953      2021        0    40            5.71"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-1",
    "href": "slides/BSMM_8740_lec_01.html#example-1",
    "title": "Tidyverse, EDA & git",
    "section": "Example",
    "text": "Example\n\n# attach package magrittr\nrequire(magrittr)\n\nurl &lt;- \n  \"https://data.cityofchicago.org/api/views/5neh-572f/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\n\nall_stations &lt;- \n  # Step 1: Read in the data.\n  readr::read_csv(url) %&gt;% \n  # Step 2: select columns and rename stationname\n  dplyr::select(station = stationname, date, rides) %&gt;% \n  # Step 3: Convert the character date field to a date encoding.\n  # Also, put the data in units of 1K rides\n  dplyr::mutate(date = lubridate::mdy(date), rides = rides / 1000) %&gt;% \n  # Step 4: Summarize the multiple records using the maximum.\n  dplyr::group_by(date, station) %&gt;% \n  dplyr::summarize(rides = max(rides), .groups = \"drop\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#magrittr-vs-native-pipe",
    "href": "slides/BSMM_8740_lec_01.html#magrittr-vs-native-pipe",
    "title": "Tidyverse, EDA & git",
    "section": "Magrittr vs native pipe",
    "text": "Magrittr vs native pipe\n\n\n\n\n\n\n\n\n\nTopic\nMagrittr 2.0.3\nBase 4.3.0\n\n\n\n\nOperator\n%&gt;% %&lt;&gt;% %T&gt;%\n|&gt; (since 4.1.0)\n\n\nFunction call\n1:3 %&gt;% sum()\n1:3 |&gt; sum()\n\n\n\n1:3 %&gt;% sum\nNeeds brackets / parentheses\n\n\n\n1:3 %&gt;% `+`(4)\nSome functions are not supported\n\n\nPlaceholder\n.\n_ (since 4.2.0)\n\n\n\n\n\nbased on a stackoverflow post comparing magrittr pipe to base R pipe."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#use-cases-for-the-magrittr-pipe",
    "href": "slides/BSMM_8740_lec_01.html#use-cases-for-the-magrittr-pipe",
    "title": "Tidyverse, EDA & git",
    "section": "Use cases for the Magrittr pipe",
    "text": "Use cases for the Magrittr pipe\n\n# functional programming\nairlines &lt;- fivethirtyeight::airline_safety %&gt;% \n  # filter rows\n  dplyr::filter( stringr::str_detect(airline, 'Air') )\n\n# assignment\nairlines %&lt;&gt;% \n  # filter columns and assign result to airlines\n  dplyr::select(avail_seat_km_per_week, incidents_85_99, fatalities_85_99)\n\n# side effects\nairlines %T&gt;% \n  # report the dimensions\n  ( \\(x) print(dim(x)) ) %&gt;% \n  # summarize\n  dplyr::summarize(avail_seat_km_per_week = sum(avail_seat_km_per_week))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#functions-in-r",
    "href": "slides/BSMM_8740_lec_01.html#functions-in-r",
    "title": "Tidyverse, EDA & git",
    "section": "Functions in R",
    "text": "Functions in R\n\n# named function\nis_awesome &lt;- function(x = 'Bob') {\n  paste(x, 'is awesome!')\n}\nis_awesome('Keith')\n\n# anonymous function\n(function (x) {paste(x, 'is awesome!')})('Keith')\n\n# also anonymous function\n(\\(x) paste(x, 'is awesome!'))('Keith')\n\n# a function from a formula in the tidyverse\nc('Bob','Ted') %&gt;% purrr::map_chr(~paste(.x, 'is awesome!'))"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#data-wrangling",
    "href": "slides/BSMM_8740_lec_01.html#data-wrangling",
    "title": "Tidyverse, EDA & git",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nThe Tidyverse offers a consistent and efficient framework for manipulating, transforming, and cleaning datasets.\nFunctions like filter(), select(), mutate(), and group_by() allow users to easily subset, reorganize, add, and aggregate data, and the pipe (%&gt;% or |&gt;) enables a sequential and readable flow of operations.\nThe following examples show a few more of the many useful data wrangling functions in the tidyverse."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-1-mutate",
    "href": "slides/BSMM_8740_lec_01.html#example-1-mutate",
    "title": "Tidyverse, EDA & git",
    "section": "Example 1: mutate",
    "text": "Example 1: mutate\n\nmutatemutate across\n\n\n\nopenintro::email %&gt;%\n  dplyr::select(-from, -sent_email) %&gt;%\n  dplyr::mutate(\n    day_of_week = lubridate::wday(time)       # new variable: day of week\n    , month = lubridate::month(time)          # new variable: month\n  ) %&gt;%\n  dplyr::select(-time) %&gt;%\n  dplyr::mutate(\n    cc       = cut(cc, breaks = c(0, 1))      # discretize cc\n    , attach = cut(attach, breaks = c(0, 1))  # discretize attach\n    , dollar = cut(dollar, breaks = c(0, 1))  # discretize dollar\n  ) %&gt;%\n  dplyr::mutate(\n    inherit = \n      cut(inherit, breaks = c(0, 1, 5, 10, 20))  # discretize inherit, by intervals\n    , password = dplyr::ntile(password, 5)       # discretize password, by quintile\n  )\n\n\n\n\niris %&gt;%\n  dplyr::mutate(across(c(Sepal.Length, Sepal.Width), round))\n\niris %&gt;%\n  dplyr::mutate(across(c(1, 2), round))\n\niris %&gt;%\n  dplyr::group_by(Species) %&gt;%\n  dplyr::summarise(\n    across( starts_with(\"Sepal\"), list(mean = mean, sd = sd) )\n  )\n\niris %&gt;%\n  dplyr::group_by(Species) %&gt;%\n  dplyr::summarise(\n    across( starts_with(\"Sepal\"), ~ mean(.x, na.rm = TRUE) )\n  )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-2-rowwise-operations",
    "href": "slides/BSMM_8740_lec_01.html#example-2-rowwise-operations",
    "title": "Tidyverse, EDA & git",
    "section": "Example 2: rowwise operations",
    "text": "Example 2: rowwise operations\n\nrowwise operationsusing c_across\n\n\nThe verb rowwise creates a special type of grouping where each group consists of a single row.\n\niris %&gt;%\n  dplyr::rowwise() %&gt;%\n  dplyr::mutate( \n    mean_length = \n      mean( c(Sepal.Length, Petal.Length) )\n    , .before = 1\n  ) %&gt;% \n  dplyr::ungroup()\n\n\n\n\niris %&gt;%\n  dplyr::rowwise() %&gt;%\n  dplyr::mutate( \n    mean_length = \n      mean(\n        dplyr::c_across(c(Sepal.Length:Petal.Width))\n      )\n    , .before = 1 \n  ) %&gt;% \n  dplyr::ungroup()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-3-nesting-operations",
    "href": "slides/BSMM_8740_lec_01.html#example-3-nesting-operations",
    "title": "Tidyverse, EDA & git",
    "section": "Example 3: nesting operations",
    "text": "Example 3: nesting operations\nA nested data frame is a data frame where one (or more) columns is a list of data frames.\n\nlist columnsgroup-nestingmapping\n\n\n\n\ncreate a list-column of data frames\n(df1 &lt;- tibble::tibble(\n  g = c(1, 2, 3),\n  data = list(\n    tibble::tibble(x = 1, y = 2),\n    tibble::tibble(x = 4:5, y = 6:7),\n    tibble::tibble(x = 10)\n  )\n) )\n\n\n# A tibble: 3 Ã— 2\n      g data            \n  &lt;dbl&gt; &lt;list&gt;          \n1     1 &lt;tibble [1 Ã— 2]&gt;\n2     2 &lt;tibble [2 Ã— 2]&gt;\n3     3 &lt;tibble [1 Ã— 1]&gt;\n\n\n\n\n\n\nnest groups by continent, country\n(gapminder_nest &lt;- gapminder::gapminder %&gt;% \n  dplyr::mutate(year1950 = year - 1950) %&gt;% \n  dplyr::group_nest(continent, country)\n)\n\n\n# A tibble: 142 Ã— 3\n   continent country                                data\n   &lt;fct&gt;     &lt;fct&gt;                    &lt;list&lt;tibble[,5]&gt;&gt;\n 1 Africa    Algeria                            [12 Ã— 5]\n 2 Africa    Angola                             [12 Ã— 5]\n 3 Africa    Benin                              [12 Ã— 5]\n 4 Africa    Botswana                           [12 Ã— 5]\n 5 Africa    Burkina Faso                       [12 Ã— 5]\n 6 Africa    Burundi                            [12 Ã— 5]\n 7 Africa    Cameroon                           [12 Ã— 5]\n 8 Africa    Central African Republic           [12 Ã— 5]\n 9 Africa    Chad                               [12 Ã— 5]\n10 Africa    Comoros                            [12 Ã— 5]\n# â„¹ 132 more rows\n\n\n\n\n\n\nFit a linear model for each country:\n(gapminder_model &lt;- gapminder_nest %&gt;% \n  dplyr::mutate(\n    model = \n      purrr::map(\n        data\n        , ~lm(lifeExp ~ year1950, data = .))\n  ))\n\n\n# A tibble: 142 Ã— 4\n   continent country                                data model \n   &lt;fct&gt;     &lt;fct&gt;                    &lt;list&lt;tibble[,5]&gt;&gt; &lt;list&gt;\n 1 Africa    Algeria                            [12 Ã— 5] &lt;lm&gt;  \n 2 Africa    Angola                             [12 Ã— 5] &lt;lm&gt;  \n 3 Africa    Benin                              [12 Ã— 5] &lt;lm&gt;  \n 4 Africa    Botswana                           [12 Ã— 5] &lt;lm&gt;  \n 5 Africa    Burkina Faso                       [12 Ã— 5] &lt;lm&gt;  \n 6 Africa    Burundi                            [12 Ã— 5] &lt;lm&gt;  \n 7 Africa    Cameroon                           [12 Ã— 5] &lt;lm&gt;  \n 8 Africa    Central African Republic           [12 Ã— 5] &lt;lm&gt;  \n 9 Africa    Chad                               [12 Ã— 5] &lt;lm&gt;  \n10 Africa    Comoros                            [12 Ã— 5] &lt;lm&gt;  \n# â„¹ 132 more rows"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-4-stringr-string-functions",
    "href": "slides/BSMM_8740_lec_01.html#example-4-stringr-string-functions",
    "title": "Tidyverse, EDA & git",
    "section": "Example 4: stringr string functions",
    "text": "Example 4: stringr string functions\nMain verbs, each taking a pattern as input\n\nstringr::str_{X}stringr::str_glue\n\n\n\nx &lt;- c(\"why\", \"video\", \"cross\", \"extra\", \"deal\", \"authority\")\n\nstringr::str_detect(x, \"[aeiou]\")       # identifies any matches\nstringr::str_count(x, \"[aeiou]\")        # counts number of patterns\nstringr::str_subset(x, \"[aeiou]\")       # extracts matching components\nstringr::str_extract(x, \"[aeiou]\")      # extracts text of the match\nstringr::str_replace(x, \"[aeiou]\", \"?\") # replaces matches with new text:\nstringr::str_split(x, \",\")              # splits up a string\n\n\n\n\nmtcars %&gt;% \n  tibble::rownames_to_column(var = \"car\") %&gt;% \n  tibble::as_tibble() %T&gt;% \n  (\\(x) print(names(x)) ) %&gt;% \n  dplyr::mutate(\n    note = stringr::str_glue(\"The {car} has {cyl} cylinders\")) %&gt;% \n  dplyr::slice_head(n=3)\n\n\n\n\ncheat sheet"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-5-database-functions",
    "href": "slides/BSMM_8740_lec_01.html#example-5-database-functions",
    "title": "Tidyverse, EDA & git",
    "section": "Example 5: Database functions",
    "text": "Example 5: Database functions\n\nCreate DBExtract SQLExecute Query\n\n\n\n# directly like a tibble\ndb &lt;- \n  dbplyr::memdb_frame(\n    x = runif(100)\n    , y = runif(100)\n    , .name = 'test_tbl'\n  )\n\n# using an existing table\nmtcars_db &lt;- dbplyr::tbl_memdb(mtcars)\n\n\n\n\n\nGenerate SQL without executing\nmtcars_db %&gt;% \n  dplyr::group_by(cyl) %&gt;% \n  dplyr::summarise(n = n()) %&gt;% \n  dplyr::show_query()\n\n\n&lt;SQL&gt;\nSELECT `cyl`, COUNT(*) AS `n`\nFROM `mtcars`\nGROUP BY `cyl`\n\n\n\n\n\n\nExecute Query on DB\nmtcars_db %&gt;% \n  dplyr::group_by(cyl) %&gt;% \n  dplyr::summarise(n = n()) %&gt;% \n  dplyr::collapse()\n\n\n# Source:   SQL [3 x 2]\n# Database: sqlite 3.46.0 [:memory:]\n    cyl     n\n  &lt;dbl&gt; &lt;int&gt;\n1     4    11\n2     6     7\n3     8    14"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#extract-sql-example",
    "href": "slides/BSMM_8740_lec_01.html#extract-sql-example",
    "title": "Tidyverse, EDA & git",
    "section": "Extract SQL Example",
    "text": "Extract SQL Example\n\n\nExecute Query on DB\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = \":memory:\")\ndplyr::copy_to(con, tibble::tibble(x = 1:100), \"temp_table\")\n\ndplyr::tbl(con, \"temp_table\") %&gt;% \n  dplyr::count(\n    x_bin = cut(\n      x\n      , breaks = c(0, 33, 66, 100)\n      , labels = c(\"low\", \"mid\", \"high\")\n    )\n  ) %&gt;% \n  dplyr::show_query()\n\n\n&lt;SQL&gt;\nSELECT `x_bin`, COUNT(*) AS `n`\nFROM (\n  SELECT\n    `temp_table`.*,\n    CASE\nWHEN (`x` &lt;= 0.0) THEN NULL\nWHEN (`x` &lt;= 33.0) THEN 'low'\nWHEN (`x` &lt;= 66.0) THEN 'mid'\nWHEN (`x` &lt;= 100.0) THEN 'high'\nWHEN (`x` &gt; 100.0) THEN NULL\nEND AS `x_bin`\n  FROM `temp_table`\n) AS `q01`\nGROUP BY `x_bin`"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#pivoting",
    "href": "slides/BSMM_8740_lec_01.html#pivoting",
    "title": "Tidyverse, EDA & git",
    "section": "Pivoting",
    "text": "Pivoting\n\nlongerwider\n\n\nWhen some of the column names are not names of variables, but values of a variable.\n\n\n\n\n# A tibble: 3 Ã— 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\n\n\nPivot longer\ntidyr::table4a %&gt;% \n  pivot_longer(\n    c(`1999`, `2000`)\n    , names_to = \"year\", values_to = \"cases\")\n\n\n# A tibble: 6 Ã— 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\n\n\nWhen an observation is scattered across multiple rows.\n\n\n\n\n# A tibble: 12 Ã— 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\nPivot wider\ntidyr::table2 %&gt;%\n    pivot_wider(\n      names_from = type\n      , values_from = count)\n\n\n# A tibble: 6 Ã— 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#relational-data1",
    "href": "slides/BSMM_8740_lec_01.html#relational-data1",
    "title": "Tidyverse, EDA & git",
    "section": "Relational data1",
    "text": "Relational data1\nWe can join related tables in a variety of ways:\n\nbased on material here"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#relational-data",
    "href": "slides/BSMM_8740_lec_01.html#relational-data",
    "title": "Tidyverse, EDA & git",
    "section": "Relational data",
    "text": "Relational data\n\n\nexample tables\nx &lt;- tibble::tibble(key=1:3, val_x= paste0('x',1:3))\ny &lt;- tibble::tibble(key=c(1,2,4), val_y= paste0('y',1:3))\n\n\n\ninner joinfull joinleft joinright join\n\n\n\nx %&gt;% dplyr::inner_join(y, by = \"key\")\n\n# A tibble: 2 Ã— 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n\n\n\n\n\nx %&gt;% dplyr::full_join(y, by = \"key\")\n\n# A tibble: 4 Ã— 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n3     3 x3    &lt;NA&gt; \n4     4 &lt;NA&gt;  y3   \n\n\n\n\n\nx %&gt;% dplyr::left_join(y, by = \"key\")\n\n# A tibble: 3 Ã— 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n3     3 x3    &lt;NA&gt; \n\n\n\n\n\nx %&gt;% dplyr::right_join(y, by = \"key\")\n\n# A tibble: 3 Ã— 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n3     4 &lt;NA&gt;  y3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#relational-data-1",
    "href": "slides/BSMM_8740_lec_01.html#relational-data-1",
    "title": "Tidyverse, EDA & git",
    "section": "Relational data",
    "text": "Relational data\nKeys used in the join:\n\ndefault (e.g.Â by=NULL): all variables that appear in both tables\na character vector (e.g.Â by = â€œxâ€) uses only the common variables named\na named character vector (e.g.Â by = c(â€œaâ€ = â€œbâ€)) matches variable â€˜aâ€™ in x with variable â€˜bâ€™ in y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#relational-data-2",
    "href": "slides/BSMM_8740_lec_01.html#relational-data-2",
    "title": "Tidyverse, EDA & git",
    "section": "Relational data",
    "text": "Relational data\nFiltering Joins:\n\nsemi_join(x, y) keeps all observations in x that have a match in y ( i.e.Â no NAs).\nanti_join(x, y) drops all observations in x that have a match in y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#set-operations",
    "href": "slides/BSMM_8740_lec_01.html#set-operations",
    "title": "Tidyverse, EDA & git",
    "section": "Set operations",
    "text": "Set operations\nWhen tidy dataset x and y have the same variables, set operations work as expected:\n\nintersect(x, y): return only observations in both x and y.\nunion(x, y): return unique observations in x and y.\nsetdiff(x, y): return observations in x, but not in y."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#tidying",
    "href": "slides/BSMM_8740_lec_01.html#tidying",
    "title": "Tidyverse, EDA & git",
    "section": "Tidying",
    "text": "Tidying\n\ntidyr::table3 %&gt;% \n  tidyr::separate_wider_delim(\n    cols = rate\n    , delim = \"/\"\n    , names = c(\"cases\", \"population\") \n)\n\n# A tibble: 6 Ã— 4\n  country      year cases  population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#exploratory-data-analysis-eda-1",
    "href": "slides/BSMM_8740_lec_01.html#exploratory-data-analysis-eda-1",
    "title": "Tidyverse, EDA & git",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nExploratory data analysis is the process of understanding a new dataset by looking at the data, constructing graphs, tables, and models. We want to understand three aspects:\n\neach individual variable by itself;\neach individual variable in the context of other, relevant, variables; and\nthe data that are not there."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#exploratory-data-analysis-eda-2",
    "href": "slides/BSMM_8740_lec_01.html#exploratory-data-analysis-eda-2",
    "title": "Tidyverse, EDA & git",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nWe will perform two broad categories of EDA:\n\nDescriptive Statistics, which includes mean, median, mode, inter-quartile range, and so on.\nGraphical Methods, which includes histogram, density estimation, box plots, and so on."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-view-all-data",
    "href": "slides/BSMM_8740_lec_01.html#eda-view-all-data",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: view all data",
    "text": "EDA: view all data\nOur first dataset is a sample of categorical variables from the General Social Survey, a long-running US survey conducted by the independent research organization NORC at the University of Chicago.\n\n\n\n\nCode\ndat &lt;- forcats::gss_cat\ndat %&gt;% utils::head()\n\n\n# A tibble: 6 Ã— 9\n   year marital         age race  rincome        partyid     relig denom tvhours\n  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;       &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1  2000 Never married    26 White $8000 to 9999  Ind,near râ€¦ Protâ€¦ Soutâ€¦      12\n2  2000 Divorced         48 White $8000 to 9999  Not str reâ€¦ Protâ€¦ Baptâ€¦      NA\n3  2000 Widowed          67 White Not applicable Independent Protâ€¦ No dâ€¦       2\n4  2000 Never married    39 White Not applicable Ind,near râ€¦ Orthâ€¦ Not â€¦       4\n5  2000 Divorced         25 White Not applicable Not str deâ€¦ None  Not â€¦       1\n6  2000 Married          25 White $20000 - 24999 Strong demâ€¦ Protâ€¦ Soutâ€¦      NA\n\n\n\n\nexecute ?forcats::gss_cat to see the data dictionary"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-view-all-columns",
    "href": "slides/BSMM_8740_lec_01.html#eda-view-all-columns",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: view all columns",
    "text": "EDA: view all columns\nUse dplyr::glimpse() to see every column in a data.frame\n\n\ndat %&gt;% dplyr::slice_head(n=10) %&gt;% dplyr::glimpse()\n\nRows: 10\nColumns: 9\n$ year    &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000\n$ marital &lt;fct&gt; Never married, Divorced, Widowed, Never married, Divorced, Marâ€¦\n$ age     &lt;int&gt; 26, 48, 67, 39, 25, 25, 36, 44, 44, 47\n$ race    &lt;fct&gt; White, White, White, White, White, White, White, White, White,â€¦\n$ rincome &lt;fct&gt; $8000 to 9999, $8000 to 9999, Not applicable, Not applicable, â€¦\n$ partyid &lt;fct&gt; \"Ind,near rep\", \"Not str republican\", \"Independent\", \"Ind,nearâ€¦\n$ relig   &lt;fct&gt; Protestant, Protestant, Protestant, Orthodox-christian, None, â€¦\n$ denom   &lt;fct&gt; \"Southern baptist\", \"Baptist-dk which\", \"No denomination\", \"Noâ€¦\n$ tvhours &lt;int&gt; 12, NA, 2, 4, 1, NA, 3, NA, 0, 3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-view-some-rows",
    "href": "slides/BSMM_8740_lec_01.html#eda-view-some-rows",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: view some rows",
    "text": "EDA: view some rows\nUse dplyr::slice_sample() to see a random selection of rows in a data.frame\n\n\ndat %&gt;% dplyr::slice_sample(n=10)\n\n# A tibble: 10 Ã— 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2002 Divorced         33 Black $8000 to 9999  Ind,near â€¦ Protâ€¦ Other       4\n 2  2014 Married          52 White $25000 or more Strong reâ€¦ Orthâ€¦ Not â€¦       0\n 3  2000 Divorced         63 White $25000 or more Strong reâ€¦ Protâ€¦ No dâ€¦      NA\n 4  2010 Divorced         46 White Not applicable Not str dâ€¦ Protâ€¦ Soutâ€¦      NA\n 5  2014 Married          72 White Not applicable Not str dâ€¦ Protâ€¦ Unitâ€¦      NA\n 6  2002 Never married    22 White Not applicable Independeâ€¦ None  Not â€¦      NA\n 7  2014 Married          47 White Not applicable Strong reâ€¦ Protâ€¦ Wi eâ€¦      NA\n 8  2000 Married          75 White $20000 - 24999 Independeâ€¦ Protâ€¦ Otheâ€¦      NA\n 9  2006 Married          50 White Not applicable Other parâ€¦ None  Not â€¦       5\n10  2006 Married          69 White Not applicable Independeâ€¦ Cathâ€¦ Not â€¦       5\n\n\nThere are many dplyr::slice_{X} variants, along with dplyr::filter"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#bad-data-rightarrow-bad-results",
    "href": "slides/BSMM_8740_lec_01.html#bad-data-rightarrow-bad-results",
    "title": "Tidyverse, EDA & git",
    "section": "bad data \\(\\rightarrow\\) bad results",
    "text": "bad data \\(\\rightarrow\\) bad results"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-descriptive-statistics",
    "href": "slides/BSMM_8740_lec_01.html#eda-descriptive-statistics",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: descriptive statistics",
    "text": "EDA: descriptive statistics\nThe base R function summary() can be used for key summary statistics of the data.\n\n\ndat %&gt;% summary()\n\n      year               marital           age                    race      \n Min.   :2000   No answer    :   17   Min.   :18.00   Other         : 1959  \n 1st Qu.:2002   Never married: 5416   1st Qu.:33.00   Black         : 3129  \n Median :2006   Separated    :  743   Median :46.00   White         :16395  \n Mean   :2007   Divorced     : 3383   Mean   :47.18   Not applicable:    0  \n 3rd Qu.:2010   Widowed      : 1807   3rd Qu.:59.00                         \n Max.   :2014   Married      :10117   Max.   :89.00                         \n                                      NA's   :76                            \n           rincome                   partyid            relig      \n $25000 or more:7363   Independent       :4119   Protestant:10846  \n Not applicable:7043   Not str democrat  :3690   Catholic  : 5124  \n $20000 - 24999:1283   Strong democrat   :3490   None      : 3523  \n $10000 - 14999:1168   Not str republican:3032   Christian :  689  \n $15000 - 19999:1048   Ind,near dem      :2499   Jewish    :  388  \n Refused       : 975   Strong republican :2314   Other     :  224  \n (Other)       :2603   (Other)           :2339   (Other)   :  689  \n              denom          tvhours      \n Not applicable  :10072   Min.   : 0.000  \n Other           : 2534   1st Qu.: 1.000  \n No denomination : 1683   Median : 2.000  \n Southern baptist: 1536   Mean   : 2.981  \n Baptist-dk which: 1457   3rd Qu.: 4.000  \n United methodist: 1067   Max.   :24.000  \n (Other)         : 3134   NA's   :10146"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-packages-for-eda",
    "href": "slides/BSMM_8740_lec_01.html#eda-packages-for-eda",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: packages for EDA",
    "text": "EDA: packages for EDA\n\nThe function skimr::skim() gives an enhanced version of base Râ€™s summary() .\nOther packages, such as DataExplorer:: rely more on graphing."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#skimrskim",
    "href": "slides/BSMM_8740_lec_01.html#skimrskim",
    "title": "Tidyverse, EDA & git",
    "section": "skimr::skim()",
    "text": "skimr::skim()\n\nnumeric variablesfactor variables\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n2006.50\n4.45\n2000\n2002\n2006\n2010\n2014\nâ–‡â–ƒâ–‡â–‚â–†\n\n\nage\n76\n1.00\n47.18\n17.29\n18\n33\n46\n59\n89\nâ–‡â–‡â–‡â–…â–‚\n\n\ntvhours\n10146\n0.53\n2.98\n2.59\n0\n1\n2\n4\n24\nâ–‡â–‚â–â–â–\n\n\n\n\n\n\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nmarital\n0\n1\nFALSE\n6\nMar: 10117, Nev: 5416, Div: 3383, Wid: 1807\n\n\nrace\n0\n1\nFALSE\n3\nWhi: 16395, Bla: 3129, Oth: 1959, Not: 0\n\n\nrincome\n0\n1\nFALSE\n16\n$25: 7363, Not: 7043, $20: 1283, $10: 1168\n\n\npartyid\n0\n1\nFALSE\n10\nInd: 4119, Not: 3690, Str: 3490, Not: 3032\n\n\nrelig\n0\n1\nFALSE\n15\nPro: 10846, Cat: 5124, Non: 3523, Chr: 689\n\n\ndenom\n0\n1\nFALSE\n30\nNot: 10072, Oth: 2534, No : 1683, Sou: 1536"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-factor-variable-counts",
    "href": "slides/BSMM_8740_lec_01.html#eda-factor-variable-counts",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: factor variable counts",
    "text": "EDA: factor variable counts\nMost of the columns here are factors (categories). Use these to count the number of observations per category.\n\nforecats::fct_countdplyr::counttable()\n\n\n\nforcats::fct_count(dat$relig) %&gt;% dplyr::arrange(desc(n))\n\n# A tibble: 16 Ã— 2\n   f                           n\n   &lt;fct&gt;                   &lt;int&gt;\n 1 Protestant              10846\n 2 Catholic                 5124\n 3 None                     3523\n 4 Christian                 689\n 5 Jewish                    388\n 6 Other                     224\n 7 Buddhism                  147\n 8 Inter-nondenominational   109\n 9 Moslem/islam              104\n10 Orthodox-christian         95\n11 No answer                  93\n12 Hinduism                   71\n13 Other eastern              32\n14 Native american            23\n15 Don't know                 15\n16 Not applicable              0\n\n\n\n\n\ndat |&gt; dplyr::count(relig) |&gt; dplyr::arrange(desc(n))\n\n# A tibble: 15 Ã— 2\n   relig                       n\n   &lt;fct&gt;                   &lt;int&gt;\n 1 Protestant              10846\n 2 Catholic                 5124\n 3 None                     3523\n 4 Christian                 689\n 5 Jewish                    388\n 6 Other                     224\n 7 Buddhism                  147\n 8 Inter-nondenominational   109\n 9 Moslem/islam              104\n10 Orthodox-christian         95\n11 No answer                  93\n12 Hinduism                   71\n13 Other eastern              32\n14 Native american            23\n15 Don't know                 15\n\n\n\n\n\ndat$relig |&gt; table() |&gt; as.data.frame() %&gt;% dplyr::arrange(desc(Freq))\n\n                      Var1  Freq\n1               Protestant 10846\n2                 Catholic  5124\n3                     None  3523\n4                Christian   689\n5                   Jewish   388\n6                    Other   224\n7                 Buddhism   147\n8  Inter-nondenominational   109\n9             Moslem/islam   104\n10      Orthodox-christian    95\n11               No answer    93\n12                Hinduism    71\n13           Other eastern    32\n14         Native american    23\n15              Don't know    15\n16          Not applicable     0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-binary-factors",
    "href": "slides/BSMM_8740_lec_01.html#eda-binary-factors",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: binary factors",
    "text": "EDA: binary factors\n\ntable()xtabs()\n\n\n\ndat_bin &lt;- dat |&gt; \n  dplyr::mutate(\n    is_protestant = dplyr::case_when(relig == 'Protestant' ~ 1, TRUE ~ 0)\n  )\n\ndat_bin$is_protestant |&gt; table() / length(dat_bin$is_protestant)\n\n\n        0         1 \n0.4951357 0.5048643 \n\n\n\n\n\ndat_bin |&gt; xtabs(~ partyid + is_protestant, data = _)\n\n                    is_protestant\npartyid                 0    1\n  No answer           102   52\n  Don't know            1    0\n  Other party         233  160\n  Strong republican   720 1594\n  Not str republican 1198 1834\n  Ind,near rep        878  913\n  Independent        2436 1683\n  Ind,near dem       1473 1026\n  Not str democrat   1972 1718\n  Strong democrat    1624 1866"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-classifying-missing-data",
    "href": "slides/BSMM_8740_lec_01.html#eda-classifying-missing-data",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: classifying missing data",
    "text": "EDA: classifying missing data\nThere are three main categories of missing data\n\nMissing Completely At Random (MCAR);\n\nmissing and independent of other measurements\n\nMissing at Random (MAR);\n\nmissing in a way related to other measurements\n\nMissing Not At Random (MNAR).\n\nmissing as a property of the variable or some other unmeasured variable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-handling-missing-data",
    "href": "slides/BSMM_8740_lec_01.html#eda-handling-missing-data",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: handling missing data",
    "text": "EDA: handling missing data\nWe can think of a few options for dealing with missing data\n\nDrop observations with missing data.\nImpute the mean of observations without missing data.\nUse multiple imputation.\n\n\n\n\n\n\n\n\nNote\n\n\nMultiple imputation involves generating several estimates for the missing values and then averaging the outcomes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mcar-or-mar",
    "href": "slides/BSMM_8740_lec_01.html#example-mcar-or-mar",
    "title": "Tidyverse, EDA & git",
    "section": "Example: MCAR or MAR?",
    "text": "Example: MCAR or MAR?\n\n\nCode\ndat %&gt;% dplyr::select(partyid) %&gt;% table() %&gt;% tibble::as_tibble() %&gt;% \n  dplyr::left_join(\n    dat %&gt;% dplyr::filter(is.na(age)) %&gt;% \n      dplyr::select(na_partyid = partyid) %&gt;% table() %&gt;% tibble::as_tibble()\n    , by = c(\"partyid\" = \"na_partyid\")\n    , suffix = c(\"_partyid\", \"_na_partyid\")\n  ) %&gt;% \n  dplyr::mutate(pct_na = n_na_partyid / n_partyid)\n\n\n# A tibble: 10 Ã— 4\n   partyid            n_partyid n_na_partyid   pct_na\n   &lt;chr&gt;                  &lt;int&gt;        &lt;int&gt;    &lt;dbl&gt;\n 1 No answer                154            9 0.0584  \n 2 Don't know                 1            0 0       \n 3 Other party              393            3 0.00763 \n 4 Strong republican       2314            8 0.00346 \n 5 Not str republican      3032            8 0.00264 \n 6 Ind,near rep            1791            2 0.00112 \n 7 Independent             4119           18 0.00437 \n 8 Ind,near dem            2499            2 0.000800\n 9 Not str democrat        3690           11 0.00298 \n10 Strong democrat         3490           15 0.00430"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mcar-mar-or-mnar",
    "href": "slides/BSMM_8740_lec_01.html#example-mcar-mar-or-mnar",
    "title": "Tidyverse, EDA & git",
    "section": "Example: MCAR, MAR or MNAR?",
    "text": "Example: MCAR, MAR or MNAR?\n\n\n\n\n\n\n\n\n\n\n\nCustomer ID\nAge\nIncome\nPurchase Frequency\nSatisfaction Rating\n\n\n\n\n1\n35\n$60,000\nHigh\n8\n\n\n2\n28\n$45,000\nLow\n-\n\n\n3\n42\n$70,000\nMedium\n7\n\n\n4\n30\n$50,000\nLow\n-\n\n\n5\n55\n$80,000\nHigh\n9\n\n\n6\n26\n$40,000\nLow\n-\n\n\n7\n50\n$75,000\nMedium\n8\n\n\n8\n29\n$48,000\nLow\n-"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#example-mcar-mar-or-mnar-1",
    "href": "slides/BSMM_8740_lec_01.html#example-mcar-mar-or-mnar-1",
    "title": "Tidyverse, EDA & git",
    "section": "Example: MCAR, MAR or MNAR?",
    "text": "Example: MCAR, MAR or MNAR?\n\n\n\n\n\n\n\n\n\n\n\nEmployee ID\nAge\nJob Tenure\nPerformance Score\nPromotion Status\n\n\n\n\n1\n30\n5 years\n85\nYes\n\n\n2\n45\n10 years\n-\nNo\n\n\n3\n28\n3 years\n90\nYes\n\n\n4\n50\n12 years\n70\nNo\n\n\n5\n35\n6 years\n-\nNo\n\n\n6\n32\n4 years\n95\nYes\n\n\n7\n40\n8 years\n-\nNo\n\n\n8\n29\n2 years\n88\nYes"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-missing-data",
    "href": "slides/BSMM_8740_lec_01.html#eda-missing-data",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: missing data",
    "text": "EDA: missing data\nFinally, be aware that how missing data is encoded depends on the dataset\n\nR defaults to NA when reading data, in joins, etc.\nThe creator(s) of the dataset may use a different encoding.\nMissing data can have multiple representations according to semantics of the measurement.\nRemember that entire measurements can be missing (i.e.Â from all observations, not just some)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#eda-summary",
    "href": "slides/BSMM_8740_lec_01.html#eda-summary",
    "title": "Tidyverse, EDA & git",
    "section": "EDA: summary",
    "text": "EDA: summary\n\nUnderstand what the measurements represent and confirm constraints (if any) and suitability of encoding.\nMake a decision on how to deal with missing data.\nUnderstand shape of measurements (may identify an issue or suggest a data transformation)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-1",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-1",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nfor continuous variables (usually the independent variables or covariates):\n\nnormalization (scale values to \\([0,1]\\))\n\n\\(X_\\text{norm} = \\frac{X-X_\\text{min}}{X_\\text{max}-X_\\text{min}}\\)\n\nstandardization (subtract mean and scale by stdev)\n\n\\(X_\\text{std} = \\frac{X-\\mu_X}{\\sigma_X}\\)\n\nscaling (multiply / divide by a constant)\n\n\\(X_\\text{scaled} = K\\times X\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-2",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-2",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nOther common transformations:\n\nBox-cox: with \\(\\tilde{x}\\) the geometric mean of the (positive) predictor data (\\(\\tilde{x}=\\left(\\prod_{i=1}^{n}x_{i}\\right)^{1/n}\\))\n\n\\[\nx_i(\\lambda) = \\left\\{\n\\begin{array}{cc}\n\\frac{x_i^{\\lambda}-1}{\\lambda\\tilde{x}^{\\lambda-1}} & \\lambda\\ne 0\\\\\n\\tilde{x}\\log x_i & \\lambda=0\n\\end{array}\n\\right .  \n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nBox-cox is an example of a power transform; it is a technique used to stabilize variance, make the data more normal distribution-like."
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-3",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-3",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nOne last common transformation:\n\nlogit transformation for bounded target variables (scaled to lie in \\([0,1]\\))\n\n\\[\n\\text{logit}\\left(p\\right)=\\log\\frac{p}{1-p},\\;p\\in [0,1]\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nThe Logit transform is primarily used to transform binary response data, such as survival/non-survival or present/absent, to provide a continuous value in the range \\(\\left(-\\infty,\\infty\\right)\\), where p is the proportion of each sample that is 1 (or 0)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-4",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-4",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nWhy normalize or standardize?\n\nvariation in the range of feature values can lead to biased model performance or difficulties during the learning process, particularly in distance-based algorithms.\n\ne.g.Â income and age\n\nreduce the impact of outliers\nmake results more explainable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-5",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-5",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nfor continuous variables (usually the target variables):\n\ntransformation (arithmetic, basis functions, polynomials, splines, differencing)\n\n\\(y = \\log(y),\\sqrt{y},\\frac{1}{y}\\), etc.\n\\(y = \\sum_i \\beta_i\\text{f}_i(x)\\;\\text{s.t.}\\;0=\\int\\text{f}_i(x)\\text{f}_j(x)\\; \\forall i\\ne j\\)\n\\(y = \\beta_0+\\beta_1 x+\\beta_2 x^2+\\beta_3 x^3+\\ldots\\)\n\\(y = \\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_3 x_1 x_2+\\ldots\\)\n\\(y'_i = y_i-y_{i-1}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-6",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-6",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nfor categorical variables (either target or explanatory variables):\n\nbinning / bucketing\n\nrepresent a numerical value as a categorical value\n\ncategorical\\(\\rightarrow\\)ordinal and ordinal\\(\\rightarrow\\)categorical\n\nfor date variables:\n\ntimestamp\\(\\rightarrow\\)date or date part"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-7",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-7",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ntransformation\n\nWhy transform?\n\nit can make your model perform better\n\ne.g.Â \\(\\log\\) transform makes exponential data linear, and log-Normal data Gaussian\n\\(\\log\\) transforms also make multiplicative models additive\ne.g.Â polynomials, basis functions and splines help model non-linearities in data"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-8",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-8",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ncreation\n\n\noutliers (due to data entry, measurement/experiment, intentional errors)\n\noutliers can be identified by quantile methods (Gaussian data)\noutliers can be removed, treated as missing, or capped\n\nlag variables (either target or explanatory variables)\n\nuseful in time series models, e.g.Â \\(y_t,y_{t-1},\\ldots y_{t-n}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-9",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-9",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering:",
    "text": "Feature engineering:\ncreation\n\n\nbinning / bucketing\n\nrepresent numerical as categorical and vice versa\n\ninterval and ratio levels"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#feature-engineering-summary",
    "href": "slides/BSMM_8740_lec_01.html#feature-engineering-summary",
    "title": "Tidyverse, EDA & git",
    "section": "Feature engineering: summary",
    "text": "Feature engineering: summary\n\nrequires an advanced technical skill set\nrequires domain expertise\nis time-consuming and resource intensive\ndifferent analytics algorithms require different feature engineering"
  },
  {
    "objectID": "slides/BSMM_8740_lec_01.html#recap",
    "href": "slides/BSMM_8740_lec_01.html#recap",
    "title": "Tidyverse, EDA & git",
    "section": "Recap",
    "text": "Recap\n\n\nToday we have introduced tidy data, tidyverse verbs and the pipe operator.\nWe briefly discussed EDA\nIn the lab we will introduce Git and the data backup workflows.\n\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html",
    "href": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html",
    "title": "Description of the airbnb dataset",
    "section": "",
    "text": "Prepared for Gaborâ€™s Data Analysis\nData Analysis for Business, Economics, and Policy by Gabor Bekes and Gabor Kezdi Cambridge University Press 2021 gabors-data-analysis.com"
  },
  {
    "objectID": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#data-source",
    "href": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#data-source",
    "title": "Description of the airbnb dataset",
    "section": "Data source",
    "text": "Data source\nThe data was collected and released by Airbnb As part or insideAirbnb http://insideairbnb.com/get-the-data.html\nThe data is a cross-section of listed apartments for one night on compiled for 04 March 2017. You may download raw listings data HERE\nDate of download for our version: 2017-03-06."
  },
  {
    "objectID": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#data-access-and-copyright",
    "href": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#data-access-and-copyright",
    "title": "Description of the airbnb dataset",
    "section": "Data access and copyright",
    "text": "Data access and copyright\nThe data behind the Inside Airbnb site is sourced from publicly available information from the Airbnb site. The data has been analyzed, cleansed and aggregated where appropriate to faciliate public discussion. Creative Commons CC0 1.0 Universal (CC0 1.0) â€œPublic Domain Dedicationâ€ license."
  },
  {
    "objectID": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#raw-data-tables",
    "href": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#raw-data-tables",
    "title": "Description of the airbnb dataset",
    "section": "Raw data tables",
    "text": "Raw data tables\nlistings.csv observations: apartments in London, n=53,904 ID variable: id\nairbnb_london_listings.csv same as listings.csv but without the techincal variables of the web listings observations: apartments in London, n=53,904 ID variable: id"
  },
  {
    "objectID": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#tidy-data-tables",
    "href": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#tidy-data-tables",
    "title": "Description of the airbnb dataset",
    "section": "Tidy data tables",
    "text": "Tidy data tables\nairbnb_london_cleaned.csv observations: apartments in London, n=53,904 ID variable: id variables: see VARIABLES_airbnb_london_clean.xls\nairbnb_london_workfile.csv observations: apartments in London, n =51 646 ID variable: ? extra variables: see VARIABLES_airbnb_london_workfile.xls\nairbnb_hackney_workfile.csv observations: apartments in the Hackney borough of London, n=4,499 ID variable: ? variables: see VARIABLES_airbnb_london_workfile.xls"
  },
  {
    "objectID": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#filtered-work-data",
    "href": "slides/R/da_data_repo/airbnb/README_dat_airbnb.html#filtered-work-data",
    "title": "Description of the airbnb dataset",
    "section": "Filtered work data",
    "text": "Filtered work data\nLondon N= 49 826\nHackney N=4393"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#recap-of-last-lecture",
    "href": "slides/origBSMM_8740_lec_04.html#recap-of-last-lecture",
    "title": "Regression methods",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\nLast time we worked with the recipes package to develop workflows for pre-processing our data.\nToday we look at regression methods we might apply to our data."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nIn the simple linear regression model, you have \\(N\\) observations of the response variable \\(Y\\) with a linear combination of \\(D\\) predictor variables \\(\\mathbf{x}\\) where\n\\[\n\\pi\\left(Y=y|\\mathbf{x,\\theta}\\right)=\\mathcal{N}\\left(\\left.y\\right|\\beta_{0}+\\mathbf{\\mathbf{\\mathbf{\\beta}}}'\\mathbf{x},\\sigma^{2}\\right)\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-1",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-1",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nwhere \\(\\mathcal{N}\\left(\\left.y\\right|\\mu,\\sigma^{2}\\right)\\) is a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), \\(\\theta=\\left(\\beta_{0},\\mathbf{\\mathbf{\\mathbf{\\beta}}},\\sigma^{2}\\right)\\) are the parameters of the model and the vector of parameters \\(\\beta_{1:D}\\) are the weights or regression coefficients."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-2",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-2",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nThe mean function \\(f(\\mathbf{x})\\equiv\\mu\\) could be any linear function in \\(\\mathbf{x}=(x_1,\\ldots,x_m)\\) in which case \\(\\beta_{0}+\\mathbf{\\mathbf{\\mathbf{\\beta}}}'\\mathbf{x}\\) is a linear approximation around \\(\\beta_{0}\\) per the Taylor series for \\(f(\\mathbf{x})\\). When \\(D=1\\) the Taylor series is1\n\\[\nf(x)=f(\\beta_{0})+f^{(1)}(\\beta_{0})(x-\\beta_{0})+\\frac{1}{2}f^{(2)}(\\beta_{0})(x-\\beta_{0})^{2}+\\ldots\n\\]\nwhere \\(\\ldots = \\sum_{n=3}^{\\infty}\\frac{1}{n!}f^{(n)}(\\beta_{0})(x-\\beta_{0})^{n}\\)"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-3",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-3",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nWhen \\(D=2\\) the Taylor series is (writing \\(f_{x}\\equiv\\frac{\\partial f}{\\partial x}\\), \\(f_{y}\\equiv\\frac{\\partial f}{\\partial y}\\), \\(f_{x,y}\\equiv\\frac{\\partial^2 f}{\\partial x,\\partial x}\\) and so on):\n\\[\n\\begin{align*}\nf(x,y) & =f(\\alpha_{0},\\beta_{0})+f_{x}(\\alpha_{0},\\beta_{0})(x-\\alpha_{0})+f_{y}(\\alpha_{0},\\beta_{0})(y-\\beta_{0})\\\\\n& = + f_{x,x}(\\alpha_{0},\\beta_{0})(x-\\alpha_{0})^{2}+f_{y,y}(\\alpha_{0},\\beta_{0})(y-\\beta_{0})^{2}\\\\\n& = + f_{x,y}(\\alpha_{0},\\beta_{0})(x-\\alpha_{0})(y-\\beta_{0})+\\ldots\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-4",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-4",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nTo fit the 1D linear regression model to \\(N\\) data samples, we minimize the negative log-likelihood on the training set.\n\\[\n\\begin{align*}\n\\text{NLL}\\left(\\beta,\\sigma^{2}\\right) & =\\sum_{n=1}^{N}\\log\\left[\\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{\\frac{1}{2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(y_{n}-\\beta'x_{n}\\right)^{2}\\right)\\right]\\\\\n& =-\\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{N}\\left(y_{n}-\\hat{y}_{n}\\right)^{2}-\\frac{N}{2}\\log\\left(2\\pi\\sigma^{2}\\right)\n\\end{align*}\n\\]\nwhere the predicted response is \\(\\hat{y}\\equiv\\beta'x_{n}\\)."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-5",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-5",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nFocusing on just the weights, the minimum NLL is (up to a constant) the minimum of the residual sum of squares (RSS):\n\\[\n\\begin{align*}\\text{RSS}\\left(\\beta\\right) & =\\frac{1}{2}\\sum_{n=1}^{N}\\left(y_{n}-\\beta'x_{n}\\right)^{2}=\\frac{1}{2}\\left\\Vert y_{n}-\\beta'x_{n}\\right\\Vert ^{2}\\\\\n& =\\frac{1}{2}\\left(y_{n}-\\beta'x_{n}\\right)'\\left(y_{n}-\\beta'x_{n}\\right)\\\\\n\\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-6",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-6",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nWe can write our regression assumption as\n\\[\ny_i=\\beta_0+\\beta_1 x_i + u_i\n\\]\nwhere \\(u_i\\) is a sample from \\(\\mathcal{N}\\left(0,\\sigma^{2}\\right)\\) which in turn implies \\(\\mathbb{E}\\left[u\\right]=0;\\;\\mathbb{E}\\left[\\left.u\\right|x\\right]=0\\)"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-7",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-7",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nIt follows that\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[y-\\beta_{0}-\\beta_{1}x\\right] & =0\\\\\n\\mathbb{E}\\left[x\\left(y-\\beta_{0}-\\beta_{1}x\\right)\\right] & =0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-8",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-8",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nWriting the same thing for our samples (where \\(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}\\) are our estimates)\n\\[\n\\begin{align*}\n\\frac{1}{N}\\sum_{i-1}^{N}y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i} & =0\\\\\n\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i}\\right) & =0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-9",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-9",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nFrom the first equation\n\\[\n\\begin{align*}\n\\bar{y}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}\\bar{x} & =0\\\\\n\\bar{y}-\\hat{\\beta}_{1}\\bar{x} & =\\hat{\\beta}_{0}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-10",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-10",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nSubstituting the expression for \\(\\hat{\\beta}_{0}\\) in the independence equation\n\\[\n\\begin{align*}\n\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(y_{i}-\\left(\\bar{y}-\\hat{\\beta}_{1}\\bar{x}\\right)-\\hat{\\beta}_{1}x_{i}\\right) & =0\\\\\n\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(y_{i}-\\bar{y}\\right) & =\\hat{\\beta}_{1}\\frac{1}{N}\\sum_{i-1}^{N}x_{i}\\left(\\bar{x}-x_{i}\\right)\\\\\n\\frac{1}{N}\\sum_{i-1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right) & =\\hat{\\beta}_{1}\\frac{1}{N}\\sum_{i-1}^{N}\\left(\\bar{x}-x_{i}\\right)^2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-11",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-11",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nOrdinary least squares (OLS)\nSo as long as \\(\\sum_{i-1}^{N}\\left(\\bar{x}-x_{i}\\right)^2\\ne 0\\)\n\\[\n\\begin{align*}\n\\hat{\\beta}_{1} & =\\frac{\\sum_{i-1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum_{i-1}^{N}\\left(\\bar{x}_{i}-x_{i}\\right)^2}\\\\\n& =\\frac{\\text{sample covariance}(x_{i}y_{i})}{\\text{sample variance}(x_{i})}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-12",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-12",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nSimilarly, in the vector equation, the minimum of the RSS is solved by (assuming \\(N&gt;D\\)):\n\\[\n\\hat{\\mathbf{\\beta}}_{OLS}=\\left(X'X\\right)^{-1}\\left(X'Y\\right) = \\frac{\\text{cov}(X,Y)}{\\text{var}(X)}\n\\]\nThere are algorithmic issues though."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-algorithms",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-algorithms",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\nComputing the inverse of \\(X'X\\) directly, while theoretically possible, can be numerically unstable.\nIn R, the \\(QR\\) decomposition is used to solve for \\(\\beta\\). Let \\(X=QR\\) where \\(Q'Q=I\\) and write:\n\\[\n\\begin{align*}\n(QR)\\beta & = y\\\\\nQ'QR\\beta & = Q'y\\\\\n\\beta & = R^{-1}(Q'y)\n\\end{align*}\n\\]\nSince \\(R\\) is upper triangular, the last equation can be solved by back-substitution."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-algorithms-1",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-algorithms-1",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\n\n&gt; A &lt;- matrix(c(1,2,5, 2,4,6, 3, 3, 3), nrow=3)\n&gt; QR &lt;- qr(A)\n\n\nQRA\n\n\n\n&gt; Q &lt;- qr.Q(QR); Q\n\n           [,1]       [,2]          [,3]\n[1,] -0.1825742 -0.4082483 -8.944272e-01\n[2,] -0.3651484 -0.8164966  4.472136e-01\n[3,] -0.9128709  0.4082483  2.593051e-16\n\n\n\n\n\n&gt; R &lt;- qr.R(QR); R\n\n          [,1]      [,2]      [,3]\n[1,] -5.477226 -7.302967 -4.381780\n[2,]  0.000000 -1.632993 -2.449490\n[3,]  0.000000  0.000000 -1.341641\n\n\n\n\n\n&gt; Q %*% R\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    2    4    3\n[3,]    5    6    3"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-algorithms-2",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-algorithms-2",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\n\nCode\n&gt; # A linear system of equations y = Ax\n&gt; cat(\"matrix A\\n\")\n&gt; A &lt;- matrix(c(3, 2, -1, 2, -2, .5, -1, 4, -1), nrow=3); A\n&gt; cat(\"vector x\\n\")\n&gt; x &lt;- c(1, -2, -2); x\n&gt; cat(\"vector y\\n\")\n&gt; y &lt;- A %*% x ; y\n\n\n\n\nmatrix A\n\n\n     [,1] [,2] [,3]\n[1,]    3  2.0   -1\n[2,]    2 -2.0    4\n[3,]   -1  0.5   -1\n\n\n\n\nvector x\n\n\n[1]  1 -2 -2\n\n\n\n\nvector y\n\n\n     [,1]\n[1,]    1\n[2,]   -2\n[3,]    0"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-algorithms-3",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-algorithms-3",
    "title": "Regression methods",
    "section": "Linear regression algorithms",
    "text": "Linear regression algorithms\n\n&gt; # Compute the QR decomposition of A\n&gt; QR &lt;- qr(A)\n&gt; Q &lt;- qr.Q(QR)\n&gt; R &lt;- qr.R(QR)\n&gt; \n&gt; # Compute b=Q'y\n&gt; b &lt;- crossprod(Q, y); b\n\n           [,1]\n[1,]  0.2672612\n[2,]  2.1472519\n[3,] -0.5638092\n\n&gt; # Solve the upper triangular system Rx=b\n&gt; backsolve(R, b)\n\n     [,1]\n[1,]    1\n[2,]   -2\n[3,]   -2"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#linear-regression-models-13",
    "href": "slides/origBSMM_8740_lec_04.html#linear-regression-models-13",
    "title": "Regression methods",
    "section": "Linear regression models",
    "text": "Linear regression models\nMinimizing the NLL by minimizing the residual sum of squares (RSS) is the same as minimizing\n\nthe mean squared error \\(\\text{MSE}\\left(\\beta\\right) = \\frac{1}{N}\\text{RSS}\\left(\\beta\\right)\\)\nthe root mean squared error \\(\\text{RMSE}\\left(\\beta\\right) = \\sqrt{\\text{MSE}\\left(\\beta\\right)}\\)\n\n\n\n\n\n\n\nNote\n\n\nThe minimum NLL estimate is also the maximum likelihood estimate (MLE)"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#empirical-risk-minimization",
    "href": "slides/origBSMM_8740_lec_04.html#empirical-risk-minimization",
    "title": "Regression methods",
    "section": "Empirical risk minimization",
    "text": "Empirical risk minimization\nThe MLE can be generalized by replacing the NLL (\\(\\ell\\left(y_{n},\\theta;x_{n}\\right)=-\\log\\pi\\left(y_n|x_n,\\theta\\right)\\)) with any other loss function to get\n\\[\n\\mathcal{L}\\left(\\theta\\right)=\\frac{1}{N}\\sum_{n=1}^{N}\\ell\\left(y_{n},\\theta;x_{n}\\right)\n\\]\nThis is known as the empirical risk minimization (ERM) - the expected loss taken with respect to the empirical distribution."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#collinearity",
    "href": "slides/origBSMM_8740_lec_04.html#collinearity",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\nOne of the important assumptions of the classical linear regression models is that there is no exact collinearity among the regressors. While high correlation between regressors is a necessary indicator of the collinearity problem, a direct linear relationship beween regressors is sufficient."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#collinearity-1",
    "href": "slides/origBSMM_8740_lec_04.html#collinearity-1",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\nData collection methods, constraints on the fitted regression model, model specification error, an overdefined model, may be some potential sources of multicollinearity. In other cases it is an artifact caused by creating new predictors from other predictors."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#collinearity-2",
    "href": "slides/origBSMM_8740_lec_04.html#collinearity-2",
    "title": "Regression methods",
    "section": "Collinearity",
    "text": "Collinearity\nThe problem of collinearity has potentially serious effect on the regression estimates such as implausible coefficient signs, impossible inversion of matrix \\(X'X\\) as it becomes near or exactly singular, large magnitude of coefficients in absolute value, large variance or standard errors with wider confidence intervals."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#ridge-regression",
    "href": "slides/origBSMM_8740_lec_04.html#ridge-regression",
    "title": "Regression methods",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nRidge regression is an example of a penalized regression model; in this case the magnitude of the weights are penalized by adding the \\(\\ell_2\\) norm of the weights to the loss function. In particular, the ridge regression weights are:\n\\[\n\\hat{\\beta}_{\\text{ridge}}=\\arg\\!\\min\\text{RSS}\\left(\\beta\\right)+\\lambda\\left\\Vert \\beta\\right\\Vert _{2}^{2}\n\\]\nwhere \\(\\lambda\\) is the strength of the regularizer term."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#ridge-regression-1",
    "href": "slides/origBSMM_8740_lec_04.html#ridge-regression-1",
    "title": "Regression methods",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nThe solution is:\n\\[\n\\begin{align*}\n\\hat{\\mathbf{\\beta}}_{ridge} & =\\left(X'X-\\lambda I_{D}\\right)^{-1}\\left(X'Y\\right)\\\\\n& =\\left(\\sum_{n}x_{n}x'_{n}+\\lambda I_{D}\\right)^{-1}\\left(\\sum_{n}y_{n}x_{n}\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#ridge-regression-2",
    "href": "slides/origBSMM_8740_lec_04.html#ridge-regression-2",
    "title": "Regression methods",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nAs for un-penalized linear regression, using matrix inversion to solve for \\(\\hat{\\mathbf{\\beta}}_{ridge}\\) can be a bad idea. The QR transformation can be used here, however, ridge regression is often used when \\(D&gt;N\\), in which case the SVD transformation is faster."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#ridge-regression-example",
    "href": "slides/origBSMM_8740_lec_04.html#ridge-regression-example",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n&gt; #define response variable\n&gt; y &lt;- mtcars %&gt;% dplyr::pull(hp)\n&gt; \n&gt; #define matrix of predictor variables\n&gt; x &lt;- mtcars %&gt;% dplyr::select(mpg, wt, drat, qsec) %&gt;% data.matrix()"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#ridge-regression-example-1",
    "href": "slides/origBSMM_8740_lec_04.html#ridge-regression-example-1",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n&gt; # fit ridge regression model\n&gt; model &lt;- glmnet::glmnet(x, y, alpha = 0)\n&gt; \n&gt; # get coefficients when lambda = 7.6\n&gt; coef(model, s = 7.6)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 477.91365858\nmpg          -3.29697140\nwt           20.31745927\ndrat         -0.09524492\nqsec        -18.48934710"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#ridge-regression-example-2",
    "href": "slides/origBSMM_8740_lec_04.html#ridge-regression-example-2",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n\nglmnet example\n&gt; # perform k-fold cross-validation to find optimal lambda value\n&gt; cv_model &lt;- glmnet::cv.glmnet(x, y, alpha = 0)\n&gt; \n&gt; # find optimal lambda value that minimizes test MSE\n&gt; best_lambda &lt;- cv_model$lambda.min\n&gt; \n&gt; # produce plot of test MSE by lambda value\n&gt; cv_model %&gt;% broom::tidy() %&gt;% \n+ ggplot(aes(x=lambda, y = estimate)) +\n+   geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#00ABFD\", alpha=0.5) +\n+   geom_point() +\n+   geom_vline(xintercept=best_lambda) +\n+   labs(title='Ridge Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}\"\n+          )\n+   ) +\n+   ggplot2::scale_x_log10()"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#ridge-regression-example-3",
    "href": "slides/origBSMM_8740_lec_04.html#ridge-regression-example-3",
    "title": "Regression methods",
    "section": "Ridge Regression Example",
    "text": "Ridge Regression Example\n\n\nglmnet coefficients\n&gt; model$beta %&gt;% \n+   as.matrix() %&gt;% \n+   t() %&gt;% \n+   tibble::as_tibble() %&gt;% \n+   tibble::add_column(lambda = model$lambda, .before = 1) %&gt;% \n+   tidyr::pivot_longer(-lambda, names_to = 'parameter') %&gt;% \n+   ggplot(aes(x=lambda, y=value, color=parameter)) +\n+   geom_line() + geom_point() +\n+   xlim(0,2000) +\n+   labs(title='Ridge Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"Parameters as a function of lambda\"\n+          )\n+   )"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#lasso-regression",
    "href": "slides/origBSMM_8740_lec_04.html#lasso-regression",
    "title": "Regression methods",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nLasso regression is another example of a penalized regression model; in this case both the magnitude of the weights and the number of parameters are penalized by using the \\(\\ell_1\\) norm of the weights to the loss function of the lasso regression. In particular, the lasso regression weights are:\n\\[\n\\hat{\\beta}_{\\text{lasso}}=\\arg\\!\\min\\text{RSS}\\left(\\beta\\right)+\\lambda\\left\\Vert \\beta\\right\\Vert _{1}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#lasso-regression-1",
    "href": "slides/origBSMM_8740_lec_04.html#lasso-regression-1",
    "title": "Regression methods",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nThe Lasso objective function is\n\\[\n\\mathcal{L}\\left(\\beta,\\lambda\\right)=\\text{NLL}+\\lambda\\left\\Vert \\beta\\right\\Vert _{1}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#lasso-regression-example",
    "href": "slides/origBSMM_8740_lec_04.html#lasso-regression-example",
    "title": "Regression methods",
    "section": "Lasso Regression Example",
    "text": "Lasso Regression Example\n\n\nlasso model\n&gt; # define response variable\n&gt; y &lt;- mtcars %&gt;% dplyr::pull(hp)\n&gt; \n&gt; # define matrix of predictor variables\n&gt; x &lt;- mtcars %&gt;% dplyr::select(mpg, wt, drat, qsec) %&gt;% data.matrix()\n&gt; \n&gt; # fit ridge regression model\n&gt; model &lt;- glmnet::glmnet(x, y, alpha = 1)\n&gt; \n&gt; # get coefficients when lambda = 3.53\n&gt; coef(model, s = 3.53)\n\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) 480.761125\nmpg          -3.036337\nwt           20.222451\ndrat          .       \nqsec        -18.944318"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#lasso-regression-example-1",
    "href": "slides/origBSMM_8740_lec_04.html#lasso-regression-example-1",
    "title": "Regression methods",
    "section": "Lasso Regression Example",
    "text": "Lasso Regression Example\n\n\nlasso example\n&gt; #perform k-fold cross-validation to find optimal lambda value\n&gt; cv_model &lt;- glmnet::cv.glmnet(x, y, alpha = 1)\n&gt; \n&gt; #find optimal lambda value that minimizes test MSE\n&gt; best_lambda &lt;- cv_model$lambda.min\n&gt; \n&gt; #produce plot of test MSE by lambda value\n&gt; cv_model %&gt;% broom::tidy() %&gt;% \n+ ggplot(aes(x=lambda, y = estimate)) +\n+   geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#00ABFD\", alpha=0.5) +\n+   geom_point() +\n+   geom_vline(xintercept=best_lambda) +\n+   labs(title='Lasso Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}\"\n+          )\n+   ) +\n+   xlim(0,exp(4)) + ggplot2::scale_x_log10()"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#lasso-regression-example-2",
    "href": "slides/origBSMM_8740_lec_04.html#lasso-regression-example-2",
    "title": "Regression methods",
    "section": "Lasso Regression Example",
    "text": "Lasso Regression Example\n\n\nlasso coefficients\n&gt; model %&gt;%\n+   broom::tidy() %&gt;%\n+   tidyr::pivot_wider(names_from=term, values_from=estimate) %&gt;%\n+   dplyr::select(-c(step,dev.ratio, `(Intercept)`)) %&gt;%\n+   dplyr::mutate_all(dplyr::coalesce, 0) %&gt;% \n+   tidyr::pivot_longer(-lambda, names_to = 'parameter') %&gt;% \n+   ggplot(aes(x=lambda, y=value, color=parameter)) +\n+   geom_line() + geom_point() +\n+   xlim(0,70) +\n+   labs(title='Ridge Regression'\n+        , subtitle = \n+          stringr::str_glue(\n+            \"Parameters as a function of lambda\"\n+          )\n+   )"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#elastic-net-regression",
    "href": "slides/origBSMM_8740_lec_04.html#elastic-net-regression",
    "title": "Regression methods",
    "section": "Elastic Net Regression",
    "text": "Elastic Net Regression\nElastic Net regression is a hybrid of ridge and lasso regression.\nThe elastic net objective function is\n\\[\n\\mathcal{L}\\left(\\beta,\\lambda,\\alpha\\right)=\\text{NLL}+\\lambda\\left(\\left(1-\\alpha\\right)\\left\\Vert \\beta\\right\\Vert _{2}^{2}+\\alpha\\left\\Vert \\beta\\right\\Vert _{1}\\right)\n\\]\nso that \\(\\alpha=0\\) is ridge regression and \\(\\alpha=1\\) is lasso regression and \\(\\alpha\\in\\left(0,1\\right)\\) is the general elastic net."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#elastic-net-regression-example",
    "href": "slides/origBSMM_8740_lec_04.html#elastic-net-regression-example",
    "title": "Regression methods",
    "section": "Elastic Net Regression Example",
    "text": "Elastic Net Regression Example\n\n\nelastic net example\n&gt; # set length of data and seed for reproducability\n&gt; n &lt;- 50\n&gt; set.seed(2467)\n&gt; # create the dataset\n&gt; dat &lt;- tibble::tibble(\n+   a = sample(1:20, n, replace = T)/10\n+   , b = sample(1:10, n, replace = T)/10\n+   , c = sort(sample(1:10, n, replace = T))\n+ ) %&gt;% \n+   dplyr::mutate(\n+     z = (a*b)/2 + c + sample(-10:10, n, replace = T)/10\n+     , .before = 1\n+   )\n&gt; # cross validate to get the best alpha\n&gt; alpha_dat &lt;- tibble::tibble( alpha = seq(0.01, 0.99, 0.01) ) %&gt;% \n+   dplyr::mutate(\n+     mse =\n+       purrr::map_dbl(\n+         alpha\n+         , (\\(a){\n+           cvg &lt;- \n+            glmnet::cv.glmnet(\n+              x = dat %&gt;% dplyr::select(-z) %&gt;% as.matrix() \n+              , y = dat$z \n+              , family = \"gaussian\"\n+              , gamma = a\n+           )\n+           min(cvg$cvm)\n+         })\n+       )\n+   ) \n&gt; \n&gt; best_alpha &lt;- alpha_dat %&gt;% \n+   dplyr::filter(mse == min(mse)) %&gt;% \n+   dplyr::pull(alpha)\n&gt; \n&gt; cat(\"best alpha:\", best_alpha)\n\n\nbest alpha: 0.64\n\n\n\n\nelastic net example, part 2\n&gt; elastic_cv &lt;- \n+   glmnet::cv.glmnet(\n+     x = dat %&gt;% dplyr::select(-z) %&gt;% as.matrix() \n+     , y = dat$z \n+     , family = \"gaussian\"\n+     , gamma = best_alpha)\n&gt; \n&gt; best_lambda &lt;- elastic_cv$lambda.min\n&gt; cat(\"best lambda:\", best_lambda)\n\n\nbest lambda: 0.01015384\n\n\nelastic net example, part 2\n&gt; elastic_mod &lt;- glmnet::glmnet(\n+   x = dat %&gt;% dplyr::select(-z) %&gt;% as.matrix() \n+   , y = dat$z \n+   , family = \"gaussian\"\n+   , gamma = best_alpha, lambda = best_lambda)\n&gt; \n&gt; elastic_mod %&gt;% broom::tidy()\n\n\n# A tibble: 4 Ã— 5\n  term         step estimate lambda dev.ratio\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1   -0.467 0.0102     0.963\n2 a               1    0.221 0.0102     0.963\n3 b               1    0.560 0.0102     0.963\n4 c               1    1.03  0.0102     0.963"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#elastic-net-regression-example-1",
    "href": "slides/origBSMM_8740_lec_04.html#elastic-net-regression-example-1",
    "title": "Regression methods",
    "section": "Elastic Net Regression Example",
    "text": "Elastic Net Regression Example\n\n\nelastic net example, part 3\n&gt; pred &lt;- predict(elastic_mod, dat %&gt;% dplyr::select(-z) %&gt;% as.matrix())\n&gt; \n&gt; rmse &lt;- sqrt(mean( (pred - dat$z)^2 ))\n&gt; R2 &lt;- 1 - (sum((dat$z - pred )^2)/sum((dat$z - mean(y))^2))\n&gt; mse &lt;- mean((dat$z - pred)^2)\n&gt; \n&gt; cat(\" RMSE:\", rmse, \"\\n\", \"R-squared:\", R2, \"\\n\", \"MSE:\", mse)\n\n\n RMSE: 0.5817823 \n R-squared: 0.9999828 \n MSE: 0.3384707\n\n\n\n\nelastic net example, part 4\n&gt; dat %&gt;% \n+   tibble::as_tibble() %&gt;% \n+   tibble::add_column(pred = pred[,1]) %&gt;% \n+   tibble::rowid_to_column(\"ID\") %&gt;% \n+   ggplot(aes(x=ID, y=z)) +\n+   geom_point() +\n+   geom_line(aes(y=pred),color='red')"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#generalized-linear-models",
    "href": "slides/origBSMM_8740_lec_04.html#generalized-linear-models",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA generalized linear model (GLM) is a flexible generalization of ordinary linear regression.\nOrdinary linear regression predicts the expected value of the outcome variable, a random variable, as a linear combination of a set of observed values (predictors). In a generalized linear model (GLM), each outcome \\(Y\\) is assumed to be generated from a particular distribution in an exponential family, The mean, \\(\\mu\\), of the distribution depends on the independent variables, \\(X\\), through:\n\\[\n\\mathbb{E}\\left[\\left.Y\\right|X\\right]=\\mu=\\text{g}^{-1}\\left(X\\beta\\right)\n\\] where \\(g\\) is called the link function."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#generalized-linear-models-1",
    "href": "slides/origBSMM_8740_lec_04.html#generalized-linear-models-1",
    "title": "Regression methods",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nFor example, if \\(Y\\) is Poisson distributed, then\n\\[\n\\mathbb{P}\\left[\\left.Y=y\\right|X,\\lambda\\right]=\\frac{\\lambda^{y}}{y!}e^{-\\lambda}=e^{y\\log\\lambda-\\lambda-\\log y!}\n\\]\nWhere \\(\\lambda\\) is both the mean and the variance. In the glm the link function is \\(\\log\\) and\n\\[\n\\log\\mathbb{E}\\left[\\left.Y\\right|X\\right] = \\beta X=\\log\\lambda\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\n\n\nCode\n&gt; dat &lt;- MASS::Boston\n\n\nThere are many methodologies for constructing regression trees but one of the oldest is known as the classification and regression tree (CART) approach.\nBasic regression trees partition a data set into smaller subgroups and then fit a simple constant for each observation in the subgroup. The partitioning is achieved by successive binary partitions (akaÂ recursive partitioning) based on the different predictors."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees-1",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees-1",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nAs a simple example, consider a continuous response variable \\(y\\) with two covariates \\(x_1,x_2\\) and the support of \\(x_1,x_2\\) partitioned into three regions. Then we write the tree regression model for \\(y\\) as:\n\\[\n\\hat{y} = \\hat{f}(x_1,x_2)=\\sum_{i=1}^{3}c_1\\times I_{(x_1,x_2)\\in R_i}\n\\] Tree algorithm differ in how they grow the regression tree, i.e.Â partition the space of the covariates."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees-2",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees-2",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nAll partitioning of variables is done in a top-down, greedy fashion. This just means that a partition performed earlier in the tree will not change based on later partitions. In general the partitions are made to minimize following objective function (support initially partitioned into 2 regions, i.e.Â a binary tree):\n\\[\n\\text{SSE}=\\left\\{ \\sum_{i\\in R_{1}}\\left(y_{i}-c_{i}\\right)^{2}+\\sum_{i\\in R_{2}}\\left(y_{i}-c_{i}\\right)^{2}\\right\\}\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees-3",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees-3",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nHaving found the best split, we repeat the splitting process on each of the two regions.\nThis process is continued until some stopping criterion is reached. What typically results is a very deep, complex tree that may produce good predictions on the training set, but is likely to overfit the data, particularly at the lower nodes.\nBy pruning these lower level nodes, we can introduce a little bit of bias in our model that help to stabilize predictions and will tend to generalize better to new, unseen data."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees-4",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees-4",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\nAs with penalized linear regression, we can us a complexity parameter \\(\\alpha\\) to penalize the number of terminal nodes of the tree (\\(T\\)), like the lasso \\(L_1\\) norm penalty, and find the smallest tree with lowest penalized error, i.e.Â the minimizing the following objective function:\n\\[\n\\text{SSE}+\\alpha\\left|T\\right|\n\\]"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees-5",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees-5",
    "title": "Regression methods",
    "section": "Regression with trees",
    "text": "Regression with trees\n\n\nStrengths\n\nThey are very interpretable.\nMaking predictions is fast; just lookup constants in the tree.\nVariables importance is easy; those variables that most reduce the SSE.\nTree models give a non-linear response; better if the true regression surface is not smooth.\nThere are fast, reliable algorithms to learn these trees.\n\n\nWeaknesses\n\nSingle regression trees have high variance, resulting in unstable predictions (an alternative subsample of training data can significantly change the terminal nodes).\nDue to the high variance single regression trees have poor predictive accuracy."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees-bagging",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees-bagging",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\nAs mentioned, single tree models suffer from high variance. Although pruning the tree helps reduce this variance, there are alternative methods that actually exploite the variability of single trees in a way that can significantly improve performance over and above that of single trees. Bootstrap aggregating (bagging) is one such approach.\nBagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees-bagging-1",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees-bagging-1",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\nBagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, improving predictive performance."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees-bagging-2",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees-bagging-2",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\nBagging follows three steps:\n\nCreate \\(m\\) bootstrap samples from the training data. Bootstrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.\nFor each bootstrap sample train a single, unpruned regression tree.\nAverage individual predictions from each tree to create an overall average predicted value."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-trees-bagging-3",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-trees-bagging-3",
    "title": "Regression methods",
    "section": "Regression with trees (Bagging)",
    "text": "Regression with trees (Bagging)\n\nFig: The bagging process."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-a-random-forest",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-a-random-forest",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\nBagging trees introduces a random component into the tree building process that reduces the variance of a single treeâ€™s prediction and improves predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree.\nSo trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships. They are correlated."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-a-random-forest-1",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-a-random-forest-1",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\nTree correlation prevents bagging from optimally reducing the variance of the predictive values. Reducing variance further can be achieved by injecting more randomness into the tree-growing process. Random forests achieve this in two ways:\n\n\nBootstrap: similar to bagging - each tree is grown from a bootstrap resampled data set, which somewhat decorrelates them.\nSplit-variable randomization: each time a split is made, the search for the split variable is limited to a random subset of \\(m\\) of the \\(p\\) variables."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-a-random-forest-2",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-a-random-forest-2",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\nFor regression trees, typical default values used in split-value randomization are \\(m=\\frac{p}{3}\\) but this should be considered a tuning parameter.\nWhen \\(m=p\\), the randomization amounts to using only step 1 and is the same as bagging."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-a-random-forest-3",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-a-random-forest-3",
    "title": "Regression methods",
    "section": "Regression with a random forest",
    "text": "Regression with a random forest\n\n\nStrengths\n\nTypically have very good performance\nRemarkably good â€œout-of-the boxâ€ - very little tuning required\nBuilt-in validation set - donâ€™t need to sacrifice data for extra validation\nNo pre-processing required\nRobust to outliers\n\n\nWeaknesses\n\nCan become slow on large data sets\nAlthough accurate, often cannot compete with advanced boosting algorithms\nLess interpretable"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nGradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-1",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-1",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nWhereas random forests build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful â€œcommitteeâ€ that are often hard to beat with other algorithms."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-2",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-2",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nThe main idea of boosting is to add new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.\n\nSequential ensemble approach."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-3",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-3",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nBoosting is a framework that iteratively improves any weak learning model. Many gradient boosting applications allow you to â€œplug inâ€ various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-4",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-4",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nA weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. With regards to decision trees, shallow trees represent a weak learner. Commonly, trees with only 1-6 splits are used."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-5",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-5",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nCombining many weak models (versus strong ones) has a few benefits:\n\n\nSpeed: Constructing weak models is computationally cheap.\nAccuracy improvement: Weak models allow the algorithm to learn slowly; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.\nAvoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation)."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-6",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-gradient-boosting-6",
    "title": "Regression methods",
    "section": "Regression with gradient boosting",
    "text": "Regression with gradient boosting\nHere is the algorithm for boosted regression trees with features \\(x\\) and response \\(y\\):\n\n\nFit a decision tree to the data: \\(F_1(x)=y\\),\nWe then fit the next decision tree to the residuals of the previous: \\(h_1(x)=yâˆ’F_1(x)\\)\nAdd this new tree to our algorithm: \\(F_2(x)=F_1(x)+h_1(x)\\),\nFit the next decision tree to the residuals of \\(F_2: h_2(x)=yâˆ’F_2(x)\\),\nAdd this new tree to our algorithm: \\(F_3(x)=F_2(x)+h_1(x)\\),\nContinue this process until some mechanism (i.e.Â cross validation) tells us to stop."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#xgboost-example",
    "href": "slides/origBSMM_8740_lec_04.html#xgboost-example",
    "title": "Regression methods",
    "section": "XGBoost Example",
    "text": "XGBoost Example\nXGBoost is short for eXtreme Gradient Boosting package.\nWhile the XGBoost model often achieves higher accuracy than a single decision tree, it sacrifices the intrinsic interpretability of decision trees. For example, following the path that a decision tree takes to make its decision is trivial and self-explained, but following the paths of hundreds or thousands of trees is much harder.\nWe will work with XGBoost in todayâ€™s lab."
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-neural-nets",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-neural-nets",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\nArchitecture of a Neural Net (NN)\n\nSingle layer NN architecture"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-neural-nets-1",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-neural-nets-1",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\n\nCommon Activation Functions"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-neural-nets-2",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-neural-nets-2",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\n\nCode\n&gt; set.seed(500)\n&gt;   \n&gt; # Boston dataset from MASS\n&gt; data &lt;- MASS::Boston\n&gt; \n&gt; # Normalize the data\n&gt; maxs &lt;- data %&gt;% dplyr::summarise_all(max) %&gt;% as.matrix() %&gt;% as.vector()\n&gt; mins &lt;- data %&gt;% dplyr::summarise_all(min) %&gt;% as.matrix() %&gt;% as.vector()\n&gt; data_scaled &lt;- data %&gt;% \n+   scale(center = mins, scale = maxs - mins) %&gt;% \n+   tibble::as_tibble()\n&gt;   \n&gt; # Split the data into training and testing set\n&gt; data_split &lt;- data_scaled %&gt;% rsample::initial_split(prop = .75)\n&gt; # extracting training data and test data as two seperate dataframes\n&gt; data_train &lt;- rsample::training(data_split)\n&gt; data_test  &lt;- rsample::testing(data_split)\n&gt; \n&gt; nn &lt;- data_train %&gt;% \n+   neuralnet::neuralnet(\n+     medv ~ .\n+     , data = .\n+     , hidden = c(5, 3)\n+     , linear.output = TRUE\n+   )\n&gt;   \n&gt; # Predict on test data\n&gt; pr.nn &lt;- neuralnet::compute( nn, data_test %&gt;% dplyr::select(-medv) )\n&gt;   \n&gt; # Compute mean squared error\n&gt; pr.nn_ &lt;- \n+   pr.nn$net.result * \n+   (max(data$medv) - min(data$medv)) +\n+   min(data$medv)\n&gt; test.r &lt;- \n+   data_test$medv * \n+   (max(data$medv) - min(data$medv)) + \n+   min(data$medv)\n&gt; MSE.nn &lt;- sum((test.r - pr.nn_)^2) / nrow(data_test)"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#regression-with-neural-nets-3",
    "href": "slides/origBSMM_8740_lec_04.html#regression-with-neural-nets-3",
    "title": "Regression methods",
    "section": "Regression with neural nets",
    "text": "Regression with neural nets\n\nNNRegression"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#recap",
    "href": "slides/origBSMM_8740_lec_04.html#recap",
    "title": "Regression methods",
    "section": "Recap",
    "text": "Recap\n\nToday we worked though a few regression methods that are useful for predicting a value given a set of covariates\nNext week we will look at the tidymodels package which will give a way to develop a workflow for fitting and comparing our models"
  },
  {
    "objectID": "slides/origBSMM_8740_lec_04.html#section",
    "href": "slides/origBSMM_8740_lec_04.html#section",
    "title": "Regression methods",
    "section": "",
    "text": "bsmm-8740-fall-2023.github.io/osb"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#announcements",
    "href": "slides/BSMM_8740_lec_02_alt.html#announcements",
    "title": "EDA and feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\nFor this week (September 25 - 29), office hours will be on Friday, from 2:00pm - 4:00pm.\nRegular Thursday office hours resume the week of October 02."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#todays-outline",
    "href": "slides/BSMM_8740_lec_02_alt.html#todays-outline",
    "title": "EDA and feature engineering",
    "section": "Todayâ€™s Outline",
    "text": "Todayâ€™s Outline\n\nComplete last weekâ€™s lab\nReview this weekâ€™s material\n\nIntroduction to exploratory data analysis (EDA), and\nFeature engineering in the tidyverse\n\nStart (and finish?) this weekâ€™s lab\n\n\n\n\n\n\n\nNote\n\n\nThe first two labs are designed to give you practice with the Tidyverse tools for manipulating data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-1",
    "title": "EDA and feature engineering",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nExploratory data analysis is the process of understanding a new dataset by looking at the data, constructing graphs, tables, and models. We want to understand three aspects:\n\neach individual variable by itself;\neach individual variable in the context of other, relevant, variables; and\nthe data that are not there."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-2",
    "href": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-2",
    "title": "EDA and feature engineering",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nDuring EDA we want to come to understand the issues and features of the dataset and how this may affect analysis decisions.\nWe are especially concerned about missing values and outliers."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-3",
    "href": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-3",
    "title": "EDA and feature engineering",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nWe are going to perform two broad categories of EDA:\n\nDescriptive Statistics, which includes mean, median, mode, inter-quartile range, and so on.\nGraphical Methods, which includes histogram, density estimation, box plots, and so on."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-4",
    "href": "slides/BSMM_8740_lec_02_alt.html#exploratory-data-analysis-eda-4",
    "title": "EDA and feature engineering",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nDescriptive statistics and graphical methods support the following process:\n\nUnderstand the distribution and properties of individual variables.\nUnderstand relationships between variables.\nUnderstand what is not there."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nOur first dataset is a sample of categorical variables from the General Social Survey, a long-running US survey conducted by the independent research organization NORC at the University of Chicago.\n\n\n\n\nCode\n&gt; dat &lt;- forcats::gss_cat\n&gt; dat %&gt;% utils::head()\n\n\n# A tibble: 6 Ã— 9\n   year marital         age race  rincome        partyid            relig    denom tvhours\n  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;              &lt;fct&gt;    &lt;fct&gt;   &lt;int&gt;\n1  2000 Never married    26 White $8000 to 9999  Ind,near rep       Protestâ€¦ Soutâ€¦      12\n2  2000 Divorced         48 White $8000 to 9999  Not str republican Protestâ€¦ Baptâ€¦      NA\n3  2000 Widowed          67 White Not applicable Independent        Protestâ€¦ No dâ€¦       2\n4  2000 Never married    39 White Not applicable Ind,near rep       Orthodoâ€¦ Not â€¦       4\n5  2000 Divorced         25 White Not applicable Not str democrat   None     Not â€¦       1\n6  2000 Married          25 White $20000 - 24999 Strong democrat    Protestâ€¦ Soutâ€¦      NA\n\n\n\n\nexecute ?forcats::gss_cat to see the data dictionary"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example-1",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nUse dplyr::glimpse() to see every column in a data.frame\n\n\n&gt; dat %&gt;% dplyr::slice_head(n=10) %&gt;% dplyr::glimpse()\n\nRows: 10\nColumns: 9\n$ year    &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000\n$ marital &lt;fct&gt; Never married, Divorced, Widowed, Never married, Divorced, Married, Neveâ€¦\n$ age     &lt;int&gt; 26, 48, 67, 39, 25, 25, 36, 44, 44, 47\n$ race    &lt;fct&gt; White, White, White, White, White, White, White, White, White, White\n$ rincome &lt;fct&gt; $8000 to 9999, $8000 to 9999, Not applicable, Not applicable, Not applicâ€¦\n$ partyid &lt;fct&gt; \"Ind,near rep\", \"Not str republican\", \"Independent\", \"Ind,near rep\", \"Noâ€¦\n$ relig   &lt;fct&gt; Protestant, Protestant, Protestant, Orthodox-christian, None, Protestantâ€¦\n$ denom   &lt;fct&gt; \"Southern baptist\", \"Baptist-dk which\", \"No denomination\", \"Not applicabâ€¦\n$ tvhours &lt;int&gt; 12, NA, 2, 4, 1, NA, 3, NA, 0, 3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example-2",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example-2",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nUse dplyr::slice_sample() to see a random selection of rows in a data.frame\n\n\n&gt; dat %&gt;% dplyr::slice_sample(n=10)\n\n# A tibble: 10 Ã— 9\n    year marital         age race  rincome        partyid            relig   denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;              &lt;fct&gt;   &lt;fct&gt;   &lt;int&gt;\n 1  2010 Never married    24 White $25000 or more Strong democrat    None    Not â€¦       4\n 2  2002 Divorced         29 White $25000 or more Strong republican  Protesâ€¦ Other      NA\n 3  2006 Divorced         49 White $25000 or more Strong republican  Protesâ€¦ Unitâ€¦      NA\n 4  2010 Married          75 White $20000 - 24999 Not str republican Protesâ€¦ Other       2\n 5  2006 Married          33 White $15000 - 19999 Not str republican Other   Not â€¦       5\n 6  2004 Married          62 White $25000 or more Strong republican  Catholâ€¦ Not â€¦      NA\n 7  2004 Never married    18 White Not applicable Independent        None    Not â€¦      NA\n 8  2012 Never married    39 White $25000 or more Not str democrat   Christâ€¦ Not â€¦       5\n 9  2010 Divorced         82 Black Not applicable Strong democrat    Protesâ€¦ Am bâ€¦      NA\n10  2004 Widowed          56 Black $4000 to 4999  Independent        Protesâ€¦ Baptâ€¦      NA"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example-3",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example-3",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nMost of the columns here are factors (categories). Use forcats::fct_count() to count the factor entries.\n\n\n&gt; forcats::fct_count(dat$relig) %&gt;% dplyr::arrange(desc(n))\n\n# A tibble: 16 Ã— 2\n   f                           n\n   &lt;fct&gt;                   &lt;int&gt;\n 1 Protestant              10846\n 2 Catholic                 5124\n 3 None                     3523\n 4 Christian                 689\n 5 Jewish                    388\n 6 Other                     224\n 7 Buddhism                  147\n 8 Inter-nondenominational   109\n 9 Moslem/islam              104\n10 Orthodox-christian         95\n11 No answer                  93\n12 Hinduism                   71\n13 Other eastern              32\n14 Native american            23\n15 Don't know                 15\n16 Not applicable              0"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-example-4",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-example-4",
    "title": "EDA and feature engineering",
    "section": "EDA: example",
    "text": "EDA: example\nThe base R function summary() can be used for key summary statistics of the data.\n\n\n&gt; dat %&gt;% summary()\n\n      year               marital           age                    race      \n Min.   :2000   No answer    :   17   Min.   :18.00   Other         : 1959  \n 1st Qu.:2002   Never married: 5416   1st Qu.:33.00   Black         : 3129  \n Median :2006   Separated    :  743   Median :46.00   White         :16395  \n Mean   :2007   Divorced     : 3383   Mean   :47.18   Not applicable:    0  \n 3rd Qu.:2010   Widowed      : 1807   3rd Qu.:59.00                         \n Max.   :2014   Married      :10117   Max.   :89.00                         \n                                      NA's   :76                            \n           rincome                   partyid            relig      \n $25000 or more:7363   Independent       :4119   Protestant:10846  \n Not applicable:7043   Not str democrat  :3690   Catholic  : 5124  \n $20000 - 24999:1283   Strong democrat   :3490   None      : 3523  \n $10000 - 14999:1168   Not str republican:3032   Christian :  689  \n $15000 - 19999:1048   Ind,near dem      :2499   Jewish    :  388  \n Refused       : 975   Strong republican :2314   Other     :  224  \n (Other)       :2603   (Other)           :2339   (Other)   :  689  \n              denom          tvhours      \n Not applicable  :10072   Min.   : 0.000  \n Other           : 2534   1st Qu.: 1.000  \n No denomination : 1683   Median : 2.000  \n Southern baptist: 1536   Mean   : 2.981  \n Baptist-dk which: 1457   3rd Qu.: 4.000  \n United methodist: 1067   Max.   :24.000  \n (Other)         : 3134   NA's   :10146"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-packages-for-eda",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-packages-for-eda",
    "title": "EDA and feature engineering",
    "section": "EDA: packages for EDA",
    "text": "EDA: packages for EDA\n\nThe function skimr::skim() gives an enhanced version of base Râ€™s summary() .\nOther packages, such as DataExplorer:: rely more on graphing.\n\nWeâ€™ll look at a few of the DataExplorer:: functions next."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerintroduce",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerintroduce",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::introduce",
    "text": "DataExplorer::introduce\nThe function DataExplorer::introduce produces a basic description of the data in a data.frame.\n\n\nCode\n&gt; dat %&gt;% DataExplorer::introduce() %&gt;% dplyr::glimpse()\n\n\nRows: 1\nColumns: 9\n$ rows                 &lt;int&gt; 21483\n$ columns              &lt;int&gt; 9\n$ discrete_columns     &lt;int&gt; 6\n$ continuous_columns   &lt;int&gt; 3\n$ all_missing_columns  &lt;int&gt; 0\n$ total_missing_values &lt;int&gt; 10222\n$ complete_rows        &lt;int&gt; 11299\n$ total_observations   &lt;int&gt; 193347\n$ memory_usage         &lt;dbl&gt; 784776"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_intro",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_intro",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::plot_intro",
    "text": "DataExplorer::plot_intro\nThe function DataExplorer::plot_intro is a visual version of DataExplorer::introduce.\n\n\nCode\n&gt; dat %&gt;% DataExplorer::plot_intro()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_missing",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_missing",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::plot_missing",
    "text": "DataExplorer::plot_missing\nThe function DataExplorer::plot_missing shows information on missing data visually.\n\n\nCode\n&gt; dat %&gt;% DataExplorer::plot_missing()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#d_explorerprofile_missing",
    "href": "slides/BSMM_8740_lec_02_alt.html#d_explorerprofile_missing",
    "title": "EDA and feature engineering",
    "section": "D_Explorer::profile_missing",
    "text": "D_Explorer::profile_missing\n\n\nCode\n&gt; dat %&gt;% DataExplorer::profile_missing() %&gt;% \n+   gt::gt('feature') %&gt;% \n+   gtExtras::gt_theme_espn() %&gt;% \n+   gt::tab_options( table.font.size = gt::px(28) ) %&gt;% \n+   gt::as_raw_html()\n\n\n\n  \n  \n\n\n\n\nnum_missing\npct_missing\n\n\n\n\nyear\n0\n0.000000000\n\n\nmarital\n0\n0.000000000\n\n\nage\n76\n0.003537681\n\n\nrace\n0\n0.000000000\n\n\nrincome\n0\n0.000000000\n\n\npartyid\n0\n0.000000000\n\n\nrelig\n0\n0.000000000\n\n\ndenom\n0\n0.000000000\n\n\ntvhours\n10146\n0.472280408"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_density",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_density",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::plot_density",
    "text": "DataExplorer::plot_density\n\n\nCode\n&gt; dat %&gt;% \n+   DataExplorer::plot_density(\n+     ggtheme = theme_bw(base_size = 18) + theme(legend.position = \"top\")\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#d_explorerplot_histogram",
    "href": "slides/BSMM_8740_lec_02_alt.html#d_explorerplot_histogram",
    "title": "EDA and feature engineering",
    "section": "D_Explorer::plot_histogram",
    "text": "D_Explorer::plot_histogram\n\n\nCode\n&gt; dat %&gt;% \n+   DataExplorer::plot_histogram(\n+     ggtheme = theme_bw(base_size = 18) + theme(legend.position = \"top\")\n+   )"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_bar",
    "href": "slides/BSMM_8740_lec_02_alt.html#dataexplorerplot_bar",
    "title": "EDA and feature engineering",
    "section": "DataExplorer::plot_bar",
    "text": "DataExplorer::plot_bar\n\n\nCode\n&gt; dat %&gt;% DataExplorer::plot_bar()"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions",
    "title": "EDA and feature engineering",
    "section": "EDA: additional DataExplorer functions",
    "text": "EDA: additional DataExplorer functions\nWhen the data has more numerical values consider looking at the relationships with the following functions:\n\nDataExplorer::plot_correlation() creates a correlation heatmap.\n\n\n&gt; iris %&gt;% DataExplorer::plot_correlation(type = \"c\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions-1",
    "title": "EDA and feature engineering",
    "section": "EDA: additional DataExplorer functions",
    "text": "EDA: additional DataExplorer functions\nWhen the data has more numerical values consider looking at the relationships with the following functions:\n\nDataExplorer::plot_scatterplot() creates a scatterplot for all measurements.\n\n\n&gt; iris %&gt;% DataExplorer::plot_scatterplot(by = \"Species\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions-2",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-additional-dataexplorer-functions-2",
    "title": "EDA and feature engineering",
    "section": "EDA: additional DataExplorer functions",
    "text": "EDA: additional DataExplorer functions\nWhen the data has more numerical values consider looking at the relationships with the following functions:\n\nDataExplorer::plot_qq() creates a quantile-quantile plot for each continuous feature.\n\n\n&gt; iris %&gt;% DataExplorer::plot_qq(by = \"Species\", ncol = 2L)\n\n\n\n\n\n\n\n\nNote\n\n\nA Qâ€“Q plot (quantile-quantile plot) is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-classifying-missing-data",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-classifying-missing-data",
    "title": "EDA and feature engineering",
    "section": "EDA: classifying missing data",
    "text": "EDA: classifying missing data\nThere are three main categories of missing data\n\nMissing Completely At Random (MCAR);\n\nmissing but independent of other measurements\n\nMissing at Random (MAR);\n\nmissing in a way related to other measurements\n\nMissing Not At Random (MNAR).\n\nmissing as a property of the variable or some other unmeasured variable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-handling-missing-data",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-handling-missing-data",
    "title": "EDA and feature engineering",
    "section": "EDA: handling missing data",
    "text": "EDA: handling missing data\nWe can think of a few options for dealing with missing data\n\nDrop observations with missing data.\nImpute the mean of observations without missing data.\nUse multiple imputation.\n\n\n\n\n\n\n\n\nNote\n\n\nMultiple imputation involves generating several estimates for the missing values and then averaging the outcomes."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#example-mcar-or-mar",
    "href": "slides/BSMM_8740_lec_02_alt.html#example-mcar-or-mar",
    "title": "EDA and feature engineering",
    "section": "Example: MCAR or MAR?",
    "text": "Example: MCAR or MAR?\n\n\nCode\n&gt; dat %&gt;% dplyr::select(partyid) %&gt;% table() %&gt;% tibble::as_tibble() %&gt;% \n+   dplyr::left_join(\n+     dat %&gt;% dplyr::filter(is.na(age)) %&gt;% \n+       dplyr::select(na_partyid = partyid) %&gt;% table() %&gt;% tibble::as_tibble()\n+     , by = c(\"partyid\" = \"na_partyid\")\n+   ) %&gt;% \n+   dplyr::mutate(pct_na = n.y / n.x)\n\n\n# A tibble: 10 Ã— 4\n   partyid              n.x   n.y   pct_na\n   &lt;chr&gt;              &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1 No answer            154     9 0.0584  \n 2 Don't know             1     0 0       \n 3 Other party          393     3 0.00763 \n 4 Strong republican   2314     8 0.00346 \n 5 Not str republican  3032     8 0.00264 \n 6 Ind,near rep        1791     2 0.00112 \n 7 Independent         4119    18 0.00437 \n 8 Ind,near dem        2499     2 0.000800\n 9 Not str democrat    3690    11 0.00298 \n10 Strong democrat     3490    15 0.00430"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-missing-data",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-missing-data",
    "title": "EDA and feature engineering",
    "section": "EDA: missing data",
    "text": "EDA: missing data\nFinally, be aware that how missing data is encoded depends on the dataset\n\nR defaults to NA when reading data, in joins, etc.\nThe creator(s) of the dataset may use a different encoding.\nMissing data can have multiple representations according to semantics of the measurement.\nRemember that entire measurements can be missing (i.e.Â from all observations, not just some)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-summary",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-summary",
    "title": "EDA and feature engineering",
    "section": "EDA: summary",
    "text": "EDA: summary\n\nUnderstand what the measurements represent and confirm constraints (if any) and suitability of encoding.\nMake a decision on how to deal with missing data.\nUnderstand shape of measurements (may identify an issue or suggest a data transformation)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#eda-bad-data-rightarrow-bad-results",
    "href": "slides/BSMM_8740_lec_02_alt.html#eda-bad-data-rightarrow-bad-results",
    "title": "EDA and feature engineering",
    "section": "EDA: bad data \\(\\rightarrow\\) bad results",
    "text": "EDA: bad data \\(\\rightarrow\\) bad results"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nfor continuous variables (usually the independent variables or covariates):\n\nnormalization (scale values to \\([0,1]\\))\n\n\\(X_\\text{norm} = \\frac{X-X_\\text{min}}{X_\\text{max}-X_\\text{min}}\\)\n\nstandardization (subtract mean and scale by stdev)\n\n\\(X_\\text{std} = \\frac{X-\\mu_X}{\\sigma_X}\\)\n\nscaling (multiply / divide by a constant)\n\n\\(X_\\text{scaled} = K\\times X\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-1",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nOther common transformations:\n\nBox-cox: with \\(\\tilde{x}\\) the geometric mean of the (positive) predictor data (\\(\\tilde{x}=\\left(\\prod_{i=1}^{n}x_{i}\\right)^{1/n}\\))\n\n\\[\nx_i(\\lambda) = \\left\\{\n\\begin{array}{cc}\n\\frac{x_i^{\\lambda}-1}{\\lambda\\tilde{x}^{\\lambda-1}} & \\lambda\\ne 0\\\\\n\\tilde{x}\\log x_i & \\lambda=0\n\\end{array}\n\\right .  \n\\]\n\n\n\n\n\n\n\nNote\n\n\nBox-cox is an example of a power transform; it is a technique used to stabilize variance, make the data more normal distribution-like."
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-2",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-2",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nOne last common transformation:\n\nlogit transformation for bounded target variables (scaled to lie in \\([0,1]\\))\n\n\\[\n\\text{logit}\\left(p\\right)=\\log\\frac{p}{1-p},\\;p\\in [0,1]\n\\]\n\n\n\n\n\n\n\nNote\n\n\nThe Logit transform is primarily used to transform binary response data, such as survival/non-survival or present/absent, to provide a continuous value in the range \\(\\left(-\\infty,\\infty\\right)\\), where p is the proportion of each sample that is 1 (or 0)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-3",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-3",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nWhy normalize or standardize?\n\nvariation in the range of feature values can lead to biased model performance or difficulties during the learning process, particularly in distance-based algorithms.\n\ne.g.Â income and age\n\nreduce the impact of outliers\nmake results more explainable"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-4",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-4",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nfor continuous variables (usually the target variables):\n\ntransformation (arithmetic, basis functions, polynomials, splines, differencing)\n\n\\(y = \\log(y),\\sqrt{y},\\frac{1}{y}\\), etc.\n\\(y = \\sum_i \\beta_i\\text{f}_i(x)\\;\\text{s.t.}\\;0=\\int\\text{f}_i(x)\\text{f}_j(x)\\; \\forall i\\ne j\\)\n\\(y = \\beta_0+\\beta_1 x+\\beta_2 x^2+\\beta_3 x^3+\\ldots\\)\n\\(y = \\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_3 x_1 x_2+\\ldots\\)\n\\(y'_i = y_i-y_{i-1}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-5",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-5",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nfor categorical variables (either target or explanatory variables):\n\nbinning / bucketing\n\nrepresent a numerical value as a categorical value\n\ncategorical\\(\\rightarrow\\)ordinal and ordinal\\(\\rightarrow\\)categorical\n\nfor date variables:\n\ntimestamp\\(\\rightarrow\\)date or date part"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-6",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-transformation-6",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: transformation",
    "text": "Feature engineering: transformation\nWhy transform?\n\nit can make your model perform better\n\ne.g.Â \\(\\log\\) transform makes exponential data linear, and log-Normal data Gaussian\n\\(\\log\\) transforms also make multiplicative models additive\ne.g.Â polynomials, basis functions and splines help model non-linearities in data"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-creation",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-creation",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: creation",
    "text": "Feature engineering: creation\n\noutliers (due to data entry, measurement/experiment, intentional errors)\n\noutliers can be identified by quantile methods (Gaussian data)\noutliers can be removed, treated as missing, or capped\n\nlag variables (either target or explanatory variables)\n\nuseful in time series models, e.g.Â \\(y_t,y_{t-1},\\ldots y_{t-n}\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-creation-1",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-creation-1",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: creation",
    "text": "Feature engineering: creation\n\nbinning / bucketing\n\nrepresent numerical as categorical and vice versa\n\ninterval and ratio levels"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-fails",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-fails",
    "title": "EDA and feature engineering",
    "section": "Feature engineering fails",
    "text": "Feature engineering fails"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-summary",
    "href": "slides/BSMM_8740_lec_02_alt.html#feature-engineering-summary",
    "title": "EDA and feature engineering",
    "section": "Feature engineering: summary",
    "text": "Feature engineering: summary\n\nrequires an advanced technical skill set\nrequires domain expertise\nis time-consuming and resource intensive\ndifferent analytics algorithms require different feature engineering"
  },
  {
    "objectID": "slides/BSMM_8740_lec_02_alt.html#recap",
    "href": "slides/BSMM_8740_lec_02_alt.html#recap",
    "title": "EDA and feature engineering",
    "section": "Recap",
    "text": "Recap\n\nToday we reviewed key elements of exploratory data analysis, the process that helps us understand the data we have and evaluate how it can help us solve the business problems we are interested in.\nWe also reviewed feature engineering - methods to we can use to facilitate our analysis and make our results more interpretable.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#recap-of-last-week",
    "href": "slides/orig BSMM_8740_lec_07.html#recap-of-last-week",
    "title": "Time series methods",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we introduced the Tidymodels framework in R\nWe showed how we can use the Tidymodels framework to create a workflow for data prep, feature engineering, model fitting and model evaluation.\nToday we look at the using the Tidymodels package to build classification and clustering models."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series",
    "title": "Time series methods",
    "section": "Time series",
    "text": "Time series\n\nToday we will explore time series - data where each observation has a time value.\nWeâ€™ll look at how to manipulate our time values, create time-based features, plot our time series, and decompose time series into components.\nFinally we will use our time series for forecasting, using regression, exponential smoothing and ARIMA1 models\n\nAuto Regressive Integrated Moving Average"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---plotting",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---plotting",
    "title": "Time series methods",
    "section": "Time series - plotting",
    "text": "Time series - plotting\n\n&gt; timetk::bike_sharing_daily %&gt;% dplyr::slice_head() %&gt;% dplyr::glimpse()\n\nRows: 1\nColumns: 16\n$ instant    &lt;dbl&gt; 1\n$ dteday     &lt;date&gt; 2011-01-01\n$ season     &lt;dbl&gt; 1\n$ yr         &lt;dbl&gt; 0\n$ mnth       &lt;dbl&gt; 1\n$ holiday    &lt;dbl&gt; 0\n$ weekday    &lt;dbl&gt; 6\n$ workingday &lt;dbl&gt; 0\n$ weathersit &lt;dbl&gt; 2\n$ temp       &lt;dbl&gt; 0.344167\n$ atemp      &lt;dbl&gt; 0.363625\n$ hum        &lt;dbl&gt; 0.805833\n$ windspeed  &lt;dbl&gt; 0.160446\n$ casual     &lt;dbl&gt; 331\n$ registered &lt;dbl&gt; 654\n$ cnt        &lt;dbl&gt; 985"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---plotting-1",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---plotting-1",
    "title": "Time series methods",
    "section": "Time series - plotting",
    "text": "Time series - plotting\nThe timetk::plot_time_series() function is a good way to to get a quick timeseries plot. From a tidy table we\n\nselect the time value and the column we want to play\npivot (longer) the columns we want to plot\nplot\n\nThe timetk::plot_time_series() function has any options that can be changed."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---plotting-2",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---plotting-2",
    "title": "Time series methods",
    "section": "Time series - plotting",
    "text": "Time series - plotting\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;% \n+   dplyr::select(dteday, casual, registered) %&gt;% \n+   tidyr::pivot_longer(-dteday) %&gt;% \n+   timetk::plot_time_series(\n+     .date_var = dteday\n+     , .value = value\n+     , .color_var = name\n+   )"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime downscaling\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;% \n+   timetk::summarise_by_time(\n+     .date_var = dteday\n+     , .by = \"week\"\n+     , .week_start = 7\n+     , causal = sum(casual)\n+     , registered = mean(registered)\n+     , max_cnt = max(cnt)\n+   )\n\n\n# A tibble: 106 Ã— 4\n   dteday     causal registered max_cnt\n   &lt;date&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 2010-12-26    331       654      985\n 2 2011-01-02    745      1235.    1606\n 3 2011-01-09    477      1167.    1421\n 4 2011-01-16    706      1183.    1927\n 5 2011-01-23    632       994.    1985\n 6 2011-01-30    550      1314.    1708\n 7 2011-02-06   1075      1450.    1746\n 8 2011-02-13   2333      1734.    2927\n 9 2011-02-20   1691      1405.    1969\n10 2011-02-27   2120      1631.    2402\n# â„¹ 96 more rows"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-1",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-1",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime upscalling\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;% \n+   dplyr::select(dteday, casual) %&gt;% \n+   timetk::pad_by_time(.date_var = dteday, .by = \"hour\") %&gt;% \n+   timetk::mutate_by_time(\n+     .date_var = dteday\n+     , .by = \"day\"\n+     , casual = sum(casual,na.rm=T)/24\n+   )\n\n\n# A tibble: 17,521 Ã— 2\n   dteday              casual\n   &lt;dttm&gt;               &lt;dbl&gt;\n 1 2011-01-01 00:00:00   13.8\n 2 2011-01-01 01:00:00   13.8\n 3 2011-01-01 02:00:00   13.8\n 4 2011-01-01 03:00:00   13.8\n 5 2011-01-01 04:00:00   13.8\n 6 2011-01-01 05:00:00   13.8\n 7 2011-01-01 06:00:00   13.8\n 8 2011-01-01 07:00:00   13.8\n 9 2011-01-01 08:00:00   13.8\n10 2011-01-01 09:00:00   13.8\n# â„¹ 17,511 more rows"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-2",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-2",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime filtering\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;%\n+   timetk::filter_by_time(\n+     .date_var = dteday\n+     , .start_date=\"2012-01-15\"\n+     , .end_date = \"2012-07-01\"\n+   ) %&gt;% \n+   timetk::plot_time_series(.date_var = dteday, casual)"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-3",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-3",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntime offsets\n\n\nCode\n&gt; require(timetk, quietly = FALSE)\n&gt; timetk::bike_sharing_daily %&gt;%\n+   timetk::filter_by_time(\n+     .date_var = dteday\n+     , .start_date=\"2012-01-15\"\n+     , .end_date = \"2012-01-15\" %+time% \"12 weeks\"\n+   ) %&gt;% \n+   timetk::plot_time_series(.date_var = dteday, casual)"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-4",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-4",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\nmutate by period\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;%\n+   dplyr::select(dteday, casual) %&gt;% \n+   timetk::mutate_by_time(\n+     .date_var = dteday\n+     , .by = \"7 days\"\n+     , casual_mean = mean(casual)\n+     , casual_median = median(casual)\n+     , casual_max = max(casual)\n+     , casual_min = min(casual)\n+   )\n\n\n# A tibble: 731 Ã— 6\n   dteday     casual casual_mean casual_median casual_max casual_min\n   &lt;date&gt;      &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 2011-01-01    331       144             120        331         82\n 2 2011-01-02    131       144             120        331         82\n 3 2011-01-03    120       144             120        331         82\n 4 2011-01-04    108       144             120        331         82\n 5 2011-01-05     82       144             120        331         82\n 6 2011-01-06     88       144             120        331         82\n 7 2011-01-07    148       144             120        331         82\n 8 2011-01-08     68        46.1            43         68         25\n 9 2011-01-09     54        46.1            43         68         25\n10 2011-01-10     41        46.1            43         68         25\n# â„¹ 721 more rows"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-5",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-5",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\nsummarize by period\n\n\nCode\n&gt; timetk::bike_sharing_daily %&gt;%\n+   timetk::summarize_by_time(\n+     .date_var = dteday\n+     , .by = \"7 days\"\n+     , casual_mean = mean(casual)\n+     , registered_mean = mean(registered)\n+     , windspeed_max = max(windspeed)\n+   )\n\n\n# A tibble: 119 Ã— 4\n   dteday     casual_mean registered_mean windspeed_max\n   &lt;date&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1 2011-01-01       144             1201.         0.249\n 2 2011-01-08        46.1           1147.         0.362\n 3 2011-01-15       119.            1203.         0.353\n 4 2011-01-22        86              981.         0.294\n 5 2011-01-29       102.            1130          0.187\n 6 2011-02-01       120.            1377.         0.278\n 7 2011-02-08       172.            1455.         0.418\n 8 2011-02-15       366             1618.         0.507\n 9 2011-02-22       233.            1546.         0.347\n10 2011-03-01       243.            1495          0.343\n# â„¹ 109 more rows"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-6",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-6",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ncreate a timeseries\n\n\nCode\n&gt; tibble::tibble(\n+   date = \n+     timetk::tk_make_timeseries(\n+       start_date = \"2024\"\n+       , length_out = 100\n+       , by = \"month\"\n+     )\n+   , values=1:100\n+ )\n\n\n# A tibble: 100 Ã— 2\n   date       values\n   &lt;date&gt;      &lt;int&gt;\n 1 2024-01-01      1\n 2 2024-02-01      2\n 3 2024-03-01      3\n 4 2024-04-01      4\n 5 2024-05-01      5\n 6 2024-06-01      6\n 7 2024-07-01      7\n 8 2024-08-01      8\n 9 2024-09-01      9\n10 2024-10-01     10\n# â„¹ 90 more rows"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-7",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-7",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ncreate a timeseries\n\n\nCode\n&gt; timetk::tk_make_holiday_sequence(\n+   start_date = \"2025\"\n+   , end_date = \"2027\"\n+   , calendar = \"TSX\"\n+ ) %&gt;% \n+   timetk::tk_get_holiday_signature(holiday_pattern = \"Thanksgiving\",locale_set = \"CA\", exchange = \"TSX\") %&gt;% \n+   dplyr::slice_head(n = 6) %&gt;% \n+   dplyr::glimpse()\n\n\nRows: 6\nColumns: 6\n$ index              &lt;date&gt; 2025-01-01, 2025-02-17, 2025-02-17, 2025-02-17, 2025-04-18, 2025-â€¦\n$ exch_TSX           &lt;dbl&gt; 1, 1, 1, 1, 1, 1\n$ locale_CA          &lt;dbl&gt; 0, 1, 1, 1, 0, 1\n$ CA_ThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0\n$ JP_ThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0\n$ US_ThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-8",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-8",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntimeseries transformations\n\nCode\n&gt; # plot wind speed\n&gt; timetk::bike_sharing_daily %&gt;% \n+   timetk::plot_time_series(dteday, windspeed, .title = \"Time Series - Raw\")\n&gt; # plot transformed speed\n&gt; timetk::bike_sharing_daily %&gt;% \n+   timetk::plot_time_series(\n+     dteday\n+     , timetk::box_cox_vec(windspeed, lambda=\"auto\",  silent = T)\n+     , .title = \"Time Series - Box Cox Tranformed\")"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-9",
    "href": "slides/orig BSMM_8740_lec_07.html#time-series---timetk-9",
    "title": "Time series methods",
    "section": "Time series - timetk::",
    "text": "Time series - timetk::\ntimeseries transformations\n\nSee Also\n\nLag Transformation: lag_vec()\nDifferencing Transformation: diff_vec()\nRolling Window Transformation: slidify_vec()\nLoess Smoothing Transformation: smooth_vec()\nFourier Series: fourier_vec()\nMissing Value Imputation for Time Series: ts_impute_vec(), ts_clean_vec()\n\nOther common transformations to reduce variance: log(), log1p() and sqrt()"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#classification",
    "href": "slides/orig BSMM_8740_lec_07.html#classification",
    "title": "Time series methods",
    "section": "Classification",
    "text": "Classification\nLazy learners or instance-based learners, do not create any model immediately from the training data, and this where the lazy aspect comes from. They just memorize the training data, and each time there is a need to make a prediction, they search for the nearest neighbor from the whole training data. Examples are:\n\nK-Nearest Neighbor.\nCase-based reasoning."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#types-of-classification",
    "href": "slides/orig BSMM_8740_lec_07.html#types-of-classification",
    "title": "Time series methods",
    "section": "Types of classification",
    "text": "Types of classification\n\nBinary classification\nMulti-Class Classification (mutually exclusive)\nMulti-Label Classification (not mutually exclusive)\nImbalanced Classification"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#binary-logistic-regression",
    "href": "slides/orig BSMM_8740_lec_07.html#binary-logistic-regression",
    "title": "Time series methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\nLogistic regression is a Generalized Linear Model where the dependent (categorical) variable \\(y\\) takes values in \\({0,1}\\). This can be interpreted as identifying two classes, and logistic regression provides a prediction for class membership based on a linear combination of the explanatory variables.\nLogistic regression is an example of supervised learning."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#binary-logistic-regression-1",
    "href": "slides/orig BSMM_8740_lec_07.html#binary-logistic-regression-1",
    "title": "Time series methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\nFor the logistic GLM:\n\nthe distribution of the observations is Binomial with parameter \\(\\pi\\)\nthe explanatory variables are linear in the parameters: \\(\\eta=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_2 x_2\\ldots+\\beta_n x_n\\)\nthe link function is the logit: \\(\\eta=\\text{logit}(\\pi) = \\log(\\frac{\\pi}{1-\\pi})\\)\n\nIt follows that \\(\\pi = \\frac{e^\\eta}{1+e^\\eta} = \\frac{1}{1+e^{-\\eta}}\\), which is a sigmoid function in the explanatory variables. The equation \\(\\eta=0\\) defines a linear decision boundary or classification threshold."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#binary-logistic-regression-2",
    "href": "slides/orig BSMM_8740_lec_07.html#binary-logistic-regression-2",
    "title": "Time series methods",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\nThe term \\(\\frac{\\pi}{1-\\pi}\\) is called the the odds-ratio. By its definition \\(\\frac{\\pi}{1-\\pi}=e^{\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_2 x_2\\ldots+\\beta_n x_n}\\)\nSo if \\(x_1\\) changes by one unit (\\(x_1\\rightarrow x_1+1\\)), then the odds ratio changes by \\(e^{\\beta_1}\\)."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#classifier-metrics",
    "href": "slides/orig BSMM_8740_lec_07.html#classifier-metrics",
    "title": "Time series methods",
    "section": "Classifier metrics",
    "text": "Classifier metrics\nConfusion matrix\nThe confusion matrix is a 2x2 table summarizing the number of correct predictions of the model:\n\n\n\n\npredict 1\npredict 0\n\n\n\n\ndata =1\ntrue positives (TP)\nfalse negatives (FN)1\n\n\ndata = 0\nfalse positives (FP)2\ntrue negatives (TN)\n\n\n\nType II errorType I error"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#classifier-metrics-1",
    "href": "slides/orig BSMM_8740_lec_07.html#classifier-metrics-1",
    "title": "Time series methods",
    "section": "Classifier metrics",
    "text": "Classifier metrics\nAccuracy\nAccuracy measure the percent of correct predictions:\n\\[\n\\frac{\\text{TP}+\\text{TN}}{\\text{total # observations}}\n\\]\nPrecision\nAccuracy measure the percent of positive predictions that are correct:\n\\[\n\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\n\\]"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#classifier-metrics-2",
    "href": "slides/orig BSMM_8740_lec_07.html#classifier-metrics-2",
    "title": "Time series methods",
    "section": "Classifier metrics",
    "text": "Classifier metrics\nRecall / Sensitivity\nMeasures the success at predicting the first class\n\\[\n\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\\qquad\\text{(True Positive Rate - TPR)}\n\\]\nRecall / Specificity\nMeasures the success at predicting the second class\n\\[\n\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}\\qquad\\text{(True Negative Rate - TNR)}\n\\]"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#classifier-metrics-3",
    "href": "slides/orig BSMM_8740_lec_07.html#classifier-metrics-3",
    "title": "Time series methods",
    "section": "Classifier metrics",
    "text": "Classifier metrics\nROC Curves\nConsider plotting the TPR against the FPR (1-TNR) at different classification thresholds. This is the ROC.\n\nthe diagonal (TPR = 1-TNR) describes a process equivalent to tossing a fair coin (i.e.Â no predictive power)\nour method should have a curve above the diagonal; which shape is better depends on the purpose of our classifier.\n\nSo, how to compute?"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#classifier-metrics-4",
    "href": "slides/orig BSMM_8740_lec_07.html#classifier-metrics-4",
    "title": "Time series methods",
    "section": "Classifier metrics",
    "text": "Classifier metrics\nThe AUC: the area under the ROC.\nIt turns out the AUC is very easy to compute and gives us the ROC at the same time.\n\nrank order your data by decreasing positive predicted probability\nagainst the cumulative percent of negative observations plot the cumulative percent of positive observations"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#example-create-the-workflow",
    "href": "slides/orig BSMM_8740_lec_07.html#example-create-the-workflow",
    "title": "Time series methods",
    "section": "Example: create the workflow",
    "text": "Example: create the workflow\nWorkflow to model credit card default\n\n\nCode\n&gt; data &lt;- ISLR::Default %&gt;% tibble::as_tibble()\n&gt; set.seed(8740)\n&gt; \n&gt; # split data\n&gt; data_split &lt;- rsample::initial_split(data)\n&gt; default_train &lt;- rsample::training(data_split)\n&gt; \n&gt; # create a recipe\n&gt; default_recipe &lt;- default_train %&gt;% \n+   recipes::recipe(formula = default ~ student + balance + income) %&gt;% \n+   recipes::step_dummy(recipes::all_nominal_predictors())\n&gt; \n&gt; # create a linear regression model\n&gt; default_model &lt;- parsnip::logistic_reg() %&gt;% \n+   parsnip::set_engine(\"glm\") %&gt;% \n+   parsnip::set_mode(\"classification\")\n&gt; \n&gt; # create a workflow\n&gt; default_workflow &lt;- workflows::workflow() %&gt;%\n+   workflows::add_recipe(default_recipe) %&gt;%\n+   workflows::add_model(default_model)"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#example-fit-the-model-using-the-data",
    "href": "slides/orig BSMM_8740_lec_07.html#example-fit-the-model-using-the-data",
    "title": "Time series methods",
    "section": "Example: fit the model using the data",
    "text": "Example: fit the model using the data\n\n&gt; # fit the model\n&gt; lm_fit &lt;- \n+   default_workflow %&gt;% \n+   parsnip::fit(default_train)\n&gt; \n&gt; # augment the data with the predictions using the model fit\n&gt; training_results &lt;- \n+   broom::augment(lm_fit , default_train)"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#example-compute-the-auc",
    "href": "slides/orig BSMM_8740_lec_07.html#example-compute-the-auc",
    "title": "Time series methods",
    "section": "Example: compute the AUC",
    "text": "Example: compute the AUC\n\n&gt; auc_roc_tbl &lt;- training_results %&gt;% \n+   # order prediction probability from high to low\n+   dplyr::arrange( desc(.pred_Yes) ) %&gt;% \n+   # make new variable for cumulative % of 'Yes' category\n+   dplyr::mutate( \n+     # scale to percent (# of all 'Yes' categories)\n+     y = ifelse(default == \"Yes\", 1/sum(default == \"Yes\"),0)\n+     # accumulate the values\n+     , y = cumsum(y)\n+   ) %&gt;% \n+   # keep the 'No' category values\n+   dplyr::filter(default == \"No\") %&gt;% \n+   # number rows & scale to % of total; compute incremental areas\n+   tibble::rowid_to_column(\"ID\") %&gt;% \n+   dplyr::mutate(\n+     auc_inc = y / max(ID) # multiply the height by the width\n+     , ID = ID / max(ID)   # scale to percent (# of all 'No' categories)\n+   )"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#example-plot-the-roc",
    "href": "slides/orig BSMM_8740_lec_07.html#example-plot-the-roc",
    "title": "Time series methods",
    "section": "Example: plot the ROC",
    "text": "Example: plot the ROC\n\n\nCode\n&gt; auc_roc_tbl %&gt;% \n+   ggplot(aes(x=ID, y = y)) +\n+   geom_line() +\n+   # xlim(c(0,1)) +\n+   geom_abline(slope=1) + \n+     coord_fixed() +\n+     labs(\n+       title = \"ROC curve for load default prediction\",\n+       subtitle = \n+         stringr::str_glue(\"Logistic Regression AUC = {scales::label_number(accuracy = 10^-7)(sum(auc_roc_tbl$auc_inc) )}\")\n+     )"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#example-yardstick",
    "href": "slides/orig BSMM_8740_lec_07.html#example-yardstick",
    "title": "Time series methods",
    "section": "Example: yardstick",
    "text": "Example: yardstick\n\n&gt; training_results %&gt;% \n+   yardstick::roc_auc(.pred_No, truth = default)"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#example-yardstick-1",
    "href": "slides/orig BSMM_8740_lec_07.html#example-yardstick-1",
    "title": "Time series methods",
    "section": "Example: yardstick",
    "text": "Example: yardstick\n\n\nCode\n&gt; training_results %&gt;% yardstick::roc_curve(.pred_No, truth = default) %&gt;% autoplot()"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#other-classification-methods",
    "href": "slides/orig BSMM_8740_lec_07.html#other-classification-methods",
    "title": "Time series methods",
    "section": "Other Classification Methods",
    "text": "Other Classification Methods"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification",
    "href": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification",
    "title": "Time series methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nThis method starts with Bayes rule: for \\(K\\) classes and \\(N\\) features, since \\(\\mathbb{P}\\left[\\left.C_{k}\\right|x_{1},\\ldots,x_{N}\\right]\\times\\mathbb{P}\\left[x_{1},\\ldots,x_{N}\\right]\\) is equal to \\(\\mathbb{P}\\left[\\left.x_{1},\\ldots,x_{N}\\right|C_{k}\\right]\\times\\mathbb{P}\\left[C_{k}\\right]\\), we can write\n\\[\n\\mathbb{P}\\left[\\left.C_{k}\\right|x_{1},\\ldots,x_{N}\\right]=\\frac{\\mathbb{P}\\left[\\left.x_{1},\\ldots,x_{N}\\right|C_{k}\\right]\\times\\mathbb{P}\\left[C_{k}\\right]}{\\mathbb{P}\\left[x_{1},\\ldots,x_{N}\\right]}\n\\]"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-1",
    "href": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-1",
    "title": "Time series methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nIf we assume that the features are all independent we can write Bayes rule as\n\\[\n\\mathbb{P}\\left[\\left.C_{k}\\right|x_{1},\\ldots,x_{N}\\right]=\\frac{\\mathbb{P}\\left[C_{k}\\right]\\times\\prod_{n=1}^{N}\\mathbb{P}\\left[\\left.x_{n}\\right|C_{k}\\right]}{\\prod_{n=1}^{N}\\mathbb{P}\\left[x_{n}\\right]}\n\\]\nand our classifier is\n\\[\nC_{k}=\\arg\\max_{C_{k}}\\mathbb{P}\\left[C_{k}\\right]\\prod_{n=1}^{N}\\mathbb{P}\\left[\\left.x_{n}\\right|C_{k}\\right]\n\\]"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-2",
    "href": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-2",
    "title": "Time series methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nSo it remains to calculate the class probability \\(\\mathbb{P}\\left[C_{k}\\right]\\) and the conditional probabilities \\(\\mathbb{P}\\left[\\left.x_{n}\\right|C_{k}\\right]\\)\nThe different naive Bayes classifiers differ mainly by the assumptions they make regarding the conditional probabilities."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-3",
    "href": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-3",
    "title": "Time series methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nIf our features are all ordinal, then\n\nThe class probabilities are simply the frequency of instances that belong to each class divided by the total number of instances.\nThe conditional probabilities are the frequency of each feature value for a given class value divided by the frequency of instances with that class value."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-4",
    "href": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-4",
    "title": "Time series methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\nIf any features are numeric, we can estimate conditional probabilities by assuming that the numeric features have a Gaussian distribution for each class"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-5",
    "href": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-5",
    "title": "Time series methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\n\nCode\n&gt; library(discrim)\n&gt; # create a naive bayes classifier\n&gt; default_model_nb &lt;- parsnip::naive_Bayes() %&gt;% \n+   parsnip::set_engine(\"klaR\") %&gt;% \n+   parsnip::set_mode(\"classification\")\n&gt; \n&gt; # create a workflow\n&gt; default_workflow_nb &lt;- workflows::workflow() %&gt;%\n+   workflows::add_recipe(default_recipe) %&gt;%\n+   workflows::add_model(default_model_nb)\n&gt; \n&gt; # fit the model\n&gt; lm_fit_nb &lt;- \n+   default_workflow_nb %&gt;% \n+   parsnip::fit(\n+     default_train\n+   , control = \n+     workflows::control_workflow(parsnip::control_parsnip(verbosity = 1L))\n+   )\n&gt; \n&gt; # augment the data with the predictions using the model fit\n&gt; training_results_nb &lt;- \n+   broom::augment(lm_fit_nb , default_train)"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-6",
    "href": "slides/orig BSMM_8740_lec_07.html#naive-bayes-classification-6",
    "title": "Time series methods",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\nAUCROC\n\n\n\n&gt; training_results_nb %&gt;% \n+   yardstick::roc_auc(.pred_No, truth = default)\n\n\n\n\n\nCode\n&gt; training_results_nb %&gt;% yardstick::roc_curve(.pred_No, truth = default) %&gt;% autoplot()"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#nearest-neighbour-classification",
    "href": "slides/orig BSMM_8740_lec_07.html#nearest-neighbour-classification",
    "title": "Time series methods",
    "section": "Nearest Neighbour Classification",
    "text": "Nearest Neighbour Classification\nThe k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.\nIt is typically used as a classification algorithm, working off the assumption that similar points can be found near one another."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#nearest-neighbour-classification-1",
    "href": "slides/orig BSMM_8740_lec_07.html#nearest-neighbour-classification-1",
    "title": "Time series methods",
    "section": "Nearest Neighbour Classification",
    "text": "Nearest Neighbour Classification\nFor classification problems, a class label is assigned on the basis of a majority voteâ€”i.e.Â the label that is most frequently represented around a given data point is used.\nBefore a classification can be made, the distance between points must be defined. Euclidean distance is most commonly used."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#nearest-neighbour-classification-2",
    "href": "slides/orig BSMM_8740_lec_07.html#nearest-neighbour-classification-2",
    "title": "Time series methods",
    "section": "Nearest Neighbour Classification",
    "text": "Nearest Neighbour Classification\nNote that the KNN algorithm is also part of a family of â€œlazy learningâ€ models, meaning that it only stores a training dataset versus undergoing a training stage. This also means that all the computation occurs when a classification or prediction is being made.\nThe k value in the k-NN algorithm determines how many neighbors will be checked to determine the classification of a specific query point."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#knn-classification-distance-measures",
    "href": "slides/orig BSMM_8740_lec_07.html#knn-classification-distance-measures",
    "title": "Time series methods",
    "section": "KNN Classification: distance measures",
    "text": "KNN Classification: distance measures\n\nEuclidean: \\(\\text{d}(x,y)=\\sqrt{\\sum_i(y_i- x_i)^2}\\)\nManhattan: \\(\\text{d}(x,y)=\\sum_{i}\\left|y_{i}-x_{i}\\right|\\)\nMinkowski: \\(\\text{d}(x,y)=\\left(\\sum_{i}\\left|y_{i}-x_{i}\\right|\\right)^{1/p}\\)"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#knn-classification-algorithm",
    "href": "slides/orig BSMM_8740_lec_07.html#knn-classification-algorithm",
    "title": "Time series methods",
    "section": "KNN Classification: algorithm",
    "text": "KNN Classification: algorithm\n\nChoose the value of K, which is the number of nearest neighbors that will be used to make the prediction.\nCalculate the distance between that point and all the points in the training set.\nSelect the K nearest neighbors based on the distances calculated.\nAssign the label of the majority class to the new data point.\nRepeat steps 2 to 4 for all the data points in the test set."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#knn-classification-model",
    "href": "slides/orig BSMM_8740_lec_07.html#knn-classification-model",
    "title": "Time series methods",
    "section": "KNN Classification: model",
    "text": "KNN Classification: model\n\n\nCode\n&gt; default_model_knn &lt;- parsnip::nearest_neighbor(neighbors = 4) %&gt;% \n+   parsnip::set_engine(\"kknn\") %&gt;% \n+   parsnip::set_mode(\"classification\")"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#support-vector-machine-classification",
    "href": "slides/orig BSMM_8740_lec_07.html#support-vector-machine-classification",
    "title": "Time series methods",
    "section": "Support Vector Machine Classification",
    "text": "Support Vector Machine Classification\nThe SVM assumes a training set of the form \\((x_1,y_1),\\ldots,(x_n,y_n)\\) where the \\(y_i\\) are either \\(-1\\) or \\(1\\), indicating the class to which each \\(x_i\\) belongs.\nThe SVM algorithm looks to find the maaximum-margin hyperplane that divides the group of points \\(x_i\\) for which \\(y_1=-1\\) from the group for which \\(Y_1=1\\), such that the distance between the hyperplane and the nearest point \\(x_i\\) from either group is maximized"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#svm-classification-large-margin",
    "href": "slides/orig BSMM_8740_lec_07.html#svm-classification-large-margin",
    "title": "Time series methods",
    "section": "SVM Classification: large margin",
    "text": "SVM Classification: large margin\nIllustration of SVM large-margin principle"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#svm-classification",
    "href": "slides/orig BSMM_8740_lec_07.html#svm-classification",
    "title": "Time series methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\n\n\n\nLet our decision boundary be given by \\(f\\left(x\\right)=w^{\\top}x+w_{0}\\), for a vector \\(w\\) perpendicular to the boundary.\nWe can express any point as \\(x=x_{\\bot}+r\\frac{w}{\\left\\Vert w\\right\\Vert }\\)\nNote that \\(f\\left(x\\right)=\\left(w^{\\top}x_{\\bot}+w_{0}\\right)+r\\left\\Vert w\\right\\Vert\\)"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#svm-classification-1",
    "href": "slides/orig BSMM_8740_lec_07.html#svm-classification-1",
    "title": "Time series methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\n\n\n\nSince \\(f\\left(x_{\\bot}\\right)=w^{\\top}x_{\\bot}+w_{0}=0\\), we have \\(f\\left(x\\right)=r\\left\\Vert w\\right\\Vert\\).\nWe also require \\(f\\left(x_{n}\\right)\\tilde{y}_{n}&gt;0\\)\nTo maximize the distance to the closest point, the objective is \\(\\max_{w,w_{0}}\\min_{n}\\left[\\tilde{y}_{n}\\left(w^{\\top}x_{n}+w_{0}\\right)\\right]\\)"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#svm-classification-2",
    "href": "slides/orig BSMM_8740_lec_07.html#svm-classification-2",
    "title": "Time series methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\nIt is common to scale the vector \\(w\\) and the offset \\(w_0\\) such that \\(f_n\\hat{y}_n=1\\) for the point nearest the decision boundary, such that \\(f_n\\hat{y}_n\\ge1\\) for all \\(n\\).\nIn addition, since minimizing \\(1/ \\left\\Vert w\\right\\Vert\\) is equivalent to minimizing \\(\\left\\Vert w\\right\\Vert^2\\), we can state the objective as\n\\[\n\\min_{w,w_{0}}\\frac{{1}}{2}\\left\\Vert w\\right\\Vert ^{2}\\quad\\text{s.t.}\\quad\\tilde{y}_{n}\\left(w^{\\top}x_{n}+w_{0}\\right)\\ge 1, \\forall n\n\\]"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#svm-classification-3",
    "href": "slides/orig BSMM_8740_lec_07.html#svm-classification-3",
    "title": "Time series methods",
    "section": "SVM Classification:",
    "text": "SVM Classification:\n\n\nIf there is no solution to the objective we can add slack variables \\(\\xi_n\\ge0\\) to replace the hard constraints that \\(f_n\\hat{y}_n\\ge1\\) with the soft margin constraints that \\(f_n\\hat{y}_n\\ge1-\\xi_n\\).\nThe new objective is\n\\[\n\\min_{w,w_{0},\\xi}\\frac{{1}}{2}\\left\\Vert w\\right\\Vert ^{2} + C\\sum_n \\xi_n \\\\\n\\text{s.t.}\\xi_n\\ge0, \\quad\\tilde{y}_{n}\\left(w^{\\top}x_{n}+w_{0}\\right)\\ge 1, \\forall n\n\\]"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#svm-data-not-separable",
    "href": "slides/orig BSMM_8740_lec_07.html#svm-data-not-separable",
    "title": "Time series methods",
    "section": "SVM: data not separable",
    "text": "SVM: data not separable\n\n\nIf the data is not separable:\n\na transformation of data may make them separable\nan embedding in a higher dimensional space might make them separable"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#svm-classification-support-vectors",
    "href": "slides/orig BSMM_8740_lec_07.html#svm-classification-support-vectors",
    "title": "Time series methods",
    "section": "SVM Classification: Support Vectors",
    "text": "SVM Classification: Support Vectors\n\nSupport vectors are the data points that lie closest to the decision surface (or hyperplane)\nThey are the data points most difficult to classify\nThey have direct bearing on the optimum location of the decision surface\nSupport vectors are the elements of the training set that would change the position of the dividing hyperplane if\nremoved"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#svm-classification-example",
    "href": "slides/orig BSMM_8740_lec_07.html#svm-classification-example",
    "title": "Time series methods",
    "section": "SVM Classification: example",
    "text": "SVM Classification: example\n\n\nCode\n&gt; # show_engines(\"svm_linear\")\n&gt; default_model_svm &lt;- parsnip::svm_linear() %&gt;% \n+   parsnip::set_engine(\"svm_linear\") %&gt;% \n+   parsnip::set_mode(\"classification\")"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#clustering",
    "href": "slides/orig BSMM_8740_lec_07.html#clustering",
    "title": "Time series methods",
    "section": "Clustering",
    "text": "Clustering\nCluster analysis refers to algorithms that group similar objects into groups called clusters. The endpoint of cluster analysis is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\nThe purpose of cluster analysis is to help reveal patterns and structures within a dataset that may provide insights into underlying relationships and associations."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#clustering-applications",
    "href": "slides/orig BSMM_8740_lec_07.html#clustering-applications",
    "title": "Time series methods",
    "section": "Clustering Applications",
    "text": "Clustering Applications\n\nMarket Segmentation:Â Cluster analysis is often used in marketing to segment customers into groups based on their buying behavior, demographics, or other characteristics.\nImage Processing:Â In image processing, cluster analysis is used to group pixels with similar properties together, allowing for the identification of objects and patterns in images.\nBiology and Medicine:Â Cluster analysis is used in biology and medicine to identify genes associated with specific diseases or to group patients with similar clinical characteristics together.\nSocial Network Analysis:Â In social network analysis, cluster analysis is used to group individuals with similar social connections and characteristics together, allowing for the identification of subgroups within a larger network.\nAnomaly Detection:Â Cluster analysis can be used to detect anomalies in data, such as fraudulent financial transactions, unusual patterns in network traffic, or outliers in medical data."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#k-means-clustering",
    "href": "slides/orig BSMM_8740_lec_07.html#k-means-clustering",
    "title": "Time series methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nk-means is a method of unsupervised learning that produces a partitioning of observations into k unique clusters.\nThe goal of k-means is to minimize the sum of squared Euclidian distances between observations in a cluster and the centroid, or geometric mean, of that cluster."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#k-means-clustering-1",
    "href": "slides/orig BSMM_8740_lec_07.html#k-means-clustering-1",
    "title": "Time series methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nIn k-means clustering, observed variables (columns) are considered to be locations on axes in multidimensional space.\nThe basic k-means algorithm has the following steps.\n\npick the number of clusters k\nChoose k random observations in the dataset. These locations in space are declared to be the initial centroids.\nAssign each observation to the nearest centroid.\nCompute the new centroids of each cluster.\nRepeat steps 3 and 4 until the centroids do not change."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#k-means-clustering-2",
    "href": "slides/orig BSMM_8740_lec_07.html#k-means-clustering-2",
    "title": "Time series methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nThere are three common methods for selecting initial centers:\n\nRandom observations: Chosing random observations to act as our initial centers is the most commonly used approach, implemented in the Forgy, Lloyd, and MacQueen methods.\nRandom partition: The observations are assigned to a cluster uniformly at random. The centroid of each cluster is computed, and these are used as the initial centers. This approach is implemented in the Hartigan-Wong method.\nk-means++: Beginning with one random set of the observations, further observations are sampled via probability-weighted sampling until \\(k\\) clusters are formed. The centroids of these clusters are used as the initial centers."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#k-means-clustering-3",
    "href": "slides/orig BSMM_8740_lec_07.html#k-means-clustering-3",
    "title": "Time series methods",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nBecause the initial conditions are based on random selection in both approaches, the k-means algorithm is not deterministic. That is, running the clustering twice on the same data may not result in the same cluster assignments."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#k-means-example",
    "href": "slides/orig BSMM_8740_lec_07.html#k-means-example",
    "title": "Time series methods",
    "section": "K-means Example",
    "text": "K-means Example\n\n\nCode\n&gt; set.seed(8740)\n&gt; \n&gt; centers &lt;- tibble::tibble(\n+   cluster = factor(1:4), \n+   num_points = c(100, 150, 50, 90),  # number points in each cluster\n+   x1 = c(5, 0, -3, -4),              # x1 coordinate of cluster center\n+   x2 = c(-1, 1, -2, 1.5)               # x2 coordinate of cluster center\n+ )\n&gt; \n&gt; labelled_points &lt;- \n+   centers %&gt;%\n+   dplyr::mutate(\n+     x1 = purrr::map2(num_points, x1, rnorm),\n+     x2 = purrr::map2(num_points, x2, rnorm)\n+   ) %&gt;% \n+   dplyr::select(-num_points) %&gt;% \n+   tidyr::unnest(cols = c(x1, x2))\n&gt; \n&gt; ggplot(labelled_points, aes(x1, x2, color = cluster)) +\n+   geom_point(alpha = 0.3)\n\n\n\n\nCode\n&gt; # penguins &lt;- modeldata::penguins\n&gt; # \n&gt; # labelled_points &lt;- penguins %&gt;%\n&gt; #   dplyr::select(bill_length_mm, bill_depth_mm) %&gt;%\n&gt; #   tidyr::drop_na() %&gt;%\n&gt; #   # shuffle rows\n&gt; #   dplyr::slice_sample( n = nrow(penguins) )\n&gt; \n&gt; # create recipe\n&gt; labelled_points_recipe &lt;- labelled_points %&gt;% \n+   recipes::recipe(~ x1 + x2, data = .)\n\n\n\n\nCode\n&gt; labelled_points\n\n\n\n\nCode\n&gt; kmeans_spec &lt;- tidyclust::k_means( num_clusters = 3 )\n\n\n\n\nCode\n&gt; wflow &lt;- workflows::workflow() %&gt;%\n+   workflows::add_model(kmeans_spec) %&gt;%\n+   workflows::add_recipe(labelled_points_recipe)\n&gt; \n&gt; set.seed(8740)\n&gt; labelled_points_resamples &lt;- labelled_points %&gt;% rsample::bootstraps(apparent = TRUE)\n\n\n\n\nCode\n&gt; wflow %&gt;%\n+   parsnip::fit(labelled_points) %&gt;% \n+   broom::tidy()\n\n\n\n&gt; wflow %&gt;%\n+   parsnip::fit(labelled_points) %&gt;% tidyclust::extract_centroids()\n\n\n&gt; # all_workflows &lt;- all_workflows %&gt;% \n&gt; #   workflowsets::workflow_map(\n&gt; #     verbose = TRUE                # enable logging\n&gt; #     , fn = \"tune_cluster\"\n&gt; #     , resamples = labelled_points_resamples # a parameter passed to tune::tune_grid()\n&gt; #   )\n\n\n&gt; # all_workflows %&gt;% \n&gt; #   dplyr::select(wflow_id,result) %&gt;% \n&gt; #   tidyr::unnest(result) %&gt;% \n&gt; #   tidyr::unnest(.metrics) %&gt;% \n&gt; #   dplyr::filter(.metric == 'sse_total') %&gt;% \n&gt; #   dplyr::group_by(wflow_id) %&gt;% \n&gt; #   dplyr::arrange(desc(.estimate) ) %&gt;% \n&gt; #   dplyr::slice(1)\n\n\n&gt; all_workflows &lt;- all_workflows %&gt;% \n+   workflowsets::workflow_map(\n+     verbose = TRUE                # enable logging\n+     , fn = \"tune_cluster\"\n+     , resamples = labelled_points_resamples # a parameter passed to tune::tune_grid()\n+   )\n\n\n&gt; all_workflows %&gt;% workflowsets::rank_results(select_best = TRUE)\n\n\n&gt; all_workflows %&gt;% \n+   dplyr::select(wflow_id,result) %&gt;% \n+   tidyr::unnest(result) %&gt;% \n+   tidyr::unnest(.metrics) %&gt;% \n+   dplyr::filter(.metric == 'sse_within_total') %&gt;% \n+   dplyr::group_by(wflow_id) %&gt;% \n+   dplyr::arrange(desc(.estimate) ) %&gt;% \n+   dplyr::slice(1)\n\n\n&gt; tune_results &lt;-\n+    all_workflows %&gt;% \n+    workflow_map(\n+       fn = \"tune_cluster\"\n+       , resamples = labelled_points_resamples\n+       , grid = dials::grid_regular(dials::num_clusters(), levels = 10)\n+       , metrics = tidyclust::cluster_metric_set(sse_within_total, sse_total, sse_ratio)\n+       , control = tune::control_grid(save_pred = TRUE, extract = identity)\n+    )"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#hierarchical-clustering",
    "href": "slides/orig BSMM_8740_lec_07.html#hierarchical-clustering",
    "title": "Time series methods",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical Clustering, sometimes called Agglomerative Clustering, is a method of unsupervised learning that produces a dendrogram, which can be used to partition observations into clusters."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#tidymodels",
    "href": "slides/orig BSMM_8740_lec_07.html#tidymodels",
    "title": "Time series methods",
    "section": "Tidymodels",
    "text": "Tidymodels\nKey Components of Tidymodels\n\nResampling: Efficient methods for handling data splitting, cross-validation, bootstrapping, and more.\nMetrics: A wide range of evaluation metrics to assess model performance and choose the best model."
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#more",
    "href": "slides/orig BSMM_8740_lec_07.html#more",
    "title": "Time series methods",
    "section": "More",
    "text": "More\n\nRead An Idiotâ€™s Guide to Support Vector Machines"
  },
  {
    "objectID": "slides/orig BSMM_8740_lec_07.html#recap",
    "href": "slides/orig BSMM_8740_lec_07.html#recap",
    "title": "Time series methods",
    "section": "Recap",
    "text": "Recap\n\nIn this section we have worked with the tidymodels package to build a workflow that facilitates building and evaluating multiple models.\nCombined with the recipes package we now have a complete data modeling framework.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08inf.html#s1",
    "href": "slides/BSMM_8740_lec_08inf.html#s1",
    "title": "Causality Part INFINITY",
    "section": "S1",
    "text": "S1\nThis is some stuff\n#| code-annotations: below\n\n\nfit_ipw &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  if(\"rsplit\" %in% class(split)){\n    .df &lt;- rsample::analysis(split)\n  }else if(\"data.frame\" %in% class(split)){\n    .df &lt;- split\n  }\n\n  # fit propensity score model\n  propensity_model &lt;- glm(\n    net ~ income + health + temperature,\n    data = .df,\n    family = binomial()\n  )\n\n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\n  # fit correctly bootstrapped ipw model\n  lm(malaria_risk ~ net, data = .df, weights = wts) |&gt;\n    broom::tidy()\n}\n\n1. bollocks\n2. more bollocks"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08inf.html#s2",
    "href": "slides/BSMM_8740_lec_08inf.html#s2",
    "title": "Causality Part INFINITY",
    "section": "S2",
    "text": "S2\n\n\nCode\nfit_ipw &lt;- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  if(\"rsplit\" %in% class(split)){\n    .df &lt;- rsample::analysis(split)\n  }else if(\"data.frame\" %in% class(split)){\n    .df &lt;- split\n  }\n\n  # fit propensity score model\n  propensity_model &lt;- glm(\n    net ~ income + health + temperature,\n    data = .df,\n    family = binomial()\n  )\n\n  # calculate inverse probability weights\n  .df &lt;- propensity_model |&gt;\n    broom::augment(type.predict = \"response\", data = .df) |&gt;\n    dplyr::mutate(wts = propensity::wt_ate(.fitted, net))\n\n  # fit correctly bootstrapped ipw model\n  lm(malaria_risk ~ net, data = .df, weights = wts) |&gt;\n    broom::tidy()\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_08inf.html#s3",
    "href": "slides/BSMM_8740_lec_08inf.html#s3",
    "title": "Causality Part INFINITY",
    "section": "S3",
    "text": "S3\nğ”¼[N1(ATETÌ‚âˆ’ATET)]=ğ”¼[N1(Î¼0(Xi)âˆ’Î¼0(Xj(i))]\n\\mathbb{E}\\left [\\sqrt{N_1}(\\hat{ATET} - ATET)\\right] = \n\\mathbb{E}\\left[\\sqrt{N_1}(\\mu_0(X_i) - \\mu_0(X_j(i))\\right]\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#recap-of-last-week",
    "href": "slides/BSMM_8740_lec_09.html#recap-of-last-week",
    "title": "Monte Carlo Methods",
    "section": "Recap of last week",
    "text": "Recap of last week\n\nLast week we looked at causal effect estimation methods in greater depth, including\n\ninverse probaiity weighting\nregression adjustment\ndoubly robust estimation\nmatching estimation\nDifference in Differences (DiD)\nPanel Data and fixed effects (FE)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#this-week",
    "href": "slides/BSMM_8740_lec_09.html#this-week",
    "title": "Monte Carlo Methods",
    "section": "This week",
    "text": "This week\n\nWe will explore Monte Carlo methods as a way to integrate difficult functions, and sample from difficult probability distributions.\nAlong the way we will look at Markov Chains, which both underlie sampling methods and provide a way to model the generation of data."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#monte-carlo-mc-methods",
    "href": "slides/BSMM_8740_lec_09.html#monte-carlo-mc-methods",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo (MC) Methods",
    "text": "Monte Carlo (MC) Methods\nMonte Carlo methods are a class of simulation-based methods that seek to avoid complicated and/or intractable mathematical computations.\nEspecially those that arise from probability distributions.\nFirst we consider how â€˜random numbersâ€™ are computer generated."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nRandom numbers from a given distribution are drawn using random numbers from a (\\([0,1]\\)) uniform distribution, using the inverse transform sampling method. Hereâ€™s how it works:\n\nUniform Distribution: Start by drawing a random number (\\(U\\)) from a (\\([0,1]\\)) uniform distribution. This gives us a random variable that is uniformly distributed between \\(0\\) and \\(1\\).\nCumulative Distribution Function (CDF): Every probability distribution has a CDF (\\(F_X(x)\\)), which gives the probability that a random variable (\\(X\\)) is less than or equal to (\\(x\\)).\nInverse CDF: The key idea is that if (\\(U\\)) is uniformly distributed in (\\([0,1]\\)), then (\\(F_X^{-1}(U)\\)) (the inverse of the CDF) will give us a random sample from the distribution of (\\(X\\))."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-1",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-1",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nProcess:\n\nDraw a random number (\\(U\\)) from the uniform (\\([0,1]\\)) distribution.\nUse the inverse CDF of the desired distribution: (\\(X = F_X^{-1}(U)\\)).\nThe result (X) is a random number that follows the target distribution.\n\nExample: For an exponential distribution with rate parameter (\\(\\lambda\\)), the CDF is (\\(F_X(x) = 1 - e^{-\\lambda x}\\)). To generate a random draw from this distribution: - Draw (\\(U \\sim \\mathcal{U}(0,1)\\)). - Use the inverse of the CDF: (\\(X = -\\frac{1}{\\lambda} \\ln(1 - U)\\)), which gives an exponentially distributed random variable.\nThis method applies broadly to any distribution where you can compute or approximate the inverse CDF."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-2",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-2",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nPseudo-random generators found in computers use a deterministic (i.e.Â repeatable) algorithm to generate a sequence of (apparently) random numbers on the \\((0, 1)\\) interval.\nWhat defines a good random number generator (RNG) is a long period â€“ how long it takes before the sequence repeats itself. A period of \\(2^{32}\\) is not enough (need at least \\(2^{40}\\)). Various statistical tests exisit to measure â€œrandomnessâ€ â€“ well validated software will have gone through these checks."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-3",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-3",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nAlternatives to inverse transform sampling are special algorithms that generate random samples from a particular distributions, e.g.Â one way to sample from a Gaussian random number generation uses the Box-Muller method:\n\ngenerate two independent uniform \\([0, 1]\\) random variables, and\nuse some trigonometry.\n\nVery important: never write your own generator, always use a well validated generator from a reputable source (e.g.Â R)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-4",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-4",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nRecall that \\(\\mathscr{N}(0, 1)\\) Normal random variables (mean 0, variance 1) have the probability density function:\n\\[\np(x)=\\frac{1}{2\\pi}e^{-\\frac{1}{2}x^2}\\equiv\\phi(x)\n\\] and if \\(X\\sim\\mathscr{N}(0, 1)\\) then its CDF is:\n\\[\n\\mathbb{P}[X\\le x] = \\int_{-\\infty}^x\\phi(x)dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-5",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-5",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nThe Box-Muller transformation method takes two independent uniform \\((0, 1)\\) random numbers \\(y_1\\), \\(y_2\\), and defines:\n\\[\n\\begin{align*}\nx_{1} & =\\sqrt{-2\\log y_{1}}\\cos(2\\pi y_{2})\\\\\nx_2& =\\sqrt{-2\\log y_{1}}\\sin(2\\pi y_{2})\n\\end{align*}\n\\]\nIt can be proved that \\(x_1\\) and \\(x_2\\) are independent \\(\\mathscr{N}(0, 1)\\) random variables"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-6",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-6",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nCode\n# The Box-Muller transformation\nsamples &lt;- matrix(runif(10000), ncol=2) |&gt; data.frame() |&gt; \n  dplyr::mutate(\n    normals = \n      purrr::map2(\n        X1, X2\n        ,(\\(x1,x2){\n          data.frame(\n            y1 = sqrt( -2 * log(x1) ) * cos(2 * pi * x2)\n            , y2 = sqrt( -2 * log(x1) ) * sin(2 * pi * x2) \n          )\n        })\n      )\n  ) |&gt; \n  tidyr::unnest(normals)  \n  \n\nsamples |&gt; \n  tidyr::pivot_longer(-c(X1,X2)) |&gt; \n  ggplot(aes(x=value, color=name, fill=name)) + \n  geom_histogram(\n    aes(y=..density..), bins = 60, position=\"identity\", alpha=0.3\n  ) + \n  labs(x=\"Value\", y=\"Density\") + theme_minimal()\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in\nggplot2 3.4.0.\nâ„¹ Please use `after_stat(density)` instead.\n\n\nCode\nsamples |&gt; \nggplot(aes(x=y1, y=y2)) + geom_point() + coord_fixed() + theme_minimal()\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs\n\n\n\n\n\n\n\nNormal y1 vs Normal y2; independent random RVs"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#random-number-generation-7",
    "href": "slides/BSMM_8740_lec_09.html#random-number-generation-7",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nYour computer is only capable of producing pseudorandom numbers. These are made by running a pseudorandom number generator algorithm which is deterministic, e.g.\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\n\nset.seed(340)\nrnorm(n=10)\n\n [1] -0.1574 -1.1989 -0.8892  1.0091  0.6130  1.0072\n [7]  0.4144 -1.8579 -1.3487  0.5189\n\n\nOnce the RNG seed is set, the â€œrandomâ€ numbers that R generates arenâ€™t random at all. But someone looking at these random numbers would have a very hard time distinguishing these numbers from truly random numbers. That is what â€œstatistical randomnessâ€ means!"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nWe can use MC methods to estimate probabilities: for a random variable \\(Z\\) with outcomes in a set \\(\\Omega\\), given a subset \\(S\\subset\\Omega\\) and defining an event \\(E\\equiv Z\\in S\\), we can compute the probability of \\(E\\) (i.e.Â \\(\\mathbb{P}(E)\\)) with of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) as\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-1",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-1",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample\nIf \\(Z\\sim\\mathscr{N}(1,3)\\) and \\(S=Z:0\\le Z\\le 3\\) then\n\\[\n\\mathbb{P}(E) = \\mathbb{P}(0\\le Z\\le 3) = \\int_0^3\\frac{1}{\\sqrt{2\\pi3}}e^{-\\frac{(t-1)^2}{2*3}}dt\n\\] In which case it is easier to just use R and calculate:\n\n\nCode\npnorm(3, mean=1, sd=sqrt(3)) - pnorm(0, mean=1, sd=sqrt(3))\n\n\n[1] 0.594\n\n\nthan it is to compute the integral."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-2",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-2",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nExample continued\nIn case we donâ€™t know that pnorm exists, we could do this:\n\n\nMC computation\n# define the event\nevent_E_happened &lt;- function( x ) {\n  if( 0 &lt;= x & x &lt;= 3 ) {\n    return( TRUE ) # The event happened\n  } else {\n    return( FALSE ) # The event DIDN'T happen\n  }\n}\nset.seed(8740)\n# generate lots of copies of Z...\nNMC &lt;- 10000; # 10000 seems like \"a lot\".\nrnorm( NMC, mean=1, sd=sqrt(3) ) |&gt; \n  purrr::map_lgl(event_E_happened) |&gt; \n  sum()/NMC\n\n\n[1] 0.5941"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-3",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-3",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nNow\n\n\\[\n\\mathbb{P}(E) = \\frac{1}{M}\\sum_{i=1}^M 1_{z_i\\in S}\n\\]\n\nis the MC estimate of \\(\\mathbb{E}[E]\\), which is unbiased because for each \\(i\\), \\(\\mathbb{E}[1_{z_i\\in S}]=\\mathbb{E}[E]\\), and the variance is\n\n\\[\n\\begin{align*}\n\\mathrm{Var}\\left({\\mathbb{P}(E)}\\right) & =\\frac{1}{M^{2}}\\mathrm{Var}\\left(\\sum_{i=1}^{M}1_{z_{i}\\in S}\\right)\\\\\n& =\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\mathrm{Var}\\left(1_{z_{i}\\in S}\\right)=\\frac{1}{M}\\mathrm{Var}\\left(1_{z_{i}\\in S}\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-4",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-4",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nThis is true for any function of the random variable \\(Z\\), and\n\\[\n\\mathbb{E}\\left[h(Z)\\right]\\approx \\hat{h}=\\frac{1}{M}\\sum_{i=1}^M h(z_i)\n\\]\nand the variance of \\(h(Z)\\) decreases as \\(1/M\\) by the same reasoning."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nThe \\(k\\)-th moment of a random variable \\(X\\) is the expected value of: \\(X\\) raised to the \\(k\\)-th power.\n\\[\n\\acute{\\mu}_k(X)=\\mathbb{E}[X^k]\n\\] which is known as the \\(k\\)-th raw moment.\nThe \\(k\\)-th moment of a random variable \\(X\\) around some value \\(c\\) is known as the \\(k\\)-th central moment of \\(X\\). It is the \\(k\\)-th raw moment of \\(X-c\\).\n\\[\n\\mu_k(X)=\\mathbb{E}[(X-c)^k]\n\\] The \\(k\\)-th standardized moment of a random variable \\(X\\) is the \\(k\\)-th central moment of \\(X\\) divided by the \\(k\\)-th power of the standard deviation of \\(X\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-1",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-1",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nThe Taylor series expansion of \\(e^{tX}\\) is\n\\[\ne^{tX} = \\sum_{k=0}^\\infty\\frac{(tX)^k}{k!}=1+\\frac{t}{1!}X+\\frac{t^2}{2!}X^2+\\cdots\n\\]\nThe Moment Generating Function (MGF) is \\(\\mathbb{E}[e^{tX}]\\)\n\\[\n\\begin{align*}\nM_X(t)=\\mathbb{E}[e^{tX}] & =\\sum_{k=0}^{\\infty}\\mathbb{E}\\left[\\frac{(tX)^{k}}{k!}\\right]\\\\\n& =1+\\frac{t}{1!}\\mathbb{E}\\left[X\\right]+\\frac{t^{2}}{2!}\\mathbb{E}\\left[X^{2}\\right]+\\cdots\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-2",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-2",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nNote that the derivatives evaluated at \\(t=0\\) give the corresponding moments\n\\[\n\\begin{align*}\n\\left.\\frac{d^{2}}{dt^{2}}M_{X}(t)\\right|_{t=0} & =\\mathbb{E}\\left[X^{2}\\right]\\\\\n\\left.\\frac{d^{3}}{dt^{3}}M_{X}(t)\\right|_{t=0} & =\\mathbb{E}\\left[X^{3}\\right]\n\\end{align*}\n\\]\nand so \\(\\left.\\frac{d^{n}}{dt^{n}}M_{X}(t)\\right|_{t=0}=\\mathbb{E}\\left[X^{n}\\right]\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-3",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-3",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nOther properties of the MGF\n\nif \\(Y=\\beta_0+\\beta_1X\\) then\n\n\\[\nM_Y(t)=\\mathbb{E}[e^{t(\\beta_0+\\beta_1X)}]=e^{\\beta_0t}M_X(\\beta_1t)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-4",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-4",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nOther properties of the MGF\n\nif \\(Y=X_1+X_2+\\cdots X_n\\) then \\(M_Y(t)=\\mathbb{E}[e^{t(X_1+X_2+\\cdots X_n)}]\\) and:\n\n\\[\nM_Y(t) = \\prod_{i=1}^n M_{X_i}(t)\n\\]\nif the \\(X\\) are independent, and if the \\(X\\) are IID:\n\\[\nM_Y(t) = \\left(M_{X}(t)\\right)^n\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-5",
    "href": "slides/BSMM_8740_lec_09.html#moments-and-the-mgf-5",
    "title": "Monte Carlo Methods",
    "section": "Moments and the MGF",
    "text": "Moments and the MGF\nOther properties of the MGF1\nif \\(Y=\\mathscr{N}(0,1)\\) then starting with the Taylor series expansion of \\(e^{t^2/2}\\):\n\\[\n\\begin{align*}\ne^{t^{2}/2} & =1+\\frac{1}{2}t^{2}+\\frac{1}{2^{2}}\\frac{t^{4}}{2!}+\\frac{1}{2^{3}}\\frac{t^{6}}{3!}+\\cdots\\\\\n\\left.\\frac{d^{n}}{dt^{n}}M_{Y}(t)\\right|_{t=0} & =\\mathbb{E}\\left[Y^{n}\\right]=\\begin{cases}\n0 & n\\,\\mathrm{\\mathrm{odd}}\\\\\n2^{-n/2}\\frac{n!}{(n/2)!} & n\\,\\mathrm{even}\n\\end{cases}\\\\\nM_{Y}(t) & =e^{t^{2}/2}\n\\end{align*}\n\\]\nsee here for a derivation of the moments of the standard Normal pdf."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#central-limit-theorem",
    "href": "slides/BSMM_8740_lec_09.html#central-limit-theorem",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nLet \\(X_1+X_2+\\cdots X_n\\) be IID with mean \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\) and standardized mean \\(\\bar{Z}_n=(\\bar{X}_n-\\mu)/(\\sigma/\\sqrt{n})\\)\n\\[\n\\begin{align*}\n\\bar{Z}_{n} & =\\frac{\\bar{X}_{n}-\\mu}{\\sigma/\\sqrt{n}}=\\frac{\\sqrt{n}}{\\sigma}\\left[\\frac{X_{1}+X_{2}+\\cdots X_{n}}{n}-\\mu\\right]\\\\\n& =\\frac{\\sqrt{n}}{\\sigma}\\left[\\frac{X_{1}+X_{2}+\\cdots X_{n}-n\\mu}{n}\\right]\\\\\n& =\\frac{\\sqrt{n}}{\\sigma}\\left[\\frac{\\frac{X_{1}-\\mu}{\\sigma}+\\frac{X_{2}-\\mu}{\\sigma}+\\cdots\\frac{X_{n}-\\mu}{\\sigma}}{n}\\right]=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}Z_{i}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#central-limit-theorem-1",
    "href": "slides/BSMM_8740_lec_09.html#central-limit-theorem-1",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\\[\nM_{\\bar{Z}_n}(t)=\\left(M_{Z/\\sqrt{n}}(t)\\right)^n\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#central-limit-theorem-2",
    "href": "slides/BSMM_8740_lec_09.html#central-limit-theorem-2",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT says that for \\(f=\\mathbb{P}(E)\\) if \\(\\sigma^2\\equiv\\mathrm{Var}\\left(f\\right)\\) is finite then the error of the MC estimate\n\\[\ne_N(f)=\\bar{f}-\\mathbb{E}[f]\n\\]\nis approximately Normal in distribution for large \\(M\\), i.e.\n\\[\ne_N(f)\\sim\\sigma M^{1/2}Z\n\\] where \\(Z\\sim\\mathscr{N}(0,1)\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-5",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-5",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nSuppose we need to compute an expectation \\(\\mathbb{E}[h(Z)]\\) for some random variable \\(Z\\) and some function \\(h:\\mathbb{R}\\to\\mathbb{R}\\).\nMonte Carlo methods avoid doing any integration and instead just generate lots of samples of \\(Z\\), say \\(z_1,z_2,\\ldots,z_M\\) and estimate \\(\\mathbb{E}[h(Z)]\\) as \\(\\frac{1}{M}\\sum_{i=1}^Mh(z_i)\\), relying on empirical probability. The law of large numbers states that this sample mean should be close to \\(\\mathbb{E}[h(Z)]\\).\nMonte Carlo replaces the work of computing an integral (i.e., an expectation) with the work of generating lots of random variables."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-6",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-6",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nsampling\n\nIf \\(X\\sim\\mathscr{N}(\\mu,\\sigma)\\) and we want to compute \\(\\mathbb{E}[\\log|X|]\\), we could set up and solve the integral\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[\\log|X|\\right] & =\\int_{-\\infty}^{\\infty}\\left(\\log|t|\\right)f(t;\\mu,\\sigma)dt\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{\\log|t|}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ \\frac{-(t-\\mu)^{2}}{2\\sigma^{2}}\\right\\} dt\n\\end{align*}\n\\]\nAlternatively, we could just draw lots of Monte Carlo replicates \\(X_1,X_2,\\cdots,X_M\\) from a normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and look at the sample mean \\(M^{-1}\\sum_{i=1}^M\\log|x_i|\\), once again appealing to the law of large numbers to ensure that this sample mean is close to its expectation."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-7",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-7",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nsampling\n\nSuppose that we want to compute an integral\n\\[\n\\int_Dg_0(x)dx\n\\] where \\(D\\) is some domain of integration and \\(g_0(.)\\) is a function.\nLet \\(f(x)\\) be the density of some random variable with \\(f(x)&gt;0, \\forall x\\in D\\). Then we can rewrite the integral as\n\\[\n\\int_Dg_0(x)dx = \\int_D\\frac{g_0(x)}{f(x)}f(x)dx = \\mathbb{E}_f[h(x)]\n\\]\nwhere \\(h(x)=g_0(x)/f(x)\\) and \\(X\\sim f\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-8",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-8",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nSimilarly, if we are given \\(h(x)\\) and we want to compute \\(\\mathbb{E}_f[h(X)]\\) where \\(X\\sim f,\\,x\\in D\\). What if we could not sample from \\(f\\) directly?\nIf there were some other distribution \\(g_1(x)\\) we could sample from, such that \\(g_1(x)&gt;0,\\,x\\in D\\), then\n\\[\n\\begin{align*}\n\\mathbb{E}_{f}\\left[h(x)\\right] & =\\int_{D}h(x)f(x)dx\\\\\n& =\\int_{S}h(x)\\frac{f(x)}{g_1(x)}g_1(x)dx=\\mathbb{E}_{g_1}\\left[h(x)\\frac{f(x)}{g_1(x)}\\right]\\\\\n& =\\frac{1}{n}\\sum_{i=1}^{n}h(x_{i})\\frac{f(x_{i})}{g_1(x_{i})}\\quad x_{i}\\sim g_1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-9",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-9",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\n\nThis method is called importance sampling (IS).\n\ndraw iid \\(x_1,x_2,\\ldots,x_n\\) from \\(g_1\\) and calculate the importance weight\n\n\\[\nw(x_i)=\\frac{f(x_{i})}{g_1(x_{i})}\n\\] 2. estimate \\(\\mathbb{E}_f\\left[h(X)\\right]\\) by\n\\[\n\\hat{\\mu}_h=\\frac{1}{n}\\sum_{i=1}^nw(x_i)h(x_i)\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-10",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-10",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nexample\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = \\sqrt{2/\\pi}e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution)\n\n\n\nintegrate the half-normal\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X) / sqrt(2 * pi)\n\nmu_h  &lt;- mean(W*X)\nvar_h &lt;- var(W*X)/n\nse_h  &lt;- sqrt(var_h)\n\ntibble::tibble(mean = mu_h,  variance = var_h, 'standard error' = se_h) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4) |&gt; \n  gt::tab_options( table.font.size = gt::px(30) ) |&gt;  \n  #gt::tab_header(title = )\n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.8208\n0.0003\n0.0186"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-11",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-11",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrepeating the prior example, but un-normalized\n\nEstimate \\(\\mathbb E_f(X)\\) where \\(f(x) = e^{-\\frac{x^2}{2}};\\;x\\ge 0\\) (this is the half-Normal distribution, un-normalized)\n\n\n\nintegrate the (un-noramlized) half-normal\n# un-normalized weights\nn &lt;- 5000\nX &lt;- rexp(n, rate=2)\nW &lt;- exp(-0.5 * X^2 + 2*X)\n\nmu_h2  &lt;- sum(W*X)/sum(W)\nvar_h2 &lt;- var(W/mean(W))\nse_h2  &lt;- sqrt(var_h2)\n\ntibble::tibble(mean = mu_h2,  variance = var_h2, 'standard error' = se_h2) |&gt; \n  gt::gt() |&gt; \n  gt::fmt_number(decimals=4) |&gt; \n  gt::tab_options( table.font.size = gt::px(30) ) |&gt;  \n  #gt::tab_header(title = )\n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\nmean\nvariance\nstandard error\n\n\n\n\n0.7994\n0.4183\n0.6468"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-12",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-12",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nAssume we have an un-normalized \\(g(x)\\), i.e.Â \\(\\pi(x)=cg(x)\\) but \\(c\\) is unknown.\nWe want to generate iid samples \\(x_1,x_2,\\ldots,x_M\\sim \\pi\\) to estimate \\(\\mathbb{E}_\\pi[h(X)]\\).\nNow assume we have an easily sampled density \\(f\\), and known \\(K&gt;0\\), such that \\(Kf(x)\\ge g(x),\\;\\forall x\\), i.e.Â \\(Kf(x)\\ge \\pi(x)/c\\) (or \\(cKf(x)\\ge \\pi(x)\\))."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-13",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-13",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nThen use the following procedure:\n\nsample \\(X\\sim f\\) and \\(U\\sim \\mathrm{uniform}[0,1]\\)\nif \\(U\\le\\frac{g(x)}{Kf(x)}\\), the accept \\(x\\) as a draw from \\(\\pi\\)\notherwise reject the sample and repeat"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-14",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-14",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\n\nSince \\(0\\le\\frac{g(x)}{Kf(x)}\\le 1\\) we know that \\(\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)=\\frac{g(x)}{Kf(x)}\\)\nand so\n\n\\[\n\\begin{align*}\n\\mathbb{E}_{f}\\left[\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right] & =\\mathbb{E}_{f}\\left[\\frac{g(X)}{Kf(X)}\\right]\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{g(x)}{Kf(x)}f(x)dx\\\\\n& =\\int_{-\\infty}^{\\infty}\\frac{g(x)}{K}dx\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-15",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-15",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\n\nSimilarly, for any \\(y\\in\\mathbb{R}\\), we can calculate the joint probability\n\\[\n\\begin{align*}\n\\mathbb{P}\\left(X\\le y,U\\le\\frac{g(X)}{Kf(X)}\\right) & =\\mathbb{E}_f\\left[1_{X\\le y}1_{U\\le\\frac{g(X)}{Kf(X)}}\\right]\\\\\n& =\\mathbb{E}_f\\left[1_{X\\le y}\\mathbb{P}\\left(U\\le\\frac{g(X)}{Kf(X)}|X=x\\right)\\right]\\\\\n& =\\mathbb{E}_f\\left[1_{X\\le y}\\frac{g(X)}{Kf(X)}\\right]=\\int_{-\\infty}^{y}\\frac{g(x)}{Kf(x)}f(x)dx\\\\\n& =\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx\n\\end{align*}\n\\]\n\nand so we have the joint probability (above - \\(\\mathbb{P}(A,B)\\)), and the probability of acceptance (previous slide - \\(\\mathbb{P}(B)\\)), so the probability, conditional on acceptance (\\(\\mathbb{P}(A|B)\\)) is \\(\\frac{\\mathbb{P}(A,B)}{\\mathbb{P}(B)}\\) by Bayes rule."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-16",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-16",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling\nSubstituting for the numerator and denominator:\n\n\\[\n\\mathbb{P}\\left(X\\le y|U\\le\\frac{g(X)}{Kf(X)}\\right)=\\frac{\\int_{-\\infty}^{y}\\frac{g(x)}{K}dx}{\\int_{-\\infty}^\\infty\\frac{g(X)}{K}dx}=\\int_{-\\infty}^{y}\\pi(x)dx\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-17",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-17",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nWe have \\(\\mathbb{P}(X=x) = \\sum_{i=1}^n\\mathbb{P}(\\mathrm{reject }\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw }\\,Y=x\\,\\mathrm{and\\, accept})\\)\nWe also have\n\\[\n\\begin{align*}\n& \\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n= & \\mathbb{P}(\\mathrm{draw}\\,Y=x)\\mathbb{P}(\\left.\\mathrm{accept}\\,Y\\right|Y=x)\\\\\n= & f(x)\\mathbb{P}\\left(\\left.U\\le\\frac{q(Y)}{Kf(Y)}\\right|Y=x\\right)\\\\\n= & \\frac{q(x)}{K} = \\frac{\\pi(x)}{cK}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-18",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-18",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nThe probability of rejection of a draw is\n\\[\n\\begin{align*}\n\\mathbb{P}(\\mathrm{{reject}}\\,Y) & =\\sum_{x\\in D}\\mathbb{P}(\\mathrm{{draw}}\\,Y=x\\,\\mathrm{and\\,reject\\,it})\\\\\n& =\\sum_{x\\in D}f(x)\\mathbb{P}\\left(\\left.U\\ge\\frac{q(Y)}{Kf(Y)}\\right|Y=x\\right)\\\\\n& =\\sum_{x\\in D}f(x)\\left(1-\\frac{q(x)}{Kf(x)}\\right)=1-\\frac{1}{cK}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mc-methods-19",
    "href": "slides/BSMM_8740_lec_09.html#mc-methods-19",
    "title": "Monte Carlo Methods",
    "section": "MC Methods",
    "text": "MC Methods\nrejection sampling: proof for discrete rv\n\nand so1\n\\[\n\\begin{align*}\n\\mathbb{P}(X=x) & =\\sum_{n=1}^{\\infty}\\mathbb{P}(\\mathrm{reject}\\,Y)^{n-1}\\mathbb{P}(\\mathrm{draw}\\,Y=x\\,\\mathrm{and\\,accept})\\\\\n& =\\sum_{n=1}^{\\infty}\\left(1-\\frac{1}{cK}\\right)^{n-1}\\frac{\\pi(x)}{cK}=\\pi(x)\n\\end{align*}\n\\]\n\nThe geometric distribution is a discrete pmf that can be interpreted as the number of failures before the first success (\\(\\mathbb{P}(X=k)=(1-p)^{k-1}p\\), with mean \\(p\\))."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#stochastic-processes",
    "href": "slides/BSMM_8740_lec_09.html#stochastic-processes",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\n\nA stochastic process, which we will usually write as \\((X_n)\\), is an indexed sequence of random variables that are possibly dependent on each other.\nEach random variable \\(X_n\\) takes a value in a state space \\(\\mathcal S\\) which is the set of possible values for the process. As with usual random variables, the state space \\(\\mathcal S\\) can be discrete or continuous.\nA discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, \\(\\mathcal S = \\{\\text{Heads},\\text{Tails}\\}\\) is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers \\(\\mathcal S = \\mathbb Z_+ = \\{0,1,2,\\dots\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#stochastic-processes-1",
    "href": "slides/BSMM_8740_lec_09.html#stochastic-processes-1",
    "title": "Monte Carlo Methods",
    "section": "Stochastic processes",
    "text": "Stochastic processes\nThe stochastic process has an index set that puts the random variables that make up the process in order.\nThe index set is usually interpreted as a time variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at distinct points, often denoted by \\(n = 0,1,2,\\dots\\), while continuous time denotes a process monitored constantly over time, often denoted by \\(t \\in \\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-property",
    "href": "slides/BSMM_8740_lec_09.html#markov-property",
    "title": "Monte Carlo Methods",
    "section": "Markov property",
    "text": "Markov property\n\nConsider a simple board game where we roll a dice and move that many squares forward on the board. Suppose we are currently on the square \\(X_n\\). What can we say about which square \\(X_{n+1}\\) we move to on our next turn?\n\n\\(X_{n+1}\\) is random, since it depends on the roll of the dice.\n\\(X_{n+1}\\) depends on where we are now \\(X_n\\), since the score of dice will be added onto the number our current square,\nGiven the square \\(X_n\\) we are now, \\(X_{n+1}\\) doesnâ€™t depend on which sequence of squares \\(X_0, X_1, \\dots, X_{n-1}\\) we used to get here."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-property-1",
    "href": "slides/BSMM_8740_lec_09.html#markov-property-1",
    "title": "Monte Carlo Methods",
    "section": "Markov property",
    "text": "Markov property\n\nThe third point is called the Markov property or memoryless property.\nThe property of Markov Chains is memoryless, because we only need to remember what square weâ€™ve reached, not which squares we visited to get here.\nThe stochastic process before this step has no bearing on the future steps, given where we are now. That is, the past and the future are conditionally independent given the present."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider the following simple random walk on the integers \\(\\mathbb Z\\):\nWe start at \\(0\\), then at each time step, we go up by one with probability \\(p\\) and down by one with probability \\(q = 1-p\\). When \\(p = q = \\frac12\\), weâ€™re equally as likely to go up as down, and we call this the simple symmetric random walk.\nThe simple random walk is a simple but very useful model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In many modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the â€œdrunkardâ€™s walk.â€"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-1",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-1",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nrandom walks\nrequire(ggplot2, quietly = TRUE)\nset.seed(315)\n\nrrw &lt;- function(n, p = 1/2) {\n  q &lt;- 1 - p\n  Z &lt;- sample(c(1, -1), n, replace = TRUE, prob = c(p, q))\n  X &lt;- c(0, cumsum(Z))\n  c(0, cumsum(Z))\n}\n\nn &lt;- 2000\nrw_dat &lt;- tibble::tibble(x=0:n) |&gt; \n  dplyr::mutate(\n    \"p = 2/3\" = rrw(n, 2/3)\n    , \"p = 1/3\" = rrw(n, 1/3)\n    , \"p = 1/2\" = rrw(n, 1/2)\n  )\n\np0 &lt;- rw_dat |&gt; dplyr::slice_head(n=20) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np1 &lt;- rw_dat |&gt; dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np3 &lt;- rw_dat |&gt; #dplyr::slice_head(n=200) |&gt; \n  tidyr::pivot_longer(cols = -x) |&gt; \n  ggplot(aes(x=x,y=value, color=name)) + \n  geom_line() + \n  theme_minimal()\n\np0+p1+p3"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-2",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-2",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWe can write this as a stochastic process \\((X_n)\\) with discrete time \\(n = \\{0,1,2,\\dots\\} = \\mathbb Z_+\\) and discrete state space \\(\\mathcal S = \\mathbb Z\\), where \\(X_0 = 0\\) and, for \\(n \\geq 0\\), we have\n$$ X_{n+1} =\n\\[\\begin{cases} X_n + 1 & \\text{with probability $p$,} \\\\\n                             X_n - 1 & \\text{with probability $1-p$.}\n                             \n              \\end{cases}\\]\n$$\nItâ€™s clear from this definition that \\(X_{n+1}\\) (the future) depends on \\(X_n\\) (the present), but, given \\(X_n\\), does not depend on \\(X_{n-1}, \\dots, X_1, X_0\\) (the past). Thus the Markov property holds, and the simple random walk is a discrete time Markov process or Markov chain."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-3",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-3",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTo define a Markov chain, we first need to say where we start from, and then what the probabilities of transitions from one state to another are.\nIn our examples of the simple random walk and gamblerâ€™s ruin, we specified the start point \\(X_0 = i\\) exactly, but we could also pick the start point at random according to some distribution \\(\\pi_i = \\mathbb P(X_0 = i)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-4",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-4",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWe also need to know the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for \\(i,j \\in \\mathcal S\\). Because of the Markov property, the transition probability only needs to condition on the state weâ€™re in now \\(X_n = i\\), and not on the whole history of the process.\nIn the case of the simple random walk, for example, we had initial distribution\n\\[\n\\lambda_i = \\mathbb P(X_0 = i) = \\begin{cases} 1 & \\text{if $i = 0$} \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\nand transition probabilities\n\\[\n\\mathbb P(X_{n+1} = j \\mid X_n = i) = \\begin{cases} p & \\text{if $j = i+1$} \\\\ q & \\text{if $j = i-1$} \\\\ 0 & \\text{otherwise.} \\end{cases}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-5",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-5",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nFor the random walk (and also the gamblerâ€™s ruin), the transition probabilities \\(\\mathbb P(X_{n+1} = j \\mid X_n = i)\\) donâ€™t depend on \\(n\\); in other words, the transition probabilities stay the same over time.\nA Markov process with this property is called time homogeneous. We will only consider time homogeneous processes in this course."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-6",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-6",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWrite \\(p_{ij} = \\mathbb P(X_{n+1} = j \\mid X_n = i)\\) for the transition probabilities, which are independent of \\(n\\). We must have \\(p_{ij} \\geq 0\\), since it is a probability, and we must also have \\(\\sum_j p_{ij} = 1\\) for all states \\(i\\), as this is the sum of the probabilities of all the places you can move to from state i.\nWhen the state space is finite (and even sometimes when itâ€™s not), itâ€™s convenient to write the transition probabilities \\((p_{ij})\\) as a matrix \\(\\mathsf P\\), called the transition matrix, whose \\((i,j)\\)th entry is \\(p_{ij}\\).\nThen the condition that \\(\\sum_j p_{ij} = 1\\) is the condition that each of the rows of \\(\\mathsf P\\) add up to \\(1\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-7",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-7",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nConsider a simple two-state Markov chain with state space \\(\\mathcal S = \\{0,1\\}\\) and transition matrix \\[ \\mathsf P = \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\beta & 1-\\beta \\end{pmatrix}  \\] for some \\(0 &lt; \\alpha&lt;1, 0&gt;\\beta &lt; 1\\). Note that the rows of \\(\\mathsf P\\) add up to \\(1\\), as they must.\nWe can illustrate \\(\\mathsf P\\) by a transition diagram/graph, where the nodes/vertices are the states and the arrows give the transition probabilities. (We donâ€™t draw the arrow if \\(p_{ij} = 0\\).) In this case, our transition diagram looks like this:"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-8",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-8",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nTransition diagram for the two-state Markov chain"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-9",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-9",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nWe can use this graph as a simple model of customer churn, for example. If the customer has closed their account (state 0) on one period, then with probability \\(\\alpha\\) we will be able to entice them to open their account again (state 1) by the next period; while if the customer has an account (state 1), then with probability \\(\\beta\\) they will have closed their account (state 0) by the next period."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-10",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-10",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\nIf the customer has an account in period \\(n\\), whatâ€™s the probability they also have an account in period \\(n+2\\)?\n\\[\np_{11}(2) = \\mathbb P (X_{n+2} = 1 \\mid X_n = 1)\n\\]\nThe key to calculating this is to condition on the first step again â€“ that is, on whether the customer has an account in period \\(n\\). We have\n\\[\n\\begin{align*}\n  p_{11}(2) &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0, X_n = 1) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1, X_n = 1) \\\\\n  &= \\mathbb P (X_{n+1} = 0 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 0) \\\\\n  &\\qquad{} + \\mathbb P (X_{n+1} = 1 \\mid X_n = 1)\\,\\mathbb P (X_{n+2} = 1 \\mid X_{n+1} = 1) \\\\\n  &= p_{10}p_{01} + p_{11}p_{11} \\\\\n  &= \\beta\\alpha + (1-\\beta)^2 .\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-11",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-11",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIn the second equality, we used the Markov property to mean conditional probabilities like \\(\\mathbb P(X_{n+2} = 1 \\mid X_{n+1} = k)\\) did not have to depend on \\(X_n\\).\nAnother way to think of this as we summing the probabilities of all length-2 paths from 1 to 1, which are \\(1\\to 0\\to 1\\) with probability \\(\\beta\\alpha\\) and \\(1 \\to 1 \\to 1\\) with probability \\((1-\\beta)^2\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-12",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-12",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIn the above example, we calculated a two-step transition probability:\n\\[\np_{ij}(2) = \\mathbb P (X_{n+2} = j \\mid X_n = i)\n\\]\nby conditioning on the first step. That is, by considering all the possible intermediate steps \\(k\\), we have\n\\[\n\\begin{align*}\np_{ij}(2) & =\\sum_{k\\in\\mathcal{S}}\\mathbb{P}(X_{n+1}=k\\mid X_{n}=i)\\mathbb{P}(X_{n+2}=j\\mid X_{n+1}=k)\\\\\n& =\\sum_{k\\in\\mathcal{S}}p_{ik}p_{kj}\n\\end{align*}\n\\]\nBut this is exactly the formula for multiplying the matrix \\(\\mathsf P\\) with itself! In other words, \\(p_{ij}(2) = \\sum_{k} p_{ik}p_{kj}\\) is the \\((i,j)\\)th entry of the matrix square \\(\\mathsf P^2 = \\mathsf{P}\\times\\mathsf{P}\\). If we write \\(\\mathsf P(2)  = (p_{ij}(2))\\) for the matrix of two-step transition probabilities, we have \\(\\mathsf P(2) = \\mathsf P^2\\).\nMore generally, we see that this rule holds over multiple steps, provided we sum over all the possible paths \\(i\\to k_1 \\to k_2 \\to \\cdots \\to k_{n-1} \\to j\\) of length \\(n\\) from \\(i\\) to \\(j\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-13",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-13",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\nTheorem 1 Let \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). For \\(i,j \\in \\mathcal S\\), write \\[ p_{ij}(n) = \\mathbb P(X_n = j \\mid X_0 = i) \\] for the \\(n\\)-step transition probability. Then\n\\[\np_{ij}(n) = \\sum_{k_1, k_2, \\dots, k_{n-1} \\in \\mathcal S} p_{ik_1} p_{k_1k_2} \\cdots p_{k_{n-2}k_{n-1}} p_{k_{n-1}j}\n\\]\nIn particular, \\(p_{ij}(n)\\) is the \\((i,j)\\)th element of the matrix power \\(\\mathsf P^n\\), and the matrix of \\(n\\)-step transition probabilities is given by \\(\\mathsf P(n) = \\mathsf P^n\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-14",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-14",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nThe so-called Chapmanâ€“Kolmogorov equations follow immediately from this.\n\nLet \\((X_n)\\) be a Markov chain with state space \\(\\mathcal S\\) and transition matrix \\(\\mathsf P = (p_{ij})\\). Then, for non-negative integers \\(n,m\\), we have \\[ p_{ij}(n+m) = \\sum_{k \\in \\mathcal S} p_{ik}(n)p_{kj}(m) , \\] or, in matrix notation, \\(\\mathsf P(n+m) = \\mathsf P(n)\\mathsf P(m)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-15",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-15",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nIn other words, a trip of length \\(n + m\\) from \\(i\\) to \\(j\\) is a trip of length \\(n\\) from \\(i\\) to some other state \\(k\\), then a trip of length \\(m\\) from \\(k\\) back to \\(j\\), and this intermediate stop \\(k\\) can be any state, so we have to sum the probabilities.\nOf course, once we know that \\(\\mathsf P(n) = \\mathsf P^n\\) is given by the matrix power, itâ€™s clear to see that \\(\\mathsf P(n+m) = \\mathsf P^{n+m} = \\mathsf P^n \\mathsf P^m = \\mathsf P(n)\\mathsf P(m)\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-16",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-16",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\ninitial distribution\n\nIf we start from a state given by a pmf \\(\\boldsymbol \\pi_0 = (\\pi_{0,i})_{i\\in S}\\), then after step 1 the probability weâ€™re in state \\(j\\) is \\(\\sum_i \\pi_{0,i} p_{ij}\\).\nGiven transition matrix \\(\\mathsf P\\) and initial state pmf \\(\\boldsymbol \\pi_0\\), the pmf for the state after one step is \\(\\boldsymbol \\pi_1=\\boldsymbol \\pi_0 \\times\\mathsf P\\), i.e.Â \\(\\boldsymbol \\pi_1\\) and \\(\\boldsymbol \\pi_0\\) are row vectors, and for any given state pmf, the pmf for the states on the next step of the Markov chain is the current state pmf multiplied1 by the transition matrix.\n\nnote: on the left"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-17",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-17",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nStarting from a state given by a distribution \\(\\boldsymbol \\pi = (\\pi_i)_{i\\in S}\\), then after step 1 the probability weâ€™re in state \\(j\\) is \\(\\sum_i \\pi_i p_{ij}\\). So if \\(\\pi_j = \\sum_i \\pi_i p_{ij}\\), we stay in this distribution forever. We call such a distribution a stationary distribution. We again recognise this formula as a matrix-vector multiplication, so this is \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\), where \\(\\boldsymbol \\pi\\) is a row vector.\n\nLet \\((X_n)\\) be a Markov chain on a state space \\(\\mathcal S\\) with transition matrix \\(\\mathsf P\\). Let \\(\\boldsymbol \\pi = (\\pi_i)\\) be a distribution on \\(\\mathcal S\\), in that \\(\\pi_i \\geq 0\\) for all \\(i \\in \\mathcal S\\) and \\(\\sum_{i \\in \\mathcal S} \\pi_i = 1\\). We call \\(\\boldsymbol \\pi\\) a stationary distribution if\n\\[\n\\pi_j = \\sum_{i\\in \\mathcal S} \\pi_i p_{ij} \\quad \\text{for all $j \\in \\mathcal S$}\n\\]\nor, equivalently, if \\(\\boldsymbol \\pi = \\boldsymbol \\pi\\mathsf P\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-18",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-18",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nNote that weâ€™re saying the distribution \\(\\mathbb P(X_n = i)\\) stays the same; the Markov chain \\((X_n)\\) itself will keep moving.\nOne way to think is that if we started off a thousand Markov chains, choosing each starting position to be \\(i\\) with probability \\(\\pi_i\\), then (roughly) \\(1000 \\pi_j\\) of them would be in state \\(j\\) at any time in the future â€“ but not necessarily the same ones each time."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-19",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-19",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nstationary distributions\n\nSo our definition of the stationary state pmf is: \\(\\pi\\) such that \\(\\pi=\\pi\\times \\mathsf P\\). We can calculate it as follows:\n\\[\n\\begin{align*}\n\\pi-\\pi\\mathsf{P} & =0\\\\\n\\pi\\left(I-\\mathsf{P}\\right) & =0\\\\\n\\left(I-\\mathsf{P}\\right)^{\\top}\\pi^{\\top} & =0\n\\end{align*}\n\\]\nalong with the additional constraint \\(\\sum_i\\pi_i=1\\).\nIf we add a row of ones at the bottom of our matrix \\(\\left(I-\\mathsf{P}\\right)^{\\top}\\) and a \\(1\\) at the bottom of the vector on the right we can solve for \\(\\pi\\), e.g.Â using qr.solve."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-20",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-20",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nstationary distribution example\n\nPt(I-P)solveconfirm\n\n\n\n\nCode\nP = matrix(\n  c(\n    0.2, 0.3, 0.5, 0,\n    0, 0.1, 0.1, 0.8,\n    0.5, 0.2, 0, 0.3,\n    0.3,0.1,0.3,0.3\n  )\n  , nrow = 4, byrow = TRUE\n)\nP\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  0.2  0.3  0.5  0.0\n[2,]  0.0  0.1  0.1  0.8\n[3,]  0.5  0.2  0.0  0.3\n[4,]  0.3  0.1  0.3  0.3\n\n\n\n\n\n\nCode\nA &lt;- t(diag(1,4) - P)\nA\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  0.8  0.0 -0.5 -0.3\n[2,] -0.3  0.9 -0.2 -0.1\n[3,] -0.5 -0.1  1.0 -0.3\n[4,]  0.0 -0.8 -0.3  0.7\n\n\n\n\n\n\nCode\npi &lt;- \n  qr.solve(\n    A |&gt; rbind(c(1,1,1,1))\n    , c(0,0,0,0,1)\n    , tol = 1e-10\n  )\npi\n\n\n[1] 0.2686 0.1782 0.2447 0.3085\n\n\n\n\n\n\nCode\npi %*% P\n\n\n       [,1]   [,2]   [,3]   [,4]\n[1,] 0.2686 0.1782 0.2447 0.3085"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-21",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-21",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\nproperties\n\n\nA Markov Chain is irreducible if you have positive probability of eventually getting to anywhere in the state space from anywhere else in the state space\nA Markov Chain is aperiodic if there are no forced cycles, i.e.Â there do not exist disjoint non-empty subsets \\(X_1,X_2,...,X_d\\) for \\(dâ‰¥2\\), such that \\(P(x,X_{i+1})=1\\) for all \\(x\\in X_i \\;(1â‰¤iâ‰¤dâˆ’1)\\), and \\(P(x,X_1)=1\\) for all \\(x\\in X_d\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chains-22",
    "href": "slides/BSMM_8740_lec_09.html#markov-chains-22",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nMC with cycle"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#markov-chain-monte-carlo",
    "href": "slides/BSMM_8740_lec_09.html#markov-chain-monte-carlo",
    "title": "Monte Carlo Methods",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\nSuppose have complicated, high-dimensional density \\(\\pi = cg\\) and we want samples \\(X_1, X_2,\\dots \\sim \\pi\\).\nDefine a Markov chain (dependent random process) \\(X_0, X_1,X_2\\dots\\) in such a way that for large enough \\(m\\), we have \\(X_n\\sim \\pi,\\,\\forall n\\ge m\\).\nThen we can estimate \\(\\mathbb{E}_{\\pi}(h) â‰¡ \\int h(x) \\pi(x) dx\\) by:\n\\[\n\\mathbb{E}_{\\pi}[h] \\approx \\frac{1}{M-B}\\sum_{i=B+1}^{M}h(x_i)\n\\]\nwhere \\(B\\) (â€œburn-inâ€) is chosen large enough so \\(X_B\\sim\\pi\\), and \\(M\\) is chosen large enough to get good Monte Carlo estimates."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\n\nchoose some initial value \\(X_0\\), then\ngiven \\(X_{n-1}\\), choose a proposal state \\(Y_n\\sim \\textrm{MVN}(X_{n-1},\\sigma^2\\textrm{I})\\), for some fixed \\(\\sigma^2&gt;0\\)\nlet \\(A_n=\\pi(Y_n)/\\pi(X_{n-1})=g(Y_n)/g(X_{n-1})\\) and \\(U_n\\sim\\textrm{U}[0,1]\\), then\nif \\(U_n&lt;A_n\\) set \\(X_n=Y_n\\) (â€œacceptâ€), otherwise set \\(X_n = X_{n-1}\\) (â€œrejectâ€)\nrepeat for \\(n=1,2,3,\\ldots,M\\)\n\nThis version is called random-walk Metropolis - if we always accepted the proposals, they would form a traditional random walk process."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-1",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\nthe burn-in period is a matter of trial and error\nthe start values donâ€™t matter much, but central. ones are â€˜betterâ€™\nif \\(\\sigma\\) is too small then we usually accept and the chain doesnâ€™t move much\nif \\(\\sigma\\) is too big then we usually reject and the chain doesnâ€™t move much\ngenerally the acceptance rate should be far from both zero and 1"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-2",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-2",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nThe variance of the estimate is\n\\[\n\\frac{1}{M-B}\\textrm{Var}_{\\pi}(h)\\times \\textrm{varfact}\n\\]\nwhere\n\\[\n\\textrm{varfact} = 1+2\\sum_{k=1}^{\\infty}\\textrm{Corr}_{\\pi} \\left(h(X_0),h(X_k)\\right)=\\sum_{-\\infty}^{\\infty}\\rho_k\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-3",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-3",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\na simple Metropolis algorithm in one dimension (continuous RV)\ng = function(y) {\n    if ( (y&lt;0) || (y&gt;1) )\n    return(0)\n    else\n    return( y^3 * sin(y^4) * cos(y^5) )\n}\n\nh = function(y) { return(y^2) }\n\nM = 11000  # run length\nB = 1000  # amount of burn-in\nX = runif(1)  # overdispersed starting distribution\nsigma = 1  # proposal scaling\nxlist = rep(0,M)  # for keeping track of chain values\nhlist = rep(0,M)  # for keeping track of h function values\nnumaccept = 0;\n\nfor (i in 1:M) {\n  Y = X + sigma * rnorm(1)  # proposal value\n  U = runif(1)              # for accept/reject\n  alpha = g(Y) / g(X)       # for accept/reject\n  if (U &lt; alpha) {\n    X = Y                   # accept proposal\n    numaccept = numaccept + 1;\n  }\n    xlist[i] = X;\n    hlist[i] = h(X);\n}\n\nu = mean(hlist[(B+1):M])\nse1 =  sd(hlist[(B+1):M]) / sqrt(M-B)\nvarfact &lt;- function(xxx) { 2 * sum(acf(xxx, plot=FALSE)$acf) - 1 }\nthevarfact = varfact(hlist[(B+1):M])\nse = se1 * sqrt( thevarfact )\n\ntibble::tibble(\n  measure = c(\"iterations\",\"burn-in\",\"mean of h\",\"iid se\",\"varfact\",\"true se\",\"95% CI lb\",\"95% CI ub\")\n  , value = c(M,B,u,se1,thevarfact,se,u-1.96*se,u+1.96*se)\n) |&gt; \ngt::gt(\"measure\") |&gt; \ngt::fmt_number(rows = 3:8, decimals = 4) |&gt; \ngt::fmt_number(rows = 1:2, decimals = 0) |&gt; \ngtExtras::gt_theme_espn()\ntibble::tibble(x=1:length(xlist), y=xlist) |&gt; \n  ggplot(aes(x=x,y=y)) + geom_line() +\n  labs(title = \"Plot of accepted values\")\n\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\niterations\n11,000\n\n\nburn-in\n1,000\n\n\nmean of h\n0.7730\n\n\niid se\n0.0017\n\n\nvarfact\n21.0156\n\n\ntrue se\n0.0076\n\n\n95% CI lb\n0.7581\n\n\n95% CI ub\n0.7879"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-4",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-4",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\n\ntargetcodesampleshistogram\n\n\n\n\ntarget density shape\np &lt;- 0.4\nmu &lt;- c(-1, 2)\nsd &lt;- c(.5, 2)\nf &lt;- function(x)\n    p     * dnorm(x, mu[1], sd[1]) +\n    (1-p) * dnorm(x, mu[2], sd[2])\npar(mar=c(10.1, 2, .5, .5), oma=c(0, 1, 0, 0))\ncurve(f(x), col=\"red\", -4, 8, n=301, las=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis sampling algrithm\npropose &lt;- function(x) rnorm(1, x, 4)\nstep &lt;- function(x, f, q) {\n    ## Pick new point\n    xp &lt;- propose(x)\n    ## Acceptance probability:\n    alpha &lt;- min(1, f(xp) / f(x))\n    ## Accept new point with probability alpha:\n    if (runif(1) &lt; alpha) x &lt;- xp\n    ## Returning the point:\n    x\n}\nrun &lt;- function(x, f, q, nsteps) {\n    res &lt;- matrix(NA, nsteps, length(x))\n    for (i in seq_len(nsteps))\n        res[i,] &lt;- x &lt;- step(x, f, q)\n    drop(res)\n}\n\n\n\n\n\n\nMetropolis samples from target\nres &lt;- run(-10, f, q, 1000)\n\nlayout(matrix(c(1, 2), 1, 2), widths=c(4, 1))\npar(mar=c(10.1, 1.5, .5, .5), oma=c(0, 1, 0, 0))\nplot(res, type=\"s\", xpd=NA, ylab=\"Parameter\", xlab=\"Sample\", las=1)\nusr &lt;- par(\"usr\")\nxx &lt;- seq(usr[3], usr[4], length=301)\nplot(f(xx), xx, type=\"l\", yaxs=\"i\", axes=FALSE, xlab=\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis samples from target: histogram\npar(mar=c(10.1, 1.5, .5, .5), oma=c(0, 1, 0, 0))\nhist(res, 50, freq=FALSE, main=\"\", ylim=c(0, .4), las=1,\n     xlab=\"x\", ylab=\"Probability density\")\nz &lt;- integrate(f, -Inf, Inf)$value\ncurve(f(x) / z, add=TRUE, col=\"red\", n=200)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-5",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-5",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nDiscrete case\n\nstationary means that if \\(X_0\\sim\\pi\\), i.e.Â \\(\\mathbb{P}(X_0=i)=\\pi(i)\\), then \\(\\mathbb{P}(X_1=j)=\\pi(j),\\;\\forall j\\)\n\\(\\mathbb{P}(X_1=j)=\\sum_{i\\in S}\\mathbb{P}\\left(X_0=i,X_1=j\\right)\\) which is \\(\\sum_{i\\in S}\\mathbb{P}(X_0=i)\\mathsf{P}(i,j)\\)\nso \\(\\pi\\) is stationary if \\(\\sum_{i\\in S}\\pi(i)\\mathsf{P}(i,j)=\\pi(j),\\;\\forall j\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-6",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-6",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nDiscrete case\nlet \\(q(x,y)=\\mathbb{P}(Y_n=y|X_{n-1}=x)\\) be the proposal distribution\n\nassume \\(q\\) is symmetic, i.e.Â \\(q(x,y)=q(y,x),\\;\\forall x,y\\)\nthen if \\(\\alpha(x,y)\\) is the acceptance probability from \\(x\\) to \\(y\\):\n\n\\[\n\\begin{align*}\n\\alpha(x,y) & =\\mathbb{P}(U_{n}&lt;A_{n}|X_{n-1}=x,Y_{n}=y)\\\\\n& \\mathbb{P}\\left(U_{n}&lt;\\frac{\\pi(y)}{\\pi(x)}\\right)=\\min\\left[1,\\frac{\\pi(y)}{\\pi(x)}\\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-7",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-7",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nDiscrete case\nThen, for \\(i,j\\) in the state space, with \\(i\\ne j\\)\n\\[\n\\begin{align*}\n\\mathsf{P}(i,j)=q(i,j)\\alpha(i,j)=q(i,j)\\min\\left[1,\\frac{\\pi(j)}{\\pi(i)}\\right]\n\\end{align*}\n\\]\nand, by symmetry of \\(q\\)\n\\[\n\\begin{align*}\n\\pi(i)\\mathsf{P}(i,j) & =q(i,j)\\min\\left(\\pi(i),\\pi(j)\\right)\\\\\n& =q(j,i)\\min\\left(\\pi(j),\\pi(i)\\right)=\\pi(j)\\mathsf{P}(j,i)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-8",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-8",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nDiscrete case\nIf \\(X_0\\sim\\pi\\), then\n\\[\n\\begin{align*}\n\\mathbb{P}(X_{1}=j) & =\\sum_{i\\in\\chi}\\mathbb{P}(X_{0}=i)\\mathsf{P}(i,j)=\\sum_{i\\in\\chi}\\pi(i)\\mathsf{P}(i,j)\\\\\n& =\\sum_{i\\in\\chi}\\pi(j)\\mathsf{P}(j,i)=\\pi(j)\\sum_{i\\in\\chi}\\mathsf{P}(j,i)\\\\\n& =\\pi(j)\n\\end{align*}\n\\]\nso \\(X_1\\sim\\pi\\) too, and the Markov chain preserves \\(\\pi\\), i.e.Â \\(\\pi\\) is a stationary distribution."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-9",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-algo-9",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis Algo",
    "text": "MCMC Metropolis Algo\nexample\n\\(\\chi=\\mathbb{Z}\\), \\(\\pi(x)=2^{-|x|}/3\\), and \\(q(x,y)=\\frac{1}{2}\\) if \\(|x-y|=1\\), otherwise \\(0\\).\n\nreversible? Yes, itâ€™s a Metropolis algorithm\n\\(\\pi\\) stationary? Yes, follows from reversibility\naperiodic? Yes, since \\(\\mathsf{P}(x,\\{x\\})&gt;0\\)\nirreducible? Yes, since \\(\\pi(x)&gt;0,\\forall x\\in\\chi\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nMCMC algorithms require that the MC chain is reversible, \\(\\pi\\)-stationary, aperiodic, and reducible.\nThe Metropolis algorithm uses the symmetry of the proposal distribution to ensure reversibility and irreducibility.\nIf the proposal distribution was not symmetric we might not have those two properties, e.g.Â if \\(q(x,y) \\gg q(y,x)\\) the the Metropolis chain would spend too much time at \\(y\\) and not enough at \\(x\\), so it accepts fewer moves \\(x\\rightarrow y\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-1",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nIf we only require that \\(q(x,y)&gt;0\\;\\textrm{iff}\\; q(y,x)&gt;0\\), then replace\n\\[\nA_n = \\frac{\\pi(Y_n)}{\\pi(X_{n-1})}\\;\\mathrm{with}\\; A_n = \\frac{\\pi(Y_n)q(Y_n,X_{n-1})}{\\pi(X_{n-1})q(X_{n-1},Y_n)}\n\\]\nand the algorithm is still valid even is \\(q\\) is not symmetric."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-2",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-2",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nWhy is this valid?\nFor metropolis, th key was that the Markov chain was reversible, i.e.Â \\(\\pi(x)\\mathsf{P}(x,y)=\\pi(y)\\mathsf{P}(y,x)\\)\nIf instead \\(A_n = \\frac{\\pi(Y_n)q(Y_n,X_{n-1})}{\\pi(X_{n-1})q(X_{n-1},Y_n)}\\), with acceptance probability \\(\\alpha(x,y)=\\min\\left[1,\\frac{\\pi(Y_n)q(Y_n,X_{n-1})}{\\pi(X_{n-1})q(X_{n-1},Y_n)}\\right]\\), then\n\\[\n\\begin{align*}\nq(x,y)\\alpha(x,y)\\pi(x) & =q(x,y)\\min\\left[1,\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}\\right]\\pi(x)\\\\\n& =\\min\\left[\\pi(x)q(x,y),\\pi(y)q(y,x)\\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-3",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-3",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nThen\n\n\\(\\pi(x)\\mathsf{P}(x,y)\\) is still symmetric, even if \\(q\\) wasnâ€™t.\nthe chain is still reversible, so there is a stationary distribution \\(\\pi\\).\nif the chain is irreducible and aperiodic, then it converges to \\(\\pi\\)."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-4",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-metropolis-hastings-algo-4",
    "title": "Monte Carlo Methods",
    "section": "MCMC Metropolis-Hastings Algo",
    "text": "MCMC Metropolis-Hastings Algo\nexample: independence sampler\nHere \\(\\{Y_n\\}\\sim q(\\cdot)\\) i,e, the \\(\\{Y_n\\}\\) are IID from some fixed density \\(q\\), independent of \\(X_{n-1}\\) and we accept \\(Y_n\\) if \\(U_n&lt;A_n\\) where \\(U_n\\sim \\textrm{Uniform}[0,1]\\) and\n\\[\nA_n = \\frac{\\pi(Y_n)q(X_{n-1})}{\\pi(X_{n-1})q(Y_n)}\n\\]"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\nThe Bayesian model for updating parameter estimates is (to within a scaling constant)\n\\[\n\\begin{align*}\n\\pi_\\theta\\left(\\left.\\theta\\right|X\\right)\\sim\\pi_X\\left(\\left.X\\right|\\theta\\right)\\times\\pi_\\theta\\left(\\theta\\right)\n\\end{align*}\n\\tag{1}\\]\nwhere the parameter set \\(\\theta\\) depends on the assumed data generating process \\(\\pi_X\\).\n\nIn words: the joint probability of the parameters given the observed data is equal to (to within a scaling constant) the probability of the observed data given the parameters, times the prior probabilities of the parameters. In practice we refer to the probabilities as likelihoods, and use log-likelihoods in equation (EquationÂ 1) to avoid numerical problems arising from the product of small probabilities."
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-1",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-1",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\ninitialupdatingplotsplot code\n\n\n\n\nInitial state: Bayesian parameter estimation, Binomial process\nset.seed(8740)\np &lt;- 0.5\nn &lt;- 10\n# range of probability [0.01 - 0.99]\np_values &lt;- seq(0.01,0.99,0.001)\n# prior probability is beta(1,1)\npr &lt;- dbeta(p_values,7,2)\n# Have to normalize given discreteness\npr &lt;- pr / sum(pr) \n# create the data\ndat &lt;- tibble::tibble(parameters = p_values, prob = pr, x = 0, step= 0) \n\n\n\n\n\n\n7 Bayesian updating steps\n# Run for M samples\nfor (i in 1:8) {\n  # have the data generating process generate a data point\n  x &lt;- rbinom(1, n, p)\n  # multiply the likelihood of observing the data point at each p-value\n  # by the prior probability of each p-value:\n  # this gives the posterior probability for each p-value\n  ps &lt;- dbinom(x, n, p_values) * pr\n  # normalize\n  ps &lt;- ps / sum(ps) \n  # lines(ps~p_values, col=(i+1))\n  # same the posterior p-value probabilities at this step\n  dat &lt;- dat |&gt;\n    dplyr::bind_rows(\n      tibble::tibble(parameters = p_values, prob = ps, x = x) |&gt;\n        dplyr::mutate(step = i)\n    )\n  # update the prior probability of each p-value to be its posterior probability\n  pr = ps\n}\n\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in\nggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlots of Bayesian updating steps\ndat &lt;- dat |&gt; dplyr::group_by(step) |&gt;\n  dplyr::mutate(\n    title =\n      dplyr::case_when(\n        step == 0 ~ stringr::str_glue(\"Step {step}: prior mean is {(parameters*prob) |&gt; sum() |&gt; round(digits=3)}\")\n        , TRUE ~ stringr::str_glue(\"Step {step}: sample is {x} & posterior mean is {(parameters*prob) |&gt; sum() |&gt; round(digits=3)}\")\n      )\n  )\n\nlabels &lt;- dat |&gt; dplyr::distinct(step, title) |&gt; dplyr::mutate(step = factor(step))\nstep_labels &lt;- split(labels$title, labels$step)\nstep_labeller &lt;- function(value){return(step_labels[value])}\n\ndat |&gt; dplyr::mutate(step = factor(step)) |&gt; dplyr::group_by(step) |&gt;\n  ggplot(aes(x = parameters, y=prob, color = step)) +\n  geom_line() +\n  geom_vline(xintercept=0.5, color=\"grey\", size=1, linetype = \"dashed\") +\n  facet_wrap(vars(step), nrow = 2, labeller=labeller(step = step_labeller)) +\n  theme(plot.margin=unit(c(5,1,5,1), 'cm')) +\n  theme_minimal() + \n  labs(title = \"Bayesian updating\", subtitle = \"Binomial data; n known, true p = 0.5\")"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-2",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-2",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\nIt is more common to have multiple data points and multiple parameters. In this case we sample from the posterior using \\(\\pi_X\\left(\\left.X\\right|\\theta\\right)\\times\\pi_\\theta\\left(\\theta\\right)\\) and a MCMC method like Metropolis Hastings.\nIn the example to follow, we observe \\((y,x)\\) and we have parameters \\(\\theta=(a,b,\\sigma)\\) that we want to estimate\n\nthe data generation process is assumed to be \\(y = \\mathscr{N}(ax+b, \\sigma^2)\\)\nwe choose priors on the parameters as follows\n\n\\(a=U[0,10]\\)\n\\(b=\\mathscr{N}(0,5)\\)\n\\(\\sigma=U[0,30]\\)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-3",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-3",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\ndatapriorlikelihoodproposalMH Algo\n\n\n\nset.seed(8740)\nb0 = 0; b1 = 5; sigma = 10; sampleSize = 31\n\ndata &lt;- \n  tibble::tibble(\n    x = (-(sampleSize-1)/2):((sampleSize-1)/2)\n    , y =  b0 + b1 * x + rnorm(n=sampleSize, mean=0, sd=sigma)\n  )\n\n\n\n\nprior = function(param){\n  aprior = dunif(param[1], min=0, max=10, log = T)\n  bprior = dnorm(param[2], sd = 5, log = T)\n  sdprior = dunif(param[3], min=0, max=30, log = T)\n  # return the log of the product of probabilities (parameters)\n  return(aprior+bprior+sdprior)\n}\n\n\n\n\nlikelihood = function(param, x, y){\n  pred = param[2] + param[1] * x \n  singlelikelihoods = dnorm(y, mean = pred, sd = param[3], log = T)\n  sumll = sum(singlelikelihoods)\n  # return the log of the product of probabilities (data)\n  return(sumll)\n}\n\n\n\n\nproposalfunction = function(param){\n  return( rnorm(3, mean = param, sd= c(0.1,0.5,0.3)) )\n}\n\n\n\n\nrun_metropolis_MCMC = function(startvalue, iterations, data){\n  # initialize\n  chain = array(dim = c(iterations+1,3))\n  chain[1,] &lt;- startvalue\n  \n  for (i in 1:iterations){\n    proposal = proposalfunction(chain[i,])\n    probab = \n      exp(\n        likelihood(proposal, data$x, data$y) + \n        prior(proposal) - \n        likelihood(chain[i,], data$x, data$y) - \n        prior(chain[i,])\n      )\n    if (runif(1) &lt; probab){\n      chain[i+1,] = proposal\n    }else{\n      chain[i+1,] = chain[i,]\n    }\n  }\n  return(chain)\n}"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-4",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-4",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\n\nMH algorithm generating posterior parameter probabilities\nstartvalue = c(4,2,8)\nchain = run_metropolis_MCMC(startvalue, 20000, data)\n\nmcm_chain &lt;- coda::mcmc(chain)\nsmry &lt;- summary(mcm_chain)\nsmry$statistics |&gt; tibble::as_tibble() |&gt; \n  dplyr::bind_cols(smry$quantiles |&gt; tibble::as_tibble()) |&gt;\n  tibble::add_column(param = c('beta1','beta0','sigma')) |&gt; \n  gt::gt(\"param\") |&gt; \n  gt::tab_spanner(label = \"percentiles\", columns = ends_with(\"%\")) |&gt; \n  gt::tab_options( table.font.size = gt::px(48) ) %&gt;% \n  gtExtras::gt_theme_espn() |&gt;\n  gt::as_raw_html()\n\n\n\n  \n  \n\n\n\n\nMean\nSD\nNaive SE\nTime-series SE\npercentiles\n\n\n2.5%\n25%\n50%\n75%\n97.5%\n\n\n\n\nbeta1\n4.8424\n0.1742\n0.001232\n0.005517\n4.498\n4.7268\n4.844\n4.959\n5.182\n\n\nbeta0\n0.6129\n1.4354\n0.010149\n0.070569\n-2.173\n-0.3511\n0.616\n1.560\n3.495\n\n\nsigma\n8.5691\n1.1006\n0.007783\n0.068802\n6.726\n7.7719\n8.496\n9.256\n10.982"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-5",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-5",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\n\nposterior parameter probability plots\nplot(mcm_chain)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-6",
    "href": "slides/BSMM_8740_lec_09.html#mcmc-and-bayesian-analysis-6",
    "title": "Monte Carlo Methods",
    "section": "MCMC and Bayesian Analysis",
    "text": "MCMC and Bayesian Analysis\n\ntrueA = 5\ntrueB = 0\ntrueSd = 10\nsampleSize = 31\n \nx = (-(sampleSize-1)/2):((sampleSize-1)/2)\ny =  trueA * x + trueB + rnorm(n=sampleSize,mean=0,sd=trueSd)\n \nlikelihood = function(param){\n  a = param[1]\n  b = param[2]\n  sd = param[3]\n \n  pred = a*x + b\n  singlelikelihoods = dnorm(y, mean = pred, sd = sd, log = T)\n  sumll = sum(singlelikelihoods)\n  return(sumll)\n}\n \nprior = function(param){\n  a = param[1]\n  b = param[2]\n  sd = param[3]\n  aprior = dunif(a, min=0, max=10, log = T)\n  bprior = dnorm(b, sd = 5, log = T)\n  sdprior = dunif(sd, min=0, max=30, log = T)\n  return(aprior+bprior+sdprior)\n}\n \nproposalfunction = function(param){\n  return(rnorm(3, mean = param, sd= c(0.1,0.5,0.3)))\n}\n \nrun_metropolis_MCMC = function(startvalue, iterations){\n  chain = array(dim = c(iterations+1,3))\n  chain[1,] = startvalue\n  for (i in 1:iterations){\n    proposal = proposalfunction(chain[i,])\n \n    probab = exp(likelihood(proposal)+ prior(proposal) - likelihood(chain[i,])- prior(chain[i,]))\n    if (runif(1) &lt; probab){\n      chain[i+1,] = proposal\n    }else{\n      chain[i+1,] = chain[i,]\n    }\n  }\n  return(mcmc(chain))\n}\n\n\nset.seed(8740)\nb0 = 0; b1 = 5; sigma = 10; sampleSize = 31\n\ndata &lt;- \n  tibble::tibble(\n    x = (-(sampleSize-1)/2):((sampleSize-1)/2)\n    , y =  b0 + b1 * x + rnorm(n=sampleSize, mean=0, sd=sigma)\n  )\n\n\nlikelihood = function(param, x, y){\n  pred = param[2] + param[1] * x \n  singlelikelihoods = dnorm(y, mean = pred, sd = param[3], log = T)\n  sumll = sum(singlelikelihoods)\n  # return the log of the product of probabilities (data)\n  return(sumll)\n}\n\nprior = function(param){\n  aprior = dunif(param[1], min=0, max=10, log = T)\n  bprior = dnorm(param[2], sd = 5, log = T)\n  sdprior = dunif(param[3], min=0, max=30, log = T)\n  # return the log of the product of probabilities (parameters)\n  return(aprior+bprior+sdprior)\n}\n\nproposalfunction = function(param){\n  return( rnorm(3, mean = param, sd= c(0.1,0.5,0.3)) )\n}\n\nrun_metropolis_MCMC = function(startvalue, iterations, data){\n  # initialize\n  chain = array(dim = c(iterations+1,3))\n  chain[1,] &lt;- startvalue\n  # x = data$x, y = data$y\n  \n  for (i in 1:iterations){\n    proposal = proposalfunction(chain[i,])\n    probab = \n      exp(\n        likelihood(proposal, data$x, data$y) + \n        prior(proposal) - \n        likelihood(chain[i,], data$x, data$y) - \n        prior(chain[i,])\n      )\n    if (runif(1) &lt; probab){\n      chain[i+1,] = proposal\n    }else{\n      chain[i+1,] = chain[i,]\n    }\n  }\n  # return(mcmc(chain))\n  return(chain)\n}\n\nstartvalue = c(4,2,8)\nchain = run_metropolis_MCMC(startvalue, 20000, data)\n\nmcm_chain &lt;- coda::mcmc(chain)\nsummary(mcm_chain)\nplot(mcm_chain)"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#more",
    "href": "slides/BSMM_8740_lec_09.html#more",
    "title": "Monte Carlo Methods",
    "section": "More",
    "text": "More\n\nThe Random Walk Metropolis: Linking Theory and Practice Through a Case Study\nâ€œThe Metropolis-Hastings Algorithm.â€ April 12, 2023. https://blog.djnavarro.net/posts/2023-04-12_metropolis-hastings/.\nWhy Metropolisâ€“Hastings Works"
  },
  {
    "objectID": "slides/BSMM_8740_lec_09.html#recap",
    "href": "slides/BSMM_8740_lec_09.html#recap",
    "title": "Monte Carlo Methods",
    "section": "Recap",
    "text": "Recap\n\nThe week weâ€™ve introduced Monte Carlo (MC) methods for sampling from probability distributions, including using those sample to estimate expected values.\nWe discus importance sampling, rejection sampling, and Markov Chain Monte Carlo (MCMC) methods, particularily the particularly the Metropolis and Metropolis-Hastings methods.\nFinally, we applied the Metropolis-Hastings methods in Bayesian analytics, generating Bayesian estimates for the parameters of linear regression.\n\n\n\n\n\nbsmm-8740-fall-2024.github.io/osb"
  },
  {
    "objectID": "supplemental/collinearity.html",
    "href": "supplemental/collinearity.html",
    "title": "Collinearity and ridge regression",
    "section": "",
    "text": "In statistics, collinearity (also multicollinearity) is a phenomenon in which one feature/predictor variable in a regression model is highly correlated with another feature variable.\nIn mathematics, a set of vectors v1,v2,â€¦,vnv_1,v_2,\\ldots,v_n (e.g.Â a set of column vectors of predictors) are linearly dependent if there are constants a1,a2,â€¦,ana_1,a_2,\\ldots,a_n, not all zero, such that a1v1+a2v2+â‹¯+anvn=0\na_1v_1+a_2v_2+ \\cdots + a_nv_n = 0\nIn other words some combination of predictor columns (vectors), after scaling and adding them together, equal one or more other predictors.\nIf the predictors are linearly dependent, they are correlated, and this means the regression coefficients are not uniquely determined.",
    "crumbs": [
      "Supplemental notes",
      "Collinearity"
    ]
  },
  {
    "objectID": "supplemental/collinearity.html#example",
    "href": "supplemental/collinearity.html#example",
    "title": "Collinearity and ridge regression",
    "section": "Example:",
    "text": "Example:\nIn this example we will simulate what happens with linearly dependent predictors.\n\nBase example: ordinary regression\nFirst weâ€™ll create a simple dataset with one outcome and one predictor and estimate the coefficients with repeated regressions. In the dataset described by 5+3x5 + 3x, i.e.Â intercept 5 and slope 3. There is no linear dependence as we have only one predictor.\n&gt; set.seed(8740)\n&gt; \n&gt; # N rows\n&gt; N &lt;- 100\n&gt; # predictor x runs from 0-5 in steps of 5/N plus a bit of noise\n&gt; x &lt;- seq( 0, 5, 5/(N-1) ) + rnorm(N, 0, 0.1)\n&gt; \n&gt; # when we regress y on x we should: \n&gt; #   - estimate an intercept close to 5 \n&gt; #   - estimate the coefficient of x as close to 3 \n&gt; dat0 &lt;- \n+   tibble::tibble( \n+     x = x \n+     # y is linearly related to x, plus some noise\n+     , y = 5 + 3*x + rnorm(N, 0, 1.5)\n+   )\n&gt; \n&gt; dat0 %&gt;% \n+   ggplot(aes(x = x, y = y)) +\n+   geom_point()\n&gt; lm(y~x, dat0)\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = y ~ x, data = dat0)\n\nCoefficients:\n(Intercept)            x  \n      5.100        2.895  \n\n\n\nWe see that an ordinary linear regression gives us results that are close to what we expect.\nNow we do the same regression on similar data (only the noise is different) and take the mean values of the coefficient estimates:\n\n&gt; # create a list with 100 elements,\n&gt; # just so we run the regression 100 times\n&gt; 1:100 %&gt;% as.list() %&gt;% \n+   # for each element of the list, run the function \n+   purrr::map(\n+     .f = function(...){ # we don't use any of arguments\n+       # run the regression on the same data\n+       tibble::tibble( \n+         x = x \n+         # y is linearly related to x, plus some noise\n+         , y = 5 + 3*x + rnorm(N, 0, 1.5)\n+       ) %&gt;% lm(y~., .) %&gt;% \n+         # extract the intercept and coefficient\n+         broom::tidy() %&gt;% \n+         dplyr::select(1:2) \n+     }\n+   ) %&gt;% \n+   # combine all the estimates\n+   dplyr::bind_rows() %&gt;% \n+   dplyr::group_by(term) %&gt;% \n+   # summarize the combined estimates\n+   dplyr::summarize(\n+     mean = mean(estimate)\n+     , variance = var(estimate) \n+   ) \n\n# A tibble: 2 Ã— 3\n  term         mean variance\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.98  0.0705 \n2 x            3.00  0.00732\n\n\nWe see that we are means of the coefficient estimates are close to what we expect.\n\n\nBase example: ordinary regression with colinearity\nNow we create two colinear predictors a,ba,b as in the code below, where b=2Ã—ab=2\\times a so a,ba,b are collinear, a+b=xa+b=x , and the regression estimates two coefficients Î²a,Î²b\\beta_a,\\beta_b such that\nÎ²aÃ—a+Î²bÃ—b=Î²a3Ã—x+2Î²b3Ã—xâ†’Î²a3+2Î²b3=3\n\\beta_a\\times a+ \\beta_b\\times b = \\frac{\\beta_a}{3}\\times x + \\frac{2\\beta_b}{3}\\times x \\rightarrow \\frac{\\beta_a}{3} + \\frac{2\\beta_b}{3} = 3\n\n&gt; dat1 &lt;- \n+   tibble::tibble( \n+     a = x/3 + rnorm(N,0, 0.01)\n+     , b = x*2/3 + rnorm(N,0, 0.01)\n+     , y = 5 + 3*(a+b) + rnorm(N, 0, 2)\n+   )\n&gt; \n&gt; dat1 %&gt;% \n+   ggplot(aes(x = x, y = y)) +\n+   geom_point()\n&gt; lm(y ~ a + b, dat1)\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = y ~ a + b, data = dat1)\n\nCoefficients:\n(Intercept)            a            b  \n      5.450       26.721       -9.104  \n\n\n\nSure enough, we have Î²a3+2Î²b3â‰ˆ3\\frac{\\beta_a}{3} + \\frac{2\\beta_b}{3} \\approx 3. The code below performs the check:\n\n&gt; lm(y ~ a + b, dat1) %&gt;% broom::tidy() %&gt;% dplyr::select(1,2) %&gt;% \n+   tidyr::pivot_wider(names_from = term, values_from = estimate) %&gt;% \n+   dplyr::mutate(\n+     check = a/3 + 2 * b/3\n+   )\n\n# A tibble: 1 Ã— 4\n  `(Intercept)`     a     b check\n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1          5.45  26.7 -9.10  2.84\n\n\nNote that we have two unknowns, but only one equation, so the problem does not have a unique solution, as seen by the wide variation of estimates in a repeated regression:\n\n&gt; # create a list with 100 elements,\n&gt; # just so we run the regression 100 times\n&gt; 1:100 %&gt;% as.list() %&gt;% \n+   # run the \n+   purrr::map(\n+     .f = function(...){ # we don't use any of arguments\n+       # run the regression on the same data\n+       tibble::tibble( \n+         a = x/3 + rnorm(N,0, 0.01)\n+         , b = x*2/3 + rnorm(N,0, 0.01)\n+         , y = 5 + 3*(a+b) + rnorm(N, 0, 2)\n+       ) %&gt;% lm(y~., .) %&gt;% \n+         # extract the intercept and coefficient\n+         broom::tidy() %&gt;% \n+         dplyr::select(1:2) \n+     }\n+   ) %&gt;% \n+   # combine all the estimates\n+   dplyr::bind_rows() %&gt;% \n+   dplyr::group_by(term) %&gt;% \n+   # summarize the combined estimates\n+   dplyr::summarize(\n+     mean = mean(estimate)\n+     , variance = var(estimate) \n+   ) \n\n# A tibble: 3 Ã— 3\n  term         mean variance\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.99    0.176\n2 a            2.99  304.   \n3 b            3.01   76.2  \n\n\nGranted this is an extreme case of collinearity, but it illustrates the issue.\nHow can we mitigate the problem?\n\n\nRidge regression example:\nIndeterminancy of the predictor coefficient estimates is a symptom of collinearity, as indicated by the large variance of the estimates.\nRidge regression penalizes large predictor coefficients, and we can use it here to address the collinearity.\nFor the similar data (same relation between y and x, but different noise) under ridge regression (glmnet::glmnet with alpha=0):\n&gt; # create the dataset\n&gt; dat1 &lt;- \n+   tibble::tibble( \n+     a = x/3 + rnorm(N,0, 0.01)\n+     , b = x*2/3 + rnorm(N,0, 0.01)\n+     , y = 5 + 3*(a+b) + rnorm(N, 0, 2)\n+   )\n&gt; \n&gt; # fit with glmnet (no cross validation and a range of penalty parameters)\n&gt; fit1 = glmnet::glmnet(\n+   y = dat1$y\n+   , x = model.matrix(y ~ a + b, data = dat1)\n+   , alpha = 0\n+ )\n&gt; \n&gt; # plot the coefficient estimates as a function of the penalty parameter lambda\n&gt; plot(fit1, xvar='lambda')\n\n\n\n\n\n\n\n\n\n\n\nEven using the defaults we can see that the coefficients estimated under the l2l_2 (sum of squared coefficients) penalty are in the right ballpark. The only step remaining is to find the best penalty coefficient Î»\\lambda.\nWe can do this with the built-in cross validation of cv.glmnet:: and alpha = 0, as follows:\n&gt; # fit with cv.glmnet (cross validation and a range of penalty parameters)\n&gt; fit_cv &lt;- glmnet::cv.glmnet(\n+   y = dat1$y\n+   , x = model.matrix(y ~ a + b, data = dat1)\n+   , alpha = 0\n+ )\n&gt; \n&gt; # get coefficients from fit1 with the penalty \n&gt; # generating the smallest mse\n&gt; coef(fit1, s = fit_cv$lambda.min)\n&gt; # do the check\n&gt; fit1 %&gt;% broom::tidy() %&gt;% \n+   dplyr::filter(lambda == fit_cv$lambda.min) %&gt;% \n+   dplyr::select(c(1,3)) %&gt;% \n+   tidyr::pivot_wider(names_from = term, values_from = estimate) %&gt;% \n+   dplyr::mutate(\n+     check = a/3 + 2 * b/3\n+   )\n\n\n\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                  s1\n(Intercept) 5.405189\n(Intercept) .       \na           4.528323\nb           2.198966\n\n\n# A tibble: 1 Ã— 4\n  `(Intercept)`     a     b check\n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1          5.41  4.53  2.20  2.98\n\n\n\nSo we can see that penalized regression, and in particular ridge regression, is useful for mitigating collinearity.",
    "crumbs": [
      "Supplemental notes",
      "Collinearity"
    ]
  },
  {
    "objectID": "supplemental/smd.html",
    "href": "supplemental/smd.html",
    "title": "Standardized Mean Differences (SMD)",
    "section": "",
    "text": "Standardized Mean Differences (SMD) are used in propensity score analysis to assess the balance of covariates between treatment and control groups. They provide a way to measure how similar the groups are in terms of observed characteristics, which is crucial for ensuring valid causal inference in observational studies.",
    "crumbs": [
      "Supplemental notes",
      "Standardized Mean Differences (SMD)"
    ]
  },
  {
    "objectID": "supplemental/smd.html#definition-and-purpose",
    "href": "supplemental/smd.html#definition-and-purpose",
    "title": "Standardized Mean Differences (SMD)",
    "section": "Definition and Purpose:",
    "text": "Definition and Purpose:\nStandardized Mean Difference:\n\nStandardized Mean Difference: a measure of effect size, used to compare the difference in means of a covariate between two groups (e.g., treatment and control groups) relative to the standard deviation of that covariate.\nPurpose: SMD is used to assess the balance of covariates in observational studies. Unlike p-values, SMD is not influenced by sample size, making it a more reliable measure of balance.",
    "crumbs": [
      "Supplemental notes",
      "Standardized Mean Differences (SMD)"
    ]
  },
  {
    "objectID": "supplemental/smd.html#calculation",
    "href": "supplemental/smd.html#calculation",
    "title": "Standardized Mean Differences (SMD)",
    "section": "Calculation",
    "text": "Calculation\nSMD=Xtâ€¾âˆ’Xcâ€¾SDpooled\n\\text{SMD}=\\frac{\\bar{X_t}-\\bar{X_c}}{\\text{SD}_\\text{pooled}}\n where:\n\nXtâ€¾\\bar{X_t} is the mean of the covariate in the treatment group\nXcâ€¾\\bar{X_c} is the mean of the covariate in the non-treatment (control) group\nSDpooled\\text{SD}_\\text{pooled} is the pooled standard deviation of the covariate across both groups.\n\nThe pooled standard deviation is calculated as\nSDpooled=(ntâˆ’1)SDt2+(ncâˆ’1)SDc2nt+ncâˆ’2\n\\text{SD}_\\text{pooled} = \\sqrt{\\frac{(n_t -1)\\text{SD}_t^2 + (n_c -1)\\text{SD}_c^2}{n_t + n_c -2}}\n where\n\nntn_t and ncn_c are the sample sizes of the treatment and control groups, respectively.\nSDt\\text{SD}_t and SDc\\text{SD}_c are the standard deviations of the covariate in the treatment and control groups, respectively.\n\nInterpretation:\n\nSMD = 0: Perfect balance. The covariate has the same mean in both groups.\nSMD &lt; 0.1: Generally considered a small and acceptable difference.\nSMD â‰¥ 0.1: Indicates a meaningful imbalance in the covariate between the groups. The threshold for what constitutes a â€œmeaningfulâ€ imbalance can vary by context.\n\nUsage in Propensity Score Analysis:\n\nBefore Matching: Calculate SMD for each covariate to assess initial imbalances between treatment and control groups.\nAfter Matching: Recalculate SMDs to ensure that the propensity score matching has adequately balanced the covariates. The goal is to achieve SMDs below a certain threshold (commonly 0.1).\n\nAdvantages:\n\nNot Sample Size Dependent: Unlike statistical significance tests, SMD is not influenced by the size of the sample, making it particularly useful in large datasets.\nEasy Comparison: Provides a straightforward way to compare balance across multiple covariates.",
    "crumbs": [
      "Supplemental notes",
      "Standardized Mean Differences (SMD)"
    ]
  },
  {
    "objectID": "supplemental/Bias_Variance_Trade_offs.html",
    "href": "supplemental/Bias_Variance_Trade_offs.html",
    "title": "Bias - Variance Trade-offs",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes are based on this page. They are provided for students who want to dive deeper into the mathematics behind regularized regression. Additional supplemental notes will be added throughout the semester.\nIn the simple linear regression model, you have nn observations of the response variable YY with a linear combination of mm predictor variables ğ±\\mathbf{x} where\nÏ€(Y=y|ğ±,ğ›‰)=ğ’©(y|Î²0+ğ›ƒâ€²ğ±,Ïƒ2)(1)\n\\pi\\left(Y=y|\\mathbf{x,\\theta}\\right)=\\mathcal{N}\\left(\\left.y\\right|\\beta_{0}+\\mathbf{\\mathbf{\\mathbf{\\beta}}}'\\mathbf{x},\\sigma^{2}\\right)\n \\qquad(1)\nwhere Î¸=(Î²0,ğ›ƒ,Ïƒ2)\\theta=\\left(\\beta_{0},\\mathbf{\\mathbf{\\mathbf{\\beta}}},\\sigma^{2}\\right) are all the parameters of the model. The vector of parameters Î²1:D\\beta_{1:D} are the weights or regression coefficients. Each coefficient Î²i\\beta_i specifies the change in the output that we expect if the corresponding input feature xix_i changes by one unit. The term Î²0\\beta_0 is the offset or bias term, and specifies the output if all the inputs are zero. This captures the unconditional response, and acts as a baseline. We sometimes write the input as (1,x1,â€¦,xD)\\left(1,x_{1},\\ldots,x_{D}\\right) so the offset can be absorbed into the weight vector.\nWe can always apply a transformation Ï•\\phi (linear or non-linear) to the input vector, replacing Î²\\beta with Ï•(Î²)\\phi(\\beta). As long as the parameters of the feature extractor are fixed, the model remain linear in the parameters even if not linear in the inputs.\nLeast squares estimation\nTo fit the linear regression model to data, we minimize the negative log-likelihood on the training set.\nNLL(Î²,Ïƒ2)=âˆ‘n=1Nlog[(12Ï€Ïƒ2)12exp(âˆ’12Ïƒ2(ynâˆ’Î²â€²xn)2)]=âˆ’12Ïƒ2âˆ‘n=1N(ynâˆ’yÌ‚n)2âˆ’Nlog(2Ï€Ïƒ2)\n\\begin{align*}\n\\text{NLL}\\left(\\beta,\\sigma^{2}\\right) & =\\sum_{n=1}^{N}\\log\\left[\\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{\\frac{1}{2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(y_{n}-\\beta'x_{n}\\right)^{2}\\right)\\right]\\\\\n & =-\\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{N}\\left(y_{n}-\\hat{y}_{n}\\right)^{2}-N\\log\\left(2\\pi\\sigma^{2}\\right)\n\\end{align*}\nwhere the predicted response is yÌ‚â‰¡Î²â€²xn\\hat{y}\\equiv\\beta'x_{n}. Focusing on just the weights, the NLL is (up to a constant):\nRSS(Î²)=12âˆ‘n=1N(ynâˆ’Î²â€²xn)2=12â€–ynâˆ’Î²â€²xnâ€–2=12(ynâˆ’Î²â€²xn)â€²(ynâˆ’Î²â€²xn) \n\\begin{align*}\n\\text{RSS}\\left(\\beta\\right) & =\\frac{1}{2}\\sum_{n=1}^{N}\\left(y_{n}-\\beta'x_{n}\\right)^{2}=\\frac{1}{2}\\left\\Vert y_{n}-\\beta'x_{n}\\right\\Vert ^{2}=\\frac{1}{2}\\left(y_{n}-\\beta'x_{n}\\right)'\\left(y_{n}-\\beta'x_{n}\\right)\\\\\n\\end{align*}\nWe must estimate the parameter values ğ›ƒÌ‚\\mathbf{\\hat{\\beta}} from the data, and using the OLS method, the loss function is\nLOLS(Î²Ì‚)=âˆ‘i=1n(yiâˆ’xiâ€²Î²Ì‚)2=â€–yâˆ’XÎ²Ì‚â€–2(2)\n\\text{L}_{OLS}\\left(\\hat{\\beta}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{'}\\hat{\\beta}\\right)^{2}=\\left\\Vert y-X\\hat{\\beta}\\right\\Vert ^{2}\n \\qquad(2)\nwhich is minimized with the estimate\nğ›ƒÌ‚OLS=(Xâ€²X)âˆ’1(Xâ€²Y)(3)\n\\hat{\\mathbf{\\beta}}_{OLS}=\\left(X'X\\right)^{-1}\\left(X'Y\\right)\n \\qquad(3)",
    "crumbs": [
      "Supplemental notes",
      "Bias-Variance Trade offs"
    ]
  },
  {
    "objectID": "supplemental/Bias_Variance_Trade_offs.html#ridge-regression",
    "href": "supplemental/Bias_Variance_Trade_offs.html#ridge-regression",
    "title": "Bias - Variance Trade-offs",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nFrom the discussion so far we have concluded that we would like to decrease the model complexity, that is the number of predictors. We could use the forward or backward selection for this, but that way we would not be able to tell anything about the removed variablesâ€™ effect on the response. Removing predictors from the model can be seen as settings their coefficients to zero. Instead of forcing them to be exactly zero, letâ€™s penalize them if they are too far from zero, thus enforcing them to be small in a continuous way. This way, we decrease model complexity while keeping all variables in the model. This, basically, is what Ridge Regression does.\n\nModel Specification\nIn Ridge Regression, the OLS loss function is augmented in such a way that we not only minimize the sum of squared residuals but also penalize the size of parameter estimates, in order to shrink them towards zero:\nLridge(Î²Ì‚)=âˆ‘i=1n(yiâˆ’xiâ€²Î²Ì‚)2+Î»âˆ‘j=1mÎ²Ì‚j2=â€–yâˆ’XÎ²Ì‚â€–2+Î»â€–Î²Ì‚â€–2\n\\text{L}_{ridge}\\left(\\hat{\\beta}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{'}\\hat{\\beta}\\right)^{2} + \\lambda\\sum_{j=1}^{m}\\hat{\\beta}^2_j=\\left\\Vert y-X\\hat{\\beta}\\right\\Vert ^{2} + \\lambda\\left\\Vert \\hat{\\beta}\\right\\Vert ^{2}\n\nSolving this for Î²Ì‚\\hat\\beta gives the the ridge regression estimates Î²Ì‚ridge=(Xâ€²X+Î»I)âˆ’1(Xâ€²Y)\\hat\\beta_{ridge} = (X'X+\\lambda I)^{-1}(X'Y), where I denotes the identity matrix.\nThe Î»\\lambda parameter is the regularization penalty. We will talk about how to choose it in the next sections of this tutorial, but for now notice that:\n\nAs Î»â†’0,Î²Ì‚ridgeâ†’Î²Ì‚OLS\\lambda \\rightarrow 0, \\quad \\hat\\beta_{ridge} \\rightarrow \\hat\\beta_{OLS};\nAs Î»â†’âˆ,Î²Ì‚ridgeâ†’0\\lambda \\rightarrow \\infty, \\quad \\hat\\beta_{ridge} \\rightarrow 0.\n\nSo, setting Î»\\lambda to 0 is the same as using the OLS, while the larger its value, the stronger is the coefficientsâ€™ size penalized.\n\n\nBias-Variance Trade-Off in Ridge Regression\nBias(ğ›ƒÌ‚ridge)=Î»(Xâ€²X+Î»I)âˆ’1Î²Var(ğ›ƒÌ‚ridge)=Ïƒ2(Xâ€²X+Î»I)âˆ’1Xâ€²X(Xâ€²X+Î»I)âˆ’1\n\\begin{align*}\n\\text{Bias}\\left(\\hat{\\mathbf{\\beta}}_{ridge}\\right) & =\\lambda(X'X+\\lambda I)^{-1}\\beta\\\\\n\\text{Var}\\left(\\hat{\\mathbf{\\beta}}_{ridge}\\right) & =\\sigma^{2}(X'X+\\lambda I)^{-1}X'X(X'X+\\lambda I)^{-1}\n\\end{align*}\n\nFrom there you can see that as Î»\\lambda becomes larger, the variance decreases, and the bias increases. This poses the question: how much bias are we willing to accept in order to decrease the variance? Or: what is the optimal value for Î»\\lambda?\nThere are two ways we could tackle this issue. A more traditional approach would be to choose Î» such that some information criterion, e.g., AIC or BIC, is the smallest. A more machine learning-like approach is to perform cross-validation and select the value of Î» that minimizes the cross-validated sum of squared residuals (or some other measure). The former approach emphasizes the modelâ€™s fit to the data, while the latter is more focused on its predictive performance. Letâ€™s discuss both.\n\n\nMinimizing Information Criteria\nThis approach boils down to estimating the model with many different values for Î»\\lambda and choosing the one that minimizes the Akaike or Bayesian Information Criterion:\nAICridge=log(eâ€²e)+2dfridgeBICridge=log(eâ€²e)+2dfridgelogn\n\\begin{align*}\n\\text{AIC}_{\\text{ridge}} & =\\log(e'e)+2\\text{df}_{\\text{ridge}}\\\\\n\\text{BIC}_{\\text{ridge}} & =\\log(e'e)+2\\text{df}_{\\text{ridge}}\\log n\n\\end{align*}\n\nwhere dfridge\\text{df}_{\\text{ridge}} is the number of degrees of freedom. Watch out here! The number of degrees of freedom in ridge regression is different than in the regular OLS! This is often overlooked which leads to incorrect inference. In both OLS and ridge regression, degrees of freedom are equal to the trace of the so-called hat matrix, which is a matrix that maps the vector of response values to the vector of fitted values as follows: yÌ‚=Hy\\hat y = H y.\nIn OLS, we find that HOLS=X(Xâ€²X)âˆ’1X\\text{H}_{\\text{OLS}}=X(Xâ€²X)^{âˆ’1}X, which gives dfOLS=trHOLS=m\\text{df}_{\\text{OLS}}=\\text{tr}\\text{H}_{\\text{OLS}}=m, where mm is the number of predictor variables. In ridge regression, however, the formula for the hat matrix should include the regularization penalty: Hridge=X(Xâ€²X+Î»I)âˆ’1X\\text{H}_{\\text{ridge}}=X(Xâ€²X+\\lambda I)^{âˆ’1}X, which gives dfridge=trHridge\\text{df}_{\\text{ridge}}=\\text{tr}\\text{H}_{\\text{ridge}}, which is no longer equal to mm. Some ridge regression software produce information criteria based on the OLS formula. To be sure you are doing things right, it is safer to compute them manually, which is what we will do later in this tutorial.\n\n\nRidge Regression: R example\nIn R, the glmnet package contains all you need to implement ridge regression. We will use the infamous mtcars dataset as an illustration, where the task is to predict miles per gallon based on carâ€™s other characteristics. One more thing: ridge regression assumes the predictors are standardized and the response is centered! You will see why this assumption is needed in a moment. For now, we will just standardize before modeling.\n\n# Load libraries, get data & set seed for reproducibility ---------------------\nset.seed(123)    # seef for reproducibility\nlibrary(glmnet)  # for ridge regression\nlibrary(dplyr)   # for data cleaning\nlibrary(psych)   # for function tr() to compute trace of a matrix\n\ndata(\"mtcars\")\n# Center y, X will be standardized in the modelling function\ny &lt;- mtcars %&gt;% select(mpg) %&gt;% scale(center = TRUE, scale = FALSE) %&gt;% as.matrix()\nX &lt;- mtcars %&gt;% select(-mpg) %&gt;% as.matrix()\n\n\n# Perform 10-fold cross-validation to select lambda ---------------------------\nlambdas_to_try &lt;- 10^seq(-3, 5, length.out = 100)\n# Setting alpha = 0 implements ridge regression\nridge_cv &lt;- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,\n                      standardize = TRUE, nfolds = 10)\n# Plot cross-validation results\nplot(ridge_cv)",
    "crumbs": [
      "Supplemental notes",
      "Bias-Variance Trade offs"
    ]
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in EquationÂ 1.\ny=Î²0+Î²1x(1)\ny = \\beta_0 + \\beta_1 x\n \\qquad(1)\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm.",
    "crumbs": [
      "Supplemental notes",
      "Log transformations"
    ]
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with log(y)\\log(y), the log-transformed yy, as the response variable. Under this model, we assume a linear relationship exists between xx and log(y)\\log(y), such that log(y)âˆ¼N(Î²0+Î²1x,Ïƒ2)\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2) for some Î²0\\beta_0, Î²1\\beta_1 and Ïƒ2\\sigma^2. In other words, we can model the relationship between xx and log(y)\\log(y) using the model in EquationÂ 2.\nlog(y)=Î²0+Î²1x(2)\n\\log(y) = \\beta_0 + \\beta_1 x\n \\qquad(2)\nIf we interpret the model in terms of log(y)\\log(y), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable yy, since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), exp{x}=ex\\exp\\{x\\} = e^x. Therefore, we use the model in EquationÂ 2 for interpretations and predictions, we will use EquationÂ 3 to state our conclusions in terms of yy.\nexp{log(y)}=exp{Î²0+Î²1x}â‡’y=exp{Î²0+Î²1x}â‡’y=exp{Î²0}exp{Î²1x}(3)\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n \\qquad(3)\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\nMean[log(y)]â‰ log[Mean(y)]\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)] - the mean and log are not commutable\nMedian[log(y)]=log[Median(y)]\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)] - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing EquationÂ 2, the mean log(y)\\log(y) for any given value of xx is Î²0+Î²1x\\beta_0 + \\beta_1 x; however, this does not indicate that the mean of y=exp{Î²0+Î²1x}y = \\exp\\{\\beta_0 + \\beta_1 x\\} (see previous section). From the assumptions of linear regression, we assume that for any given value of xx, the distribution of log(y)\\log(y) is Normal, and therefore symmetric. Thus the median of log(y)\\log(y) is equal to the mean of log(y)\\log(y), i.e Median(log(y))=Î²0+Î²1x\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x.\nSince the log and the median are commutable, Median(log(y))=Î²0+Î²1xâ‡’Median(y)=exp{Î²0+Î²1x}\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}. Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of yy.\nIntercept: The intercept is expected median of yy when the predictor variable equals 0. Therefore, when x=0x=0,\nlog(y)=Î²0+Î²1Ã—0=Î²0â‡’y=exp{Î²0}\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\nInterpretation: When x=0x=0, the median of yy is expected to be exp{Î²0}\\exp\\{\\beta_0\\}.\nSlope: The slope is the expected change in the median of yy when xx increases by 1 unit. The change in the median of yy is\nexp{[Î²0+Î²1(x+1)]âˆ’[Î²0+Î²1x]}=exp{Î²0+Î²1(x+1)}exp{Î²0+Î²1x}=exp{Î²0}exp{Î²1x}exp{Î²1}exp{Î²0}exp{Î²1x}=exp{Î²1}\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\nThus, the median of yy for x+1x+1 is exp{Î²1}\\exp\\{\\beta_1\\} times the median of yy for xx.\nInterpretation: When xx increases by one unit, the median of yy is expected to multiply by a factor of exp{Î²1}\\exp\\{\\beta_1\\}.",
    "crumbs": [
      "Supplemental notes",
      "Log transformations"
    ]
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with log(x)\\log(x), the log-transformed xx, as the predictor variable. Under this model, we assume a linear relationship exists between log(x)\\log(x) and yy, such that yâˆ¼N(Î²0+Î²1log(x),Ïƒ2)y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2) for some Î²0\\beta_0, Î²1\\beta_1 and Ïƒ2\\sigma^2. In other words, we can model the relationship between log(x)\\log(x) and yy using the model in #eq-log-x.\ny=Î²0+Î²1log(x)(4)\ny = \\beta_0 + \\beta_1 \\log(x)\n \\qquad(4)\nIntercept: The intercept is the mean of yy when log(x)=0\\log(x) = 0, i.e.Â x=1x = 1.\nInterpretation: When x=1x = 1 (log(x)=0)(\\log(x) = 0), the mean of yy is expected to be Î²0\\beta_0.\nSlope: The slope is interpreted in terms of the change in the mean of yy when xx is multiplied by a factor of CC, since log(Cx)=log(x)+log(C)\\log(Cx) = \\log(x) + \\log(C). Thus, when xx is multiplied by a factor of CC, the change in the mean of yy is\n[Î²0+Î²1log(Cx)]âˆ’[Î²0+Î²1log(x)]=Î²1[log(Cx)âˆ’log(x)]=Î²1[log(C)+log(x)âˆ’log(x)]=Î²1log(C)\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt] \n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt] \n& = \\beta_1 \\log(C)\n\\end{aligned}\n\nThus the mean of yy changes by Î²1log(C)\\beta_1 \\log(C) units.\nInterpretation: When xx is multiplied by a factor of CC, the mean of yy is expected to change by Î²1log(C)\\beta_1 \\log(C) units. For example, if xx is doubled, then the mean of yy is expected to change by Î²1log(2)\\beta_1 \\log(2) units.",
    "crumbs": [
      "Supplemental notes",
      "Log transformations"
    ]
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with log(x)\\log(x), the log-transformed xx, as the predictor variable and log(y)\\log(y), the log-transformed yy, as the response variable. Under this model, we assume a linear relationship exists between log(x)\\log(x) and log(y)\\log(y), such that log(y)âˆ¼N(Î²0+Î²1log(x),Ïƒ2)\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2) for some Î²0\\beta_0, Î²1\\beta_1 and Ïƒ2\\sigma^2. In other words, we can model the relationship between log(x)\\log(x) and log(y)\\log(y) using the model in EquationÂ 5.\nlog(y)=Î²0+Î²1log(x)(5)\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n \\qquad(5)\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of yy (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of yy when log(x)=0\\log(x) = 0, i.e.Â x=1x = 1. Therefore, when log(x)=0\\log(x) = 0,\nlog(y)=Î²0+Î²1Ã—0=Î²0â‡’y=exp{Î²0}\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\nInterpretation: When x=1x = 1 (log(x)=0)(\\log(x) = 0), the median of yy is expected to be exp{Î²0}\\exp\\{\\beta_0\\}.\nSlope: The slope is interpreted in terms of the change in the median yy when xx is multiplied by a factor of CC, since log(Cx)=log(x)+log(C)\\log(Cx) = \\log(x) + \\log(C). Thus, when xx is multiplied by a factor of CC, the change in the median of yy is\nexp{[Î²0+Î²1log(Cx)]âˆ’[Î²0+Î²1log(x)]}=exp{Î²1[log(Cx)âˆ’log(x)]}=exp{Î²1[log(C)+log(x)âˆ’log(x)]}=exp{Î²1log(C)}=CÎ²1\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &= \n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt] \n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt] \n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\nThus, the median of yy for CxCx is CÎ²1C^{\\beta_1} times the median of yy for xx.\nInterpretation: When xx is multiplied by a factor of CC, the median of yy is expected to multiple by a factor of CÎ²1C^{\\beta_1}. For example, if xx is doubled, then the median of yy is expected to multiply by 2Î²12^{\\beta_1}.",
    "crumbs": [
      "Supplemental notes",
      "Log transformations"
    ]
  },
  {
    "objectID": "supplemental/PCA_and_component_selection.html",
    "href": "supplemental/PCA_and_component_selection.html",
    "title": "Principal Component Analysis (PCA) and Component Selection",
    "section": "",
    "text": "Note\n\n\n\nThe PCA material here is taken from Probabilistic View of Principal Component Analysis\nWeâ€™ll assume all columns in our data have been normalized - with zero mean and unit standard deviation.\n\n\n\n\nFor any square matrix AA, xx is an eigenvector of AA and Î»\\lambda the corresponding eigenvalue of AA if Ax=Î»xAx=\\lambda x. Suppose AA has a full set of NN independent eigenvectors (most matrices do, but not all).\nIf we put the eigenvectors into a matrix QQ (the eigenvectors are the column vectors of the matrix), then AQ=QÎ›AQ=Q\\Lambda, where Î›\\Lambda is a diagonal matrix, with the eigenvalues on the diagonal. Thus\nA=QÎ›Qâˆ’1=QÎ›QâŠ¤(1)\nA = Q\\Lambda Q^{-1} =  Q\\Lambda Q^\\top\n \\qquad(1)\nThis is the Eigenvalue Decomposition: a square NÃ—NN\\times N matrix (AA) which is diagonalizable can be factorized as:\nA=QÎ›QâŠ¤\nA = Q\\Lambda Q^\\top \n\nwhere QQ is the square NÃ—NN\\times N matrix whose iith column is the eigenvector qiq_i of BB, and Î›\\Lambda is the diagonal matrix whose diagonal elements are the corresponding eigenvalues. QQ is square and orthogonal (Q=Qâˆ’1=QâŠ¤Q=Q^{-1}=Q^\\top), because the eigenvectors are orthogonal.\n\n\n\nIf AA is not square we need a different decomposition. Suppose AA is an NÃ—DN\\times D matrix (NN rows and DD columns) - then we need a square, orthogonal NÃ—NN\\times N matrix VV to multiply on the right, and a square, orthogonal DÃ—DD\\times D matrix UU to multiply on the left. In which case our matrix (say A) can be factorized as:\nA=UÎ£VâŠ¤(2)\nA = U\\Sigma V^\\top \n \\qquad(2)\nÎ£\\Sigma will then be an NÃ—DN\\times D matrix where the DÃ—DD\\times D subset will be diagonal with rr singular values Ïƒiiâˆˆ{1,2,â€¦,r}\\sigma_i\\;i\\in\\{1,2,\\ldots,r\\} and the remaining entries will be zero.\nWe have\nA=UÎ£VâŠ¤=Ïƒ1u1v1âŠ¤+Ïƒ2u2v2âŠ¤+â€¦+ÏƒrurvrâŠ¤\nA = U\\Sigma V^\\top =\\sigma_1u_1v_1^\\top + \\sigma_2u_2v_2^\\top + \\ldots+\\sigma_ru_rv_r^\\top\n Note that this is a sum of matrices. The first is the best rank 1 approximation to AA; the first kk is the best rank kk approximation to AA\n\n\n\nThe PCA decomposition requires that one compute the eigenvalues and eigenvectors of the covariance matrix of AA (again an NÃ—DN\\times D matrix), which is the product 1nâˆ’1AAâŠ¤\\frac{1}{n-1}AA^\\top. Since the covariance matrix is symmetric, the matrix is diagonalizable, and the eigenvectors can be normalized such that they are orthonormal.\nThe square corvariance matrix (1nâˆ’1AAâŠ¤\\frac{1}{n-1}AA^\\top) is symmetric and thus diagonizable, and so from equation (EquationÂ 1), it can be factorized as:\n1nâˆ’1AAâŠ¤=QÎ›QâŠ¤\n\\frac{1}{n-1}AA^\\top = Q\\Lambda Q^\\top \n where Î›\\Lambda is the diagonal matrix with the eigenvalues of the covariance matrix and QQ has column vectors that are the eigenvectors of the covariance matrix.\nHowever, using the SVD (EquationÂ 2) we can also write\n1nâˆ’1AAâŠ¤=1nâˆ’1(UÎ£VâŠ¤)(VÎ£UâŠ¤)=UÎ£2UâŠ¤\n\\frac{1}{n-1}AA^\\top = \\frac{1}{n-1} \\left(U\\Sigma V^\\top\\right)\\left(V\\Sigma U^\\top\\right) = U\\Sigma^2U^\\top\n\nUsing the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the formation of AAâŠ¤AA^\\top can cause loss of precision.\nHere is a simple PCA example:\n\nDataCorrelationPCA\n\n\n\n\nCode\nset.seed(1) # For data reproducibility\n# create some data\ndat1 &lt;- tibble::tibble(\n  x = rnorm(50, 50, sd = 3)\n  , y = .5*x + rnorm(50, sd = sqrt(3))\n)\n# plot it\ndat1 |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point(,color = \"blue\", size = 2) +\n  xlab(\"Variable 1\") +\n  ylab(\"Variable 2\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n# center and\ndat1 &lt;- dat1 |&gt;  \n  dplyr::mutate(x = x - mean(x), y = y - mean(y))\n# convert to matrix\ndat2 &lt;- dat1 |&gt; dplyr::select(x,y) |&gt; as.matrix()\n\n\n\n\n\n# Calculate the covariance matrix\ncov_m &lt;- (t(dat2) %*% dat2) / (nrow(dat2) - 1) \ncov_m\n\n         x        y\nx 6.220943 2.946877\ny 2.946877 4.207523\n\n\n\n# we can also use the cov function in base R\ncov(dat2)\n\n         x        y\nx 6.220943 2.946877\ny 2.946877 4.207523\n\n\n\n\n\n\nCode\n# Use eigen() to obtain eigenvectors and eigenvalues\ncov_e &lt;- eigen(cov_m)\n\n# Eigenvectors\ne_vec &lt;- cov_e$vectors\n\n# Eigenvalues\ne_val &lt;- cov_e$values\n\n# First eigenvector \nev_1 &lt;- e_vec[,1]\n\n# Second eigenvector \nev_2 &lt;- e_vec[,2]\n\n# eigenvectors are orthogonal and eigenvalues capture total variance\nc( ev_1 %*% ev_2, e_val, e_val |&gt; sum(), cov_m |&gt; diag() |&gt; sum())\n\n\n[1] 2.749766e-17 8.328321e+00 2.100145e+00 1.042847e+01 1.042847e+01\n\n\n\n\n\nFor a positive semi-definite matrix SVD and eigendecomposition are equivalent. PCA boils down to the eigendecomposition of the covariance matrix. Finding the maximum eigenvalue(s) and corresponding eigenvector(s) can be thought of as finding the direction of maximum variance.\nIf we have a lot of data (many rows or many columns or both), weâ€™ll have a large covariance matrix and large number of eigenvalues and their corresponding eigenvectors (though there can be duplicates).\nDo we need them all? How many are just due to noise or measurement error? First look at random matrices, then at covariance matrices formed from random matrices.\n\n\n\nLetâ€™s perform an experiment, generating a large random NÃ—NN\\times N data set using N(0,1)N(0,1) measurements.\n\n\neigenvalues from random symmetric matrix (Normally distributed measurements)\nrequire(ggplot2, quietly=TRUE)\n\n# 5000 rows and columns\nn &lt;- 5000\n# generate n^2 samples from N(0,1)\nm &lt;- array( rnorm(n^2) ,c(n,n))\n# make it symmetric\nm2 &lt;- (m + t(m))/sqrt(2*n) # t(m) %*% m\n# compute eigenvalues and vectors\nlambda &lt;- eigen(m2, symmetric=T, only.values = T)\n\n# plot the eignevalues\ntibble::tibble(lambda = lambda$values) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  labs(x = 'eigenvalues', title = 'Normal random symmetric matrix') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLetâ€™s do the same, but with uniform U(0,1)U(0,1) distributed measurements.\n\n\neigenvalues from random symmetric matrix (Uniformly distributed measurements)\n# 5000 rows and columns\nn &lt;- 5000\n# generate n^2 samples from U(0,1)\nm &lt;- array( runif(n^2) ,c(n,n))\n# make it symmetric\nm2 &lt;- sqrt(12)*(m + t(m) -1)/sqrt(2*n) # t(m) %*% m\n# compute eigenvalues and vectors\nlambda &lt;- eigen(m2, symmetric=T, only.values = T)\n\n# plot the eignevalues\ntibble::tibble(lambda = lambda$values) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  labs(x = 'eigenvalues', title = 'Uniform random symmetric matrix') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNote the striking pattern: the density of eigenvalues is a semicircle.\n\n\n\nLet AÌƒ\\tilde{A} be an NÃ—NN\\times N matrix with entries AÌƒi,jâˆ¼ğ’©(0,Ïƒ2)\\tilde{A}_{i,j}\\sim\\mathcal{N}\\left(0,\\sigma^2\\right). Define\nAN=1N(A+AâŠ¤2)\nA_N=\\frac{1}{\\sqrt{N}}\\left(\\frac{A+A^\\top}{2}\\right)\n then ANA_N is symmetric with variance\nVar[ai,j]={Ïƒ2/Nifiâ‰ jÏƒ2/Nifi=j\n\\mathrm{Var}\\left[a_{i,j}\\right]=\\left\\{ \\begin{array}{cc}\n\\sigma^{2}/N & \\mathrm{if}\\,i\\ne j\\\\\n\\sigma^{2}/N & \\mathrm{if}\\,i=j\n\\end{array}\\right.\n and the density of the eigenvalues of ANA_N is given by\nÏN(Î»)â‰¡1Nâˆ‘i=1NÎ´(Î»âˆ’Î»j)\n\\rho_N\\left(\\lambda\\right)\\equiv\\frac{1}{N}\\sum_{i=1}^N\\delta\\left(\\lambda-\\lambda_j\\right)\n which, as shown by Wigner, as\nnâ†’âˆâ†’12Ï€Ïƒ24Ïƒ2âˆ’Î±2if|Î»|â‰¤2Ïƒ0otherwise\nn\\rightarrow\\infty\\rightarrow\\begin{array}{cc}\n\\frac{1}{2\\pi\\sigma^{2}}\\sqrt{4\\sigma^{2}-\\alpha^{2}} & \\mathrm{if}\\,\\left|\\lambda\\right|\\le2\\sigma\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\n\n\n\nWe have MM variables with TT rows. The elements of the MÃ—MM\\times M empirical correlation matrix EE are given by:\nEi,j=1Tâˆ‘t=1Txi,jxj,i\nE_{i,j}=\\frac{1}{T}\\sum_{t=1}^Tx_{i,j}x_{j,i}\n where xi,jx_{i,j} denotes the jj-th (normalized) value of variable ii. This can be written as E=HâŠ¤HE=H^\\top H where HH is the TÃ—MT\\times M dataset.\nAssuming the values of HH are random with variance Ïƒ2\\sigma^2 then in the limit T,Mâ†’âˆT,M\\rightarrow\\infty, while keeping the ratio Qâ‰¡TMâ‰¥1Q\\equiv\\frac{T}{M}\\ge1 constant, the density of eigenvalues of EE is given by\nÏ(Î»)=Q2Ï€Ïƒ2(Î»+âˆ’Î»)(Î»âˆ’Î»âˆ’)Î»\n\\rho\\left(\\lambda\\right) = \\frac{Q}{2\\pi\\sigma^2}\\frac{\\sqrt{\\left(\\lambda_+-\\lambda\\right)\\left(\\lambda-\\lambda_-\\right)}}{\\lambda}\n where the minimum and maximum eigenvalues are given by\nÎ»Â±=Ïƒ2(1Â±1Q)2\n\\lambda_\\pm=\\sigma^2\\left(1\\pm\\sqrt{\\frac{1}{Q}}\\right)^2\n\nis also known as the Marchenko-Pastur distribution that describes the asymptotic behavior of eigenvalues of large random correlation matrices.\n\n\ncode for Marchenko-Pastur distribution\nmpd &lt;- function(lambda,T,M,sigma=1){\n  Q &lt;- T/M\n  lambda_plus  &lt;- (1+sqrt(1/Q))^2 * sigma^2\n  lambda_minus &lt;- (1-sqrt(1/Q))^2 * sigma^2\n  if(lambda &lt; lambda_minus | lambda &gt; lambda_plus){\n    0\n  }else{\n    (Q/(2*pi*sigma^2)) * sqrt((lambda_plus-lambda)*(lambda-lambda_minus)) / lambda\n  }\n}\n\n\n\nM = 1000, T = 5000M = 100, T = 500M = 10, T = 50\n\n\n\n\nCode\nt &lt;- 5000;\nm &lt;- 1000;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=TRUE, only.values = TRUE);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nt &lt;- 500;\nm &lt;- 100;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=T, only.values = T);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=30) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nt &lt;- 50;\nm &lt;- 10;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=T, only.values = T);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=15) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()",
    "crumbs": [
      "Supplemental notes",
      "PCA and component selection"
    ]
  },
  {
    "objectID": "supplemental/PCA_and_component_selection.html#pca-short-version",
    "href": "supplemental/PCA_and_component_selection.html#pca-short-version",
    "title": "Principal Component Analysis (PCA) and Component Selection",
    "section": "",
    "text": "Note\n\n\n\nThe PCA material here is taken from Probabilistic View of Principal Component Analysis\nWeâ€™ll assume all columns in our data have been normalized - with zero mean and unit standard deviation.\n\n\n\n\nFor any square matrix AA, xx is an eigenvector of AA and Î»\\lambda the corresponding eigenvalue of AA if Ax=Î»xAx=\\lambda x. Suppose AA has a full set of NN independent eigenvectors (most matrices do, but not all).\nIf we put the eigenvectors into a matrix QQ (the eigenvectors are the column vectors of the matrix), then AQ=QÎ›AQ=Q\\Lambda, where Î›\\Lambda is a diagonal matrix, with the eigenvalues on the diagonal. Thus\nA=QÎ›Qâˆ’1=QÎ›QâŠ¤(1)\nA = Q\\Lambda Q^{-1} =  Q\\Lambda Q^\\top\n \\qquad(1)\nThis is the Eigenvalue Decomposition: a square NÃ—NN\\times N matrix (AA) which is diagonalizable can be factorized as:\nA=QÎ›QâŠ¤\nA = Q\\Lambda Q^\\top \n\nwhere QQ is the square NÃ—NN\\times N matrix whose iith column is the eigenvector qiq_i of BB, and Î›\\Lambda is the diagonal matrix whose diagonal elements are the corresponding eigenvalues. QQ is square and orthogonal (Q=Qâˆ’1=QâŠ¤Q=Q^{-1}=Q^\\top), because the eigenvectors are orthogonal.\n\n\n\nIf AA is not square we need a different decomposition. Suppose AA is an NÃ—DN\\times D matrix (NN rows and DD columns) - then we need a square, orthogonal NÃ—NN\\times N matrix VV to multiply on the right, and a square, orthogonal DÃ—DD\\times D matrix UU to multiply on the left. In which case our matrix (say A) can be factorized as:\nA=UÎ£VâŠ¤(2)\nA = U\\Sigma V^\\top \n \\qquad(2)\nÎ£\\Sigma will then be an NÃ—DN\\times D matrix where the DÃ—DD\\times D subset will be diagonal with rr singular values Ïƒiiâˆˆ{1,2,â€¦,r}\\sigma_i\\;i\\in\\{1,2,\\ldots,r\\} and the remaining entries will be zero.\nWe have\nA=UÎ£VâŠ¤=Ïƒ1u1v1âŠ¤+Ïƒ2u2v2âŠ¤+â€¦+ÏƒrurvrâŠ¤\nA = U\\Sigma V^\\top =\\sigma_1u_1v_1^\\top + \\sigma_2u_2v_2^\\top + \\ldots+\\sigma_ru_rv_r^\\top\n Note that this is a sum of matrices. The first is the best rank 1 approximation to AA; the first kk is the best rank kk approximation to AA\n\n\n\nThe PCA decomposition requires that one compute the eigenvalues and eigenvectors of the covariance matrix of AA (again an NÃ—DN\\times D matrix), which is the product 1nâˆ’1AAâŠ¤\\frac{1}{n-1}AA^\\top. Since the covariance matrix is symmetric, the matrix is diagonalizable, and the eigenvectors can be normalized such that they are orthonormal.\nThe square corvariance matrix (1nâˆ’1AAâŠ¤\\frac{1}{n-1}AA^\\top) is symmetric and thus diagonizable, and so from equation (EquationÂ 1), it can be factorized as:\n1nâˆ’1AAâŠ¤=QÎ›QâŠ¤\n\\frac{1}{n-1}AA^\\top = Q\\Lambda Q^\\top \n where Î›\\Lambda is the diagonal matrix with the eigenvalues of the covariance matrix and QQ has column vectors that are the eigenvectors of the covariance matrix.\nHowever, using the SVD (EquationÂ 2) we can also write\n1nâˆ’1AAâŠ¤=1nâˆ’1(UÎ£VâŠ¤)(VÎ£UâŠ¤)=UÎ£2UâŠ¤\n\\frac{1}{n-1}AA^\\top = \\frac{1}{n-1} \\left(U\\Sigma V^\\top\\right)\\left(V\\Sigma U^\\top\\right) = U\\Sigma^2U^\\top\n\nUsing the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the formation of AAâŠ¤AA^\\top can cause loss of precision.\nHere is a simple PCA example:\n\nDataCorrelationPCA\n\n\n\n\nCode\nset.seed(1) # For data reproducibility\n# create some data\ndat1 &lt;- tibble::tibble(\n  x = rnorm(50, 50, sd = 3)\n  , y = .5*x + rnorm(50, sd = sqrt(3))\n)\n# plot it\ndat1 |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point(,color = \"blue\", size = 2) +\n  xlab(\"Variable 1\") +\n  ylab(\"Variable 2\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n# center and\ndat1 &lt;- dat1 |&gt;  \n  dplyr::mutate(x = x - mean(x), y = y - mean(y))\n# convert to matrix\ndat2 &lt;- dat1 |&gt; dplyr::select(x,y) |&gt; as.matrix()\n\n\n\n\n\n# Calculate the covariance matrix\ncov_m &lt;- (t(dat2) %*% dat2) / (nrow(dat2) - 1) \ncov_m\n\n         x        y\nx 6.220943 2.946877\ny 2.946877 4.207523\n\n\n\n# we can also use the cov function in base R\ncov(dat2)\n\n         x        y\nx 6.220943 2.946877\ny 2.946877 4.207523\n\n\n\n\n\n\nCode\n# Use eigen() to obtain eigenvectors and eigenvalues\ncov_e &lt;- eigen(cov_m)\n\n# Eigenvectors\ne_vec &lt;- cov_e$vectors\n\n# Eigenvalues\ne_val &lt;- cov_e$values\n\n# First eigenvector \nev_1 &lt;- e_vec[,1]\n\n# Second eigenvector \nev_2 &lt;- e_vec[,2]\n\n# eigenvectors are orthogonal and eigenvalues capture total variance\nc( ev_1 %*% ev_2, e_val, e_val |&gt; sum(), cov_m |&gt; diag() |&gt; sum())\n\n\n[1] 2.749766e-17 8.328321e+00 2.100145e+00 1.042847e+01 1.042847e+01\n\n\n\n\n\nFor a positive semi-definite matrix SVD and eigendecomposition are equivalent. PCA boils down to the eigendecomposition of the covariance matrix. Finding the maximum eigenvalue(s) and corresponding eigenvector(s) can be thought of as finding the direction of maximum variance.\nIf we have a lot of data (many rows or many columns or both), weâ€™ll have a large covariance matrix and large number of eigenvalues and their corresponding eigenvectors (though there can be duplicates).\nDo we need them all? How many are just due to noise or measurement error? First look at random matrices, then at covariance matrices formed from random matrices.\n\n\n\nLetâ€™s perform an experiment, generating a large random NÃ—NN\\times N data set using N(0,1)N(0,1) measurements.\n\n\neigenvalues from random symmetric matrix (Normally distributed measurements)\nrequire(ggplot2, quietly=TRUE)\n\n# 5000 rows and columns\nn &lt;- 5000\n# generate n^2 samples from N(0,1)\nm &lt;- array( rnorm(n^2) ,c(n,n))\n# make it symmetric\nm2 &lt;- (m + t(m))/sqrt(2*n) # t(m) %*% m\n# compute eigenvalues and vectors\nlambda &lt;- eigen(m2, symmetric=T, only.values = T)\n\n# plot the eignevalues\ntibble::tibble(lambda = lambda$values) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  labs(x = 'eigenvalues', title = 'Normal random symmetric matrix') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLetâ€™s do the same, but with uniform U(0,1)U(0,1) distributed measurements.\n\n\neigenvalues from random symmetric matrix (Uniformly distributed measurements)\n# 5000 rows and columns\nn &lt;- 5000\n# generate n^2 samples from U(0,1)\nm &lt;- array( runif(n^2) ,c(n,n))\n# make it symmetric\nm2 &lt;- sqrt(12)*(m + t(m) -1)/sqrt(2*n) # t(m) %*% m\n# compute eigenvalues and vectors\nlambda &lt;- eigen(m2, symmetric=T, only.values = T)\n\n# plot the eignevalues\ntibble::tibble(lambda = lambda$values) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  labs(x = 'eigenvalues', title = 'Uniform random symmetric matrix') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNote the striking pattern: the density of eigenvalues is a semicircle.\n\n\n\nLet AÌƒ\\tilde{A} be an NÃ—NN\\times N matrix with entries AÌƒi,jâˆ¼ğ’©(0,Ïƒ2)\\tilde{A}_{i,j}\\sim\\mathcal{N}\\left(0,\\sigma^2\\right). Define\nAN=1N(A+AâŠ¤2)\nA_N=\\frac{1}{\\sqrt{N}}\\left(\\frac{A+A^\\top}{2}\\right)\n then ANA_N is symmetric with variance\nVar[ai,j]={Ïƒ2/Nifiâ‰ jÏƒ2/Nifi=j\n\\mathrm{Var}\\left[a_{i,j}\\right]=\\left\\{ \\begin{array}{cc}\n\\sigma^{2}/N & \\mathrm{if}\\,i\\ne j\\\\\n\\sigma^{2}/N & \\mathrm{if}\\,i=j\n\\end{array}\\right.\n and the density of the eigenvalues of ANA_N is given by\nÏN(Î»)â‰¡1Nâˆ‘i=1NÎ´(Î»âˆ’Î»j)\n\\rho_N\\left(\\lambda\\right)\\equiv\\frac{1}{N}\\sum_{i=1}^N\\delta\\left(\\lambda-\\lambda_j\\right)\n which, as shown by Wigner, as\nnâ†’âˆâ†’12Ï€Ïƒ24Ïƒ2âˆ’Î±2if|Î»|â‰¤2Ïƒ0otherwise\nn\\rightarrow\\infty\\rightarrow\\begin{array}{cc}\n\\frac{1}{2\\pi\\sigma^{2}}\\sqrt{4\\sigma^{2}-\\alpha^{2}} & \\mathrm{if}\\,\\left|\\lambda\\right|\\le2\\sigma\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\n\n\n\nWe have MM variables with TT rows. The elements of the MÃ—MM\\times M empirical correlation matrix EE are given by:\nEi,j=1Tâˆ‘t=1Txi,jxj,i\nE_{i,j}=\\frac{1}{T}\\sum_{t=1}^Tx_{i,j}x_{j,i}\n where xi,jx_{i,j} denotes the jj-th (normalized) value of variable ii. This can be written as E=HâŠ¤HE=H^\\top H where HH is the TÃ—MT\\times M dataset.\nAssuming the values of HH are random with variance Ïƒ2\\sigma^2 then in the limit T,Mâ†’âˆT,M\\rightarrow\\infty, while keeping the ratio Qâ‰¡TMâ‰¥1Q\\equiv\\frac{T}{M}\\ge1 constant, the density of eigenvalues of EE is given by\nÏ(Î»)=Q2Ï€Ïƒ2(Î»+âˆ’Î»)(Î»âˆ’Î»âˆ’)Î»\n\\rho\\left(\\lambda\\right) = \\frac{Q}{2\\pi\\sigma^2}\\frac{\\sqrt{\\left(\\lambda_+-\\lambda\\right)\\left(\\lambda-\\lambda_-\\right)}}{\\lambda}\n where the minimum and maximum eigenvalues are given by\nÎ»Â±=Ïƒ2(1Â±1Q)2\n\\lambda_\\pm=\\sigma^2\\left(1\\pm\\sqrt{\\frac{1}{Q}}\\right)^2\n\nis also known as the Marchenko-Pastur distribution that describes the asymptotic behavior of eigenvalues of large random correlation matrices.\n\n\ncode for Marchenko-Pastur distribution\nmpd &lt;- function(lambda,T,M,sigma=1){\n  Q &lt;- T/M\n  lambda_plus  &lt;- (1+sqrt(1/Q))^2 * sigma^2\n  lambda_minus &lt;- (1-sqrt(1/Q))^2 * sigma^2\n  if(lambda &lt; lambda_minus | lambda &gt; lambda_plus){\n    0\n  }else{\n    (Q/(2*pi*sigma^2)) * sqrt((lambda_plus-lambda)*(lambda-lambda_minus)) / lambda\n  }\n}\n\n\n\nM = 1000, T = 5000M = 100, T = 500M = 10, T = 50\n\n\n\n\nCode\nt &lt;- 5000;\nm &lt;- 1000;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=TRUE, only.values = TRUE);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=100) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nt &lt;- 500;\nm &lt;- 100;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=T, only.values = T);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=30) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nt &lt;- 50;\nm &lt;- 10;\nh = array(rnorm(m*t),c(m,t)); # Time series in rows\ne = h %*% t(h)/t; # Form the correlation matrix\nlambdae = eigen(e, symmetric=T, only.values = T);\n\n# create the mp distribution\nmpd_tbl &lt;- tibble::tibble(lambda = c(lambdae$values, seq(0,3,0.1)) ) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m)))\n\n# plot the eigenvalues\ntibble::tibble(lambda = lambdae$values) |&gt; \n  dplyr::mutate(mp_dist = purrr::map_dbl(lambda, ~mpd(lambda = ., t,m))) |&gt; \n  ggplot(aes(x = lambda, y = after_stat(density))) + \n  geom_histogram(color = \"white\", fill=\"lightblue\", bins=15) + \n  geom_line(data = mpd_tbl, aes(y=mp_dist)) +\n  labs(x = 'eigenvalues', title = 'Empirical density'\n  , subtitle = \n    stringr::str_glue(\"with superimposed Marchenko-Pastur density | M={t}, T={m}\")\n  ) +\n  xlim(0,3) +\n  theme_minimal()",
    "crumbs": [
      "Supplemental notes",
      "PCA and component selection"
    ]
  },
  {
    "objectID": "supplemental/PCA_and_component_selection.html#application-to-correlation-matrices",
    "href": "supplemental/PCA_and_component_selection.html#application-to-correlation-matrices",
    "title": "Principal Component Analysis (PCA) and Component Selection",
    "section": "Application to correlation matrices",
    "text": "Application to correlation matrices\nFor the special case of correlation matrices (e.g.Â PCA), we know that Ïƒ2=1\\sigma^2=1 and Q=T/MQ = T/M. This bounds the probability mass over the interval defined by (1Â±1Q)2\\left(1\\pm\\sqrt{\\frac{1}{Q}}\\right)^2.\nSince this distribution describes the spectrum of random matrices with mean 0, the eigenvalues of correlation matrices (read PCA component weights) that fall inside of the aforementioned interval could be considered spurious or noise. For instance, obtaining a correlation matrix of 10 variables with 252 observations would render\nÎ»+=(1Â±1Q)2â‰ˆ1.43\n\\lambda_+=\\left(1\\pm\\sqrt{\\frac{1}{Q}}\\right)^2\\approx1.43\n\nThus, out of 10 eigenvalues/components of said correlation matrix, only the values higher than 1.43 would be considered significantly different from random.",
    "crumbs": [
      "Supplemental notes",
      "PCA and component selection"
    ]
  },
  {
    "objectID": "supplemental/regularization-derivations.html",
    "href": "supplemental/regularization-derivations.html",
    "title": "Regularized Regression: Ridge, Lasso and Elastic Net",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes are based on this page. They are provided for students who want to dive deeper into the mathematics behind regularized regression. Additional supplemental notes will be added throughout the semester.\nIn the simple linear regression model, you have nn observations of the response variable YY with a linear combination of mm predictor variables ğ±\\mathbf{x} where\nÏ€(Y=y|ğ±,ğ›‰)=ğ’©(y|Î²0+ğ›ƒâ€²ğ±,Ïƒ2)(1)\n\\pi\\left(Y=y|\\mathbf{x,\\theta}\\right)=\\mathcal{N}\\left(\\left.y\\right|\\beta_{0}+\\mathbf{\\mathbf{\\mathbf{\\beta}}}'\\mathbf{x},\\sigma^{2}\\right)\n \\qquad(1)\nwhere Î¸=(Î²0,ğ›ƒ,Ïƒ2)\\theta=\\left(\\beta_{0},\\mathbf{\\mathbf{\\mathbf{\\beta}}},\\sigma^{2}\\right) are all the parameters of the model. The vector of parameters Î²1:D\\beta_{1:D} are the weights or regression coefficients. Each coefficient Î²i\\beta_i specifies the change in the output that we expect if the corresponding input feature xix_i changes by one unit. The term Î²0\\beta_0 is the offset or bias term, and specifies the output if all the inputs are zero. This captures the unconditional response, and acts as a baseline. We sometimes write the input as (1,x1,â€¦,xD)\\left(1,x_{1},\\ldots,x_{D}\\right) so the offset can be absorbed into the weight vector.\nWe can always apply a transformation Ï•\\phi (linear or non-linear) to the input vector, replacing Î²\\beta with Ï•(Î²)\\phi(\\beta). As long as the parameters of the feature extractor are fixed, the model remain linear in the parameters even if not linear in the inputs.\nLeast squares estimation\nTo fit the linear regression model to data, we minimize the negative log-likelihood on the training set.\nNLL(Î²,Ïƒ2)=âˆ‘n=1Nlog[(12Ï€Ïƒ2)12exp(âˆ’12Ïƒ2(ynâˆ’Î²â€²xn)2)]=âˆ’12Ïƒ2âˆ‘n=1N(ynâˆ’yÌ‚n)2âˆ’Nlog(2Ï€Ïƒ2)\n\\begin{align*}\n\\text{NLL}\\left(\\beta,\\sigma^{2}\\right) & =\\sum_{n=1}^{N}\\log\\left[\\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{\\frac{1}{2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(y_{n}-\\beta'x_{n}\\right)^{2}\\right)\\right]\\\\\n & =-\\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{N}\\left(y_{n}-\\hat{y}_{n}\\right)^{2}-N\\log\\left(2\\pi\\sigma^{2}\\right)\n\\end{align*}\nwhere the predicted response is yÌ‚â‰¡Î²â€²xn\\hat{y}\\equiv\\beta'x_{n}. Focusing on just the weights, the NLL is (up to a constant):\nRSS(Î²)=12âˆ‘n=1N(ynâˆ’Î²â€²xn)2=12â€–ynâˆ’Î²â€²xnâ€–2=12(ynâˆ’Î²â€²xn)â€²(ynâˆ’Î²â€²xn) \n\\begin{align*}\n\\text{RSS}\\left(\\beta\\right) & =\\frac{1}{2}\\sum_{n=1}^{N}\\left(y_{n}-\\beta'x_{n}\\right)^{2}=\\frac{1}{2}\\left\\Vert y_{n}-\\beta'x_{n}\\right\\Vert ^{2}=\\frac{1}{2}\\left(y_{n}-\\beta'x_{n}\\right)'\\left(y_{n}-\\beta'x_{n}\\right)\\\\\n\\end{align*}\nWe must estimate the parameter values ğ›ƒÌ‚\\mathbf{\\hat{\\beta}} from the data, and using the OLS method, the loss function is\nLOLS(Î²Ì‚)=âˆ‘i=1n(yiâˆ’xiâ€²Î²Ì‚)2=â€–yâˆ’XÎ²Ì‚â€–2(2)\n\\text{L}_{OLS}\\left(\\hat{\\beta}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{'}\\hat{\\beta}\\right)^{2}=\\left\\Vert y-X\\hat{\\beta}\\right\\Vert ^{2}\n \\qquad(2)\nwhich is minimized with the estimate\nğ›ƒÌ‚OLS=(Xâ€²X)âˆ’1(Xâ€²Y)(3)\n\\hat{\\mathbf{\\beta}}_{OLS}=\\left(X'X\\right)^{-1}\\left(X'Y\\right)\n \\qquad(3)"
  },
  {
    "objectID": "supplemental/regularization-derivations.html#ridge-regression",
    "href": "supplemental/regularization-derivations.html#ridge-regression",
    "title": "Regularized Regression: Ridge, Lasso and Elastic Net",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nFrom the discussion so far we have concluded that we would like to decrease the model complexity, that is the number of predictors. We could use the forward or backward selection for this, but that way we would not be able to tell anything about the removed variablesâ€™ effect on the response. Removing predictors from the model can be seen as settings their coefficients to zero. Instead of forcing them to be exactly zero, letâ€™s penalize them if they are too far from zero, thus enforcing them to be small in a continuous way. This way, we decrease model complexity while keeping all variables in the model. This, basically, is what Ridge Regression does.\n\nModel Specification\nIn Ridge Regression, the OLS loss function is augmented in such a way that we not only minimize the sum of squared residuals but also penalize the size of parameter estimates, in order to shrink them towards zero:\nLridge(Î²Ì‚)=âˆ‘i=1n(yiâˆ’xiâ€²Î²Ì‚)2+Î»âˆ‘j=1mÎ²Ì‚j2=â€–yâˆ’XÎ²Ì‚â€–2+Î»â€–Î²Ì‚â€–2\n\\text{L}_{ridge}\\left(\\hat{\\beta}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{'}\\hat{\\beta}\\right)^{2} + \\lambda\\sum_{j=1}^{m}\\hat{\\beta}^2_j=\\left\\Vert y-X\\hat{\\beta}\\right\\Vert ^{2} + \\lambda\\left\\Vert \\hat{\\beta}\\right\\Vert ^{2}\n\nSolving this for Î²Ì‚\\hat\\beta gives the the ridge regression estimates Î²Ì‚ridge=(Xâ€²X+Î»I)âˆ’1(Xâ€²Y)\\hat\\beta_{ridge} = (X'X+\\lambda I)^{-1}(X'Y), where I denotes the identity matrix.\nThe Î»\\lambda parameter is the regularization penalty. We will talk about how to choose it in the next sections of this tutorial, but for now notice that:\n\nAs Î»â†’0,Î²Ì‚ridgeâ†’Î²Ì‚OLS\\lambda \\rightarrow 0, \\quad \\hat\\beta_{ridge} \\rightarrow \\hat\\beta_{OLS};\nAs Î»â†’âˆ,Î²Ì‚ridgeâ†’0\\lambda \\rightarrow \\infty, \\quad \\hat\\beta_{ridge} \\rightarrow 0.\n\nSo, setting Î»\\lambda to 0 is the same as using the OLS, while the larger its value, the stronger is the coefficientsâ€™ size penalized.\n\n\nBias-Variance Trade-Off in Ridge Regression\nBias(ğ›ƒÌ‚ridge)=Î»(Xâ€²X+Î»I)âˆ’1Î²Var(ğ›ƒÌ‚ridge)=Ïƒ2(Xâ€²X+Î»I)âˆ’1Xâ€²X(Xâ€²X+Î»I)âˆ’1\n\\begin{align*}\n\\text{Bias}\\left(\\hat{\\mathbf{\\beta}}_{ridge}\\right) & =\\lambda(X'X+\\lambda I)^{-1}\\beta\\\\\n\\text{Var}\\left(\\hat{\\mathbf{\\beta}}_{ridge}\\right) & =\\sigma^{2}(X'X+\\lambda I)^{-1}X'X(X'X+\\lambda I)^{-1}\n\\end{align*}\n\nFrom there you can see that as Î»\\lambda becomes larger, the variance decreases, and the bias increases. This poses the question: how much bias are we willing to accept in order to decrease the variance? Or: what is the optimal value for Î»\\lambda?\nThere are two ways we could tackle this issue. A more traditional approach would be to choose Î» such that some information criterion, e.g., AIC or BIC, is the smallest. A more machine learning-like approach is to perform cross-validation and select the value of Î» that minimizes the cross-validated sum of squared residuals (or some other measure). The former approach emphasizes the modelâ€™s fit to the data, while the latter is more focused on its predictive performance. Letâ€™s discuss both.\n\n\nMinimizing Information Criteria\nThis approach boils down to estimating the model with many different values for Î»\\lambda and choosing the one that minimizes the Akaike or Bayesian Information Criterion:\nAICridge=log(eâ€²e)+2dfridgeBICridge=log(eâ€²e)+2dfridgelogn\n\\begin{align*}\n\\text{AIC}_{\\text{ridge}} & =\\log(e'e)+2\\text{df}_{\\text{ridge}}\\\\\n\\text{BIC}_{\\text{ridge}} & =\\log(e'e)+2\\text{df}_{\\text{ridge}}\\log n\n\\end{align*}\n\nwhere dfridge\\text{df}_{\\text{ridge}} is the number of degrees of freedom. Watch out here! The number of degrees of freedom in ridge regression is different than in the regular OLS! This is often overlooked which leads to incorrect inference. In both OLS and ridge regression, degrees of freedom are equal to the trace of the so-called hat matrix, which is a matrix that maps the vector of response values to the vector of fitted values as follows: yÌ‚=Hy\\hat y = H y.\nIn OLS, we find that HOLS=X(Xâ€²X)âˆ’1X\\text{H}_{\\text{OLS}}=X(Xâ€²X)^{âˆ’1}X, which gives dfOLS=trHOLS=m\\text{df}_{\\text{OLS}}=\\text{tr}\\text{H}_{\\text{OLS}}=m, where mm is the number of predictor variables. In ridge regression, however, the formula for the hat matrix should include the regularization penalty: Hridge=X(Xâ€²X+Î»I)âˆ’1X\\text{H}_{\\text{ridge}}=X(Xâ€²X+\\lambda I)^{âˆ’1}X, which gives dfridge=trHridge\\text{df}_{\\text{ridge}}=\\text{tr}\\text{H}_{\\text{ridge}}, which is no longer equal to mm. Some ridge regression software produce information criteria based on the OLS formula. To be sure you are doing things right, it is safer to compute them manually, which is what we will do later in this tutorial.\n\n\nRidge Regression: R example\nIn R, the glmnet package contains all you need to implement ridge regression. We will use the infamous mtcars dataset as an illustration, where the task is to predict miles per gallon based on carâ€™s other characteristics. One more thing: ridge regression assumes the predictors are standardized and the response is centered! You will see why this assumption is needed in a moment. For now, we will just standardize before modeling.\n\n# Load libraries, get data & set seed for reproducibility ---------------------\nset.seed(123)    # seef for reproducibility\nlibrary(glmnet)  # for ridge regression\nlibrary(dplyr)   # for data cleaning\nlibrary(psych)   # for function tr() to compute trace of a matrix\n\ndata(\"mtcars\")\n# Center y, X will be standardized in the modelling function\ny &lt;- mtcars %&gt;% select(mpg) %&gt;% scale(center = TRUE, scale = FALSE) %&gt;% as.matrix()\nX &lt;- mtcars %&gt;% select(-mpg) %&gt;% as.matrix()\n\n\n# Perform 10-fold cross-validation to select lambda ---------------------------\nlambdas_to_try &lt;- 10^seq(-3, 5, length.out = 100)\n# Setting alpha = 0 implements ridge regression\nridge_cv &lt;- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,\n                      standardize = TRUE, nfolds = 10)\n# Plot cross-validation results\nplot(ridge_cv)\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (Î²1\\beta_1) and intercept (Î²0\\beta_0). We obtain the estimates, Î²Ì‚1\\hat{\\beta}_1 and Î²Ì‚0\\hat{\\beta}_0 by finding the values that minimize the sum of squared residuals, as shown in EquationÂ 8.\nSSR=âˆ‘i=1n[yiâˆ’yÌ‚i]2=[yiâˆ’(Î²Ì‚0+Î²Ì‚1xi)]2=[yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi]2(8)\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n \\qquad(8)\nRecall that we can find the values of Î²Ì‚1\\hat{\\beta}_1 and Î²Ì‚0\\hat{\\beta}_0 that minimize /eq-ssr by taking the partial derivatives of EquationÂ 8 and setting them to 0. Thus, the values of Î²Ì‚1\\hat{\\beta}_1 and Î²Ì‚0\\hat{\\beta}_0 that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in EquationÂ 9.\nâˆ‚SSRâˆ‚Î²Ì‚1=âˆ’2âˆ‘i=1nxi(yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi)âˆ‚SSRâˆ‚Î²Ì‚0=âˆ’2âˆ‘i=1n(yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi)(9)\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n \\qquad(9)\nThe derivation of deriving Î²Ì‚0\\hat{\\beta}_0 is shown in EquationÂ 10.\nâˆ‚SSRâˆ‚Î²Ì‚0=âˆ’2âˆ‘i=1n(yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi)=0â‡’âˆ’âˆ‘i=1n(yi+Î²Ì‚0+Î²Ì‚1xi)=0â‡’âˆ’âˆ‘i=1nyi+nÎ²Ì‚0+Î²Ì‚1âˆ‘i=1nxi=0â‡’nÎ²Ì‚0=âˆ‘i=1nyiâˆ’Î²Ì‚1âˆ‘i=1nxiâ‡’Î²Ì‚0=1n(âˆ‘i=1nyiâˆ’Î²Ì‚1âˆ‘i=1nxi)â‡’Î²Ì‚0=yâ€¾âˆ’Î²Ì‚1xâ€¾(10)\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n \\qquad(10)\nThe derivation of Î²Ì‚1\\hat{\\beta}_1 using the Î²Ì‚0\\hat{\\beta}_0 we just derived is shown in EquationÂ 11.\nâˆ‚SSRâˆ‚Î²Ì‚1=âˆ’2âˆ‘i=1nxi(yiâˆ’Î²Ì‚0âˆ’Î²Ì‚1xi)=0â‡’âˆ’âˆ‘i=1nxiyi+Î²Ì‚0âˆ‘i=1nxi+Î²Ì‚1âˆ‘i=1nxi2=0(Fill in Î²Ì‚0)â‡’âˆ’âˆ‘i=1nxiyi+(yâ€¾âˆ’Î²Ì‚1xâ€¾)âˆ‘i=1nxi+Î²Ì‚1âˆ‘i=1nxi2=0â‡’(yâ€¾âˆ’Î²Ì‚1xâ€¾)âˆ‘i=1nxi+Î²Ì‚1âˆ‘i=1nxi2=âˆ‘i=1nxiyiâ‡’yâ€¾âˆ‘i=1nxiâˆ’Î²Ì‚1xâ€¾âˆ‘i=1nxi+Î²Ì‚1âˆ‘i=1nxi2=âˆ‘i=1nxiyiâ‡’nyâ€¾xâ€¾âˆ’Î²Ì‚1nxâ€¾2+Î²Ì‚1âˆ‘i=1nxi2=âˆ‘i=1nxiyiâ‡’Î²Ì‚1âˆ‘i=1nxi2âˆ’Î²Ì‚1nxâ€¾2=âˆ‘i=1nxiyiâˆ’nyâ€¾xâ€¾â‡’Î²Ì‚1(âˆ‘i=1nxi2âˆ’nxâ€¾2)=âˆ‘i=1nxiyiâˆ’nyâ€¾xâ€¾Î²Ì‚1=âˆ‘i=1nxiyiâˆ’nyâ€¾xâ€¾âˆ‘i=1nxi2âˆ’nxâ€¾2(11)\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n \\qquad(11)\nTo write Î²Ì‚1\\hat{\\beta}_1 in a form thatâ€™s more recognizable, we will use the following:\nâˆ‘xiyiâˆ’nyâ€¾xâ€¾=âˆ‘(xâˆ’xâ€¾)(yâˆ’yâ€¾)=(nâˆ’1)Cov(x,y)(12)\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n \\qquad(12)\nâˆ‘xi2âˆ’nxâ€¾2âˆ’âˆ‘(xâˆ’xâ€¾)2=(nâˆ’1)sx2(13)\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n \\qquad(13)\nwhere Cov(x,y)\\text{Cov}(x,y) is the covariance of xx and yy, and sx2s_x^2 is the sample variance of xx (sxs_x is the sample standard deviation).\nThus, applying EquationÂ 12 and EquationÂ 13, we have\nÎ²Ì‚1=âˆ‘i=1nxiyiâˆ’nyâ€¾xâ€¾âˆ‘i=1nxi2âˆ’nxâ€¾2=âˆ‘i=1n(xâˆ’xâ€¾)(yâˆ’yâ€¾)âˆ‘i=1n(xâˆ’xâ€¾)2=(nâˆ’1)Cov(x,y)(nâˆ’1)sx2=Cov(x,y)sx2(14)\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n \\qquad(14)\nThe correlation between xx and yy is r=Cov(x,y)sxsyr = \\frac{\\text{Cov}(x,y)}{s_x s_y}. Thus, Cov(x,y)=rsxsy\\text{Cov}(x,y) = r s_xs_y. Plugging this into EquationÂ 14, we have\nÎ²Ì‚1=Cov(x,y)sx2=rsysxsx2=rsysx(15)\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n \\qquad(15)"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Generative Artificial Intelligence (AI) models, such as ChatGPT, Google Gemini, Claude, Jenni, Github Co-pilot, DaLL- E, and Midjourney, may be used for any lab assignment in this course with appropriate acknowledgement and citation. Examples of citing AI language models are available here. You are responsible for fact-checking statements and code composed by AI language models. Failure to acknowledge or cite GAI use will constitute academic misconduct and may be subject to discipline under Bylaw 31: Academic Integrity.\nUse of Generative Artificial Intelligence (AI) models will not be permitted for quizzes, or exams.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#can-i-use-generative-ai-for-class-assignments",
    "href": "course-faq.html#can-i-use-generative-ai-for-class-assignments",
    "title": "FAQ",
    "section": "",
    "text": "Generative Artificial Intelligence (AI) models, such as ChatGPT, Google Gemini, Claude, Jenni, Github Co-pilot, DaLL- E, and Midjourney, may be used for any lab assignment in this course with appropriate acknowledgement and citation. Examples of citing AI language models are available here. You are responsible for fact-checking statements and code composed by AI language models. Failure to acknowledge or cite GAI use will constitute academic misconduct and may be subject to discipline under Bylaw 31: Academic Integrity.\nUse of Generative Artificial Intelligence (AI) models will not be permitted for quizzes, or exams.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-lab",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-lab",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the Lab?",
    "text": "Can I use a local install of R and RStudio instead of using the Lab?\nThe short answer is, Iâ€™d rather you didnâ€™t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages weâ€™re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TA are always happy to provide help with any computational questions when youâ€™re working in the lab. If youâ€™re working on your local setup, we canâ€™t guarantee being able to resolve your issues, though weâ€™re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.3: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd Iâ€™d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment set up for you in the lab.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html",
    "href": "exams/BSMM_8740_quiz_1_solutions.html",
    "title": "BSMM-quiz-1",
    "section": "",
    "text": "# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  ggplot2, magrittr, tidymodels, tidyverse, rsample, broom, recipes, parsnip, modeldata\n)\n\n# set the efault theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#packages",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#packages",
    "title": "BSMM-quiz-1",
    "section": "",
    "text": "# check if 'librarian' is installed and if not, install it\nif (! \"librarian\" %in% rownames(installed.packages()) ){\n  install.packages(\"librarian\")\n}\n  \n# load packages if not already loaded\nlibrarian::shelf(\n  ggplot2, magrittr, tidymodels, tidyverse, rsample, broom, recipes, parsnip, modeldata\n)\n\n# set the efault theme for plotting\ntheme_set(theme_bw(base_size = 18) + theme(legend.position = \"top\"))"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-1",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-1",
    "title": "BSMM-quiz-1",
    "section": "Q-1",
    "text": "Q-1\nIs this data a tidy dataset?\n\n\n\n\n\n\n\n\n\n\n\nRegion\n&lt; $1M\n$1 - $5M\n$5 - $10M\n$10 - $100M\n&gt; $100M\n\n\n\n\nN America\n$50M\n$324M\n$1045M\n$941M\n$1200M\n\n\nEMEA\n$10M\n$121M\n$77M\n$80M\n$0M\n\n\n\nDelete the wrong answer:\n\n\n\n\n\n\nSOLUTION: the answer is No\n\n\n\nThe values of categories appear as column names, while the corresponding dollar values are spread out across the tables. The Tidy version would have three columns - Region, income_range, and dollar value so that all measurements that go together can be found in a row.\nThis could be achieved by transforming the original table using tidyr::pivot_longer()\n\n\n\n# original table\ndat &lt;- tibble::tibble(\n  Region = c('N America', 'EMEA')\n  , '&lt; $1M' = c('$50M', '$10M')\n  , '$1 - $5M' = c('$324M', '$121M')\n  , '$5 - $10M' = c('$1045M', '$77M')\n  , '$10 - $100M' = c('$941M', '$80M')\n  , '&gt; $100M' = c('$1200M', '$0M')\n) \ndat |&gt; gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nRegion\n&lt; $1M\n$1 - $5M\n$5 - $10M\n$10 - $100M\n&gt; $100M\n\n\n\n\nN America\n$50M\n$324M\n$1045M\n$941M\n$1200M\n\n\nEMEA\n$10M\n$121M\n$77M\n$80M\n$0M\n\n\n\n\n\n\n\n\n# transformed table\ndat |&gt; \n  tidyr::pivot_longer(-Region, names_to = \"range\", values_to = \"$ amount\")|&gt; \n  gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nRegion\nrange\n$ amount\n\n\n\n\nN America\n&lt; $1M\n$50M\n\n\nN America\n$1 - $5M\n$324M\n\n\nN America\n$5 - $10M\n$1045M\n\n\nN America\n$10 - $100M\n$941M\n\n\nN America\n&gt; $100M\n$1200M\n\n\nEMEA\n&lt; $1M\n$10M\n\n\nEMEA\n$1 - $5M\n$121M\n\n\nEMEA\n$5 - $10M\n$77M\n\n\nEMEA\n$10 - $100M\n$80M\n\n\nEMEA\n&gt; $100M\n$0M"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-2",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-2",
    "title": "BSMM-quiz-1",
    "section": "Q-2",
    "text": "Q-2\nWhich resampling method from the resample:: package randomly partitions the data into V sets of roughly equal size?\n\n\n\n\n\n\nSOLUTION: The answer is V-fold cross-validation\n\n\n\nV-fold cross-validation (also known as k-fold cross-validation) randomly splits the data into V groups of roughly equal size (called â€œfoldsâ€). In the tidyverse you create a dataset for V-fold cross-validation using rsample::vfold_cv() ."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-3",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-3",
    "title": "BSMM-quiz-1",
    "section": "Q-3",
    "text": "Q-3\nIf I join the two tables below as follows:\ndplyr::????_join(employees, departments, by = \"department_id\")\nwhich type of join would include employee_name == Moe Syzslak?\n\n\n\n\n\n\nSOLUTION: the answer is left_join\n\n\n\nInner join will return all the rows with common department ID, and since Moe Syzslak has a NA department ID, with no match in the department ID table,his name wonâ€™t appear in the result of the join.\nRight join will return the departments table and all the rows of employees with department ID in common with the departments table, and since Moe Syzslak has a NA department ID, with no match in the department ID table,his name wonâ€™t appear in the result of the join.\nLeft join will return the employees table and all the rows of departments with department ID in common with the employees table. Since Moe Syzslak appears in the employees table his name will appear in the result of the join. Since\nSee the code below.\n\n\n\ninner\nleft\nright\nall of the above\n\nDelete the incorrect answers.\nemployees - This table contains each employeeâ€™s ID, name, and department ID.\n\n\n\nid\nemployee_name\ndepartment_id\n\n\n\n\n1\nHomer Simpson\n4\n\n\n2\nNed Flanders\n1\n\n\n3\nBarney Gumble\n5\n\n\n4\nClancy Wiggum\n3\n\n\n5\nMoe Syzslak\nNA\n\n\n\ndepartments - This table contains each departmentâ€™s ID and name.\n\n\n\ndepartment_id\ndepartment_name\n\n\n\n\n1\nSales\n\n\n2\nEngineering\n\n\n3\nHuman Resources\n\n\n4\nCustomer Service\n\n\n5\nResearch And Development\n\n\n\ntbl1 &lt;- tibble::tribble(\n~id , ~employee_name,   ~department_id\n,1  ,'Homer Simpson'    ,4\n,2  ,'Ned Flanders'   ,1\n,3  ,'Barney Gumble'    ,5\n,4  ,'Clancy Wiggum'    ,3\n,5  ,'Moe Syzslak'    ,NA\n)\n\ntbl2 &lt;- tibble::tribble(\n~department_id  ,~department_name\n,1  ,\"Sales\"\n,2  ,\"Engineering\"\n,3  ,\"Human Resources\"\n,4  ,\"Customer Service\"\n,5  ,\"Research And Development\"\n)\n\n# left_join\ndplyr::left_join(tbl1,tbl2,by = \"department_id\") |&gt; \n  gt::gt() |&gt; gt::tab_header(title = \"Left Join\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\n\nLeft Join\n\n\nid\nemployee_name\ndepartment_id\ndepartment_name\n\n\n\n\n1\nHomer Simpson\n4\nCustomer Service\n\n\n2\nNed Flanders\n1\nSales\n\n\n3\nBarney Gumble\n5\nResearch And Development\n\n\n4\nClancy Wiggum\n3\nHuman Resources\n\n\n5\nMoe Syzslak\nNA\nNA\n\n\n\n\n\n\n\n\n# right_join\ndplyr::right_join(tbl1,tbl2,by = \"department_id\") |&gt; \n  gt::gt() |&gt; gt::tab_header(title = \"Right Join\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nRight Join\n\n\nid\nemployee_name\ndepartment_id\ndepartment_name\n\n\n\n\n1\nHomer Simpson\n4\nCustomer Service\n\n\n2\nNed Flanders\n1\nSales\n\n\n3\nBarney Gumble\n5\nResearch And Development\n\n\n4\nClancy Wiggum\n3\nHuman Resources\n\n\nNA\nNA\n2\nEngineering\n\n\n\n\n\n\n\n\n# inner_join\ndplyr::inner_join(tbl1,tbl2,by = \"department_id\") |&gt; \n  gt::gt() |&gt; gt::tab_header(title = \"Inner Join\") |&gt; \n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nInner Join\n\n\nid\nemployee_name\ndepartment_id\ndepartment_name\n\n\n\n\n1\nHomer Simpson\n4\nCustomer Service\n\n\n2\nNed Flanders\n1\nSales\n\n\n3\nBarney Gumble\n5\nResearch And Development\n\n\n4\nClancy Wiggum\n3\nHuman Resources"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-4",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-4",
    "title": "BSMM-quiz-1",
    "section": "Q-4",
    "text": "Q-4\nRecall that the first step of a decision-tree regression model will divide the space of predictors into 2 parts and estimate constant prediction values for each part. For a single predictor, the result of the first step estimates the outcome as:\nyÌ‚=âˆ‘i=12ciÃ—I(xâˆˆRi)\n\\hat{y} =\\sum_{i=1}^{2}c_i\\times I_{(x\\in R_i)}\nsuch that\nSSE={âˆ‘iâˆˆR1(yiâˆ’ci)2+âˆ‘iâˆˆR2(yiâˆ’ci)2}\n\\text{SSE}=\\left\\{ \\sum_{i\\in R_{1}}\\left(y_{i}-c_{i}\\right)^{2}+\\sum_{i\\in R_{2}}\\left(y_{i}-c_{i}\\right)^{2}\\right\\} \n\nis minimized.\nOn the first split of a decision tree regression model for the following data:\n\n\n\n\n\nThe first two regions that partition xx will be (Delete the wrong answer(s) below):\n\n\n\n\n\n\nSOLUTION: the answer is [0,1/2] and (1/2, 2/2]\n\n\n\nSince the decision tree is minimizing the SSE at each split, you want to minimize the range (max-min) of y values in each split. You can find a cic_i value to minimize the SSE within each split, but a wider range of yy values will have a larger SSE than a smaller range of yy values, due to the squares, and so the splits should have equal ranges.\nSince it looks like yi=xi+eiy_i = x_i + e_i (where eie_i is an error term), equal x ranges will give equal y ranges, so the split should be [0,1/2] and (1/2, 2/2]."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q-5",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q-5",
    "title": "BSMM-quiz-1",
    "section": "Q-5",
    "text": "Q-5\nIn an ordinary linear regression, regressing the outcome yy on a single predictor xx, the regression coefficient can be estimated as:\n\n\n\n\n\n\nSOLUTION:\n\n\n\nIn class we showed that the regression coefficient can be estimated as\ncovar(x,y)var(x)\n\\displaystyle\\frac{\\text{covar(x,y)}}{\\text{var(x)}}"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q6",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q6",
    "title": "BSMM-quiz-1",
    "section": "Q6",
    "text": "Q6\nWrite code to determine the number of species of penguin in the dataset. How many are there?\n\n\n\n\n\n\nSOLUTION: there are 3 penguin species in the dataset\n\n\n\n\npalmerpenguins::penguins |&gt; \n  dplyr::distinct(species)\n\n# A tibble: 3 Ã— 1\n  species  \n  &lt;fct&gt;    \n1 Adelie   \n2 Gentoo   \n3 Chinstrap\n\n\n\n# == OR ==\npalmerpenguins::penguins$species |&gt;  \n  unique() |&gt; length()\n\n[1] 3"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q7",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q7",
    "title": "BSMM-quiz-1",
    "section": "Q7",
    "text": "Q7\nExecute the following code to read sales data from a csv file.\n\n# read sales data\nsales_dat &lt;-\n  readr::read_csv(\"data/sales_data_sample.csv\", show_col_types = FALSE) |&gt;\n  janitor::clean_names() |&gt; \n  dplyr::mutate(\n    orderdate = lubridate::as_date(orderdate, format = \"%m/%d/%Y %H:%M\")\n    , orderdate = lubridate::year(orderdate)\n  )\n\nDescribe what the group_by step does in the code below, and complete the code to produce a sales summary by year, i.e.Â a data.frame where productline and orderdate are the columns (one column for each year), while each year column contains the sales for each productline that year.\n\n  sales_dat |&gt; \n    dplyr::group_by(orderdate, productline) |&gt; \n    dplyr::summarize( sales = sum(___) ) |&gt; \n    tidyr::pivot_wider(names_from = ___, values_from = ___)\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\nthe result of the group_by step is: order first by the values of the orderdate column, and then, within each orderdate value, order the rows by the values of the productline column.\nthe sales summary table produced by the code is given below\n\n\n# executed code\nsales_dat |&gt; \n    dplyr::group_by(orderdate, productline) |&gt; \n    dplyr::summarize( sales = sum(sales) ) |&gt; \n    tidyr::pivot_wider(names_from = orderdate, values_from = sales)\n\n# A tibble: 7 Ã— 4\n  productline        `2003`   `2004`  `2005`\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 Classic Cars     1484785. 1762257. 672573.\n2 Motorcycles       370896.  560545. 234948.\n3 Planes            272258.  502672. 200074.\n4 Ships             244821.  341438. 128178.\n5 Trains             72802.  116524.  36917.\n6 Trucks and Buses  420430.  529303. 178057.\n7 Vintage Cars      650988.  911424. 340739."
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q8",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q8",
    "title": "BSMM-quiz-1",
    "section": "Q8",
    "text": "Q8\nFor the data below, it is expected that the response variable yy can be described by the independent variables x1x1 and x2x2. This implies that the parameters of the following model should be estimated and tested per the model:\ny=Î²0+Î²1x1+Î²2x2+Ïµ,Ïµâˆ¼ğ’©(0,Ïƒ2)\ny = \\beta_0 + \\beta_1x1 + \\beta_2x2 + \\epsilon, \\epsilon âˆ¼ \\mathcal{N}(0, \\sigma^2)\n\n\ndat &lt;- tibble::tibble(\n  x1=c(0.58, 0.86, 0.29, 0.20, 0.56, 0.28, 0.08, 0.41, 0.22, 0.35, 0.59, 0.22, 0.26, 0.12, 0.65, 0.70, 0.30\n        , 0.70, 0.39, 0.72, 0.45, 0.81, 0.04, 0.20, 0.95)\n  , x2=c(0.71, 0.13, 0.79, 0.20, 0.56, 0.92, 0.01, 0.60, 0.70, 0.73, 0.13, 0.96, 0.27, 0.21, 0.88, 0.30\n        , 0.15, 0.09, 0.17, 0.25, 0.30, 0.32, 0.82, 0.98, 0.00)\n  , y=c(1.45, 1.93, 0.81, 0.61, 1.55, 0.95, 0.45, 1.14, 0.74, 0.98, 1.41, 0.81, 0.89, 0.68, 1.39, 1.53\n        , 0.91, 1.49, 1.38, 1.73, 1.11, 1.68, 0.66, 0.69, 1.98)\n)\n\nCalculate the parameter estimates ( Î²Ì‚0\\hat{\\beta}_0, Î²Ì‚1\\hat{\\beta}_1, and Î²Ì‚2\\hat{\\beta}_2); in addition find the usual 95% confidence intervals for Î²0\\beta_0, Î²1\\beta_1, Î²2\\beta_2.\n\n\n\n\n\n\nSOLUTION:\n\n\n\nUsing broom::tidy(conf.int = TRUE) with a regression model:\n\n# your code goes here\nfit_Q8 &lt;- lm(y ~ ., data = dat)\nfit_Q8 |&gt; broom::tidy(conf.int = TRUE) |&gt; \n  gt::gt() |&gt; gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.433547115\n0.06598300\n6.57058782\n1.313239e-06\n0.2967067\n0.5703875\n\n\nx1\n1.652993451\n0.09524539\n17.35510141\n2.525004e-14\n1.4554666\n1.8505203\n\n\nx2\n0.003944875\n0.07485382\n0.05270105\n9.584457e-01\n-0.1512924\n0.1591822"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q9",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q9",
    "title": "BSMM-quiz-1",
    "section": "Q9",
    "text": "Q9\nUsing the .resid column created by broom::augment(___, dat) , calculate ÏƒÌ‚2\\hat{\\sigma}^2.\n\n\n\n\n\n\nSOLUTION: the variance of the residual is 0.0116\n\n\n\n\nbroom::augment(fit_Q8, dat) |&gt; \n  dplyr::pull(.resid) |&gt; \n  var()\n\n[1] 0.01164646"
  },
  {
    "objectID": "exams/BSMM_8740_quiz_1_solutions.html#q10",
    "href": "exams/BSMM_8740_quiz_1_solutions.html#q10",
    "title": "BSMM-quiz-1",
    "section": "Q10",
    "text": "Q10\nDoes the following code train a model on the full training set of the modeldata::ames housing dataset and then evaluate the model using a test set?\n\nIs any step missing?\nWhen the recipe is baked and prepped, do you think all categories will be converted to dummy variables and all numeric predictors will be normalized?\n\n\n\n\n\n\n\nSOLUTION:\n\n\n\n\n# Load the ames housing dataset\ndata(ames)\n\n# Create an initial split of the data\nset.seed(123)\names_split &lt;- initial_split(ames, prop = 0.8, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\n# Create a recipe\names_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt;\n  step_log(Sale_Price, base = 10) |&gt;  \n  step_dummy(all_nominal_predictors()) |&gt;  \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())  \n\n\n# Create a workflow\names_workflow &lt;- workflow() |&gt;\n  add_recipe(ames_recipe) |&gt;\n  add_model(lm_spec)\n\n# Fit the model and evaluate on the test set\names_fit &lt;- ames_workflow |&gt; last_fit(ames_split)\n\n# View the metrics\names_fit |&gt; collect_metrics()\n\n\nIf you run the code chunk above you will get an error error, â€œâ€˜lm_specâ€™ is missingâ€. A workflow requires a pre-processing step (by formula or recipe) and a model specification. That is what is missing here.\nLooking at the data in the ames dataset, there are 40 factor variables and 34 numeric variables.\n\n\nskim_data &lt;- ames |&gt; skimr::skim()\nskim_data$skim_type |&gt; table()\n\n\n factor numeric \n     40      34 \n\n\nThe author of the recipe (see below) likely wanted the factor variables to be converted to dummy variables, and the numeric variables to be normalized.\nHowever, the order of the steps in the recipe turns the factor variables into dummy variables before the numeric variable were normalized, and dummy variables are numeric. Given the order of the steps, all the dummy variables will be normalized and all variables will be normalized numeric variables.\nNo dummy variables remain after the bake operation!\n\names_recipe |&gt; prep() |&gt; \n  bake(new_data = ames) |&gt; \n  dplyr::glimpse()\n\nRows: 2,930\nColumns: 273\n$ Lot_Frontage                                          &lt;dbl&gt; 2.46645944, 0.66â€¦\n$ Lot_Area                                              &lt;dbl&gt; 2.553049311, 0.1â€¦\n$ Year_Built                                            &lt;dbl&gt; -0.39216369, -0.â€¦\n$ Year_Remod_Add                                        &lt;dbl&gt; -1.17030684, -1.â€¦\n$ Mas_Vnr_Area                                          &lt;dbl&gt; 0.05161557, -0.5â€¦\n$ BsmtFin_SF_1                                          &lt;dbl&gt; -0.97323408, 0.8â€¦\n$ BsmtFin_SF_2                                          &lt;dbl&gt; -0.2948976, 0.54â€¦\n$ Bsmt_Unf_SF                                           &lt;dbl&gt; -0.26264166, -0.â€¦\n$ Total_Bsmt_SF                                         &lt;dbl&gt; 0.066772012, -0.â€¦\n$ First_Flr_SF                                          &lt;dbl&gt; 1.24235505, -0.6â€¦\n$ Second_Flr_SF                                         &lt;dbl&gt; -0.7768908, -0.7â€¦\n$ Gr_Liv_Area                                           &lt;dbl&gt; 0.3012894541, -1â€¦\n$ Bsmt_Full_Bath                                        &lt;dbl&gt; 1.0688790, -0.82â€¦\n$ Bsmt_Half_Bath                                        &lt;dbl&gt; -0.2447658, -0.2â€¦\n$ Full_Bath                                             &lt;dbl&gt; -1.0350779, -1.0â€¦\n$ Half_Bath                                             &lt;dbl&gt; -0.7505082, -0.7â€¦\n$ Bedroom_AbvGr                                         &lt;dbl&gt; 0.1586909, -1.04â€¦\n$ Kitchen_AbvGr                                         &lt;dbl&gt; -0.2142401, -0.2â€¦\n$ TotRms_AbvGrd                                         &lt;dbl&gt; 0.3483003, -0.91â€¦\n$ Fireplaces                                            &lt;dbl&gt; 2.1485023, -0.92â€¦\n$ Garage_Cars                                           &lt;dbl&gt; 0.3157983, -1.00â€¦\n$ Garage_Area                                           &lt;dbl&gt; 0.269501338, 1.2â€¦\n$ Wood_Deck_SF                                          &lt;dbl&gt; 0.9247677, 0.372â€¦\n$ Open_Porch_SF                                         &lt;dbl&gt; 0.21567955, -0.7â€¦\n$ Enclosed_Porch                                        &lt;dbl&gt; -0.364656, -0.36â€¦\n$ Three_season_porch                                    &lt;dbl&gt; -0.1027822, -0.1â€¦\n$ Screen_Porch                                          &lt;dbl&gt; -0.2849706, 1.83â€¦\n$ Pool_Area                                             &lt;dbl&gt; -0.06118388, -0.â€¦\n$ Misc_Val                                              &lt;dbl&gt; -0.08608582, -0.â€¦\n$ Mo_Sold                                               &lt;dbl&gt; -0.42445939, -0.â€¦\n$ Year_Sold                                             &lt;dbl&gt; 1.666407, 1.6664â€¦\n$ Longitude                                             &lt;dbl&gt; 0.9003850, 0.900â€¦\n$ Latitude                                              &lt;dbl&gt; 1.0642009, 1.008â€¦\n$ Sale_Price                                            &lt;dbl&gt; 5.332438, 5.0211â€¦\n$ MS_SubClass_One_Story_1945_and_Older                  &lt;dbl&gt; -0.2198252, -0.2â€¦\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    &lt;dbl&gt; -0.04135376, -0.â€¦\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    &lt;dbl&gt; -0.08027027, -0.â€¦\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      &lt;dbl&gt; -0.3211104, -0.3â€¦\n$ MS_SubClass_Two_Story_1946_and_Newer                  &lt;dbl&gt; -0.4996264, -0.4â€¦\n$ MS_SubClass_Two_Story_1945_and_Older                  &lt;dbl&gt; -0.2066988, -0.2â€¦\n$ MS_SubClass_Two_and_Half_Story_All_Ages               &lt;dbl&gt; -0.08549097, -0.â€¦\n$ MS_SubClass_Split_or_Multilevel                       &lt;dbl&gt; -0.2066988, -0.2â€¦\n$ MS_SubClass_Split_Foyer                               &lt;dbl&gt; -0.1283979, -0.1â€¦\n$ MS_SubClass_Duplex_All_Styles_and_Ages                &lt;dbl&gt; -0.2055737, -0.2â€¦\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              &lt;dbl&gt; -0.2707322, -0.2â€¦\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           &lt;dbl&gt; -0.02066363, -0.â€¦\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              &lt;dbl&gt; -0.2187562, -0.2â€¦\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          &lt;dbl&gt; -0.07469546, -0.â€¦\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; -0.1399371, -0.1â€¦\n$ MS_Zoning_Residential_High_Density                    &lt;dbl&gt; -0.09509974, 10.â€¦\n$ MS_Zoning_Residential_Low_Density                     &lt;dbl&gt; 0.5420306, -1.84â€¦\n$ MS_Zoning_Residential_Medium_Density                  &lt;dbl&gt; -0.4344558, -0.4â€¦\n$ MS_Zoning_A_agr                                       &lt;dbl&gt; -0.02066363, -0.â€¦\n$ MS_Zoning_C_all                                       &lt;dbl&gt; -0.09735866, -0.â€¦\n$ MS_Zoning_I_all                                       &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Street_Pave                                           &lt;dbl&gt; 0.06868034, 0.06â€¦\n$ Alley_No_Alley_Access                                 &lt;dbl&gt; 0.2661636, 0.266â€¦\n$ Alley_Paved                                           &lt;dbl&gt; -0.1648677, -0.1â€¦\n$ Lot_Shape_Slightly_Irregular                          &lt;dbl&gt; 1.4134589, -0.70â€¦\n$ Lot_Shape_Moderately_Irregular                        &lt;dbl&gt; -0.1607238, -0.1â€¦\n$ Lot_Shape_Irregular                                   &lt;dbl&gt; -0.07174967, -0.â€¦\n$ Land_Contour_HLS                                      &lt;dbl&gt; -0.2066988, -0.2â€¦\n$ Land_Contour_Low                                      &lt;dbl&gt; -0.1430754, -0.1â€¦\n$ Land_Contour_Lvl                                      &lt;dbl&gt; 0.3401764, 0.340â€¦\n$ Utilities_NoSeWa                                      &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Utilities_NoSewr                                      &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Lot_Config_CulDSac                                    &lt;dbl&gt; -0.2577909, -0.2â€¦\n$ Lot_Config_FR2                                        &lt;dbl&gt; -0.1793294, -0.1â€¦\n$ Lot_Config_FR3                                        &lt;dbl&gt; -0.07174967, -0.â€¦\n$ Lot_Config_Inside                                     &lt;dbl&gt; -1.6286599, 0.61â€¦\n$ Land_Slope_Mod                                        &lt;dbl&gt; -0.2187562, -0.2â€¦\n$ Land_Slope_Sev                                        &lt;dbl&gt; -0.07174967, -0.â€¦\n$ Neighborhood_College_Creek                            &lt;dbl&gt; -0.3186783, -0.3â€¦\n$ Neighborhood_Old_Town                                 &lt;dbl&gt; -0.287611, -0.28â€¦\n$ Neighborhood_Edwards                                  &lt;dbl&gt; -0.2679979, -0.2â€¦\n$ Neighborhood_Somerset                                 &lt;dbl&gt; -0.2596688, -0.2â€¦\n$ Neighborhood_Northridge_Heights                       &lt;dbl&gt; -0.2384008, -0.2â€¦\n$ Neighborhood_Gilbert                                  &lt;dbl&gt; -0.2423742, -0.2â€¦\n$ Neighborhood_Sawyer                                   &lt;dbl&gt; -0.2302931, -0.2â€¦\n$ Neighborhood_Northwest_Ames                           &lt;dbl&gt; -0.2166052, -0.2â€¦\n$ Neighborhood_Sawyer_West                              &lt;dbl&gt; -0.2155231, -0.2â€¦\n$ Neighborhood_Mitchell                                 &lt;dbl&gt; -0.2010204, -0.2â€¦\n$ Neighborhood_Brookside                                &lt;dbl&gt; -0.1904408, -0.1â€¦\n$ Neighborhood_Crawford                                 &lt;dbl&gt; -0.1928346, -0.1â€¦\n$ Neighborhood_Iowa_DOT_and_Rail_Road                   &lt;dbl&gt; -0.1916409, -0.1â€¦\n$ Neighborhood_Timberland                               &lt;dbl&gt; -0.1550442, -0.1â€¦\n$ Neighborhood_Northridge                               &lt;dbl&gt; -0.1607238, -0.1â€¦\n$ Neighborhood_Stone_Brook                              &lt;dbl&gt; -0.1351039, -0.1â€¦\n$ Neighborhood_South_and_West_of_Iowa_State_University  &lt;dbl&gt; -0.1157946, -0.1â€¦\n$ Neighborhood_Clear_Creek                              &lt;dbl&gt; -0.1213469, -0.1â€¦\n$ Neighborhood_Meadow_Village                           &lt;dbl&gt; -0.113887, -0.11â€¦\n$ Neighborhood_Briardale                                &lt;dbl&gt; -0.1079726, -0.1â€¦\n$ Neighborhood_Bloomington_Heights                      &lt;dbl&gt; -0.09956823, -0.â€¦\n$ Neighborhood_Veenker                                  &lt;dbl&gt; -0.09041895, -0.â€¦\n$ Neighborhood_Northpark_Villa                          &lt;dbl&gt; -0.09278786, -0.â€¦\n$ Neighborhood_Blueste                                  &lt;dbl&gt; -0.05853314, -0.â€¦\n$ Neighborhood_Greens                                   &lt;dbl&gt; -0.05066948, -0.â€¦\n$ Neighborhood_Green_Hills                              &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Neighborhood_Landmark                                 &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Condition_1_Feedr                                     &lt;dbl&gt; -0.2462976, 4.05â€¦\n$ Condition_1_Norm                                      &lt;dbl&gt; 0.403473, -2.477â€¦\n$ Condition_1_PosA                                      &lt;dbl&gt; -0.08549097, -0.â€¦\n$ Condition_1_PosN                                      &lt;dbl&gt; -0.1176728, -0.1â€¦\n$ Condition_1_RRAe                                      &lt;dbl&gt; -0.09735866, -0.â€¦\n$ Condition_1_RRAn                                      &lt;dbl&gt; -0.1301046, -0.1â€¦\n$ Condition_1_RRNe                                      &lt;dbl&gt; -0.05066948, -0.â€¦\n$ Condition_1_RRNn                                      &lt;dbl&gt; -0.05474101, -0.â€¦\n$ Condition_2_Feedr                                     &lt;dbl&gt; -0.0654701, -0.0â€¦\n$ Condition_2_Norm                                      &lt;dbl&gt; 0.09509974, 0.09â€¦\n$ Condition_2_PosA                                      &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Condition_2_PosN                                      &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Condition_2_RRAe                                      &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Condition_2_RRAn                                      &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Condition_2_RRNn                                      &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Bldg_Type_TwoFmCon                                    &lt;dbl&gt; -0.1415143, -0.1â€¦\n$ Bldg_Type_Duplex                                      &lt;dbl&gt; -0.2055737, -0.2â€¦\n$ Bldg_Type_Twnhs                                       &lt;dbl&gt; -0.1880208, -0.1â€¦\n$ Bldg_Type_TwnhsE                                      &lt;dbl&gt; -0.3029889, -0.3â€¦\n$ House_Style_One_and_Half_Unf                          &lt;dbl&gt; -0.08549097, -0.â€¦\n$ House_Style_One_Story                                 &lt;dbl&gt; 0.983694, 0.9836â€¦\n$ House_Style_SFoyer                                    &lt;dbl&gt; -0.1689205, -0.1â€¦\n$ House_Style_SLvl                                      &lt;dbl&gt; -0.2166052, -0.2â€¦\n$ House_Style_Two_and_Half_Fin                          &lt;dbl&gt; -0.05474101, -0.â€¦\n$ House_Style_Two_and_Half_Unf                          &lt;dbl&gt; -0.08549097, -0.â€¦\n$ House_Style_Two_Story                                 &lt;dbl&gt; -0.6547801, -0.6â€¦\n$ Overall_Cond_Poor                                     &lt;dbl&gt; -0.06209708, -0.â€¦\n$ Overall_Cond_Fair                                     &lt;dbl&gt; -0.1351039, -0.1â€¦\n$ Overall_Cond_Below_Average                            &lt;dbl&gt; -0.1892341, -0.1â€¦\n$ Overall_Cond_Average                                  &lt;dbl&gt; 0.876672, -1.140â€¦\n$ Overall_Cond_Above_Average                            &lt;dbl&gt; -0.4700736, 2.12â€¦\n$ Overall_Cond_Good                                     &lt;dbl&gt; -0.3875958, -0.3â€¦\n$ Overall_Cond_Very_Good                                &lt;dbl&gt; -0.232341, -0.23â€¦\n$ Overall_Cond_Excellent                                &lt;dbl&gt; -0.113887, -0.11â€¦\n$ Roof_Style_Gable                                      &lt;dbl&gt; -1.9639943, 0.50â€¦\n$ Roof_Style_Gambrel                                    &lt;dbl&gt; -0.07753179, -0.â€¦\n$ Roof_Style_Hip                                        &lt;dbl&gt; 2.0815862, -0.48â€¦\n$ Roof_Style_Mansard                                    &lt;dbl&gt; -0.06209708, -0.â€¦\n$ Roof_Style_Shed                                       &lt;dbl&gt; -0.04135376, -0.â€¦\n$ Roof_Matl_CompShg                                     &lt;dbl&gt; 0.1157946, 0.115â€¦\n$ Roof_Matl_Membran                                     &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Roof_Matl_Metal                                       &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Roof_Matl_Tar.Grv                                     &lt;dbl&gt; -0.08292059, -0.â€¦\n$ Roof_Matl_WdShake                                     &lt;dbl&gt; -0.05853314, -0.â€¦\n$ Roof_Matl_WdShngl                                     &lt;dbl&gt; -0.04135376, -0.â€¦\n$ Exterior_1st_AsphShn                                  &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Exterior_1st_BrkComm                                  &lt;dbl&gt; -0.0462448, -0.0â€¦\n$ Exterior_1st_BrkFace                                  &lt;dbl&gt; 5.7382892, -0.17â€¦\n$ Exterior_1st_CemntBd                                  &lt;dbl&gt; -0.2155231, -0.2â€¦\n$ Exterior_1st_HdBoard                                  &lt;dbl&gt; -0.4267943, -0.4â€¦\n$ Exterior_1st_ImStucc                                  &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Exterior_1st_MetalSd                                  &lt;dbl&gt; -0.4232945, -0.4â€¦\n$ Exterior_1st_Plywood                                  &lt;dbl&gt; -0.288480, -0.28â€¦\n$ Exterior_1st_PreCast                                  &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Exterior_1st_Stone                                    &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Exterior_1st_Stucco                                   &lt;dbl&gt; -0.1195232, -0.1â€¦\n$ Exterior_1st_VinylSd                                  &lt;dbl&gt; -0.7345379, 1.36â€¦\n$ Exterior_1st_Wd.Sdng                                  &lt;dbl&gt; -0.3991715, -0.3â€¦\n$ Exterior_1st_WdShing                                  &lt;dbl&gt; -0.1461515, -0.1â€¦\n$ Exterior_2nd_AsphShn                                  &lt;dbl&gt; -0.03580575, -0.â€¦\n$ Exterior_2nd_Brk.Cmn                                  &lt;dbl&gt; -0.09041895, -0.â€¦\n$ Exterior_2nd_BrkFace                                  &lt;dbl&gt; -0.1249191, -0.1â€¦\n$ Exterior_2nd_CBlock                                   &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Exterior_2nd_CmentBd                                  &lt;dbl&gt; -0.2155231, -0.2â€¦\n$ Exterior_2nd_HdBoard                                  &lt;dbl&gt; -0.4070422, -0.4â€¦\n$ Exterior_2nd_ImStucc                                  &lt;dbl&gt; -0.07469546, -0.â€¦\n$ Exterior_2nd_MetalSd                                  &lt;dbl&gt; -0.4232945, -0.4â€¦\n$ Exterior_2nd_Other                                    &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Exterior_2nd_Plywood                                  &lt;dbl&gt; 3.0517540, -0.32â€¦\n$ Exterior_2nd_PreCast                                  &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Exterior_2nd_Stone                                    &lt;dbl&gt; -0.0462448, -0.0â€¦\n$ Exterior_2nd_Stucco                                   &lt;dbl&gt; -0.1195232, -0.1â€¦\n$ Exterior_2nd_VinylSd                                  &lt;dbl&gt; -0.7283491, 1.37â€¦\n$ Exterior_2nd_Wd.Sdng                                  &lt;dbl&gt; -0.3854073, -0.3â€¦\n$ Exterior_2nd_Wd.Shng                                  &lt;dbl&gt; -0.1689205, -0.1â€¦\n$ Mas_Vnr_Type_BrkFace                                  &lt;dbl&gt; -0.6634428, -0.6â€¦\n$ Mas_Vnr_Type_CBlock                                   &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Mas_Vnr_Type_None                                     &lt;dbl&gt; -1.2199205, 0.81â€¦\n$ Mas_Vnr_Type_Stone                                    &lt;dbl&gt; 3.2366522, -0.30â€¦\n$ Exter_Cond_Fair                                       &lt;dbl&gt; -0.1550442, -0.1â€¦\n$ Exter_Cond_Good                                       &lt;dbl&gt; -0.3393949, -0.3â€¦\n$ Exter_Cond_Poor                                       &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Exter_Cond_Typical                                    &lt;dbl&gt; 0.3890515, 0.389â€¦\n$ Foundation_CBlock                                     &lt;dbl&gt; 1.1411815, 1.141â€¦\n$ Foundation_PConc                                      &lt;dbl&gt; -0.8927767, -0.8â€¦\n$ Foundation_Slab                                       &lt;dbl&gt; -0.1367326, -0.1â€¦\n$ Foundation_Stone                                      &lt;dbl&gt; -0.06868034, -0.â€¦\n$ Foundation_Wood                                       &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Bsmt_Cond_Fair                                        &lt;dbl&gt; -0.1975475, -0.1â€¦\n$ Bsmt_Cond_Good                                        &lt;dbl&gt; 4.9442976, -0.20â€¦\n$ Bsmt_Cond_No_Basement                                 &lt;dbl&gt; -0.1728887, -0.1â€¦\n$ Bsmt_Cond_Poor                                        &lt;dbl&gt; -0.0462448, -0.0â€¦\n$ Bsmt_Cond_Typical                                     &lt;dbl&gt; -2.8539373, 0.35â€¦\n$ Bsmt_Exposure_Gd                                      &lt;dbl&gt; 3.0742999, -0.32â€¦\n$ Bsmt_Exposure_Mn                                      &lt;dbl&gt; -0.3055009, -0.3â€¦\n$ Bsmt_Exposure_No                                      &lt;dbl&gt; -1.3381530, 0.74â€¦\n$ Bsmt_Exposure_No_Basement                             &lt;dbl&gt; -0.1767779, -0.1â€¦\n$ BsmtFin_Type_1_BLQ                                    &lt;dbl&gt; 3.0895747, -0.32â€¦\n$ BsmtFin_Type_1_GLQ                                    &lt;dbl&gt; -0.6481321, -0.6â€¦\n$ BsmtFin_Type_1_LwQ                                    &lt;dbl&gt; -0.2333596, -0.2â€¦\n$ BsmtFin_Type_1_No_Basement                            &lt;dbl&gt; -0.1728887, -0.1â€¦\n$ BsmtFin_Type_1_Rec                                    &lt;dbl&gt; -0.3291359, 3.03â€¦\n$ BsmtFin_Type_1_Unf                                    &lt;dbl&gt; -0.6342108, -0.6â€¦\n$ BsmtFin_Type_2_BLQ                                    &lt;dbl&gt; -0.1491694, -0.1â€¦\n$ BsmtFin_Type_2_GLQ                                    &lt;dbl&gt; -0.1176728, -0.1â€¦\n$ BsmtFin_Type_2_LwQ                                    &lt;dbl&gt; -0.1780576, 5.61â€¦\n$ BsmtFin_Type_2_No_Basement                            &lt;dbl&gt; -0.1741936, -0.1â€¦\n$ BsmtFin_Type_2_Rec                                    &lt;dbl&gt; -0.194022, -0.19â€¦\n$ BsmtFin_Type_2_Unf                                    &lt;dbl&gt; 0.4183756, -2.38â€¦\n$ Heating_GasA                                          &lt;dbl&gt; 0.1249191, 0.124â€¦\n$ Heating_GasW                                          &lt;dbl&gt; -0.09278786, -0.â€¦\n$ Heating_Grav                                          &lt;dbl&gt; -0.05853314, -0.â€¦\n$ Heating_OthW                                          &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Heating_Wall                                          &lt;dbl&gt; -0.05066948, -0.â€¦\n$ Heating_QC_Fair                                       &lt;dbl&gt; 5.2487339, -0.19â€¦\n$ Heating_QC_Good                                       &lt;dbl&gt; -0.4379219, -0.4â€¦\n$ Heating_QC_Poor                                       &lt;dbl&gt; -0.03580575, -0.â€¦\n$ Heating_QC_Typical                                    &lt;dbl&gt; -0.6441494, 1.55â€¦\n$ Central_Air_Y                                         &lt;dbl&gt; 0.265243, 0.2652â€¦\n$ Electrical_FuseF                                      &lt;dbl&gt; -0.1351039, -0.1â€¦\n$ Electrical_FuseP                                      &lt;dbl&gt; -0.05066948, -0.â€¦\n$ Electrical_Mix                                        &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Electrical_SBrkr                                      &lt;dbl&gt; 0.3088293, 0.308â€¦\n$ Electrical_Unknown                                    &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Functional_Maj2                                       &lt;dbl&gt; -0.06209708, -0.â€¦\n$ Functional_Min1                                       &lt;dbl&gt; -0.1506577, -0.1â€¦\n$ Functional_Min2                                       &lt;dbl&gt; -0.1621157, -0.1â€¦\n$ Functional_Mod                                        &lt;dbl&gt; -0.1119485, -0.1â€¦\n$ Functional_Sal                                        &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Functional_Sev                                        &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Functional_Typ                                        &lt;dbl&gt; 0.2814762, 0.281â€¦\n$ Garage_Type_Basment                                   &lt;dbl&gt; -0.1176728, -0.1â€¦\n$ Garage_Type_BuiltIn                                   &lt;dbl&gt; -0.2587311, -0.2â€¦\n$ Garage_Type_CarPort                                   &lt;dbl&gt; -0.07174967, -0.â€¦\n$ Garage_Type_Detchd                                    &lt;dbl&gt; -0.5966212, -0.5â€¦\n$ Garage_Type_More_Than_Two_Types                       &lt;dbl&gt; -0.08549097, -0.â€¦\n$ Garage_Type_No_Garage                                 &lt;dbl&gt; -0.2423742, -0.2â€¦\n$ Garage_Finish_No_Garage                               &lt;dbl&gt; -0.244342, -0.24â€¦\n$ Garage_Finish_RFn                                     &lt;dbl&gt; -0.6229746, -0.6â€¦\n$ Garage_Finish_Unf                                     &lt;dbl&gt; -0.8406517, 1.18â€¦\n$ Garage_Cond_Fair                                      &lt;dbl&gt; -0.1550442, -0.1â€¦\n$ Garage_Cond_Good                                      &lt;dbl&gt; -0.06209708, -0.â€¦\n$ Garage_Cond_No_Garage                                 &lt;dbl&gt; -0.244342, -0.24â€¦\n$ Garage_Cond_Poor                                      &lt;dbl&gt; -0.07469546, -0.â€¦\n$ Garage_Cond_Typical                                   &lt;dbl&gt; 0.3154172, 0.315â€¦\n$ Paved_Drive_Partial_Pavement                          &lt;dbl&gt; 6.9116757, -0.14â€¦\n$ Paved_Drive_Paved                                     &lt;dbl&gt; -3.1286491, 0.31â€¦\n$ Pool_QC_Fair                                          &lt;dbl&gt; -0.02922903, -0.â€¦\n$ Pool_QC_Good                                          &lt;dbl&gt; -0.03580575, -0.â€¦\n$ Pool_QC_No_Pool                                       &lt;dbl&gt; 0.0654701, 0.065â€¦\n$ Pool_QC_Typical                                       &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Fence_Good_Wood                                       &lt;dbl&gt; -0.1975475, -0.1â€¦\n$ Fence_Minimum_Privacy                                 &lt;dbl&gt; -0.3548348, 2.81â€¦\n$ Fence_Minimum_Wood_Wire                               &lt;dbl&gt; -0.06209708, -0.â€¦\n$ Fence_No_Fence                                        &lt;dbl&gt; 0.4902687, -2.03â€¦\n$ Misc_Feature_Gar2                                     &lt;dbl&gt; -0.04135376, -0.â€¦\n$ Misc_Feature_None                                     &lt;dbl&gt; 0.1855737, 0.185â€¦\n$ Misc_Feature_Othr                                     &lt;dbl&gt; -0.03580575, -0.â€¦\n$ Misc_Feature_Shed                                     &lt;dbl&gt; -0.1741936, -0.1â€¦\n$ Misc_Feature_TenC                                     &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Sale_Type_Con                                         &lt;dbl&gt; -0.04135376, -0.â€¦\n$ Sale_Type_ConLD                                       &lt;dbl&gt; -0.09278786, -0.â€¦\n$ Sale_Type_ConLI                                       &lt;dbl&gt; -0.05474101, -0.â€¦\n$ Sale_Type_ConLw                                       &lt;dbl&gt; -0.0462448, -0.0â€¦\n$ Sale_Type_CWD                                         &lt;dbl&gt; -0.06868034, -0.â€¦\n$ Sale_Type_New                                         &lt;dbl&gt; -0.2893471, -0.2â€¦\n$ Sale_Type_Oth                                         &lt;dbl&gt; -0.05066948, -0.â€¦\n$ Sale_Type_VWD                                         &lt;dbl&gt; -0.02066363, -0.â€¦\n$ Sale_Type_WD.                                         &lt;dbl&gt; 0.3890515, 0.389â€¦\n$ Sale_Condition_AdjLand                                &lt;dbl&gt; -0.0654701, -0.0â€¦\n$ Sale_Condition_Alloca                                 &lt;dbl&gt; -0.09735866, -0.â€¦\n$ Sale_Condition_Family                                 &lt;dbl&gt; -0.1266697, -0.1â€¦\n$ Sale_Condition_Normal                                 &lt;dbl&gt; 0.4619311, 0.461â€¦\n$ Sale_Condition_Partial                                &lt;dbl&gt; -0.292798, -0.29â€¦\n\n\nThe code used in this question was written by chatGPT. Use these tools with caution."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "BSMM 8740 Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus.\nClass section\n002\n\n\nClass meetings\nWednesdays, 11:30 AM to 2:20 PM - Odette Building 507\n\n\nInstructor\nDr. Lou Odette\n\n\nCourse Website\nhttps://brightspace.uwindsor.ca\n\n\nTextbook\nThe Elements of Statistical Learning - Data Mining, Inference, and Prediction. Second Edition by Hastie, Tibshirani, and Firedman. Available for free at https://web.stanford.edu/~hastie/ElemStatLearn/\n\n\n\n\n\n\n\n\n\n\n\nOffice hours\nThursday 2:00 PM\nOffice\nZoom\n\n\nTelephone\n-\nEmail\nlodette@uwindsor.ca\n\n\nAcademic Director\nDr. Brent Furneaux\nEmail\nbrent.furneaux@uwindsor.ca:\n\n\nProgram Administrator\nTBD\nEmail\n\n\n\nStudent Experience Coordinator\nSamantha DesRosiers\nEmail\nSamantha.Desrosiers@uwindsor.ca\n\n\nCareer Advisor Coordinator\nClementa Stan\nEmail\ncstan@uwindsor.ca\n\n\nGraduate Secretary\nLisa Power\nEmail\nlisa.power@uwindsor.ca",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#recording",
    "href": "course-syllabus.html#recording",
    "title": "BSMM 8740 Syllabus",
    "section": "Recording",
    "text": "Recording\nRecording or reproduction of class sessions in whole or in part in any format including audio, video, or photographic format is not permitted without prior written permission from the course instructor or presenter. In addition, course materials cannot be shared, distributed, emailed, posted online, or otherwise disseminated or communicated in any form to any other person (including fellow classmates) unless written consent has first been obtained from the instructor or presenter. Course materials include but are not limited to slides, instructor notes, assignment instructions, audio and video recordings of course lectures, and audio and video recordings of software demonstrations.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#calendar-description",
    "href": "course-syllabus.html#calendar-description",
    "title": "BSMM 8740 Syllabus",
    "section": "Calendar Description",
    "text": "Calendar Description\nThis course is the exploration of an analytical framework for method selection and model building to help students develop professional capability in data-based techniques of data analytics. A focus will be placed on comparing and selecting appropriate methodology to conduct advanced statistical analysis and on building predictive modeling in order to create a competitive advantage in business operations with efficient analytical methods and data modeling.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-objectives-expected-outcomes",
    "href": "course-syllabus.html#learning-objectives-expected-outcomes",
    "title": "BSMM 8740 Syllabus",
    "section": "Learning Objectives & Expected Outcomes",
    "text": "Learning Objectives & Expected Outcomes\nThe general objectives of this course are to:\n\nDescribe the concepts and issues associated with analytical framework for method selection and model building\nDescribe the assumptions, limitations, and advantages of various statistical techniques for building predictive models\nDevelop an understanding of various data analytics algorithms\nDemonstrate a capacity for interpersonal interactions",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#master-of-management-competencies",
    "href": "course-syllabus.html#master-of-management-competencies",
    "title": "BSMM 8740 Syllabus",
    "section": "Master of Management Competencies",
    "text": "Master of Management Competencies\n\n\n\n\n\n\n\n\nProgram Competencies\nCourse Competencies\nTested by\n\n\n\n\nApply an evidence-based decision model to evaluate and recommend the best available alternative to resolve an international business problem.\nApply an evidence-based decision model to evaluate and recommend the best available alternative to resolve an international business problem.\nLab Assessments\n\n\nAnalyze both qualitative and quantitative data and findings, distinguishing and evaluating their relevance to the resolution of international business issues.\nAnalyze both qualitative and quantitative data and findings, distinguishing and evaluating their relevance to the resolution of international business issues.\nQuizzes, Midterm Examination, and Final Examination",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "BSMM 8740 Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nWhile there is no official textbook for the course, the following textbooks are useful references for the class material, and additional suggested reading for each lecture can be found in the weekly materials.\n\nThe Elements of Statistical Learning Learning - Data Mining, Inference, and Prediction by Hastie, Tibshirani, and Firedman.\nr Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine Ã‡etinkaya-Rundel and Johanna Hardin\nTidy modeling with R by Max Kuhn and Julia Silge\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-content",
    "href": "course-syllabus.html#course-content",
    "title": "BSMM 8740 Syllabus",
    "section": "Course Content",
    "text": "Course Content\n\n\n\nDate\nTopic\n\n\n\n\nSep 11, 2024\nThe Tidyverse, EDA & Git\n\n\nSep 18, 2024\nThe Recipes Package\n\n\nSep 25, 2024\nRegression Methods\n\n\nOct 02, 2024\nThe TidyModels Package\n\n\nOct 09, 2024\nClassification & Clustering Methods\n\n\nOct 23, 2024\nTime Series Methods\n\n\nOct 30, 2024\nCausality: DAGs\n\n\nNov 06, 2024\nCausality: Methods\n\n\nNov 13, 2024\nMonte Carlo Methods\n\n\nNov 20, 2024\nBayesian Methods\n\n\nNov 27, 2024\nAdvanced Topics\n\n\nDec 04, 2024\nFinal Exam",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#key-dates-for-examsassignments1",
    "href": "course-syllabus.html#key-dates-for-examsassignments1",
    "title": "BSMM 8740 Syllabus",
    "section": "Key Dates For Exams/Assignments1",
    "text": "Key Dates For Exams/Assignments1\n\n\n\nDate\nExam/Assignment\n\n\n\n\nOct 09, 2024\nQuiz1\n\n\nNov 06, 2024\nMidterm Examination\n\n\nNov 20, 2024\nQuiz 2\n\n\nDec 04, 2024\nFinal Examination",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "BSMM 8740 Syllabus",
    "section": "Grading",
    "text": "Grading\n\n\n\n\n%\n\n\n\n\nQuizzes\n20\n\n\nLab Assessments\n30\n\n\nMidterm Examination\n25\n\n\nFinal Examination\n25\n\n\nTOTAL\n100",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading-scale-policies",
    "href": "course-syllabus.html#grading-scale-policies",
    "title": "BSMM 8740 Syllabus",
    "section": "Grading Scale Policies",
    "text": "Grading Scale Policies\nAll course work is to be marked and final grades submitted using the 100% scale beginning September 1, 2013. In accordance with the Senate resolution, instructors are to submit whole numbers (e.g., 88, 76, etc.) as percentages. The following University-wide grade descriptors are in effect and will be printed on the back of transcripts:\n\n\n\nLetter Grade\nPercentage Range\n\n\n\n\nA+\n90-100\n\n\nA\n85-89.9\n\n\nA-\n80-84.9\n\n\nB+\n77-79.9\n\n\nB\n73-76.9\n\n\nB-\n70-72.9\n\n\nC+\n67-69.9\n\n\nC\n63-66.9\n\n\nC-\n60-62.9\n\n\nF\n0-59.9",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#examassignment-descriptions",
    "href": "course-syllabus.html#examassignment-descriptions",
    "title": "BSMM 8740 Syllabus",
    "section": "Exam/Assignment Descriptions",
    "text": "Exam/Assignment Descriptions\nThe use of generative artificial intelligence (AI) tools is strictly prohibited in all course assessments unless explicitly indicated otherwise in guidelines provided by the instructor for an assessment. This includes the use of ChatGPT, Google Gemini, Claude, Jenni, Github Co-pilot, DaLL-E, Midjourney, and all other tools that provide artificial intelligence capabilities. When the use of generative AI is permitted this use must be acknowledged and cited following citation instructions given in the assessment guidelines. Use of generative AI outside of assessment guidelines or without required citation will constitute academic misconduct and may be subject to discipline under Bylaw 31: Academic Integrity. It is the studentâ€™s responsibility to be clear concerning constraints on the use of generative AI for each assessment and to comply with these constraints.\n\nQuizzes\nThe quizzes can consist of true/false, multiple choice, short answer, and essay questions from all material covered before the date of the quiz. When writing quizzes, you must abide by University of Windsor policies governing plagiarism and academic integrity. Quiz submissions may be subjected to review by automated tools to verify their originality.\n\n\nLab Assessments\nLab assessments will require learners to demonstrate the ability to apply methods and techniques to machine learning problems using the R language and explain the steps they followed to solve a problem. Assessments should be started in person during lab sessions. Deadlines for each lab assessment will be posted.\n\n\nMidterm Examination\nThe midterm exam can consist of true/false, multiple choice, short answer, and essay questions from all material covered before the date of the mid-term exam. When writing this exam, you must abide by University of Windsor policies governing plagiarism and academic integrity. Exam submissions may be subjected to review by automated tools to verify their originality.\nFinal Examination\nThe final exam can consist of true/false, multiple choice, short answer, and essay questions covering all course material, including material discussed during lab sessions. When writing this exam, you must abide by University of Windsor policies governing plagiarism and academic integrity. Exam submissions may be subjected to review by automated tools to verify their originality.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#odette-school-of-business-course-policies",
    "href": "course-syllabus.html#odette-school-of-business-course-policies",
    "title": "BSMM 8740 Syllabus",
    "section": "Odette School Of Business Course Policies",
    "text": "Odette School Of Business Course Policies\nPlease refer to the Odette School of Business Course Policies document for specific information on the following subjects. This Course Policies document is available electronically on each course website, on the Brightspace Master of Management Program page at https://brightspace.uwindsor.ca/d2l/le/content/136263/viewContent/654482/View?ou=136263 and also in paper form outside each Area Secretaryâ€™s office on the 4th floor of the Odette building. (Adopted Fall 2009).",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#master-of-management-program-etiquette",
    "href": "course-syllabus.html#master-of-management-program-etiquette",
    "title": "BSMM 8740 Syllabus",
    "section": "Master Of Management Program Etiquette",
    "text": "Master Of Management Program Etiquette\nThe Master of Management program is a culturally inclusive program where it is expected that students, faculty, and staff will recognize, appreciate, and benefit from diversity so as to enhance the learning experience. Promoting a culturally inclusive learning environment encourages individuals to collaborate and develop intercultural respect. The following outlines the protocol for Master of Management students while they are at the University of Windsor:\n\nAll students will communicate in English at all times. It is important for students to continually improve language skills and be inclusive of others from different backgrounds.Â \nStudents will demonstrate respectful behavior toward their peers and professors, regardless of culture, language, values, beliefs, or ideas.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#secondary-data-use-evaluation-focus-groups-and-interviews",
    "href": "course-syllabus.html#secondary-data-use-evaluation-focus-groups-and-interviews",
    "title": "BSMM 8740 Syllabus",
    "section": "Secondary Data Use, Evaluation, Focus Groups And Interviews",
    "text": "Secondary Data Use, Evaluation, Focus Groups And Interviews\nThis course will be evaluated as part of internal or external quality assurance processes and reporting requirements to funding agencies and as research data for scholarly use. As a student in this course your online student data will be used for evaluating the course delivery and your engagement in the various aspects of the course. This will only occur after final grades have been submitted and approved so it will no effect on your grade. This course data provides information about your individual course usage and activity during the time that you are enrolled in the course. Your anonymized, aggregated data may also be used in the future in reports, articles or presentations.\nDuring the final week of the course you may also be invited to participate in further research about the course. If you decide to participate you may be asked to fill out anonymous online questionnaires that solicit your impressions about the course design and student learning in the course.Â  The survey participation is voluntary and no questions of a personal nature will be asked. Your participation will have no effect on your grade and your instructor will not know who participated in the surveys.\nFinally, at the end of the survey you may also be asked if you want to participate in a focus group or interviews after final grades have been assigned to gather yours and other student opinions about specific course delivery methods and technologies used.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#commitment-to-student-wellness",
    "href": "course-syllabus.html#commitment-to-student-wellness",
    "title": "BSMM 8740 Syllabus",
    "section": "Commitment To Student Wellness:",
    "text": "Commitment To Student Wellness:\nFeeling Overwhelmed?\nFrom time to time, students face obstacles that can affect academic performance. If you experience difficulties and need help, it is important to reach out to someone.\nFor help addressing mental or physical health concernsÂ on campus, contact (519) 253-3000:Â \n\nStudent Health Services at ext. 7002 (http://www.uwindsor.ca/studenthealthservices/)\nStudent Counselling Centre at ext. 4616 (http://www.uwindsor.ca/studentcounselling/)\nPeer Support Centre at ext. 4551\n\n24 Hour Support is Available\n\nMy Student Support Program (MySSP) is an immediate and fully confidential 24/7 mental health support that can be accessed for free through chat, online, and telephone. This service is available to all University of Windsor students and offered in over 30 languages. Call: 1-844-451-9700, visit https://keepmesafe.myissp.com/Â Â or download the My SSP app:Â Apple App Store/Google Play.\n\nA full list of on- and off-campus resources is available at Â http://www.uwindsor.ca/wellness.\nShould you need to request alternative accommodation contact your Instructor, Program Administrator, or Director.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#lectures-and-labs",
    "href": "course-syllabus.html#lectures-and-labs",
    "title": "BSMM 8740 Syllabus",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nThe goal of both the lectures and the labs is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to attend all lecture and lab sessions and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature [application exercises] that will be graded. In addition to application exercises will be periodic activities help build a learning community. These will be short, fun activities that will help everyone in the class connect throughout the semester.\nYou are expected to make use of the provided GitHub repository as your central code source platform. Commits to this repository will be used as a metric (one of several) of each team memberâ€™s relative contribution for each project.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#five-tips-for-success",
    "href": "course-syllabus.html#five-tips-for-success",
    "title": "BSMM 8740 Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. Your TAs and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TAs, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If youâ€™re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework and lab.The earlier you start, the better. Itâ€™s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDonâ€™t procrastinate. If something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, and eventually you wonâ€™t know where to begin asking questions. Donâ€™t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours, and let me help you identify a good (re)starting point.\n\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the courseâ€™s policy is that you may make use of any online resources (e.g.Â RStudio Community, StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. On individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 3 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThere is no late work accepted for application exercises, since these are designed to help you prepare for labs and homework.\nThe late work policy for exams will be provided with the exam instructions.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email Dr.Â Lou Odette and our head TA TODO before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns this semester. All course lectures will be recorded and available to enrolled students after class. If you miss a lecture, make sure to watch the recording and review the material before the next class session. Lab time is dedicated to working on your lab assignments and collaborating with your teammates on your project. If you miss a lab session, make sure to communicate with your team about how you can make up your contribution. Given the technologies we use in the course, this is straightforward to do asynchronously. If you know youâ€™re going to miss a lab session and youâ€™re feeling well enough to do so, notify your teammates ahead of time. Overall these policies are put in place to ensure communication between team members, respect for each othersâ€™ time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the universityâ€™s top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have symptoms related to COVID-19, have had a known exposure to COVID-19, or have tested positive for COVID-19. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously or watching a recording of the class.\nNote: If youâ€™ve read this far in the syllabus, email me a picture of your pet if you have one or your favourite meme!",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-academic-dates",
    "href": "course-syllabus.html#important-academic-dates",
    "title": "BSMM 8740 Syllabus",
    "section": "Important Academic Dates",
    "text": "Important Academic Dates\n\n\n\nFALL 2024\n\n\n\n\n\nSeptember 2\nLabour Day (University closed)\n\n\nSeptember 5\nall 2024 classes begin\n\n\nSeptember 18\nAdd/Drop Date\n\n\nOctober 3\nFinancial Drop Date\n\n\nOctober 14\nThanksgiving Day (University closed)\n\n\nOctober 12 - 20\nReading Week (no classes)\n\n\nNovember 13\nVoluntary Withdrawal Date\n\n\nDecember 4\nClasses end\n\n\nDecember 7\nFinal Exams Begin\n\n\nDecember 18\nFinal Exams End\n\n\nDecember 19\nAlternate Final Exam Day\n\n\nDecember 23 â€“ January 1\nUniversity closed â€“ Winter holiday",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "BSMM 8740 Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLab assessments will be part of each class session.â†©ï¸",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  }
]