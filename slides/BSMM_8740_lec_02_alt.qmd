---
title: "EDA and feature engineering"
subtitle: "BSMM8740-2-R-2025F [WEEK - 2]"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2025.github.io/osb](https://bsmm-8740-fall-2025.github.io/osb/)"
logo: "images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

```{r opts, include = FALSE}
options(width = 90)
library(knitr)
opts_chunk$set(comment="", 
               digits = 3, 
               tidy = FALSE, 
               prompt = TRUE,
               fig.align = 'center')
require(magrittr)
require(ggplot2)
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

## Announcements

-   For this week (September 25 - 29), office hours will be on **Friday, from 2:00pm - 4:00pm**.

-   Regular Thursday office hours resume the week of October 02.

## Today's Outline

1.  Complete last week's lab
2.  Review this week's material
    -   Introduction to exploratory data analysis (EDA), and
    -   Feature engineering in the tidyverse
3.  Start (and finish?) this week's lab

::: callout-note
The first two labs are designed to give you practice with the `Tidyverse` tools for manipulating data.
:::

# Exploratory Data Analysis (EDA)

## Exploratory Data Analysis (EDA)

Exploratory data analysis is the process of understanding a new dataset by looking at the data, constructing graphs, tables, and models. We want to understand three aspects:

1.  each individual variable by itself;

2.  each individual variable in the context of other, relevant, variables; and

3.  the data that are not there.

## Exploratory Data Analysis (EDA)

During EDA we want to come to understand the issues and features of the dataset and how this may affect analysis decisions.

We are especially concerned about missing values and outliers.

## Exploratory Data Analysis (EDA)

We are going to perform two broad categories of EDA:

-   Descriptive Statistics, which includes mean, median, mode, inter-quartile range, and so on.
-   Graphical Methods, which includes histogram, density estimation, box plots, and so on.

## Exploratory Data Analysis (EDA)

Descriptive statistics and graphical methods support the following process:

-   Understand the distribution and properties of individual variables.

-   Understand relationships between variables.

-   Understand what is not there.

## EDA: example

Our first dataset is a sample of categorical variables from the [General Social Survey](https://gss.norc.org/Documents/other/2021%20XSEC%20R1%20Methodological%20Primer.pdf), a long-running US survey conducted by the independent research organization NORC at the University of Chicago.

::: aside
execute ?forcats::gss_cat to see the data dictionary
:::

::: {style="font-size: 32px"}
```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "1|2"
dat <- forcats::gss_cat
dat %>% utils::head()
```
:::

## EDA: example

Use `dplyr::glimpse()` to see every column in a data.frame

::: {style="font-size: 32px"}
```{r}
#| echo: true
#| warning: false
#| message: false
dat %>% dplyr::slice_head(n=10) %>% dplyr::glimpse()
```
:::

## EDA: example

Use `dplyr::slice_sample()` to see a random selection of rows in a data.frame

::: {style="font-size: 32px"}
```{r}
#| echo: true
#| warning: false
#| message: false
dat %>% dplyr::slice_sample(n=10)
```
:::

## EDA: example

Most of the columns here are `factors` (categories). Use `forcats::fct_count()` to count the factor entries.

::: {style="font-size: 32px"}
```{r}
#| echo: true
#| warning: false
#| message: false
forcats::fct_count(dat$relig) %>% dplyr::arrange(desc(n))
```
:::

## EDA: example

The base R function `summary()` can be used for key summary statistics of the data.

::: {style="font-size: 32px"}
```{r}
#| echo: true
#| warning: false
#| message: false
dat %>% summary()
```
:::

## EDA: packages for EDA

-   The function `skimr::skim()` gives an enhanced version of base R's `summary()` .

-   Other packages, such as `DataExplorer::` rely more on graphing.

    -   We'll look at a few of the `DataExplorer::` functions next.

## `DataExplorer::introduce`

The function `DataExplorer::introduce` produces a basic description of the data in a `data.frame`.

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::introduce() %>% dplyr::glimpse()
```

## `DataExplorer::plot_intro`

The function `DataExplorer::plot_intro` is a visual version of `DataExplorer::introduce`.

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::plot_intro()
```

## `DataExplorer::plot_missing`

The function `DataExplorer::plot_missing` shows information on missing data visually.

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::plot_missing()
```

## `D_Explorer::profile_missing`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "1|2|3|4|5"
dat %>% DataExplorer::profile_missing() %>% 
  gt::gt('feature') %>% 
  gtExtras::gt_theme_espn() %>% 
  gt::tab_options( table.font.size = gt::px(28) ) %>% 
  gt::as_raw_html()
```

## `DataExplorer::plot_density`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "2|3"
dat %>% 
  DataExplorer::plot_density(
    ggtheme = theme_bw(base_size = 18) + theme(legend.position = "top")
  )
```

## `D_Explorer::plot_histogram`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "2|3"
dat %>% 
  DataExplorer::plot_histogram(
    ggtheme = theme_bw(base_size = 18) + theme(legend.position = "top")
  )
```

## `DataExplorer::plot_bar`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::plot_bar()

```

## EDA: additional `DataExplorer` functions

When the data has more numerical values consider looking at the relationships with the following functions:

-   `DataExplorer::plot_correlation()` creates a correlation heatmap.

```{r}
#| echo: true
#| eval: false
iris %>% DataExplorer::plot_correlation(type = "c")
```

## EDA: additional `DataExplorer` functions

When the data has more numerical values consider looking at the relationships with the following functions:

-   `DataExplorer::plot_scatterplot()` creates a scatterplot for all measurements.

```{r}
#| echo: true
#| eval: false
iris %>% DataExplorer::plot_scatterplot(by = "Species")
```

## EDA: additional `DataExplorer` functions

When the data has more numerical values consider looking at the relationships with the following functions:

-   `DataExplorer::plot_qq()` creates a quantile-quantile plot for each continuous feature.

```{r}
#| echo: true
#| eval: false
iris %>% DataExplorer::plot_qq(by = "Species", ncol = 2L)
```

::: {.callout-note style="font-size: smaller"}
A **Q--Q plot** (**quantile-quantile plot**) is a probability plot, a graphical method for comparing two probability distributions by plotting their *quantiles* against each other.
:::

## EDA: classifying missing data

There are three main categories of missing data

1.  Missing Completely At Random (MCAR);
    -   missing but independent of other measurements
2.  Missing at Random (MAR);
    -   missing in a way related to other measurements
3.  Missing Not At Random (MNAR).
    -   missing as a property of the variable or some other unmeasured variable

## EDA: handling missing data

We can think of a few options for dealing with missing data

1.  Drop observations with missing data.

2.  Impute the mean of observations without missing data.

3.  Use multiple imputation.

::: {.callout-note style="font-size: smaller"}
Multiple imputation involves generating several estimates for the missing values and then averaging the outcomes.
:::

## Example: MCAR or MAR?

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "1|2|3-4|5|7"
dat %>% dplyr::select(partyid) %>% table() %>% tibble::as_tibble() %>% 
  dplyr::left_join(
    dat %>% dplyr::filter(is.na(age)) %>% 
      dplyr::select(na_partyid = partyid) %>% table() %>% tibble::as_tibble()
    , by = c("partyid" = "na_partyid")
  ) %>% 
  dplyr::mutate(pct_na = n.y / n.x)
```

## EDA: missing data

Finally, be aware that how missing data is encoded depends on the dataset

-   R defaults to NA when reading data, in joins, etc.
-   The creator(s) of the dataset may use a different encoding.
-   Missing data can have multiple representations according to semantics of the measurement.
-   Remember that entire measurements can be missing (i.e. from all observations, not just some).

## EDA: summary

-   Understand what the measurements represent and confirm constraints (if any) and suitability of encoding.
-   Make a decision on how to deal with missing data.
-   Understand shape of measurements (may identify an issue or suggest a data transformation)

## EDA: bad data $\rightarrow$ bad results

![](/images/category_fail.png)

# Feature engineering

Feature engineering is the act of converting raw observations into desired features using statistical, mathematical, or machine learning approaches.

## Feature engineering: transformation

for continuous variables (usually the independent variables or covariates):

-   **normalization** (scale values to $[0,1]$)
    -   $X_\text{norm} = \frac{X-X_\text{min}}{X_\text{max}-X_\text{min}}$
-   **standardization** (subtract mean and scale by stdev)
    -   $X_\text{std} = \frac{X-\mu_X}{\sigma_X}$
-   **scaling** (multiply / divide by a constant)
    -   $X_\text{scaled} = K\times X$

## Feature engineering: transformation

Other common transformations:

-   Box-cox: with $\tilde{x}$ the geometric mean of the (**positive**) predictor data ($\tilde{x}=\left(\prod_{i=1}^{n}x_{i}\right)^{1/n}$)

$$
x_i(\lambda) = \left\{ 
\begin{array}{cc}
\frac{x_i^{\lambda}-1}{\lambda\tilde{x}^{\lambda-1}} & \lambda\ne 0\\
\tilde{x}\log x_i & \lambda=0
\end{array}
\right .  
$$

::: {.callout-note style="font-size: smaller"}
Box-cox is an example of a power transform; it is a technique used to stabilize variance, make the data more normal distribution-like.
:::

## Feature engineering: transformation

One last common transformation:

-   logit transformation for bounded target variables (scaled to lie in $[0,1]$)

$$
\text{logit}\left(p\right)=\log\frac{p}{1-p},\;p\in [0,1]
$$

::: {.callout-note style="font-size: small"}
The Logit transform is primarily used to transform binary response data, such as survival/non-survival or present/absent, to provide a continuous value in the range $\left(-\infty,\infty\right)$, where p is the proportion of each sample that is 1 (or 0)
:::

## Feature engineering: transformation

Why normalize or standardize?

-   variation in the range of feature values can lead to biased model performance or difficulties during the learning process, particularly in distance-based algorithms.
    -   e.g. income and age
-   reduce the impact of outliers
-   make results more explainable

## Feature engineering: transformation

for continuous variables (usually the target variables):

-   **transformation** (arithmetic, basis functions, polynomials, splines, differencing)
    -   $y = \log(y),\sqrt{y},\frac{1}{y}$, etc.
    -   $y = \sum_i \beta_i\text{f}_i(x)\;\text{s.t.}\;0=\int\text{f}_i(x)\text{f}_j(x)\; \forall i\ne j$
    -   $y = \beta_0+\beta_1 x+\beta_2 x^2+\beta_3 x^3+\ldots$
    -   $y = \beta_0+\beta_1 x_1+\beta_2 x_2+\beta_3 x_1 x_2+\ldots$
    -   $y'_i = y_i-y_{i-1}$

## Feature engineering: transformation

for categorical variables (either target or explanatory variables):

-   binning / bucketing
    -   represent a numerical value as a categorical value
-   categorical$\rightarrow$ordinal and ordinal$\rightarrow$categorical

for date variables:

-   timestamp$\rightarrow$date or date part

## Feature engineering: transformation

Why transform?

-   it can make your model perform better

    -   e.g. $\log$ transform makes exponential data linear, and log-Normal data Gaussian
    -   $\log$ transforms also make multiplicative models additive
    -   e.g. polynomials, basis functions and splines help model non-linearities in data

## Feature engineering: creation

-   outliers (due to data entry, measurement/experiment, intentional errors)
    -   outliers can be identified by quantile methods (Gaussian data)
    -   outliers can be removed, treated as missing, or capped
-   lag variables (either target or explanatory variables)
    -   useful in time series models, e.g. $y_t,y_{t-1},\ldots y_{t-n}$

## Feature engineering: creation

-   binning / bucketing
    -   represent numerical as categorical and vice versa
-   interval and ratio levels

## Feature engineering fails

![](/images/category_fail.png)

## Feature engineering: summary

-   requires an advanced technical skill set

-   requires domain expertise

-   is time-consuming and resource intensive

-   different analytics algorithms require different feature engineering

## Recap

-   Today we reviewed key elements of exploratory data analysis, the process that helps us understand the data we have and evaluate how it can help us solve the business problems we are interested in.

-   We also reviewed feature engineering - methods to we can use to facilitate our analysis and make our results more interpretable.
