<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.39">

  <meta name="author" content="L.L. Odette">
  <title>BSMM-8740 - Fall 2024 – Regression methods</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-a8986e45384d4e2047f3d84b21c5a23c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Regression methods – BSMM-8740 - Fall 2024">
<meta property="og:description" content="BSMM8740-2-R-2024F [WEEK - 3]">
<meta property="og:site_name" content="BSMM-8740 - Fall 2024">
<meta name="twitter:title" content="Regression methods – BSMM-8740 - Fall 2024">
<meta name="twitter:description" content="BSMM8740-2-R-2024F [WEEK - 3]">
<meta name="twitter:image" content="https://bsmm-8740-fall-2024.github.io/osb/slides/images/twitter-card.png">
<meta name="twitter:creator" content="@lodette">
<meta name="twitter:card" content="summary_large_image">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Regression methods</h1>
  <p class="subtitle">BSMM8740-2-R-2024F [WEEK - 3]</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
L.L. Odette 
</div>
</div>
</div>

</section>
<section id="recap-of-last-lecture" class="slide level2">
<h2>Recap of last lecture</h2>
<ul>
<li><p>Last time we worked with the <code>recipes</code> package to develop workflows for pre-processing our data.</p></li>
<li><p>Today we look at regression methods we might apply to understand relationships between measurements in our data.</p></li>
</ul>
</section>
<section>
<section id="linear-regression" class="title-slide slide level1 center">
<h1>Linear regression</h1>

</section>
<section id="linear-regression-models" class="slide level2">
<h2>Linear regression models</h2>
<p>In the simple linear regression (SLR) model, we have <span class="math inline">\(N\)</span> observations of a single outcome variable <span class="math inline">\(Y\)</span> along with <span class="math inline">\(D\)</span> predictor (aka co-variate) variables <span class="math inline">\(\mathbf{x}\)</span> where the likelihood<sup>1</sup> of observing <span class="math inline">\(Y=y\)</span> is conditional on the predictor values <span class="math inline">\(x\)</span> and parameters <span class="math inline">\(\theta=\{\beta,\sigma^2\}\)</span>:</p>
<p><span class="math display">\[
\pi\left(Y=y|\mathbf{x,\theta}\right)=\mathscr{N}\left(\left.y\right|\mu(\mathbf{x};\beta),\sigma^{2}\right)
\]</span></p>
<aside><ol class="aside-footnotes"><li id="fn1"><p>In SLR models we assume a Normal (Gaussian) probability model for the data generation process. For non-Normal data generation processes we use generalized linear models, which we’ll discuss later.</p></li></ol></aside></section>
<section id="linear-regression-models-1" class="slide level2">
<h2>Linear regression models</h2>
<p>In the SLR model, <span class="math inline">\(\mathscr{N}\left(\left.y\right|\mu(\mathbf{x};\beta),\sigma^{2}\right)\)</span> is a Normal probability density with mean <span class="math inline">\(\mu(\mathbf{x};\beta)\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, where <span class="math inline">\(\sigma^2\)</span> is a constant and the mean is a function of the predictors <span class="math inline">\(\mathbf{x}\)</span> and a vector of parameters <span class="math inline">\(\beta\)</span>.</p>
<p>The mean function <span class="math inline">\(\mu(\mathbf{x};\beta)\)</span> is often assumed to be continuous, i.e.&nbsp;a small change in the predictors implies a small change in the outcome. In addition, it is often convenient to decompose the mean function into a sum of simpler functions, e.g.&nbsp;polynomial functions (like straight lines, parabolas, and more).</p>
</section>
<section id="taylor-series" class="slide level2">
<h2>Taylor Series</h2>
<p>The decomposition of a function <span class="math inline">\(f\)</span> of a single variable <span class="math inline">\(x\)</span> into a sum of simpler polynomial functions is called a Taylor series and is defined as follows:</p>
<div style="font-size: xx-large">
<p><span class="math display">\[
f(x;x_0)=\sum_{n=0}\beta_n(x-x_0)^n=\beta_0+\beta_1(x-x_0)+\beta_2(x-x_0)^2+\beta_3(x-x_0)^3+\ldots
\]</span></p>
</div>
<p>where <span class="math inline">\(\frac{d^nf(x)}{dx^n}|_{x=x_0} \equiv f^{(n)}(x_0) = n!\beta_n\;\rightarrow \beta_n=\frac{1}{n!}f^{(n)}(x_0)\)</span></p>
</section>
<section id="taylor-series-1" class="slide level2">
<h2>Taylor Series</h2>
<div style="font-size: x-large">
<p><span class="math display">\[
f(x;x_0)=\sum_n\beta_n(x-x_0)^n=\beta_0+\beta_1(x-x_0)+\beta_2(x-x_0)^2+\beta_3(x-x_0)^3+\ldots
\]</span></p>
</div>
<p>We can use the following constructive proof to find the coefficients in the series:</p>
<div style="font-size: x-large">
<ol type="1">
<li><span class="math inline">\(0^{th}\)</span> derivative at <span class="math inline">\(x=x_0\)</span>: <span class="math inline">\(f(x_0) = \beta_0\)</span></li>
<li><span class="math inline">\(1^{st}\)</span> derivative at <span class="math inline">\(x=x_0\)</span>: <span class="math inline">\(f'(x_0) = \beta_1\)</span></li>
<li><span class="math inline">\(2^{nd}\)</span> derivative at <span class="math inline">\(x=x_0\)</span>: <span class="math inline">\(f''(x_0) = 2\beta_2\)</span></li>
<li><span class="math inline">\(3^{rd}\)</span> derivative at <span class="math inline">\(x=x_0\)</span>: <span class="math inline">\(f'''(x_0) = 6\beta_3\)</span></li>
<li><span class="math inline">\(n^{th}\)</span> derivative at <span class="math inline">\(x=x_0\)</span>: <span class="math inline">\(f^{(n)}(x_0) = n!\beta_n\)</span></li>
</ol>
</div>
</section>
<section id="linear-regression-models-2" class="slide level2">
<h2>Linear regression models</h2>
<p>Similarly, when the number of variables is <span class="math inline">\(D=2\)</span> the Taylor series is (writing <span class="math inline">\(f_{x}\equiv\frac{\partial f}{\partial x}\)</span>, <span class="math inline">\(f_{y}\equiv\frac{\partial f}{\partial y}\)</span>, <span class="math inline">\(f_{x,y}\equiv\frac{\partial^2 f}{\partial x,\partial x}\)</span> and so on):</p>
<div style="font-size: xx-large">
<p><span class="math display">\[
\begin{align*}
f(x,y;x_0,y_0) &amp; =f(x_{0},y_{0})+f_{x}(x_{0},y_{0})(x-x_{0})+f_{y}(x_{0},y_{0})(y-y_{0})\\
&amp; = + \frac{1}{2}f_{x,x}(x_{0},y_{0})(x-x_{0})^{2}+\frac{1}{2}f_{y,y}(x_{0},y_{0})(y-y_{0})^{2}\\
&amp; = + \frac{1}{2}f_{x,y}(x_{0},y_{0})(x-x_{0})(y-y_{0})+\ldots
\end{align*}
\]</span></p>
<p>which decomposes a function of two variables into a sum of simpler functions, e.g.&nbsp;polynomial functions (like 2-D straight lines, parabolas, and more).</p>
</div>
</section>
<section id="linear-regression-models-3" class="slide level2">
<h2>Linear regression models</h2>
<p>For one co-variate, if the mean function is smooth (i.e.&nbsp;not changing rapidly with the co-variate) then <span class="math inline">\(\mu^{(n)}\)</span> will be decreasing in <span class="math inline">\(n\)</span>, and furthermore <span class="math inline">\(\beta_n\)</span> decreases as <span class="math inline">\(1/n!\)</span>, so SLR models in one co-variate typically use only the first two or at most three <span class="math inline">\(\beta\)</span> coefficients.</p>
<p>Thus the likelihood of observing <span class="math inline">\(Y=y\)</span> is conditional on the predictor values <span class="math inline">\(x\)</span> and parameters <span class="math inline">\(\theta=\{\beta_0,\beta_1,\sigma^2\}\)</span>:</p>
<p><span class="math inline">\(\theta=\{\beta_{0},\mathbf{\beta},\sigma^{2}\}\)</span> are the <em>parameters</em> of the model, where <span class="math inline">\(\beta_0\)</span> is a constant and <span class="math inline">\(\beta_1\)</span> is the co-variate <em>weight</em> or <em>regression</em> coefficient.</p>
</section>
<section id="linear-regression-models-4" class="slide level2">
<h2>Linear regression models</h2>
<p>For multiple co-variates, If the mean function is smooth (i.e.&nbsp;not changing rapidly with the co-variates) then under similar (reasonable) assumptions on the differentials, it is common to see SLR models using only the first order coefficients, i.e.&nbsp;a constant and one coefficient for each co-variate.</p>
</section>
<section id="linear-regression-models-5" class="slide level2">
<h2>Linear regression models</h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>It is common to add the unit constant to the covariate vector <span class="math inline">\(x=(1,x_1,x_2,\ldots)\)</span> so that the coefficient vector is <span class="math inline">\(\mathbf{\beta}=(\beta_0,\beta_{x_1},\beta_{x_2},\ldots)\)</span>, and the model can be expressed<sup>1</sup> as a vector equation: <span class="math inline">\(y=\mathbf{\beta}\cdot \mathbf{x}\)</span>.</p>
</div>
</div>
</div>
<aside><ol class="aside-footnotes"><li id="fn2"><p>sometime written as <span class="math inline">\(\beta'\mathbf{x}\)</span></p></li></ol></aside></section>
<section id="linear-regression-models-6" class="slide level2">
<h2>Linear regression models</h2>
<p>Recall that the one dimensional Normal/Gaussian probability density (aka likelihood) is:</p>
<p><span class="math display">\[
\mathscr{N}\left(x;\mu,\sigma^2\right)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{1}{2}\frac{x-\mu}{\sigma^2}}
\]</span></p>
<p>The likelihood of multiple observations is the product of the likelihoods for each observation.</p>
</section>
<section id="linear-regression-models-7" class="slide level2">
<h2>Linear regression models</h2>
<p>To fit the 1D linear regression model given <span class="math inline">\(N\)</span> data samples, we minimize the negative log-likelihood (NLL) on the training set.</p>
<div style="font-size: xx-large">
<p><span class="math display">\[
\begin{align*}
\text{NLL}\left(\beta,\sigma^{2}\right) &amp; =-\sum_{n=1}^{N}\log\left[\left(\frac{1}{2\pi\sigma^{2}}\right)^{\frac{1}{2}}\exp\left(-\frac{1}{2\sigma^{2}}\left(y_{n}-\beta'x_{n}\right)^{2}\right)\right]\\
&amp; =\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}\left(y_{n}-\hat{y}_{n}\right)^{2}-\frac{N}{2}\log\left(2\pi\sigma^{2}\right)
\end{align*}
\]</span></p>
</div>
<p>where the predicted response is <span class="math inline">\(\hat{y}\equiv\beta'x_{n}\)</span>. This is also the maximum likelihood estimation (MLE) method.</p>
</section>
<section id="linear-regression-models-8" class="slide level2">
<h2>Linear regression models</h2>
<p>Minimizing the NLL by minimizing the residual sum of squares (RSS) is the same as minimizing</p>
<ul>
<li>the <strong>mean squared error</strong> <span class="math inline">\(\text{MSE}\left(\beta\right) = \frac{1}{N}\text{RSS}\left(\beta\right)\)</span></li>
<li>the <strong>root mean squared error</strong> <span class="math inline">\(\text{RMSE}\left(\beta\right) = \sqrt{\text{MSE}\left(\beta\right)}\)</span></li>
</ul>
</section>
<section id="aside-empirical-risk-minimization" class="slide level2">
<h2>Aside: empirical risk minimization</h2>
<p>The MLE can be generalized by replacing the NLL (<span class="math inline">\(\ell\left(y_{n},\theta;x_{n}\right)=-\log\pi\left(y_n|x_n,\theta\right)\)</span>) with any other loss function to get</p>
<p><span class="math display">\[
\mathscr{L}\left(\theta\right)=\frac{1}{N}\sum_{n=1}^{N}\ell\left(y_{n},\theta;x_{n}\right)
\]</span></p>
<p>This is known as the empirical risk minimization (ERM) - the expected loss taken with respect to the empirical distribution.</p>
</section>
<section id="linear-regression-models-9" class="slide level2">
<h2>Linear regression models</h2>
<p>Focusing on just the coefficients <span class="math inline">\(\beta\)</span>, the minimum NLL is (up to a constant) the minimum of the residual sum of squares (RSS)<sup>1</sup> with coefficient estimates <span class="math inline">\(\hat\beta\)</span> :</p>
<p><span class="math display">\[
\begin{align*}\text{RSS}\left(\beta\right) &amp; =\frac{1}{2}\sum_{n=1}^{N}\left(y_{n}-\beta'x_{n}\right)^{2}=\frac{1}{2}\left\Vert y_{n}-\beta'x_{n}\right\Vert ^{2}\\
&amp; =\frac{1}{2}\left(y_{n}-\beta'x_{n}\right)'\left(y_{n}-\beta'x_{n}\right)\\
\\
\end{align*}
\]</span></p>
<aside><ol class="aside-footnotes"><li id="fn3"><p>i.e.&nbsp;minimizing the squared prediction error, aka ordinary least squares.</p></li></ol></aside></section>
<section id="linear-regression-models-10" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols">Ordinary least squares (OLS)</h4>
<p>Note that, given the assumption that the data generation process is Normal/Gaussian, we can write our regression equation in terms of individual observations as</p>
<p><span class="math display">\[
y_i=\beta_0+\beta_1 x_i + u_i
\]</span></p>
<p>where error term <span class="math inline">\(u_i\)</span> is a sample from <span class="math inline">\(\mathscr{N}\left(0,\sigma^{2}\right)\)</span> which in turn implies <span class="math inline">\(\mathbb{E}\left[u\right]=0;\;\mathbb{E}\left[\left.u\right|x\right]=0\)</span></p>
<p>The independence of the covariates and the errors/residuals is a testable assumption.</p>
</section>
<section id="linear-regression-models-11" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-1">Ordinary least squares (OLS)</h4>
<p>It follows independence of the covariates and the errors that</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left[y-\beta_{0}-\beta_{1}x\right] &amp; =0\\
\mathbb{E}\left[x\left(y-\beta_{0}-\beta_{1}x\right)\right] &amp; =0
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-12" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-2">Ordinary least squares (OLS)</h4>
<p>Writing these equations in terms of our samples (where <span class="math inline">\(\hat{\beta}_{0}, \hat{\beta}_{1}\)</span> are our coefficient estimates)</p>
<p><span class="math display">\[
\begin{align*}
\frac{1}{N}\sum_{i-1}^{N}y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i} &amp; =0\\
\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right) &amp; =0
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-13" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-3">Ordinary least squares (OLS)</h4>
<p>From the first equation</p>
<p><span class="math display">\[
\begin{align*}
\bar{y}-\hat{\beta}_{0}-\hat{\beta}_{1}\bar{x} &amp; =0\\
\bar{y}-\hat{\beta}_{1}\bar{x} &amp; =\hat{\beta}_{0}
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-14" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-4">Ordinary least squares (OLS)</h4>
<p>Substituting the expression for <span class="math inline">\(\hat{\beta}_{0}\)</span> in the independence equation</p>
<div style="font-size: xx-large">
<p><span class="math display">\[
\begin{align*}
\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(y_{i}-\left(\bar{y}-\hat{\beta}_{1}\bar{x}\right)-\hat{\beta}_{1}x_{i}\right) &amp; =0\\
\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(y_{i}-\bar{y}\right) &amp; =\hat{\beta}_{1}\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(\bar{x}-x_{i}\right)\\
\frac{1}{N}\sum_{i-1}^{N}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right) &amp; =\hat{\beta}_{1}\frac{1}{N}\sum_{i-1}^{N}\left(\bar{x}-x_{i}\right)^2
\end{align*}
\]</span></p>
</div>
</section>
<section id="linear-regression-models-15" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-5">Ordinary least squares (OLS)</h4>
<p>So as long as <span class="math inline">\(\sum_{i-1}^{N}\left(\bar{x}-x_{i}\right)^2\ne 0\)</span></p>
<p><span class="math display">\[
\begin{align*}
\hat{\beta}_{1} &amp; =\frac{\sum_{i-1}^{N}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i-1}^{N}\left(\bar{x}_{i}-x_{i}\right)^2}\\
&amp; =\frac{\text{sample covariance}(x_{i}y_{i})}{\text{sample variance}(x_{i})}
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-16" class="slide level2">
<h2>Linear regression models</h2>
<p>Similarly, in the vector equation, the minimum of the RSS is solved by (assuming <span class="math inline">\(N&gt;D\)</span>):</p>
<p><span class="math display">\[
\hat{\mathbf{\beta}}_{OLS}=\left(X'X\right)^{-1}\left(X'Y\right) = \frac{\text{cov}(X,Y)}{\text{var}(X)}
\]</span></p>
<p>There are algorithmic issues with computing <span class="math inline">\(\left(X'X\right)^{-1}\)</span> though, so we could instead start with <span class="math inline">\(X\beta=y\)</span> and write <span class="math inline">\(\hat{\mathbf{\beta}}_{OLS}=X^{-1}y\)</span> .</p>
</section>
<section id="linear-regression-algorithms" class="slide level2">
<h2>Linear regression algorithms</h2>
<p>Computing the inverse of <span class="math inline">\(X'X\)</span> directly, while theoretically possible, can be numerically unstable.</p>
<p>In R, the <span class="math inline">\(QR\)</span> decomposition is used to solve for <span class="math inline">\(\beta\)</span>. Let <span class="math inline">\(X=QR\)</span> where <span class="math inline">\(Q'Q=I\)</span> and write:</p>
<p><span class="math display">\[
\begin{align*}
(QR)\beta &amp; = y\\
Q'QR\beta &amp; = Q'y\\
\beta &amp; = R^{-1}(Q'y)
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(R\)</span> is upper triangular, the last equation can be solved by back-substitution.</p>
</section>
<section id="linear-regression-algorithms-1" class="slide level2">
<h2>Linear regression algorithms</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1" data-code-line-numbers="1|2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a></a><span class="sc">&gt;</span> A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>, <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>), <span class="at">nrow=</span><span class="dv">3</span>)</span>
<span id="cb1-2"><a></a><span class="sc">&gt;</span> QR <span class="ot">&lt;-</span> <span class="fu">qr</span>(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="panel-tabset">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1">Q</a></li><li><a href="#tabset-1-2">R</a></li><li><a href="#tabset-1-3">A</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a></a><span class="sc">&gt;</span> Q <span class="ot">&lt;-</span> <span class="fu">qr.Q</span>(QR); Q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]       [,2]          [,3]
[1,] -0.1825742 -0.4082483 -8.944272e-01
[2,] -0.3651484 -0.8164966  4.472136e-01
[3,] -0.9128709  0.4082483  2.593051e-16</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a></a><span class="sc">&gt;</span> Q <span class="sc">%*%</span> <span class="fu">t</span>(Q) <span class="sc">|&gt;</span> <span class="fu">round</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1</code></pre>
</div>
</div>
</div>
<div id="tabset-1-2">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a></a><span class="sc">&gt;</span> R <span class="ot">&lt;-</span> <span class="fu">qr.R</span>(QR); R</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]      [,2]      [,3]
[1,] -5.477226 -7.302967 -4.381780
[2,]  0.000000 -1.632993 -2.449490
[3,]  0.000000  0.000000 -1.341641</code></pre>
</div>
</div>
</div>
<div id="tabset-1-3">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a></a><span class="sc">&gt;</span> Q <span class="sc">%*%</span> R</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    2    4    3
[3,]    5    6    3</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="linear-regression-algorithms-2" class="slide level2">
<h2>Linear regression algorithms</h2>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" data-code-line-numbers="1|2-3|5-6|8-9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a></a><span class="sc">&gt;</span> <span class="co"># A linear system of equations y = Ax</span></span>
<span id="cb10-2"><a></a><span class="er">&gt;</span> <span class="fu">cat</span>(<span class="st">"matrix A</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb10-3"><a></a><span class="sc">&gt;</span> A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>, .<span class="dv">5</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">4</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="at">nrow=</span><span class="dv">3</span>); A</span>
<span id="cb10-4"><a></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">"vector x</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb10-5"><a></a><span class="sc">&gt;</span> x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>); x</span>
<span id="cb10-6"><a></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">"vector y</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb10-7"><a></a><span class="sc">&gt;</span> y <span class="ot">&lt;-</span> A <span class="sc">%*%</span> x ; y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell quarto-layout-panel" data-layout-nrow="3" data-layout-align="center">
<div class="quarto-layout-row">
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>matrix A</code></pre>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>     [,1] [,2] [,3]
[1,]    3  2.0   -1
[2,]    2 -2.0    4
[3,]   -1  0.5   -1</code></pre>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>vector x</code></pre>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>[1]  1 -2 -2</code></pre>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>vector y</code></pre>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>     [,1]
[1,]    1
[2,]   -2
[3,]    0</code></pre>
</div>
</div>
</div>
</section>
<section id="linear-regression-algorithms-3" class="slide level2">
<h2>Linear regression algorithms</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb17" data-code-line-numbers="1-4|6-7|9-10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a></a><span class="sc">&gt;</span> <span class="co"># Compute the QR decomposition of A</span></span>
<span id="cb17-2"><a></a><span class="er">&gt;</span> QR <span class="ot">&lt;-</span> <span class="fu">qr</span>(A)</span>
<span id="cb17-3"><a></a><span class="sc">&gt;</span> Q <span class="ot">&lt;-</span> <span class="fu">qr.Q</span>(QR)</span>
<span id="cb17-4"><a></a><span class="sc">&gt;</span> R <span class="ot">&lt;-</span> <span class="fu">qr.R</span>(QR)</span>
<span id="cb17-5"><a></a><span class="sc">&gt;</span> </span>
<span id="cb17-6"><a></a><span class="er">&gt;</span> <span class="co"># Compute b=Q'y</span></span>
<span id="cb17-7"><a></a><span class="er">&gt;</span> b <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(Q, y); b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]
[1,]  0.2672612
[2,]  2.1472519
[3,] -0.5638092</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19" data-code-line-numbers="1-4|6-7|9-10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a></a><span class="sc">&gt;</span> <span class="co"># Solve the upper triangular system Rx=b</span></span>
<span id="cb19-2"><a></a><span class="er">&gt;</span> <span class="fu">backsolve</span>(R, b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]
[1,]    1
[2,]   -2
[3,]   -2</code></pre>
</div>
</div>
</section>
<section id="collinearity" class="slide level2">
<h2>Collinearity</h2>
<ul>
<li><p>One of the important assumptions of the classical linear regression models is that there is no exact collinearity among the regressors.</p></li>
<li><p>While high correlation between regressors is a necessary indicator of the collinearity problem, a direct linear relationship beween regressors is sufficient.</p></li>
</ul>
</section>
<section id="collinearity-1" class="slide level2">
<h2>Collinearity</h2>
<ul>
<li><p>Data collection methods, constraints on the fitted regression model, model specification error, an overdefined model, may be some potential sources of multicollinearity.</p></li>
<li><p>In other cases it is an artifact caused by creating new predictors from other predictors.</p></li>
</ul>
</section>
<section id="collinearity-2" class="slide level2">
<h2>Collinearity</h2>
<p>The problem of collinearity has potentially serious effect on the regression estimates such as:</p>
<ul>
<li>implausible coefficient signs,</li>
<li>impossible inversion of matrix <span class="math inline">\(X'X\)</span> as it becomes near or exactly singular,</li>
<li>large magnitude of coefficients in absolute value,</li>
<li>large variance or standard errors with wider confidence intervals.</li>
</ul>
</section>
<section id="collinearity-3" class="slide level2">
<h2>Collinearity</h2>
<p>Mitigating Collinearity:</p>
<ul>
<li>Remove Highly Correlated Variables: If two variables are highly correlated, consider removing one of them.</li>
<li>Combine Variables: Create a new variable that combines the collinear variables</li>
<li>Principal Component Analysis (PCA): Use PCA to transform the correlated variables into a smaller set of uncorrelated variables.</li>
</ul>
</section>
<section id="bias-vs-variance" class="slide level2">
<h2>Bias vs Variance</h2>
<p>We introduced truncated Taylor series approximations to motivate using simplified models of the mean function when using regression.</p>
<p>But bias is the error introduced by approximating a real-world problem, which may be complex, by a simplified model.</p>
<p>So to reduce bias, why not include more Taylor series terms, or more covariates in a first-order model?</p>
</section>
<section id="bias-vs-variance-1" class="slide level2">
<h2>Bias vs Variance</h2>
<p>Note that for random variables in general and Gaussian random variables in particular</p>
<ul>
<li>the mean of the sum of random variables is the sum of the means of the random variables.</li>
<li>the variance of the sum of random variables is the sum of the variances of the random variables.</li>
</ul>
<p>So adding more terms or more covariates may reduce bias by improving the mean estimate, but will certainly increase the variance of the estimate.</p>
</section>
<section id="bias-b-vs-variance-v-tradeoffs" class="slide level2">
<h2>Bias (B) vs Variance (V) tradeoffs</h2>
<div class="panel-tabset">
<ul id="tabset-2" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-2-1"><span class="math inline">\(\downarrow\)</span> B <span class="math inline">\(\uparrow\)</span> V</a></li><li><a href="#tabset-2-2"><span class="math inline">\(\uparrow\)</span> B <span class="math inline">\(\downarrow\)</span> V</a></li><li><a href="#tabset-2-3"><span class="math inline">\(-\)</span> B <span class="math inline">\(-\)</span> V</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1">
<p><strong>Low Bias and High Variance</strong></p>
<ul>
<li>A model with low bias fits the training data very closely, capturing all the details and fluctuations.</li>
<li>This leads to overfitting, where the model performs well on the training data but poorly on new data because it has learned the noise in the training data as if it were a signal.</li>
</ul>
</div>
<div id="tabset-2-2">
<p><strong>High Bias and Low Variance</strong></p>
<ul>
<li><p>A model with high bias makes oversimplified assumptions about the data, ignoring relevant complexities.</p></li>
<li><p>This leads to underfitting, where the model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and new data.</p></li>
</ul>
</div>
<div id="tabset-2-3">
<p><strong>Balancing Bias and Variance</strong></p>
<ul>
<li>The goal is to find a sweet spot where the model is complex enough to capture the underlying patterns (low bias) but simple enough not to capture the noise (low variance).</li>
<li>Achieving this balance ensures the model generalizes well to new data, providing good performance overall.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="bias-b-vs-variance-v-examples" class="slide level2">
<h2>Bias (B) vs Variance (V) examples</h2>
<div class="panel-tabset">
<ul id="tabset-3" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-3-1">Underfitting</a></li><li><a href="#tabset-3-2">Overfitting</a></li><li><a href="#tabset-3-3">Balanced</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1">
<p><strong>Underfitting (High Bias, Low Variance)</strong></p>
<ul>
<li>Suppose you’re predicting house prices using just the size of the house (one variable) in a linear regression model.</li>
<li>If the true relationship is complex (e.g., non-linear, involving multiple factors), this simple model will have high bias and underfit the data, missing important patterns.</li>
</ul>
</div>
<div id="tabset-3-2">
<p><strong>Overfitting (Low Bias, High Variance)</strong></p>
<ul>
<li>Now, imagine you use a very complex model, like a high-degree polynomial regression, that uses many variables and interactions.</li>
<li>This model fits the training data very well but captures noise as well. When applied to new data, its performance drops because it has learned patterns that don’t generalize (high variance).</li>
</ul>
</div>
<div id="tabset-3-3">
<p><strong>Balanced Model</strong></p>
<ul>
<li>A balanced model might use a moderate number of relevant variables and a reasonable complexity (like a linear regression with interaction terms or a low-degree polynomial).</li>
<li>This model captures the essential patterns without fitting the noise, resulting in good generalization to new data.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="bias-vs-variance-2" class="slide level2">
<h2>Bias vs Variance</h2>
<p>The following regression models techniques with the higher variance that follows from a large number of covariates by adding a bit of bias. The variance is reduced by penalizing covariate coefficients, shrinking then towards zero.</p>
<p>The resulting simpler models may not fully capture the patterns in the data, thus underfitting the data.</p>
</section>
<section id="ridge-regression" class="slide level2">
<h2>Ridge Regression</h2>
<div style="font-size: xx-large">
<p>Ridge regression is an example of a penalized regression model; in this case the magnitude of the weights are penalized by adding the <span class="math inline">\(\ell_2\)</span> norm of the weights to the loss function. In particular, the ridge regression weights are:</p>
<p><span class="math display">\[
\hat{\beta}_{\text{ridge}}=\arg\!\min\text{RSS}\left(\beta\right)+\lambda\left\Vert \beta\right\Vert _{2}^{2}
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the strength of the penalty term.</p>
<p>The Ridge objective function is</p>
<p><span class="math display">\[
\mathscr{L}\left(\beta,\lambda\right)=\text{NLL}+\lambda\left\Vert \beta\right\Vert_2^2
\]</span></p>
</div>
</section>
<section id="ridge-regression-1" class="slide level2">
<h2>Ridge Regression</h2>
<p>The solution is:</p>
<p><span class="math display">\[
\begin{align*}
\hat{\mathbf{\beta}}_{ridge} &amp; =\left(X'X-\lambda I_{D}\right)^{-1}\left(X'Y\right)\\
&amp; =\left(\sum_{n}x_{n}x'_{n}+\lambda I_{D}\right)^{-1}\left(\sum_{n}y_{n}x_{n}\right)
\end{align*}
\]</span></p>
</section>
<section id="ridge-regression-2" class="slide level2">
<h2>Ridge Regression</h2>
<p>As for un-penalized linear regression, using matrix inversion to solve for <span class="math inline">\(\hat{\mathbf{\beta}}_{ridge}\)</span> can be a bad idea. The QR transformation can be used here, however, ridge regression is often used when <span class="math inline">\(D&gt;N\)</span>, in which case the SVD transformation is faster.</p>
</section>
<section id="ridge-regression-example" class="slide level2">
<h2>Ridge Regression Example</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb21" data-code-line-numbers="2|5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a></a><span class="sc">&gt;</span> <span class="co">#define response variable</span></span>
<span id="cb21-2"><a></a><span class="er">&gt;</span> y <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">pull</span>(hp)</span>
<span id="cb21-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb21-4"><a></a><span class="er">&gt;</span> <span class="co">#define matrix of predictor variables</span></span>
<span id="cb21-5"><a></a><span class="er">&gt;</span> x <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(mpg, wt, drat, qsec) <span class="sc">%&gt;%</span> <span class="fu">data.matrix</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ridge-regression-example-1" class="slide level2">
<h2>Ridge Regression Example</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb22" data-code-line-numbers="1-2|4-5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a></a><span class="sc">&gt;</span> <span class="co"># fit ridge regression model</span></span>
<span id="cb22-2"><a></a><span class="er">&gt;</span> model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb22-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb22-4"><a></a><span class="er">&gt;</span> <span class="co"># get coefficients when lambda = 7.6</span></span>
<span id="cb22-5"><a></a><span class="er">&gt;</span> <span class="fu">coef</span>(model, <span class="at">s =</span> <span class="fl">7.6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>5 x 1 sparse Matrix of class "dgCMatrix"
                      s1
(Intercept) 477.91365858
mpg          -3.29697140
wt           20.31745927
drat         -0.09524492
qsec        -18.48934710</code></pre>
</div>
</div>
</section>
<section id="ridge-regression-example-2" class="slide level2">
<h2>Ridge Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>glmnet example</summary>
<div class="sourceCode cell-code" id="cb24" data-code-line-numbers="1-2|4-5|7-8|9|10|11|12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a></a><span class="sc">&gt;</span> <span class="co"># perform k-fold cross-validation to find optimal lambda value</span></span>
<span id="cb24-2"><a></a><span class="er">&gt;</span> cv_model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb24-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb24-4"><a></a><span class="er">&gt;</span> <span class="co"># find optimal lambda value that minimizes test MSE</span></span>
<span id="cb24-5"><a></a><span class="er">&gt;</span> best_lambda <span class="ot">&lt;-</span> cv_model<span class="sc">$</span>lambda.min</span>
<span id="cb24-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb24-7"><a></a><span class="er">&gt;</span> <span class="co"># produce plot of test MSE by lambda value</span></span>
<span id="cb24-8"><a></a><span class="er">&gt;</span> cv_model <span class="sc">%&gt;%</span> broom<span class="sc">::</span><span class="fu">tidy</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb24-9"><a></a><span class="sc">+</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>lambda, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb24-10"><a></a><span class="sc">+</span>   <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> conf.low, <span class="at">ymax =</span> conf.high), <span class="at">fill =</span> <span class="st">"#00ABFD"</span>, <span class="at">alpha=</span><span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb24-11"><a></a><span class="sc">+</span>   <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb24-12"><a></a><span class="sc">+</span>   <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>best_lambda) <span class="sc">+</span></span>
<span id="cb24-13"><a></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title=</span><span class="st">'Ridge Regression'</span></span>
<span id="cb24-14"><a></a><span class="sc">+</span>        , <span class="at">subtitle =</span> </span>
<span id="cb24-15"><a></a><span class="sc">+</span>          stringr<span class="sc">::</span><span class="fu">str_glue</span>(</span>
<span id="cb24-16"><a></a><span class="sc">+</span>            <span class="st">"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}"</span></span>
<span id="cb24-17"><a></a><span class="sc">+</span>          )</span>
<span id="cb24-18"><a></a><span class="sc">+</span>   ) <span class="sc">+</span></span>
<span id="cb24-19"><a></a><span class="sc">+</span>   ggplot2<span class="sc">::</span><span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="BSMM_8740_lec_03_files/figure-revealjs/unnamed-chunk-10-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="ridge-regression-example-3" class="slide level2">
<h2>Ridge Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>glmnet coefficients</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a></a><span class="sc">&gt;</span> model<span class="sc">$</span>beta <span class="sc">%&gt;%</span> </span>
<span id="cb25-2"><a></a><span class="sc">+</span>   <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-3"><a></a><span class="sc">+</span>   <span class="fu">t</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-4"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-5"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">add_column</span>(<span class="at">lambda =</span> model<span class="sc">$</span>lambda, <span class="at">.before =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-6"><a></a><span class="sc">+</span>   tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(<span class="sc">-</span>lambda, <span class="at">names_to =</span> <span class="st">'parameter'</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-7"><a></a><span class="sc">+</span>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>lambda, <span class="at">y=</span>value, <span class="at">color=</span>parameter)) <span class="sc">+</span></span>
<span id="cb25-8"><a></a><span class="sc">+</span>   <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb25-9"><a></a><span class="sc">+</span>   <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">2000</span>) <span class="sc">+</span></span>
<span id="cb25-10"><a></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title=</span><span class="st">'Ridge Regression'</span></span>
<span id="cb25-11"><a></a><span class="sc">+</span>        , <span class="at">subtitle =</span> </span>
<span id="cb25-12"><a></a><span class="sc">+</span>          stringr<span class="sc">::</span><span class="fu">str_glue</span>(</span>
<span id="cb25-13"><a></a><span class="sc">+</span>            <span class="st">"Parameters as a function of lambda"</span></span>
<span id="cb25-14"><a></a><span class="sc">+</span>          )</span>
<span id="cb25-15"><a></a><span class="sc">+</span>   )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="BSMM_8740_lec_03_files/figure-revealjs/unnamed-chunk-11-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="lasso-regression" class="slide level2">
<h2>Lasso Regression</h2>
<div style="font-size: xx-large">
<p>Lasso regression is another example of a penalized regression model; in this case both the magnitude of the weights and the number of parameters are penalized by using the <span class="math inline">\(\ell_1\)</span> norm of the weights to the loss function of the lasso regression. In particular, the lasso regression weights are:</p>
<p><span class="math display">\[
\hat{\beta}_{\text{lasso}}=\arg\!\min\text{RSS}\left(\beta\right)+\lambda\left\Vert \beta\right\Vert _{1}
\]</span></p>
<p>The Lasso objective function is</p>
<p><span class="math display">\[
\mathscr{L}\left(\beta,\lambda\right)=\text{NLL}+\lambda\left\Vert \beta\right\Vert _{1}
\]</span></p>
</div>
</section>
<section id="lasso-regression-example" class="slide level2">
<h2>Lasso Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>lasso model</summary>
<div class="sourceCode cell-code" id="cb26" data-code-line-numbers="1-2|4-5|7-8|10-11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a></a><span class="sc">&gt;</span> <span class="co"># define response variable</span></span>
<span id="cb26-2"><a></a><span class="er">&gt;</span> y <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">pull</span>(hp)</span>
<span id="cb26-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb26-4"><a></a><span class="er">&gt;</span> <span class="co"># define matrix of predictor variables</span></span>
<span id="cb26-5"><a></a><span class="er">&gt;</span> x <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(mpg, wt, drat, qsec) <span class="sc">%&gt;%</span> <span class="fu">data.matrix</span>()</span>
<span id="cb26-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb26-7"><a></a><span class="er">&gt;</span> <span class="co"># fit ridge regression model</span></span>
<span id="cb26-8"><a></a><span class="er">&gt;</span> model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb26-9"><a></a><span class="sc">&gt;</span> </span>
<span id="cb26-10"><a></a><span class="er">&gt;</span> <span class="co"># get coefficients when lambda = 3.53</span></span>
<span id="cb26-11"><a></a><span class="er">&gt;</span> <span class="fu">coef</span>(model, <span class="at">s =</span> <span class="fl">3.53</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>5 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept) 480.761125
mpg          -3.036337
wt           20.222451
drat          .       
qsec        -18.944318</code></pre>
</div>
</div>
</section>
<section id="lasso-regression-example-1" class="slide level2">
<h2>Lasso Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>lasso example</summary>
<div class="sourceCode cell-code" id="cb28" data-code-line-numbers="2|5|8|9|10|11|12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a></a><span class="sc">&gt;</span> <span class="co">#perform k-fold cross-validation to find optimal lambda value</span></span>
<span id="cb28-2"><a></a><span class="er">&gt;</span> cv_model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb28-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb28-4"><a></a><span class="er">&gt;</span> <span class="co">#find optimal lambda value that minimizes test MSE</span></span>
<span id="cb28-5"><a></a><span class="er">&gt;</span> best_lambda <span class="ot">&lt;-</span> cv_model<span class="sc">$</span>lambda.min</span>
<span id="cb28-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb28-7"><a></a><span class="er">&gt;</span> <span class="co">#produce plot of test MSE by lambda value</span></span>
<span id="cb28-8"><a></a><span class="er">&gt;</span> cv_model <span class="sc">%&gt;%</span> broom<span class="sc">::</span><span class="fu">tidy</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb28-9"><a></a><span class="sc">+</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>lambda, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb28-10"><a></a><span class="sc">+</span>   <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> conf.low, <span class="at">ymax =</span> conf.high), <span class="at">fill =</span> <span class="st">"#00ABFD"</span>, <span class="at">alpha=</span><span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb28-11"><a></a><span class="sc">+</span>   <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb28-12"><a></a><span class="sc">+</span>   <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>best_lambda) <span class="sc">+</span></span>
<span id="cb28-13"><a></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title=</span><span class="st">'Lasso Regression'</span></span>
<span id="cb28-14"><a></a><span class="sc">+</span>        , <span class="at">subtitle =</span> </span>
<span id="cb28-15"><a></a><span class="sc">+</span>          stringr<span class="sc">::</span><span class="fu">str_glue</span>(</span>
<span id="cb28-16"><a></a><span class="sc">+</span>            <span class="st">"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}"</span></span>
<span id="cb28-17"><a></a><span class="sc">+</span>          )</span>
<span id="cb28-18"><a></a><span class="sc">+</span>   ) <span class="sc">+</span></span>
<span id="cb28-19"><a></a><span class="sc">+</span>   <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="fu">exp</span>(<span class="dv">4</span>)) <span class="sc">+</span> ggplot2<span class="sc">::</span><span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="BSMM_8740_lec_03_files/figure-revealjs/unnamed-chunk-13-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="lasso-regression-example-2" class="slide level2">
<h2>Lasso Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>lasso coefficients</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a></a><span class="sc">&gt;</span> model <span class="sc">%&gt;%</span></span>
<span id="cb29-2"><a></a><span class="sc">+</span>   broom<span class="sc">::</span><span class="fu">tidy</span>() <span class="sc">%&gt;%</span></span>
<span id="cb29-3"><a></a><span class="sc">+</span>   tidyr<span class="sc">::</span><span class="fu">pivot_wider</span>(<span class="at">names_from=</span>term, <span class="at">values_from=</span>estimate) <span class="sc">%&gt;%</span></span>
<span id="cb29-4"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(step,dev.ratio, <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb29-5"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">mutate_all</span>(dplyr<span class="sc">::</span>coalesce, <span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb29-6"><a></a><span class="sc">+</span>   tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(<span class="sc">-</span>lambda, <span class="at">names_to =</span> <span class="st">'parameter'</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb29-7"><a></a><span class="sc">+</span>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>lambda, <span class="at">y=</span>value, <span class="at">color=</span>parameter)) <span class="sc">+</span></span>
<span id="cb29-8"><a></a><span class="sc">+</span>   <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb29-9"><a></a><span class="sc">+</span>   <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">70</span>) <span class="sc">+</span></span>
<span id="cb29-10"><a></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title=</span><span class="st">'Ridge Regression'</span></span>
<span id="cb29-11"><a></a><span class="sc">+</span>        , <span class="at">subtitle =</span> </span>
<span id="cb29-12"><a></a><span class="sc">+</span>          stringr<span class="sc">::</span><span class="fu">str_glue</span>(</span>
<span id="cb29-13"><a></a><span class="sc">+</span>            <span class="st">"Parameters as a function of lambda"</span></span>
<span id="cb29-14"><a></a><span class="sc">+</span>          )</span>
<span id="cb29-15"><a></a><span class="sc">+</span>   )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="BSMM_8740_lec_03_files/figure-revealjs/unnamed-chunk-14-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="elastic-net-regression" class="slide level2">
<h2>Elastic Net Regression</h2>
<p>Elastic Net regression is a hybrid of ridge and lasso regression.</p>
<p>The elastic net objective function is</p>
<p><span class="math display">\[
\mathscr{L}\left(\beta,\lambda,\alpha\right)=\text{NLL}+\lambda\left(\left(1-\alpha\right)\left\Vert \beta\right\Vert _{2}^{2}+\alpha\left\Vert \beta\right\Vert _{1}\right)
\]</span></p>
<p>so that <span class="math inline">\(\alpha=0\)</span> is ridge regression and <span class="math inline">\(\alpha=1\)</span> is lasso regression and <span class="math inline">\(\alpha\in\left(0,1\right)\)</span> is the general elastic net.</p>
</section>
<section id="elastic-net-regression-example" class="slide level2">
<h2>Elastic Net Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>elastic net example</summary>
<div class="sourceCode cell-code" id="cb30" data-code-line-numbers="5-9|10-13|15|16-17|18-29|33-35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a></a><span class="sc">&gt;</span> <span class="co"># set length of data and seed for reproducability</span></span>
<span id="cb30-2"><a></a><span class="er">&gt;</span> n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb30-3"><a></a><span class="sc">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">2467</span>)</span>
<span id="cb30-4"><a></a><span class="sc">&gt;</span> <span class="co"># create the dataset</span></span>
<span id="cb30-5"><a></a><span class="er">&gt;</span> dat <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(</span>
<span id="cb30-6"><a></a><span class="sc">+</span>   <span class="at">a =</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, n, <span class="at">replace =</span> T)<span class="sc">/</span><span class="dv">10</span></span>
<span id="cb30-7"><a></a><span class="sc">+</span>   , <span class="at">b =</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, n, <span class="at">replace =</span> T)<span class="sc">/</span><span class="dv">10</span></span>
<span id="cb30-8"><a></a><span class="sc">+</span>   , <span class="at">c =</span> <span class="fu">sort</span>(<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, n, <span class="at">replace =</span> T))</span>
<span id="cb30-9"><a></a><span class="sc">+</span> ) <span class="sc">%&gt;%</span> </span>
<span id="cb30-10"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb30-11"><a></a><span class="sc">+</span>     <span class="at">z =</span> (a<span class="sc">*</span>b)<span class="sc">/</span><span class="dv">2</span> <span class="sc">+</span> c <span class="sc">+</span> <span class="fu">sample</span>(<span class="sc">-</span><span class="dv">10</span><span class="sc">:</span><span class="dv">10</span>, n, <span class="at">replace =</span> T)<span class="sc">/</span><span class="dv">10</span></span>
<span id="cb30-12"><a></a><span class="sc">+</span>     , <span class="at">.before =</span> <span class="dv">1</span></span>
<span id="cb30-13"><a></a><span class="sc">+</span>   )</span>
<span id="cb30-14"><a></a><span class="sc">&gt;</span> <span class="co"># cross validate to get the best alpha</span></span>
<span id="cb30-15"><a></a><span class="er">&gt;</span> alpha_dat <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>( <span class="at">alpha =</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="fl">0.01</span>) ) <span class="sc">%&gt;%</span> </span>
<span id="cb30-16"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb30-17"><a></a><span class="sc">+</span>     <span class="at">mse =</span></span>
<span id="cb30-18"><a></a><span class="sc">+</span>       purrr<span class="sc">::</span><span class="fu">map_dbl</span>(</span>
<span id="cb30-19"><a></a><span class="sc">+</span>         alpha</span>
<span id="cb30-20"><a></a><span class="sc">+</span>         , (\(a){</span>
<span id="cb30-21"><a></a><span class="sc">+</span>           cvg <span class="ot">&lt;-</span> </span>
<span id="cb30-22"><a></a><span class="sc">+</span>            glmnet<span class="sc">::</span><span class="fu">cv.glmnet</span>(</span>
<span id="cb30-23"><a></a><span class="sc">+</span>              <span class="at">x =</span> dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>z) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb30-24"><a></a><span class="sc">+</span>              , <span class="at">y =</span> dat<span class="sc">$</span>z </span>
<span id="cb30-25"><a></a><span class="sc">+</span>              , <span class="at">family =</span> <span class="st">"gaussian"</span></span>
<span id="cb30-26"><a></a><span class="sc">+</span>              , <span class="at">gamma =</span> a</span>
<span id="cb30-27"><a></a><span class="sc">+</span>           )</span>
<span id="cb30-28"><a></a><span class="sc">+</span>           <span class="fu">min</span>(cvg<span class="sc">$</span>cvm)</span>
<span id="cb30-29"><a></a><span class="sc">+</span>         })</span>
<span id="cb30-30"><a></a><span class="sc">+</span>       )</span>
<span id="cb30-31"><a></a><span class="sc">+</span>   ) </span>
<span id="cb30-32"><a></a><span class="sc">&gt;</span> </span>
<span id="cb30-33"><a></a><span class="er">&gt;</span> best_alpha <span class="ot">&lt;-</span> alpha_dat <span class="sc">%&gt;%</span> </span>
<span id="cb30-34"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">filter</span>(mse <span class="sc">==</span> <span class="fu">min</span>(mse)) <span class="sc">%&gt;%</span> </span>
<span id="cb30-35"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">pull</span>(alpha)</span>
<span id="cb30-36"><a></a><span class="sc">&gt;</span> </span>
<span id="cb30-37"><a></a><span class="er">&gt;</span> <span class="fu">cat</span>(<span class="st">"best alpha:"</span>, best_alpha)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>best alpha: 0.64</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>elastic net example, part 2</summary>
<div class="sourceCode cell-code" id="cb32" data-code-line-numbers="1-6|8-9|11-15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a></a><span class="sc">&gt;</span> elastic_cv <span class="ot">&lt;-</span> </span>
<span id="cb32-2"><a></a><span class="sc">+</span>   glmnet<span class="sc">::</span><span class="fu">cv.glmnet</span>(</span>
<span id="cb32-3"><a></a><span class="sc">+</span>     <span class="at">x =</span> dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>z) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb32-4"><a></a><span class="sc">+</span>     , <span class="at">y =</span> dat<span class="sc">$</span>z </span>
<span id="cb32-5"><a></a><span class="sc">+</span>     , <span class="at">family =</span> <span class="st">"gaussian"</span></span>
<span id="cb32-6"><a></a><span class="sc">+</span>     , <span class="at">gamma =</span> best_alpha)</span>
<span id="cb32-7"><a></a><span class="sc">&gt;</span> </span>
<span id="cb32-8"><a></a><span class="er">&gt;</span> best_lambda <span class="ot">&lt;-</span> elastic_cv<span class="sc">$</span>lambda.min</span>
<span id="cb32-9"><a></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">"best lambda:"</span>, best_lambda)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>best lambda: 0.01015384</code></pre>
</div>
<details class="code-fold">
<summary>elastic net example, part 2</summary>
<div class="sourceCode cell-code" id="cb34" data-code-line-numbers="1-6|8-9|11-15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a></a><span class="sc">&gt;</span> elastic_mod <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">glmnet</span>(</span>
<span id="cb34-2"><a></a><span class="sc">+</span>   <span class="at">x =</span> dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>z) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb34-3"><a></a><span class="sc">+</span>   , <span class="at">y =</span> dat<span class="sc">$</span>z </span>
<span id="cb34-4"><a></a><span class="sc">+</span>   , <span class="at">family =</span> <span class="st">"gaussian"</span></span>
<span id="cb34-5"><a></a><span class="sc">+</span>   , <span class="at">gamma =</span> best_alpha, <span class="at">lambda =</span> best_lambda)</span>
<span id="cb34-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb34-7"><a></a><span class="er">&gt;</span> elastic_mod <span class="sc">%&gt;%</span> broom<span class="sc">::</span><span class="fu">tidy</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 5
  term         step estimate lambda dev.ratio
  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)     1   -0.467 0.0102     0.963
2 a               1    0.221 0.0102     0.963
3 b               1    0.560 0.0102     0.963
4 c               1    1.03  0.0102     0.963</code></pre>
</div>
</div>
</section>
<section id="elastic-net-regression-example-1" class="slide level2">
<h2>Elastic Net Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>elastic net example, part 3</summary>
<div class="sourceCode cell-code" id="cb36" data-code-line-numbers="1|3-5|7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a></a><span class="sc">&gt;</span> pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(elastic_mod, dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>z) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>())</span>
<span id="cb36-2"><a></a><span class="sc">&gt;</span> </span>
<span id="cb36-3"><a></a><span class="er">&gt;</span> rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>( (pred <span class="sc">-</span> dat<span class="sc">$</span>z)<span class="sc">^</span><span class="dv">2</span> ))</span>
<span id="cb36-4"><a></a><span class="sc">&gt;</span> R2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> (<span class="fu">sum</span>((dat<span class="sc">$</span>z <span class="sc">-</span> pred )<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">sum</span>((dat<span class="sc">$</span>z <span class="sc">-</span> <span class="fu">mean</span>(y))<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb36-5"><a></a><span class="sc">&gt;</span> mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((dat<span class="sc">$</span>z <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb36-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb36-7"><a></a><span class="er">&gt;</span> <span class="fu">cat</span>(<span class="st">" RMSE:"</span>, rmse, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, <span class="st">"R-squared:"</span>, R2, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, <span class="st">"MSE:"</span>, mse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> RMSE: 0.5817823 
 R-squared: 0.9999828 
 MSE: 0.3384707</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>elastic net example, part 4</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a></a><span class="sc">&gt;</span> dat <span class="sc">%&gt;%</span> </span>
<span id="cb38-2"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb38-3"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">add_column</span>(<span class="at">pred =</span> pred[,<span class="dv">1</span>]) <span class="sc">%&gt;%</span> </span>
<span id="cb38-4"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">rowid_to_column</span>(<span class="st">"ID"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb38-5"><a></a><span class="sc">+</span>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>ID, <span class="at">y=</span>z)) <span class="sc">+</span></span>
<span id="cb38-6"><a></a><span class="sc">+</span>   <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb38-7"><a></a><span class="sc">+</span>   <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>pred),<span class="at">color=</span><span class="st">'red'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="BSMM_8740_lec_03_files/figure-revealjs/unnamed-chunk-18-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="generalized-linear-models" class="slide level2">
<h2>Generalized Linear Models</h2>
<p>A <strong>generalized linear model</strong> (<strong>GLM</strong>) is a flexible generalization of ordinary linear regression.</p>
<p>Ordinary linear regression predicts the expected value of the outcome variable, a random variable, as a linear combination of a set of observed values (<em>predictors</em>). In a generalized linear model (GLM), each outcome <span class="math inline">\(Y\)</span> is assumed to be generated from a particular distribution in an exponential family, The mean, <span class="math inline">\(\mu\)</span>, of the distribution depends on the independent variables, <span class="math inline">\(X\)</span>, through:</p>
<p><span class="math display">\[
\mathbb{E}\left[\left.Y\right|X\right]=\mu=\text{g}^{-1}\left(X\beta\right)
\]</span> where <span class="math inline">\(g\)</span> is called the <strong>link function</strong>.</p>
</section>
<section id="generalized-linear-models-1" class="slide level2">
<h2>Generalized Linear Models</h2>
<p>For example, if <span class="math inline">\(Y\)</span> is Poisson distributed, then</p>
<p><span class="math display">\[
\mathbb{P}\left[\left.Y=y\right|X,\lambda\right]=\frac{\lambda^{y}}{y!}e^{-\lambda}=e^{y\log\lambda-\lambda-\log y!}
\]</span></p>
<p>Where <span class="math inline">\(\lambda\)</span> is both the mean and the variance. In the glm the link function is <span class="math inline">\(\log\)</span> and</p>
<p><span class="math display">\[
\log\mathbb{E}\left[\left.Y\right|X\right] = \beta X=\log\lambda
\]</span></p>
</section>
<section id="generalized-linear-models-2" class="slide level2">
<h2>Generalized Linear Models</h2>
<h3 id="key-components-of-glms">Key Components of GLMs</h3>
<div class="panel-tabset">
<ul id="tabset-4" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-4-1">Random</a></li><li><a href="#tabset-4-2">Systemic</a></li><li><a href="#tabset-4-3">Link</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1">
<p><strong>Random Component</strong>:</p>
<ul>
<li>Specifies the probability distribution of the data generation process of the response variable (<span class="math inline">\(Y\)</span>). Examples include Normal, Binomial, Poisson, etc.</li>
</ul>
</div>
<div id="tabset-4-2">
<p><strong>Systematic Component</strong>:</p>
<ul>
<li>Specifies the linear predictor (<span class="math inline">\(\eta = X\beta)\)</span>, where (<span class="math inline">\(X\)</span>) is the matrix of predictors and (<span class="math inline">\(\beta\)</span>) is the vector of coefficients.</li>
</ul>
</div>
<div id="tabset-4-3">
<p><strong>Link Function</strong>:</p>
<ul>
<li>Connects the mean of the response variable (<span class="math inline">\(\mathbb{E}(Y)\)</span>) to the linear predictor (<span class="math inline">\(\eta\)</span>). It transforms the expected value of the response variable to the linear predictor scale.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="generalized-linear-models-3" class="slide level2">
<h2>Generalized Linear Models</h2>
<h3 id="common-types-of-glms">Common Types of GLMs</h3>
<div class="panel-tabset">
<ul id="tabset-5" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-5-1">Linear</a></li><li><a href="#tabset-5-2">Logistic</a></li><li><a href="#tabset-5-3">Poisson</a></li><li><a href="#tabset-5-4">Gamma</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1">
<div style="font-size: xx-large">
<p>Linear Regression (Binomial Distribution)</p>
<ul>
<li><strong>Response Variable</strong>: Continuous</li>
<li><strong>Link Function</strong>: Identity (<span class="math inline">\((g(\mu) = \mu\)</span>))</li>
<li><strong>Example</strong>: Predicting house prices based on square footage, number of bedrooms, etc.</li>
<li><strong>Formula</strong>: <span class="math inline">\((Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \epsilon)\)</span>, where <span class="math inline">\((Y\)</span>) is normally distributed.</li>
</ul>
</div>
</div>
<div id="tabset-5-2">
<div style="font-size: x-large">
<p>Logistic Regression (binomial Distribution)</p>
<ul>
<li><strong>Response Variable</strong>: Binary (0 or 1)</li>
<li><strong>Link Function</strong>: Logit (<span class="math inline">\((g(\mu) = \log(\frac{\mu}{1-\mu})\)</span>))</li>
<li><strong>Example</strong>: Predicting whether a customer will buy a product (yes/no) based on age, income, etc.</li>
<li><strong>Formula</strong>: <span class="math inline">\((\log(\frac{p}{1-p}) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots\)</span>), where (<span class="math inline">\(p\)</span>) is the probability of the event occurring.</li>
</ul>
</div>
</div>
<div id="tabset-5-3">
<div style="font-size: xx-large">
<p>Poisson Regression (Poisson Distribution)</p>
<ul>
<li><strong>Response Variable</strong>: Count data (non-negative integers)</li>
<li><strong>Link Function</strong>: Log (<span class="math inline">\((g(\mu) = \log(\mu)\)</span>))</li>
<li><strong>Example</strong>: Predicting the number of insurance claims in a year based on driver age, vehicle type, etc.</li>
<li><strong>Formula</strong>: <span class="math inline">\((\log(\lambda) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots)\)</span>, where <span class="math inline">\((\lambda)\)</span> is the expected count.</li>
</ul>
</div>
</div>
<div id="tabset-5-4">
<div style="font-size: xx-large">
<p>Gamma Regression (Gamma Distribution)</p>
<ul>
<li><strong>Response Variable</strong>: Continuous and positive</li>
<li><strong>Link Function</strong>: Inverse (<span class="math inline">\((g(\mu) = \frac{1}{\mu}\)</span>))</li>
<li><strong>Example</strong>: Predicting the time until failure of a machine based on temperature, pressure, etc.</li>
<li><strong>Formula</strong>: <span class="math inline">\((\frac{1}{\mu} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots)\)</span>, where (<span class="math inline">\(\mu\)</span>) is the mean of the response variable.</li>
</ul>
</div>
</div>
</div>
</div>
</section>
<section id="generalized-linear-models-4" class="slide level2">
<h2>Generalized Linear Models</h2>
<h3 id="examples-of-glms">Examples of GLMs</h3>
<div class="panel-tabset">
<ul id="tabset-6" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-6-1">Logistic</a></li><li><a href="#tabset-6-2">Poisson</a></li><li><a href="#tabset-6-3">Gamma</a></li></ul>
<div class="tab-content">
<div id="tabset-6-1">
<div style="font-size: xx-large">
<p><strong>Logistic Regression Example</strong>:</p>
<ul>
<li><strong>Scenario</strong>: A marketing team wants to predict whether a customer will buy a product.</li>
<li><strong>Variables</strong>: Customer age, income, and previous purchase history.</li>
<li><strong>Model</strong>: <span class="math inline">\((\log(\frac{p}{1-p}) = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Income} + \beta_3 \text{History}\)</span>)</li>
<li><strong>Interpretation</strong>: The coefficients (_1, _2, _3) indicate how each predictor affects the log odds of making a purchase.</li>
</ul>
</div>
</div>
<div id="tabset-6-2">
<div style="font-size: xx-large">
<p><strong>Poisson Regression Example</strong>:</p>
<ul>
<li><strong>Scenario</strong>: An insurance company wants to predict the number of claims a policyholder will file.</li>
<li><strong>Variables</strong>: Age of the policyholder, type of vehicle, and driving experience.</li>
<li><strong>Model</strong>: <span class="math inline">\((\log(\lambda) = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{VehicleType} + \beta_3 \text{Experience}\)</span>)</li>
<li><strong>Interpretation</strong>: The coefficients <span class="math inline">\((\beta_1, \beta_2, \beta_3)\)</span> indicate how each predictor affects the expected number of claims.</li>
</ul>
</div>
</div>
<div id="tabset-6-3">
<div style="font-size: xx-large">
<p><strong>Gamma Regression Example</strong>:</p>
<ul>
<li><strong>Scenario</strong>: A manufacturing company wants to predict the lifetime of a machine part.</li>
<li><strong>Variables</strong>: Operating temperature, pressure, and usage frequency.</li>
<li><strong>Model</strong>: <span class="math inline">\((\frac{1}{\mu} = \beta_0 + \beta_1 \text{Temperature} + \beta_2 \text{Pressure} + \beta_3 \text{Frequency})\)</span></li>
<li><strong>Interpretation</strong>: The coefficients <span class="math inline">\((\beta_1, \beta_2, \beta_3)\)</span> indicate how each predictor affects the inverse of the expected lifetime.</li>
</ul>
</div>
</div>
</div>
</div>
</section></section>
<section>
<section id="non-parametric-regression" class="title-slide slide level1 center">
<h1>Non-parametric regression</h1>

</section>
<section id="regression-with-trees" class="slide level2">
<h2>Regression with trees</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a></a><span class="sc">&gt;</span> dat <span class="ot">&lt;-</span> MASS<span class="sc">::</span>Boston</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>There are many methodologies for constructing regression trees but one of the oldest is known as the <strong>c</strong>lassification <strong>a</strong>nd <strong>r</strong>egression <strong>t</strong>ree (CART) approach.</p>
<p>Basic regression trees <em>partition</em> a data set into smaller subgroups and then fit a simple <em>constant</em> for each observation in the subgroup. The partitioning is achieved by successive binary partitions (aka&nbsp;<em>recursive partitioning</em>) based on the different predictors.</p>
</section>
<section id="regression-with-trees-1" class="slide level2">
<h2>Regression with trees</h2>
<p>As a simple example, consider a continuous response variable <span class="math inline">\(y\)</span> with two covariates <span class="math inline">\(x_1,x_2\)</span> and the support of <span class="math inline">\(x_1,x_2\)</span> partitioned into three regions. Then we write the tree regression model for <span class="math inline">\(y\)</span> as:</p>
<p><span class="math display">\[
\hat{y} = \hat{f}(x_1,x_2)=\sum_{i=1}^{3}c_1\times I_{(x_1,x_2)\in R_i}
\]</span></p>
<p>Tree algorithms differ in how they grow the regression tree, i.e.&nbsp;partition the space of the covariates.</p>
</section>
<section id="regression-with-trees-2" class="slide level2">
<h2>Regression with trees</h2>
<p>All partitioning of variables is done in a top-down, greedy fashion. This just means that a partition performed earlier in the tree will not change based on later partitions. In general the partitions are made to minimize following objective function (support initially partitioned into 2 regions, i.e.&nbsp;a binary tree):</p>
<p><span class="math display">\[
\text{SSE}=\left\{ \sum_{i\in R_{1}}\left(y_{i}-c_{i}\right)^{2}+\sum_{i\in R_{2}}\left(y_{i}-c_{i}\right)^{2}\right\}
\]</span></p>
</section>
<section id="regression-with-trees-3" class="slide level2">
<h2>Regression with trees</h2>
<p>Having found the best split, we repeat the splitting process on each of the two regions.</p>
<p>This process is continued until some stopping criterion is reached. What typically results is a very deep, complex tree that may produce good predictions on the training set, but is likely to overfit the data, particularly at the lower nodes.</p>
<p>By pruning these lower level nodes, we can introduce a little bit of bias in our model that help to stabilize predictions and will tend to generalize better to new, unseen data.</p>
</section>
<section id="regression-with-trees-4" class="slide level2">
<h2>Regression with trees</h2>
<p>As with penalized linear regression, we can use a complexity parameter <span class="math inline">\(\alpha\)</span> to penalize the number of terminal nodes of the tree (<span class="math inline">\(T\)</span>), like the lasso <span class="math inline">\(L_1\)</span> norm penalty, and find the smallest tree with lowest penalized error, i.e.&nbsp;the minimizing the following objective function:</p>
<p><span class="math display">\[
\text{SSE}+\alpha\left|T\right|
\]</span></p>
</section>
<section id="regression-with-trees-5" class="slide level2">
<h2>Regression with trees</h2>
<div class="columns">
<div class="column" style="font-size: 32px">
<p>Strengths</p>
<ul>
<li>They are very interpretable.</li>
<li>Making predictions is fast; just lookup constants in the tree.</li>
<li>Variables importance is easy; those variables that most reduce the SSE.</li>
<li>Tree models give a non-linear response; better if the true regression surface is not smooth.</li>
<li>There are fast, reliable algorithms to learn these trees.</li>
</ul>
</div><div class="column" style="font-size: 32px">
<p>Weaknesses</p>
<ul>
<li>Single regression trees have high variance, resulting in unstable predictions (an alternative subsample of training data can significantly change the terminal nodes).</li>
<li>Due to the high variance single regression trees have poor predictive accuracy.</li>
</ul>
</div></div>
</section>
<section id="regression-with-trees-bagging" class="slide level2">
<h2>Regression with trees (Bagging)</h2>
<p>As mentioned, single tree models suffer from high variance. Although pruning the tree helps reduce this variance, there are alternative methods that actually exploite the variability of single trees in a way that can significantly improve performance over and above that of single trees. <em><strong>B</strong>ootstrap</em> <em><strong>agg</strong>regat<strong>ing</strong></em> (<strong><em>bagging</em></strong>) is one such approach.</p>
<p>Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance.</p>
</section>
<section id="regression-with-trees-bagging-1" class="slide level2">
<h2>Regression with trees (Bagging)</h2>
<p>Bagging combines and averages multiple tree models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, improving predictive performance.</p>
</section>
<section id="regression-with-trees-bagging-2" class="slide level2">
<h2>Regression with trees (Bagging)</h2>
<p>Bagging follows three steps:</p>
<ul>
<li>Create <span class="math inline">\(m\)</span> <a href="http://uc-r.github.io/bootstrapping">bootstrap samples</a> from the training data. Bootstrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.</li>
<li>For each bootstrap sample train a single, unpruned regression tree.</li>
<li>Average individual predictions from each tree to create an overall average predicted value.</li>
</ul>
</section>
<section id="regression-with-trees-bagging-3" class="slide level2">
<h2>Regression with trees (Bagging)</h2>

<img data-src="https://uc-r.github.io/public/images/analytics/regression_trees/bagging3.png" class="r-stretch quarto-figure-center"><p class="caption">Fig: The bagging process.</p></section>
<section id="regression-with-a-random-forest" class="slide level2">
<h2>Regression with a random forest</h2>
<p>Bagging trees introduces a random component into the tree building process that reduces the variance of a single tree’s prediction and improves predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree.</p>
<p>So trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships. They are correlated.</p>
</section>
<section id="regression-with-a-random-forest-1" class="slide level2">
<h2>Regression with a random forest</h2>
<p>Tree correlation prevents bagging from optimally reducing the variance of the predictive values. Reducing variance further can be achieved by injecting more randomness into the tree-growing process. Random forests achieve this in two ways:</p>
<div style="font-size: smaller">
<ol type="1">
<li><strong>Bootstrap</strong>: similar to bagging - each tree is grown from a bootstrap resampled data set, which <em>somewhat</em> decorrelates them.</li>
<li><strong>Split-variable randomization</strong>: each time a split is made, the search for the split variable is limited to a random subset of <span class="math inline">\(m\)</span> of the <span class="math inline">\(p\)</span> variables.</li>
</ol>
</div>
</section>
<section id="regression-with-a-random-forest-2" class="slide level2">
<h2>Regression with a random forest</h2>
<p>For regression trees, typical default values used in split-value randomization are <span class="math inline">\(m=\frac{p}{3}\)</span> but this should be considered a tuning parameter.</p>
<p>When <span class="math inline">\(m=p\)</span>, the randomization amounts to using only step 1 and is the same as <em>bagging</em>.</p>
</section>
<section id="regression-with-a-random-forest-3" class="slide level2">
<h2>Regression with a random forest</h2>
<div class="columns">
<div class="column" style="font-size: 32px">
<p>Strengths</p>
<ul>
<li>Typically have very good performance</li>
<li>Remarkably good “out-of-the box” - very little tuning required</li>
<li>Built-in validation set - don’t need to sacrifice data for extra validation</li>
<li>No pre-processing required</li>
<li>Robust to outliers</li>
</ul>
</div><div class="column" style="font-size: 32px">
<p>Weaknesses</p>
<ul>
<li>Can become slow on large data sets</li>
<li>Although accurate, often cannot compete with advanced boosting algorithms</li>
<li>Less interpretable</li>
</ul>
</div></div>
</section>
<section id="regression-with-gradient-boosting" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions.</p>
</section>
<section id="regression-with-gradient-boosting-1" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Whereas <a href="http://uc-r.github.io/random_forests">random forests</a> build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms.</p>
</section>
<section id="regression-with-gradient-boosting-2" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>The main idea of boosting is to add new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.</p>

<img data-src="../images/boosted-trees-process.png" class="r-stretch quarto-figure-center"><p class="caption">Sequential ensemble approach.</p></section>
<section id="regression-with-gradient-boosting-3" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Boosting is a framework that iteratively improves <em>any</em> weak learning model. Many gradient boosting applications allow you to “plug in” various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner.</p>
</section>
<section id="regression-with-gradient-boosting-4" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>A weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. With regards to decision trees, shallow trees represent a weak learner. Commonly, trees with only 1-6 splits are used.</p>
</section>
<section id="regression-with-gradient-boosting-5" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Combining many weak models (versus strong ones) has a few benefits:</p>
<div style="font-size: smaller">
<ul>
<li>Speed: Constructing weak models is computationally cheap.</li>
<li>Accuracy improvement: Weak models allow the algorithm to <em>learn slowly</em>; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.</li>
<li>Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).</li>
</ul>
</div>
</section>
<section id="regression-with-gradient-boosting-6" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Here is the algorithm for boosted regression trees with features <span class="math inline">\(x\)</span> and response <span class="math inline">\(y\)</span>:</p>
<div style="font-size: smaller">
<ol type="1">
<li>Fit a decision tree to the data: <span class="math inline">\(F_1(x)=y\)</span>,</li>
<li>We then fit the next decision tree to the residuals of the previous: <span class="math inline">\(h_1(x)=y−F_1(x)\)</span></li>
<li>Add this new tree to our algorithm: <span class="math inline">\(F_2(x)=F_1(x)+h_1(x)\)</span>,</li>
<li>Fit the next decision tree to the residuals of <span class="math inline">\(F_2: h_2(x)=y−F_2(x)\)</span>,</li>
<li>Add this new tree to our algorithm: <span class="math inline">\(F_3(x)=F_2(x)+h_1(x)\)</span>,</li>
<li>Continue this process until some mechanism (i.e.&nbsp;cross validation) tells us to stop.</li>
</ol>
</div>
</section>
<section id="xgboost-example" class="slide level2">
<h2>XGBoost Example</h2>
<p><strong>XGBoost</strong> is short for e<strong>X</strong>treme <strong>G</strong>radient <strong>Boost</strong>ing package.</p>
<p>While the <code>XGBoost</code> model often achieves higher accuracy than a single decision tree, it sacrifices the intrinsic interpretability of decision trees. For example, following the path that a decision tree takes to make its decision is trivial and self-explained, but following the paths of hundreds or thousands of trees is much harder.</p>
<p>We will work with <code>XGBoost</code> in today’s lab.</p>
</section>
<section id="kernel-regression" class="slide level2">
<h2>Kernel Regression</h2>
<p>Kernel Regression is a non-parametric technique in machine learning used to estimate the relationship between a dependent variable and one or more independent variables.</p>
<p>Unlike linear regression, Kernel Regression does not assume a specific form for the relationship between the variables. Instead, it uses a weighted average of nearby observed data points to make predictions.</p>
</section>
<section id="kernel-regression-1" class="slide level2">
<h2>Kernel Regression</h2>
<div style="font-size: x-large">
<ol type="1">
<li><strong>Select a Kernel Function</strong>:
<ul>
<li>Choose a kernel function that will determine how weights are assigned to nearby data points. The Gaussian kernel is a common choice, where weights decrease with distance according to a normal distribution.</li>
</ul></li>
<li><strong>Choose a Bandwidth</strong>:
<ul>
<li>Decide on the bandwidth parameter that will control the spread of the kernel function. This affects the smoothness of the regression curve.</li>
</ul></li>
<li><strong>Compute Weights</strong>:
<ul>
<li>For each point where you want to estimate the dependent variable, compute the weights for all observed data points using the kernel function.</li>
</ul></li>
<li><strong>Calculate Weighted Average</strong>:
<ul>
<li>Use the weights to compute a weighted average of the dependent variable values, giving more influence to points closer to the point of interest.</li>
</ul></li>
</ol>
</div>
</section>
<section id="kernel-regression-example" class="slide level2">
<h2>Kernel Regression: example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a></a><span class="sc">&gt;</span> <span class="co">#Kernel regression</span></span>
<span id="cb40-2"><a></a><span class="er">&gt;</span> <span class="co"># from https://towardsdatascience.com/kernel-regression-made-easy-to-understand-86caf2d2b844</span></span>
<span id="cb40-3"><a></a><span class="er">&gt;</span> Kdata <span class="ot">&lt;-</span> </span>
<span id="cb40-4"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">tibble</span>(</span>
<span id="cb40-5"><a></a><span class="sc">+</span>     <span class="at">Area =</span> <span class="fu">c</span>(<span class="dv">11</span>,<span class="dv">22</span>,<span class="dv">33</span>,<span class="dv">44</span>,<span class="dv">50</span>,<span class="dv">56</span>,<span class="dv">67</span>,<span class="dv">70</span>,<span class="dv">78</span>,<span class="dv">89</span>,<span class="dv">90</span>,<span class="dv">100</span>)</span>
<span id="cb40-6"><a></a><span class="sc">+</span>     , <span class="at">RiverFlow =</span> <span class="fu">c</span>(<span class="dv">2337</span>,<span class="dv">2750</span>,<span class="dv">2301</span>,<span class="dv">2500</span>,<span class="dv">1700</span>,<span class="dv">2100</span>,<span class="dv">1100</span>,<span class="dv">1750</span>,<span class="dv">1000</span>,<span class="dv">1642</span>, <span class="dv">2000</span>,<span class="dv">1932</span>)</span>
<span id="cb40-7"><a></a><span class="sc">+</span>   )</span>
<span id="cb40-8"><a></a><span class="sc">&gt;</span> </span>
<span id="cb40-9"><a></a><span class="er">&gt;</span> <span class="co">#function to calculate Gaussian kernel</span></span>
<span id="cb40-10"><a></a><span class="er">&gt;</span> gausinKernel <span class="ot">&lt;-</span> <span class="cf">function</span>(x,b){<span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span>(x<span class="sc">/</span>b)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">*</span>pi))}</span>
<span id="cb40-11"><a></a><span class="sc">&gt;</span> <span class="co">#plotting function</span></span>
<span id="cb40-12"><a></a><span class="er">&gt;</span> plt_fit <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">bandwidth =</span> <span class="dv">10</span>, <span class="at">support =</span> <span class="fu">seq</span>(<span class="dv">5</span>,<span class="dv">110</span>,<span class="dv">1</span>)){</span>
<span id="cb40-13"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">x_hat =</span> support) <span class="sc">|&gt;</span> </span>
<span id="cb40-14"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb40-15"><a></a><span class="sc">+</span>     <span class="at">y_hat =</span></span>
<span id="cb40-16"><a></a><span class="sc">+</span>       purrr<span class="sc">::</span><span class="fu">map_dbl</span>(</span>
<span id="cb40-17"><a></a><span class="sc">+</span>         x_hat</span>
<span id="cb40-18"><a></a><span class="sc">+</span>         , (</span>
<span id="cb40-19"><a></a><span class="sc">+</span>         \(x){</span>
<span id="cb40-20"><a></a><span class="sc">+</span>           K <span class="ot">&lt;-</span> <span class="fu">gausinKernel</span>(Kdata<span class="sc">$</span>Area<span class="sc">-</span>x, bandwidth)</span>
<span id="cb40-21"><a></a><span class="sc">+</span>           <span class="fu">sum</span>( Kdata<span class="sc">$</span>RiverFlow <span class="sc">*</span> K<span class="sc">/</span><span class="fu">sum</span>(K) )</span>
<span id="cb40-22"><a></a><span class="sc">+</span>         })</span>
<span id="cb40-23"><a></a><span class="sc">+</span>       )</span>
<span id="cb40-24"><a></a><span class="sc">+</span>   ) <span class="sc">|&gt;</span> </span>
<span id="cb40-25"><a></a><span class="sc">+</span>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>x_hat, <span class="at">y=</span>y_hat)) <span class="sc">+</span> </span>
<span id="cb40-26"><a></a><span class="sc">+</span>   <span class="fu">geom_line</span>(<span class="at">color=</span><span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb40-27"><a></a><span class="sc">+</span>   <span class="fu">geom_point</span>(<span class="at">data =</span> Kdata, <span class="fu">aes</span>(<span class="at">x=</span>Area, <span class="at">y=</span>RiverFlow), <span class="at">size=</span><span class="dv">4</span>, <span class="at">color=</span><span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb40-28"><a></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Kernel regression"</span>, <span class="at">subtitle =</span> stringr<span class="sc">::</span><span class="fu">str_glue</span>(<span class="st">"bandwith = {bandwidth}; data = red | fit = blue"</span>) ) <span class="sc">+</span></span>
<span id="cb40-29"><a></a><span class="sc">+</span>   <span class="fu">theme_minimal</span>()</span>
<span id="cb40-30"><a></a><span class="sc">+</span> }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="panel-tabset">
<ul id="tabset-7" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-7-1">B = 5</a></li><li><a href="#tabset-7-2">B=10</a></li><li><a href="#tabset-7-3">B=15</a></li></ul>
<div class="tab-content">
<div id="tabset-7-1">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="BSMM_8740_lec_03_files/figure-revealjs/unnamed-chunk-21-1.png" class="quarto-figure quarto-figure-center" width="960"></p>
</figure>
</div>
</div>
</div>
</div>
<div id="tabset-7-2">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="BSMM_8740_lec_03_files/figure-revealjs/unnamed-chunk-22-1.png" class="quarto-figure quarto-figure-center" width="960"></p>
</figure>
</div>
</div>
</div>
</div>
<div id="tabset-7-3">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="BSMM_8740_lec_03_files/figure-revealjs/unnamed-chunk-23-1.png" class="quarto-figure quarto-figure-center" width="960"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="kernel-regression-2" class="slide level2">
<h2>Kernel Regression:</h2>
<div style="font-size: xx-large">
<h3 id="advantages-of-kernel-regression">Advantages of Kernel Regression</h3>
<ul>
<li><strong>Flexibility</strong>: Can capture complex, non-linear relationships between variables.</li>
<li><strong>No Assumptions</strong>: Does not require the assumption of a specific functional form for the relationship.</li>
</ul>
<h3 id="disadvantages-of-kernel-regression">Disadvantages of Kernel Regression</h3>
<ul>
<li><strong>Computationally Intensive</strong>: Can be slow, especially with large datasets, since it requires calculating weights for all data points for each estimate.</li>
<li><strong>Choice of Parameters</strong>: The results can be sensitive to the choice of kernel function and bandwidth, requiring careful tuning.</li>
</ul>
</div>
</section>
<section id="regression-with-neural-nets" class="slide level2">
<h2>Regression with neural nets</h2>
<p>Regression with neural nets involves using artificial neural networks (ANNs) to predict a continuous output variable based on one or more input variables. Neural nets are powerful, flexible models that can capture complex relationships and patterns in the data.</p>
</section>
<section id="regression-with-anns-components" class="slide level2">
<h2>Regression with ANNs: Components</h2>
<div style="font-size: x-large">
<ol type="1">
<li><strong>Neurons</strong>:
<ul>
<li>The building blocks of neural networks. Each neuron takes an input, processes it, and passes the output to the next layer.</li>
</ul></li>
<li><strong>Layers</strong>:
<ul>
<li><strong>Input Layer</strong>: Receives the input data.</li>
<li><strong>Hidden Layers</strong>: Intermediate layers that process the input data through neurons. There can be one or more hidden layers.</li>
<li><strong>Output Layer</strong>: Produces the final prediction.</li>
</ul></li>
<li><strong>Weights and Biases</strong>:
<ul>
<li>Each connection between neurons has a weight, which adjusts the strength of the signal.</li>
<li>Each neuron has a bias, which adjusts the output along with the weighted sum of inputs.</li>
</ul></li>
</ol>
</div>
</section>
<section id="regression-with-anns-components-1" class="slide level2">
<h2>Regression with ANNs: Components</h2>
<div style="font-size: x-large">
<ol start="4" type="1">
<li><strong>Activation Functions</strong>:
<ul>
<li>Functions applied to the output of each neuron in hidden layers to introduce non-linearity. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.</li>
</ul></li>
<li><strong>Loss Function</strong>:
<ul>
<li>Measures the difference between the predicted output and the actual output. For regression tasks, common loss functions include Mean Squared Error (MSE) and Mean Absolute Error (MAE).</li>
</ul></li>
<li><strong>Optimization Algorithm</strong>:
<ul>
<li>Adjusts the weights and biases to minimize the loss function. The most common optimization algorithm is Gradient Descent and its variants like Adam.</li>
</ul></li>
</ol>
</div>
</section>
<section id="regression-with-anns-algorithm" class="slide level2">
<h2>Regression with ANNs: Algorithm</h2>
<div style="font-size: x-large">
<ol type="1">
<li><strong>Forward Propagation</strong>:
<ul>
<li>Input data is passed through the network, layer by layer, with each neuron applying its weights, bias, and activation function, until the output layer produces the prediction.</li>
</ul></li>
<li><strong>Loss Calculation</strong>:
<ul>
<li>The loss function calculates the error between the predicted output and the actual target value.</li>
</ul></li>
<li><strong>Backward Propagation</strong>:
<ul>
<li>The network uses the error to adjust the weights and biases. This involves calculating the gradient of the loss function with respect to each weight and bias (using the chain rule), and then updating the weights and biases to reduce the error.</li>
</ul></li>
<li><strong>Iterative Training</strong>:
<ul>
<li>The process of forward propagation, loss calculation, and backward propagation is repeated for many iterations (epochs) until the loss converges to a minimum value.</li>
</ul></li>
</ol>
</div>
</section>
<section id="regression-with-anns" class="slide level2">
<h2>Regression with ANNs:</h2>
<div class="panel-tabset">
<ul id="tabset-8" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-8-1">Advantages</a></li><li><a href="#tabset-8-2">Disadvantages</a></li></ul>
<div class="tab-content">
<div id="tabset-8-1">
<h3 id="advantages-of-anns-for-regression">Advantages of ANNs for Regression</h3>
<ul>
<li><strong>Flexibility</strong>: Can model complex, non-linear relationships between inputs and outputs.</li>
<li><strong>High Performance</strong>: Can achieve high accuracy with sufficient data and proper tuning.</li>
<li><strong>Feature Learning</strong>: Automatically learns relevant features from raw input data.</li>
</ul>
</div>
<div id="tabset-8-2">
<h3 id="disadvantages-of-anns-for-regression">Disadvantages of ANNs for Regression</h3>
<ul>
<li><strong>Computationally Intensive</strong>: Requires significant computational resources.</li>
<li><strong>Data Hungry</strong>: Needs a large amount of training data to perform well.</li>
<li><strong>Complexity</strong>: Requires careful tuning of hyperparameters (e.g., number of layers, neurons, learning rate) and can be prone to overfitting if not properly regularized.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="regression-with-neural-nets-1" class="slide level2">
<h2>Regression with neural nets</h2>
<p>Architecture of an ANN</p>

<img data-src="../images/single_layer_nn.png" alt="credit deep learning.a" class="r-stretch quarto-figure-center"><p class="caption">Single layer NN architecture</p></section>
<section id="regression-with-neural-nets-2" class="slide level2">
<h2>Regression with neural nets</h2>

<img data-src="../images/activation_functions.png" alt="credits - analyticsindiamag" class="r-stretch quarto-figure-center"><p class="caption">Common Activation Functions</p></section>
<section id="regression-with-anns-example" class="slide level2">
<h2>Regression with ANNs: example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41" data-code-line-numbers="1|3-4|6-11|13-17|19-25|27-28|30-39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a></a><span class="sc">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">500</span>)</span>
<span id="cb41-2"><a></a><span class="sc">&gt;</span>   </span>
<span id="cb41-3"><a></a><span class="er">&gt;</span> <span class="co"># Boston dataset from MASS</span></span>
<span id="cb41-4"><a></a><span class="er">&gt;</span> data <span class="ot">&lt;-</span> MASS<span class="sc">::</span>Boston</span>
<span id="cb41-5"><a></a><span class="sc">&gt;</span> </span>
<span id="cb41-6"><a></a><span class="er">&gt;</span> <span class="co"># Normalize the data</span></span>
<span id="cb41-7"><a></a><span class="er">&gt;</span> maxs <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">summarise_all</span>(max) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> <span class="fu">as.vector</span>()</span>
<span id="cb41-8"><a></a><span class="sc">&gt;</span> mins <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">summarise_all</span>(min) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> <span class="fu">as.vector</span>()</span>
<span id="cb41-9"><a></a><span class="sc">&gt;</span> data_scaled <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> </span>
<span id="cb41-10"><a></a><span class="sc">+</span>   <span class="fu">scale</span>(<span class="at">center =</span> mins, <span class="at">scale =</span> maxs <span class="sc">-</span> mins) <span class="sc">%&gt;%</span> </span>
<span id="cb41-11"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">as_tibble</span>()</span>
<span id="cb41-12"><a></a><span class="sc">&gt;</span>   </span>
<span id="cb41-13"><a></a><span class="er">&gt;</span> <span class="co"># Split the data into training and testing set</span></span>
<span id="cb41-14"><a></a><span class="er">&gt;</span> data_split <span class="ot">&lt;-</span> data_scaled <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(<span class="at">prop =</span> .<span class="dv">75</span>)</span>
<span id="cb41-15"><a></a><span class="sc">&gt;</span> <span class="co"># extracting training data and test data as two seperate dataframes</span></span>
<span id="cb41-16"><a></a><span class="er">&gt;</span> data_train <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(data_split)</span>
<span id="cb41-17"><a></a><span class="sc">&gt;</span> data_test  <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(data_split)</span>
<span id="cb41-18"><a></a><span class="sc">&gt;</span> </span>
<span id="cb41-19"><a></a><span class="er">&gt;</span> nn <span class="ot">&lt;-</span> data_train <span class="sc">%&gt;%</span> </span>
<span id="cb41-20"><a></a><span class="sc">+</span>   neuralnet<span class="sc">::</span><span class="fu">neuralnet</span>(</span>
<span id="cb41-21"><a></a><span class="sc">+</span>     medv <span class="sc">~</span> .</span>
<span id="cb41-22"><a></a><span class="sc">+</span>     , <span class="at">data =</span> .</span>
<span id="cb41-23"><a></a><span class="sc">+</span>     , <span class="at">hidden =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb41-24"><a></a><span class="sc">+</span>     , <span class="at">linear.output =</span> <span class="cn">TRUE</span></span>
<span id="cb41-25"><a></a><span class="sc">+</span>   )</span>
<span id="cb41-26"><a></a><span class="sc">&gt;</span>   </span>
<span id="cb41-27"><a></a><span class="er">&gt;</span> <span class="co"># Predict on test data</span></span>
<span id="cb41-28"><a></a><span class="er">&gt;</span> pr.nn <span class="ot">&lt;-</span> neuralnet<span class="sc">::</span><span class="fu">compute</span>( nn, data_test <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>medv) )</span>
<span id="cb41-29"><a></a><span class="sc">&gt;</span>   </span>
<span id="cb41-30"><a></a><span class="er">&gt;</span> <span class="co"># Compute mean squared error</span></span>
<span id="cb41-31"><a></a><span class="er">&gt;</span> pr.nn_ <span class="ot">&lt;-</span> </span>
<span id="cb41-32"><a></a><span class="sc">+</span>   pr.nn<span class="sc">$</span>net.result <span class="sc">*</span> </span>
<span id="cb41-33"><a></a><span class="sc">+</span>   (<span class="fu">max</span>(data<span class="sc">$</span>medv) <span class="sc">-</span> <span class="fu">min</span>(data<span class="sc">$</span>medv)) <span class="sc">+</span></span>
<span id="cb41-34"><a></a><span class="sc">+</span>   <span class="fu">min</span>(data<span class="sc">$</span>medv)</span>
<span id="cb41-35"><a></a><span class="sc">&gt;</span> test.r <span class="ot">&lt;-</span> </span>
<span id="cb41-36"><a></a><span class="sc">+</span>   data_test<span class="sc">$</span>medv <span class="sc">*</span> </span>
<span id="cb41-37"><a></a><span class="sc">+</span>   (<span class="fu">max</span>(data<span class="sc">$</span>medv) <span class="sc">-</span> <span class="fu">min</span>(data<span class="sc">$</span>medv)) <span class="sc">+</span> </span>
<span id="cb41-38"><a></a><span class="sc">+</span>   <span class="fu">min</span>(data<span class="sc">$</span>medv)</span>
<span id="cb41-39"><a></a><span class="sc">&gt;</span> MSE.nn <span class="ot">&lt;-</span> <span class="fu">sum</span>((test.r <span class="sc">-</span> pr.nn_)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">nrow</span>(data_test)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="regression-with-neural-nets-3" class="slide level2">
<h2>Regression with neural nets</h2>
<div class="panel-tabset">
<ul id="tabset-9" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-9-1">NN</a></li><li><a href="#tabset-9-2">Regression</a></li></ul>
<div class="tab-content">
<div id="tabset-9-1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/nn_plot.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
<div id="tabset-9-2">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="BSMM_8740_lec_03_files/figure-revealjs/unnamed-chunk-25-1.png" class="quarto-figure quarto-figure-center" width="960"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap" class="slide level2">
<h2>Recap</h2>
<ul>
<li><p>Today we worked though a parametric and non-parametric regression methods that are useful for predicting a value given a set of covariates.</p></li>
<li><p>Next week we will look in detail at the <code>tidymodels</code> package which will give a way to develop a workflow for fitting and comparing our models across different feature sets.</p></li>
</ul>
</section>
<section id="section" class="slide level2">
<h2></h2>


</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="images/logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://bsmm-8740-fall-2024.github.io/osb/">bsmm-8740-fall-2024.github.io/osb</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="../site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true,"numbers":true},
'chalkboard': {"buttons":true},
'multiplex': {"secret":null,"id":"acd02a16f3876c22","url":"https://reveal-multiplex.glitch.me/"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 5.0e-2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/bsmm-8740-fall-2024\.github\.io\/osb\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>