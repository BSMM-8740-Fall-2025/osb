---
title: "Regression methods"
subtitle: "BSMM8740-2-R-2023F [WEEK - 4]"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2023.github.io/osb](https://bsmm-8740-fall-2023.github.io/osb/)"
logo: "images/logo.png"
# title-slide-attributes:
#   data-background-image: images/my-DRAFT.png
#   data-background-size: contain
#   data-background-opacity: "0.40"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

```{r opts, include = FALSE}
options(width = 90)
library(knitr)
opts_chunk$set(comment="", 
               digits = 3, 
               tidy = FALSE, 
               prompt = TRUE,
               fig.align = 'center')
require(magrittr)
require(ggplot2)
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

## Recap of last lecture

-   Last time we worked with the `recipes` package to develop workflows for pre-processing our data.

-   Today we look at regression methods we might apply to our data.

# Linear regression

## Linear regression models

In the simple linear regression model, you have $N$ observations of the response variable $Y$ with a linear combination of $D$ predictor variables $\mathbf{x}$ where

$$
\pi\left(Y=y|\mathbf{x,\theta}\right)=\mathcal{N}\left(\left.y\right|\beta_{0}+\mathbf{\mathbf{\mathbf{\beta}}}'\mathbf{x},\sigma^{2}\right)
$$

## Linear regression models

where $\mathcal{N}\left(\left.y\right|\mu,\sigma^{2}\right)$ is a Normal distribution with mean $\mu$ and variance $\sigma^2$, $\theta=\left(\beta_{0},\mathbf{\mathbf{\mathbf{\beta}}},\sigma^{2}\right)$ are the *parameters* of the model and the vector of parameters $\beta_{1:D}$ are the *weights* or *regression* coefficients.

## Linear regression models

The mean function $f(\mathbf{x})\equiv\mu$ could be any linear function in $\mathbf{x}=(x_1,\ldots,x_m)$ in which case $\beta_{0}+\mathbf{\mathbf{\mathbf{\beta}}}'\mathbf{x}$ is a linear approximation around $\beta_{0}$ per the Taylor series for $f(\mathbf{x})$. When $D=1$ the Taylor series is[^1]

[^1]: where $\ldots = \sum_{n=3}^{\infty}\frac{1}{n!}f^{(n)}(\beta_{0})(x-\beta_{0})^{n}$

$$
f(x)=f(\beta_{0})+f^{(1)}(\beta_{0})(x-\beta_{0})+\frac{1}{2}f^{(2)}(\beta_{0})(x-\beta_{0})^{2}+\ldots
$$

## Linear regression models

When $D=2$ the Taylor series is (writing $f_{x}\equiv\frac{\partial f}{\partial x}$, $f_{y}\equiv\frac{\partial f}{\partial y}$, $f_{x,y}\equiv\frac{\partial^2 f}{\partial x,\partial x}$ and so on):

$$
\begin{align*}
f(x,y) & =f(\alpha_{0},\beta_{0})+f_{x}(\alpha_{0},\beta_{0})(x-\alpha_{0})+f_{y}(\alpha_{0},\beta_{0})(y-\beta_{0})\\
 & = + f_{x,x}(\alpha_{0},\beta_{0})(x-\alpha_{0})^{2}+f_{y,y}(\alpha_{0},\beta_{0})(y-\beta_{0})^{2}\\
 & = + f_{x,y}(\alpha_{0},\beta_{0})(x-\alpha_{0})(y-\beta_{0})+\ldots
\end{align*}
$$

## Linear regression models

To fit the 1D linear regression model to $N$ data samples, we minimize the negative log-likelihood on the training set.

$$
\begin{align*}
\text{NLL}\left(\beta,\sigma^{2}\right) & =\sum_{n=1}^{N}\log\left[\left(\frac{1}{2\pi\sigma^{2}}\right)^{\frac{1}{2}}\exp\left(-\frac{1}{2\sigma^{2}}\left(y_{n}-\beta'x_{n}\right)^{2}\right)\right]\\
 & =-\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}\left(y_{n}-\hat{y}_{n}\right)^{2}-\frac{N}{2}\log\left(2\pi\sigma^{2}\right)
\end{align*}
$$

where the predicted response is $\hat{y}\equiv\beta'x_{n}$.

## Linear regression models

Focusing on just the weights, the minimum NLL is (up to a constant) the minimum of the residual sum of squares (RSS):

$$ 
\begin{align*}\text{RSS}\left(\beta\right) & =\frac{1}{2}\sum_{n=1}^{N}\left(y_{n}-\beta'x_{n}\right)^{2}=\frac{1}{2}\left\Vert y_{n}-\beta'x_{n}\right\Vert ^{2}\\
 & =\frac{1}{2}\left(y_{n}-\beta'x_{n}\right)'\left(y_{n}-\beta'x_{n}\right)\\
\\
\end{align*}
$$

## Linear regression models

#### Ordinary least squares (OLS)

We can write our regression assumption as

$$
y_i=\beta_0+\beta_1 x_i + u_i
$$

where $u_i$ is a sample from $\mathcal{N}\left(0,\sigma^{2}\right)$ which in turn implies $\mathbb{E}\left[u\right]=0;\;\mathbb{E}\left[\left.u\right|x\right]=0$

## Linear regression models

#### Ordinary least squares (OLS)

It follows that

$$
\begin{align*}
\mathbb{E}\left[y-\beta_{0}-\beta_{1}x\right] & =0\\
\mathbb{E}\left[x\left(y-\beta_{0}-\beta_{1}x\right)\right] & =0
\end{align*}
$$

## Linear regression models

#### Ordinary least squares (OLS)

Writing the same thing for our samples (where $\hat{\beta}_{0}, \hat{\beta}_{1}$ are our estimates)

$$
\begin{align*}
\frac{1}{N}\sum_{i-1}^{N}y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i} & =0\\
\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right) & =0
\end{align*}
$$

## Linear regression models

#### Ordinary least squares (OLS)

From the first equation

$$
\begin{align*}
\bar{y}-\hat{\beta}_{0}-\hat{\beta}_{1}\bar{x}_{i} & =0\\
\bar{y}-\hat{\beta}_{1}\bar{x} & =\hat{\beta}_{0}
\end{align*}
$$

## Linear regression models

#### Ordinary least squares (OLS)

Substituting the expression for $\hat{\beta}_{0}$ in the independence equation

$$
\begin{align*}
\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(y_{i}-\left(\bar{y}-\hat{\beta}_{1}\bar{x}\right)-\hat{\beta}_{1}x_{i}\right) & =0\\
\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(y_{i}-\bar{y}\right) & =\hat{\beta}_{1}\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(\bar{x}-x_{i}\right)\\
\frac{1}{N}\sum_{i-1}^{N}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right) & =\hat{\beta}_{1}\frac{1}{N}\sum_{i-1}^{N}\left(\bar{x}-x_{i}\right)^2
\end{align*}
$$

## Linear regression models

#### Ordinary least squares (OLS)

So as long as $\sum_{i-1}^{N}\left(\bar{x}-x_{i}\right)^2\ne 0$

$$
\begin{align*}
\hat{\beta}_{1} & =\frac{\sum_{i-1}^{N}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i-1}^{N}\left(\bar{x}_{i}-x_{i}\right)^2}\\
 & =\frac{\text{sample covariance}(x_{i}y_{i})}{\text{sample variance}(x_{i})}
\end{align*}
$$

## Linear regression models

Similarly, in the vector equation, the minimum of the RSS is solved by (assuming $N>D$):

$$
\hat{\mathbf{\beta}}_{OLS}=\left(X'X\right)^{-1}\left(X'Y\right) = \frac{\text{cov}(X,Y)}{\text{var}(X)}
$$

There are algorithmic issues though.

## Linear regression algorithms

Computing the inverse of $X'X$ directly, while theoretically possible, can be numerically unstable.

In R, the $QR$ decomposition is used to solve for $\beta$. Let $X=QR$ where $Q'Q=I$ and write:

$$
\begin{align*}
(QR)\beta & = y\\
Q'QR\beta & = Q'y\\
\beta & = R^{-1}(Q'y)
\end{align*}
$$

Since $R$ is upper triangular, the last equation can be solved by back-substitution.

## Linear regression algorithms

```{r}
#| echo: true
#| message: false
#| code-line-numbers: "1|2"
A <- matrix(c(1,2,5, 2,4,6, 3, 3, 3), nrow=3)
QR <- qr(A)
```

::: panel-tabset
## Q

```{r}
#| echo: true
#| message: false
Q <- qr.Q(QR); Q
```

## R

```{r}
#| echo: true
#| message: false
R <- qr.R(QR); R
```

## A

```{r}
#| echo: true
#| message: false
Q %*% R
```
:::

## Linear regression algorithms

```{r}
#| echo: true
#| message: false
#| layout-nrow: 3
#| code-fold: true
#| code-line-numbers: "1|2-3|5-6|8-9"
# A linear system of equations y = Ax
cat("matrix A\n")
A <- matrix(c(3, 2, -1, 2, -2, .5, -1, 4, -1), nrow=3); A

cat("vector x\n")
x <- c(1, -2, -2); x

cat("vector y\n")
y <- A %*% x ; y
```

## Linear regression algorithms

```{r}
#| echo: true
#| message: false
#| code-line-numbers: "1-4|6-7|9-10"
# Compute the QR decomposition of A
QR <- qr(A)
Q <- qr.Q(QR)
R <- qr.R(QR)

# Compute b=Q'y
b <- crossprod(Q, y); b

# Solve the upper triangular system Rx=b
backsolve(R, b)
```

## Linear regression models

Minimizing the NLL by minimizing the residual sum of squares (RSS) is the same as minimizing

-   the **mean squared error** $\text{MSE}\left(\beta\right) = \frac{1}{N}\text{RSS}\left(\beta\right)$
-   the **root mean squared error** $\text{RMSE}\left(\beta\right) = \sqrt{\text{MSE}\left(\beta\right)}$

::: callout-note
The minimum NLL estimate is also the maximum likelihood estimate (MLE)
:::

## Empirical risk minimization

The MLE can be generalized by replacing the NLL ($\ell\left(y_{n},\theta;x_{n}\right)=-\log\pi\left(y_n|x_n,\theta\right)$) with any other loss function to get

$$
\mathcal{L}\left(\theta\right)=\frac{1}{N}\sum_{n=1}^{N}\ell\left(y_{n},\theta;x_{n}\right)
$$

This is known as the empirical risk minimization (ERM) - the expected loss taken with respect to the empirical distribution.

## Collinearity

One of the important assumptions of the classical linear regression models is that there is no exact collinearity among the regressors. While high correlation between regressors is a necessary indicator of the collinearity problem, a direct linear relationship beween regressors is sufficient.

## Collinearity

Data collection methods, constraints on the fitted regression model, model specification error, an overdefined model, may be some potential sources of multicollinearity. In other cases it is an artifact caused by creating new predictors from other predictors.

## Collinearity

The problem of collinearity has potentially serious effect on the regression estimates such as implausible coefficient signs, impossible inversion of matrix $X'X$ as it becomes near or exactly singular, large magnitude of coefficients in absolute value, large variance or standard errors with wider confidence intervals.

## Ridge Regression

Ridge regression is an example of a penalized regression model; in this case the magnitude of the weights are penalized by adding the $\ell_2$ norm of the weights to the loss function. In particular, the ridge regression weights are:

$$
\hat{\beta}_{\text{ridge}}=\arg\!\min\text{RSS}\left(\beta\right)+\lambda\left\Vert \beta\right\Vert _{2}^{2}
$$

where $\lambda$ is the strength of the regularizer term.

## Ridge Regression

The solution is:

$$
\begin{align*}
\hat{\mathbf{\beta}}_{ridge} & =\left(X'X-\lambda I_{D}\right)^{-1}\left(X'Y\right)\\
 & =\left(\sum_{n}x_{n}x'_{n}+\lambda I_{D}\right)^{-1}\left(\sum_{n}y_{n}x_{n}\right)
\end{align*}
$$

## Ridge Regression

As for un-penalized linear regression, using matrix inversion to solve for $\hat{\mathbf{\beta}}_{ridge}$ can be a bad idea. The QR transformation can be used here, however, ridge regression is often used when $D>N$, in which case the SVD transformation is faster.

## Ridge Regression Example

```{r}
#| echo: true
#| message: false
#| code-line-numbers: "2|5"
#define response variable
y <- mtcars %>% dplyr::pull(hp)

#define matrix of predictor variables
x <- mtcars %>% dplyr::select(mpg, wt, drat, qsec) %>% data.matrix()
```

## Ridge Regression Example

```{r}
#| echo: true
#| message: false
#| code-line-numbers: "1-2|4-5"
# fit ridge regression model
model <- glmnet::glmnet(x, y, alpha = 0)

# get coefficients when lambda = 7.6
coef(model, s = 7.6)
```

## Ridge Regression Example

```{r}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "glmnet example"
#| code-line-numbers: "1-2|4-5|7-8|9|10|11|12"
# perform k-fold cross-validation to find optimal lambda value
cv_model <- glmnet::cv.glmnet(x, y, alpha = 0)

# find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min

# produce plot of test MSE by lambda value
cv_model %>% broom::tidy() %>% 
ggplot(aes(x=lambda, y = estimate)) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "#00ABFD", alpha=0.5) +
  geom_point() +
  geom_vline(xintercept=best_lambda) +
  labs(title='Ridge Regression'
       , subtitle = 
         stringr::str_glue(
           "The best lambda value is {scales::number(best_lambda, accuracy=0.01)}"
         )
  ) +
  ggplot2::scale_x_log10()
```

## Ridge Regression Example

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "glmnet coefficients"
model$beta %>% 
  as.matrix() %>% 
  t() %>% 
  tibble::as_tibble() %>% 
  tibble::add_column(lambda = model$lambda, .before = 1) %>% 
  tidyr::pivot_longer(-lambda, names_to = 'parameter') %>% 
  ggplot(aes(x=lambda, y=value, color=parameter)) +
  geom_line() + geom_point() +
  xlim(0,2000) +
  labs(title='Ridge Regression'
       , subtitle = 
         stringr::str_glue(
           "Parameters as a function of lambda"
         )
  )
```

## Lasso Regression

Lasso regression is another example of a penalized regression model; in this case both the magnitude of the weights and the number of parameters are penalized by using the $\ell_1$ norm of the weights to the loss function of the lasso regression. In particular, the lasso regression weights are:

$$
\hat{\beta}_{\text{lasso}}=\arg\!\min\text{RSS}\left(\beta\right)+\lambda\left\Vert \beta\right\Vert _{1}
$$

## Lasso Regression

The Lasso objective function is

$$
\mathcal{L}\left(\beta,\lambda\right)=\text{NLL}+\lambda\left\Vert \beta\right\Vert _{1}
$$

## Lasso Regression Example

```{r}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "lasso model"
#| code-line-numbers: "1-2|4-5|7-8|10-11"
# define response variable
y <- mtcars %>% dplyr::pull(hp)

# define matrix of predictor variables
x <- mtcars %>% dplyr::select(mpg, wt, drat, qsec) %>% data.matrix()

# fit ridge regression model
model <- glmnet::glmnet(x, y, alpha = 1)

# get coefficients when lambda = 3.53
coef(model, s = 3.53)
```

## Lasso Regression Example

```{r}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "lasso example"
#| code-line-numbers: "2|5|8|9|10|11|12"
#perform k-fold cross-validation to find optimal lambda value
cv_model <- glmnet::cv.glmnet(x, y, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min

#produce plot of test MSE by lambda value
cv_model %>% broom::tidy() %>% 
ggplot(aes(x=lambda, y = estimate)) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "#00ABFD", alpha=0.5) +
  geom_point() +
  geom_vline(xintercept=best_lambda) +
  labs(title='Lasso Regression'
       , subtitle = 
         stringr::str_glue(
           "The best lambda value is {scales::number(best_lambda, accuracy=0.01)}"
         )
  ) +
  xlim(0,exp(4)) + ggplot2::scale_x_log10()
```

## Lasso Regression Example

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "lasso coefficients"
model %>%
  broom::tidy() %>%
  tidyr::pivot_wider(names_from=term, values_from=estimate) %>%
  dplyr::select(-c(step,dev.ratio, `(Intercept)`)) %>%
  dplyr::mutate_all(dplyr::coalesce, 0) %>% 
  tidyr::pivot_longer(-lambda, names_to = 'parameter') %>% 
  ggplot(aes(x=lambda, y=value, color=parameter)) +
  geom_line() + geom_point() +
  xlim(0,70) +
  labs(title='Ridge Regression'
       , subtitle = 
         stringr::str_glue(
           "Parameters as a function of lambda"
         )
  )
```

## Elastic Net Regression

Elastic Net regression is a hybrid of ridge and lasso regression.

The elastic net objective function is

$$
\mathcal{L}\left(\beta,\lambda,\alpha\right)=\text{NLL}+\lambda\left(\left(1-\alpha\right)\left\Vert \beta\right\Vert _{2}^{2}+\alpha\left\Vert \beta\right\Vert _{1}\right)
$$

so that $\alpha=0$ is ridge regression and $\alpha=1$ is lasso regression and $\alpha\in\left(0,1\right)$ is the general elastic net.

## Elastic Net Regression Example

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "elastic net example"
#| code-line-numbers: "5-9|10-13|15|16-17|18-29|33-35"
# set length of data and seed for reproducability
n <- 50
set.seed(2467)
# create the dataset
dat <- tibble::tibble(
  a = sample(1:20, n, replace = T)/10
  , b = sample(1:10, n, replace = T)/10
  , c = sort(sample(1:10, n, replace = T))
) %>% 
  dplyr::mutate(
    z = (a*b)/2 + c + sample(-10:10, n, replace = T)/10
    , .before = 1
  )
# cross validate to get the best alpha
alpha_dat <- tibble::tibble( alpha = seq(0.01, 0.99, 0.01) ) %>% 
  dplyr::mutate(
    mse =
      purrr::map_dbl(
        alpha
        , (\(a){
          cvg <- 
           glmnet::cv.glmnet(
             x = dat %>% dplyr::select(-z) %>% as.matrix() 
             , y = dat$z 
             , family = "gaussian"
             , gamma = a
          )
          min(cvg$cvm)
        })
      )
  ) 

best_alpha <- alpha_dat %>% 
  dplyr::filter(mse == min(mse)) %>% 
  dplyr::pull(alpha)

cat("best alpha:", best_alpha)
```

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "1-6|8-9|11-15"
#| code-summary: "elastic net example, part 2"
elastic_cv <- 
  glmnet::cv.glmnet(
    x = dat %>% dplyr::select(-z) %>% as.matrix() 
    , y = dat$z 
    , family = "gaussian"
    , gamma = best_alpha)

best_lambda <- elastic_cv$lambda.min
cat("best lambda:", best_lambda)

elastic_mod <- glmnet::glmnet(
  x = dat %>% dplyr::select(-z) %>% as.matrix() 
  , y = dat$z 
  , family = "gaussian"
  , gamma = best_alpha, lambda = best_lambda)

elastic_mod %>% broom::tidy()
```

## Elastic Net Regression Example

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "1|3-5|7"
#| code-summary: "elastic net example, part 3"
pred <- predict(elastic_mod, dat %>% dplyr::select(-z) %>% as.matrix())

rmse <- sqrt(mean( (pred - dat$z)^2 ))
R2 <- 1 - (sum((dat$z - pred )^2)/sum((dat$z - mean(y))^2))
mse <- mean((dat$z - pred)^2)

cat(" RMSE:", rmse, "\n", "R-squared:", R2, "\n", "MSE:", mse)
```

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "elastic net example, part 4"
dat %>% 
  tibble::as_tibble() %>% 
  tibble::add_column(pred = pred[,1]) %>% 
  tibble::rowid_to_column("ID") %>% 
  ggplot(aes(x=ID, y=z)) +
  geom_point() +
  geom_line(aes(y=pred),color='red')
```

## Generalized Linear Models

A **generalized linear model** (**GLM**) is a flexible generalization of ordinary linear regression.

Ordinary linear regression predicts the expected value of the outcome variable, a random variable, as a linear combination of a set of observed values (*predictors*). In a generalized linear model (GLM), each outcome $Y$ is assumed to be generated from a particular distribution in an exponential family, The mean, $\mu$, of the distribution depends on the independent variables, $X$, through:

$$
\mathbb{E}\left[\left.Y\right|X\right]=\mu=\text{g}^{-1}\left(X\beta\right)
$$ where $g$ is called the **link function**.

## Generalized Linear Models

For example, if $Y$ is Poisson distributed, then

$$
\mathbb{P}\left[\left.Y=y\right|X,\lambda\right]=\frac{\lambda^{y}}{y!}e^{-\lambda}=e^{y\log\lambda-\lambda-\log y!}
$$

Where $\lambda$ is both the mean and the variance. In the glm the link function is $\log$ and

$$
\log\mathbb{E}\left[\left.Y\right|X\right] = \beta X=\log\lambda
$$

# Tree regression

## Regression with trees

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat <- MASS::Boston
```

There are many methodologies for constructing regression trees but one of the oldest is known as the **c**lassification **a**nd **r**egression **t**ree (CART) approach.

Basic regression trees *partition* a data set into smaller subgroups and then fit a simple *constant* for each observation in the subgroup. The partitioning is achieved by successive binary partitions (aka *recursive partitioning*) based on the different predictors.

## Regression with trees

As a simple example, consider a continuous response variable $y$ with two covariates $x_1,x_2$ and the support of $x_1,x_2$ partitioned into three regions. Then we write the tree regression model for $y$ as:

$$
\hat{y} = \hat{f}(x_1,x_2)=\sum_{i=1}^{3}c_1\times I_{(x_1,x_2)\in R_i}
$$ Tree algorithm differ in how they grow the regression tree, i.e. partition the space of the covariates.

## Regression with trees

All partitioning of variables is done in a top-down, greedy fashion. This just means that a partition performed earlier in the tree will not change based on later partitions. In general the partitions are made to minimize following objective function (support initially partitioned into 2 regions, i.e. a binary tree):

$$
\text{SSE}=\left\{ \sum_{i\in R_{1}}\left(y_{i}-c_{i}\right)^{2}+\sum_{i\in R_{2}}\left(y_{i}-c_{i}\right)^{2}\right\} 
$$

## Regression with trees

Having found the best split, we repeat the splitting process on each of the two regions.

This process is continued until some stopping criterion is reached. What typically results is a very deep, complex tree that may produce good predictions on the training set, but is likely to overfit the data, particularly at the lower nodes.

By pruning these lower level nodes, we can introduce a little bit of bias in our model that help to stabilize predictions and will tend to generalize better to new, unseen data.

## Regression with trees

As with penalized linear regression, we can us a complexity parameter $\alpha$ to penalize the number of terminal nodes of the tree ($T$), like the lasso $L_1$ norm penalty, and find the smallest tree with lowest penalized error, i.e. the minimizing the following objective function:

$$
\text{SSE}+\alpha\left|T\right|
$$

## Regression with trees

::: columns
::: {.column width="50%" style="font-size: 32px"}
Strengths

-   They are very interpretable.
-   Making predictions is fast; just lookup constants in the tree.
-   Variables importance is easy; those variables that most reduce the SSE.
-   Tree models give a non-linear response; better if the true regression surface is not smooth.
-   There are fast, reliable algorithms to learn these trees.
:::

::: {.column width="50%" style="font-size: 32px"}
Weaknesses

-   Single regression trees have high variance, resulting in unstable predictions (an alternative subsample of training data can significantly change the terminal nodes).
-   Due to the high variance single regression trees have poor predictive accuracy.
:::
:::

## Regression with trees (Bagging)

As mentioned, single tree models suffer from high variance. Although pruning the tree helps reduce this variance, there are alternative methods that actually exploite the variability of single trees in a way that can significantly improve performance over and above that of single trees. ***B**ootstrap* ***agg**regat**ing*** (***bagging***) is one such approach.

Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance.

## Regression with trees (Bagging)

Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, improving predictive performance.

## Regression with trees (Bagging)

Bagging follows three steps:

-   Create $m$ [bootstrap samples](http://uc-r.github.io/bootstrapping) from the training data. Bootstrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.
-   For each bootstrap sample train a single, unpruned regression tree.
-   Average individual predictions from each tree to create an overall average predicted value.

## Regression with trees (Bagging)

![Fig: The bagging process.](https://uc-r.github.io/public/images/analytics/regression_trees/bagging3.png)

## Regression with a random forest

Bagging trees introduces a random component into the tree building process that reduces the variance of a single tree's prediction and improves predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree.

So trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships. They are correlated.

## Regression with a random forest

Tree correlation prevents bagging from optimally reducing the variance of the predictive values. Reducing variance further can be achieved by injecting more randomness into the tree-growing process. Random forests achieve this in two ways:

::: {style="font-size: smaller"}
1.  **Bootstrap**: similar to bagging - each tree is grown from a bootstrap resampled data set, which *somewhat* decorrelates them.
2.  **Split-variable randomization**: each time a split is made, the search for the split variable is limited to a random subset of $m$ of the $p$ variables.
:::

## Regression with a random forest

For regression trees, typical default values used in split-value randomization are $m=\frac{p}{3}$ but this should be considered a tuning parameter.

When $m=p$, the randomization amounts to using only step 1 and is the same as *bagging*.

## Regression with a random forest

::: columns
::: {.column width="50%" style="font-size: 32px"}
Strengths

-   Typically have very good performance
-   Remarkably good "out-of-the box" - very little tuning required
-   Built-in validation set - don't need to sacrifice data for extra validation
-   No pre-processing required
-   Robust to outliers
:::

::: {.column width="50%" style="font-size: 32px"}
Weaknesses

-   Can become slow on large data sets
-   Although accurate, often cannot compete with advanced boosting algorithms
-   Less interpretable
:::
:::

## Regression with gradient boosting

Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions.

## Regression with gradient boosting

Whereas [random forests](http://uc-r.github.io/random_forests) build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful "committee" that are often hard to beat with other algorithms.

## Regression with gradient boosting

The main idea of boosting is to add new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.

![Sequential ensemble approach.](/images/boosted-trees-process.png)

## Regression with gradient boosting

Boosting is a framework that iteratively improves *any* weak learning model. Many gradient boosting applications allow you to "plug in" various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner.

## Regression with gradient boosting

A weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. With regards to decision trees, shallow trees represent a weak learner. Commonly, trees with only 1-6 splits are used.

## Regression with gradient boosting

Combining many weak models (versus strong ones) has a few benefits:

::: {style="font-size: smaller"}
-   Speed: Constructing weak models is computationally cheap.
-   Accuracy improvement: Weak models allow the algorithm to *learn slowly*; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.
-   Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).
:::

## Regression with gradient boosting

Here is the algorithm for boosted regression trees with features $x$ and response $y$:

::: {style="font-size: smaller"}
1.  Fit a decision tree to the data: $F_1(x)=y$,
2.  We then fit the next decision tree to the residuals of the previous: $h_1(x)=y−F_1(x)$
3.  Add this new tree to our algorithm: $F_2(x)=F_1(x)+h_1(x)$,
4.  Fit the next decision tree to the residuals of $F_2: h_2(x)=y−F_2(x)$,
5.  Add this new tree to our algorithm: $F_3(x)=F_2(x)+h_1(x)$,
6.  Continue this process until some mechanism (i.e. cross validation) tells us to stop.
:::

## XGBoost Example

**XGBoost** is short for e**X**treme **G**radient **Boost**ing package.

While the `XGBoost` model often achieves higher accuracy than a single decision tree, it sacrifices the intrinsic interpretability of decision trees. For example, following the path that a decision tree takes to make its decision is trivial and self-explained, but following the paths of hundreds or thousands of trees is much harder.

We will work with `XGBoost` in today's lab.

## Regression with neural nets

Architecture of a Neural Net (NN)

![Single layer NN architecture](/images/single_layer_nn.png){fig-alt="credit deep learning.a"}

## Regression with neural nets

![Common Activation Functions](/images/activation_functions.png){fig-alt="credits - analyticsindiamag"}

## Regression with neural nets

```{r}
#| echo: true
#| warning: false
#| message: false
# layout-nrow: 3
#| code-line-numbers: "1|3-4|6-11|13-17|19-25|27-28|30-39"
#| code-fold: true
set.seed(500)
  
# Boston dataset from MASS
data <- MASS::Boston

# Normalize the data
maxs <- data %>% dplyr::summarise_all(max) %>% as.matrix() %>% as.vector()
mins <- data %>% dplyr::summarise_all(min) %>% as.matrix() %>% as.vector()
data_scaled <- data %>% 
  scale(center = mins, scale = maxs - mins) %>% 
  tibble::as_tibble()
  
# Split the data into training and testing set
data_split <- data_scaled %>% rsample::initial_split(prop = .75)
# extracting training data and test data as two seperate dataframes
data_train <- rsample::training(data_split)
data_test  <- rsample::testing(data_split)

nn <- data_train %>% 
  neuralnet::neuralnet(
    medv ~ .
    , data = .
    , hidden = c(5, 3)
    , linear.output = TRUE
  )
  
# Predict on test data
pr.nn <- neuralnet::compute( nn, data_test %>% dplyr::select(-medv) )
  
# Compute mean squared error
pr.nn_ <- 
  pr.nn$net.result * 
  (max(data$medv) - min(data$medv)) +
  min(data$medv)
test.r <- 
  data_test$medv * 
  (max(data$medv) - min(data$medv)) + 
  min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2) / nrow(data_test)  
```

## Regression with neural nets

::: panel-tabset
## NN

![](images/nn_plot.png){fig-align="center"}

## Regression

```{r}
tibble::tibble(test = data_test$medv, predicted = pr.nn$net.result) %>% 
  ggplot(aes(x=test, y=predicted)) +
  geom_point(color='red') +
  geom_abline(intercept = 0, slope = 1, color="blue", 
                 linetype="dashed", linewidth=1.5) +
  labs(title='Neural Net Regression'
     , subtitle = 
       stringr::str_glue(
         "Mean squared prediction error is {scales::number(MSE.nn, accuracy=0.01)}"
       )
  )
```
:::

## Recap

-   Today we worked though a few regression methods that are useful for predicting a value given a set of covariates

-   Next week we will look at the `tidymodels` package which will give a way to develop a workflow for fitting and comparing our models

## 
