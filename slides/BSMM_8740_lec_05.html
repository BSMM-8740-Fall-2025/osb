<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="L.L. Odette">
  <title>BSMM-8740 - Fall 2025 – Classification &amp; clustering methods</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-156995b38130bdaa56919fb0a5db863b.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Classification &amp; clustering methods – BSMM-8740 - Fall 2025">
<meta property="og:description" content="BSMM8740-2-R-2024F [WEEK - 5]">
<meta property="og:site_name" content="BSMM-8740 - Fall 2025">
<meta name="twitter:title" content="Classification &amp; clustering methods – BSMM-8740 - Fall 2025">
<meta name="twitter:description" content="BSMM8740-2-R-2024F [WEEK - 5]">
<meta name="twitter:image" content="https://bsmm-8740-fall-2025.github.io/osb/slides/images/twitter-card.png">
<meta name="twitter:creator" content="@lodette">
<meta name="twitter:card" content="summary_large_image">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Classification &amp; clustering methods</h1>
  <p class="subtitle">BSMM8740-2-R-2024F [WEEK - 5]</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
L.L. Odette 
</div>
</div>
</div>

</section>
<section id="recap-of-last-week" class="slide level2">
<h2>Recap of last week</h2>
<ul>
<li><p>Last time we introduced the Tidymodels framework in R</p></li>
<li><p>We showed how we can use the Tidymodels framework to create a workflow for data prep, feature engineering, model fitting and model evaluation.</p></li>
<li><p>Today we look at the using the Tidymodels package to build classification and clustering models.</p></li>
</ul>
</section>
<section>
<section id="classification-methods" class="title-slide slide level1 center">
<h1>Classification Methods</h1>

</section>
<section id="classification" class="slide level2">
<h2>Classification</h2>
<ul>
<li>Classification is a supervised machine learning method where the model tries to predict a categorical outcome for given input data.</li>
<li>These models are essential in various business applications, such as credit scoring, customer segmentation, fraud detection, and more. Classification methods can broadly be categorized into two types: eager learners and lazy (instance-based) learners.</li>
</ul>
</section>
<section id="eager-learners" class="slide level2">
<h2>Eager Learners</h2>
<p><strong>Eager learners</strong> are machine learning algorithms that first build a model from the training dataset before making any prediction on future datasets. They spend more time on the training process to better generalize from the data.</p>
<p>They usually require less time to make predictions.</p>
</section>
<section id="eager-learners-1" class="slide level2">
<h2>Eager Learners</h2>
<h3 id="example-eager-learners-are">Example eager learners are:</h3>
<div class="panel-tabset">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1">LR</a></li><li><a href="#tabset-1-2">DT</a></li><li><a href="#tabset-1-3">RF</a></li><li><a href="#tabset-1-4">SVM</a></li><li><a href="#tabset-1-5">ANN</a></li><li><a href="#tabset-1-6">GMB</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1">
<div style="font-size: x-large">
<p><strong>Logistic Regression</strong>:</p>
<ul>
<li><strong>Overview</strong>: A statistical method for binary classification that models the probability of a binary outcome based on one or more predictor variables.</li>
<li><strong>Advantages</strong>: Simple, interpretable, and works well with linear decision boundaries.</li>
<li><strong>Disadvantages</strong>: Assumes linearity between predictors and the log-odds of the outcome.</li>
</ul>
</div>
</div>
<div id="tabset-1-2">
<div style="font-size: x-large">
<p><strong>Decision Trees</strong>:</p>
<ul>
<li><strong>Overview</strong>: A model that splits the data into subsets based on feature values, resulting in a tree structure where each leaf node represents a class label.</li>
<li><strong>Advantages</strong>: Easy to interpret and visualize, handles both numerical and categorical data.</li>
<li><strong>Disadvantages</strong>: Prone to overfitting, especially with deep trees.</li>
</ul>
</div>
</div>
<div id="tabset-1-3">
<div style="font-size: x-large">
<p><strong>Random Forests</strong>:</p>
<ul>
<li><strong>Overview</strong>: An ensemble method that builds multiple decision trees and aggregates their predictions to improve accuracy and reduce overfitting.</li>
<li><strong>Advantages</strong>: Robust to overfitting, handles large datasets well.</li>
<li><strong>Disadvantages</strong>: Less interpretable than single decision trees.</li>
</ul>
</div>
</div>
<div id="tabset-1-4">
<div style="font-size: x-large">
<p><strong>Support Vector Machines (SVM)</strong>:</p>
<ul>
<li><strong>Overview</strong>: A model that finds the hyperplane that best separates the classes in the feature space.</li>
<li><strong>Advantages</strong>: Effective in high-dimensional spaces, works well with a clear margin of separation.</li>
<li><strong>Disadvantages</strong>: Computationally intensive, less effective with noisy data or overlapping classes.</li>
</ul>
</div>
</div>
<div id="tabset-1-5">
<div style="font-size: x-large">
<p><strong>Neural Networks</strong>:</p>
<ul>
<li><strong>Overview</strong>: Models inspired by the human brain, consisting of layers of interconnected neurons that learn to map inputs to outputs.</li>
<li><strong>Advantages</strong>: Capable of capturing complex patterns and relationships.</li>
<li><strong>Disadvantages</strong>: Requires large amounts of data and computational power, less interpretable.</li>
</ul>
</div>
</div>
<div id="tabset-1-6">
<div style="font-size: x-large">
<p><strong>Gradient Boosting Machines (GBM)</strong>:</p>
<ul>
<li><strong>Overview</strong>: An ensemble technique that builds trees sequentially, where each new tree corrects errors made by the previous ones.</li>
<li><strong>Advantages</strong>: High accuracy, effective at handling various data types.</li>
<li><strong>Disadvantages</strong>: Computationally intensive, requires careful tuning.</li>
</ul>
</div>
</div>
</div>
</div>
</section>
<section id="lazy-learners" class="slide level2">
<h2>Lazy Learners</h2>
<p><strong>Lazy learners or instance-based learners</strong>, do not create any model immediately from the training data, and this where the lazy aspect comes from. They just memorize the training data, and each time there is a need to make a prediction, they predict based on similarity between the query instance and stored instances.</p>
</section>
<section id="lazy-learners-1" class="slide level2">
<h2>Lazy Learners</h2>
<p>Example lazy learners are:</p>
<div class="panel-tabset">
<ul id="tabset-2" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-2-1">KNN</a></li><li><a href="#tabset-2-2">Case-based</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1">
<div style="font-size: x-large">
<p><strong>k-Nearest Neighbors (k-NN)</strong>:</p>
<ul>
<li><strong>Overview</strong>: Predicts the class of a query instance based on the majority class among its k nearest neighbors in the training data.</li>
<li><strong>Advantages</strong>: Simple, intuitive, and effective for small datasets.</li>
<li><strong>Disadvantages</strong>: Computationally intensive during prediction, performance degrades with high-dimensional data.</li>
</ul>
</div>
</div>
<div id="tabset-2-2">
<div style="font-size: x-large">
<p><strong>Case-based Learning</strong>:</p>
<ul>
<li><strong>Overview</strong>: Makes predictions by fitting simple models (like linear regression) to localized regions of the data.</li>
<li><strong>Advantages</strong>: Flexible, can adapt to complex patterns locally.</li>
<li><strong>Disadvantages</strong>: Computationally expensive, sensitive to the choice of kernel and bandwidth parameters.</li>
</ul>
</div>
</div>
</div>
</div>
</section>
<section id="comparing-eager-and-lazy-learners" class="slide level2">
<h2>Comparing Eager and Lazy Learners</h2>
<div style="font-size: x-large">
<ul>
<li><strong>Training vs.&nbsp;Prediction Time</strong>:
<ul>
<li>Eager learners invest time in building the model during training, resulting in faster predictions.</li>
<li>Lazy learners have negligible training time but are computationally intensive during prediction.</li>
</ul></li>
<li><strong>Model Interpretability</strong>:
<ul>
<li>Eager learners like decision trees and logistic regression are generally more interpretable.</li>
<li>Lazy learners like k-NN are less interpretable as they rely on instance-based comparisons.</li>
</ul></li>
</ul>
</div>
</section>
<section id="comparing-eager-and-lazy-learners-1" class="slide level2">
<h2>Comparing Eager and Lazy Learners</h2>
<div style="font-size: x-large">
<ul>
<li><strong>Handling of High-Dimensional Data</strong>:
<ul>
<li>Eager learners like SVMs and neural networks can handle high-dimensional data effectively.</li>
<li>Lazy learners like k-NN can struggle with the curse of dimensionality.</li>
</ul></li>
<li><strong>Flexibility and Complexity</strong>:
<ul>
<li>Eager learners can capture complex relationships and interactions through models like neural networks and gradient boosting.</li>
<li>Lazy learners are simpler but can be flexible in capturing local patterns.</li>
</ul></li>
</ul>
</div>
</section>
<section id="types-of-classification" class="slide level2">
<h2>Types of classification</h2>
<ul>
<li>Binary classification</li>
<li>Multi-Class Classification (mutually exclusive)
<ul>
<li><a href="https://juliasilge.com/blog/datasaurus-multiclass/">multiclass</a></li>
</ul></li>
<li>Multi-Label Classification (not mutually exclusive)
<ul>
<li><a href="https://en.wikipedia.org/wiki/Multi-label_classification">multilabel</a></li>
</ul></li>
<li>Imbalanced Classification
<ul>
<li><a href="https://www.tidymodels.org/learn/models/sub-sampling/">class imbalance</a></li>
</ul></li>
</ul>
</section>
<section id="binary-logistic-regression" class="slide level2">
<h2>Binary Logistic Regression</h2>
<p>Logistic regression is a Generalized Linear Model where the dependent (categorical) variable <span class="math inline">\(y\)</span> is binary, i.e.&nbsp;takes values in <span class="math inline">\(\{0,1\}\)</span> (e.g., yes/no, success/failure).</p>
<p>This can be interpreted as identifying two classes, and logistic regression provides a prediction for class membership based on a linear combination of the explanatory variables.</p>
<p>Logistic regression is an example of supervised learning.</p>
</section>
<section id="binary-logistic-regression-1" class="slide level2">
<h2>Binary Logistic Regression</h2>
<p>For the logistic GLM:</p>
<ul>
<li>the distribution of the observations is Binomial with parameter <span class="math inline">\(\pi\equiv\mathbb{P}(\left.Y=1\right|\eta)\)</span></li>
<li>the explanatory variables are linear in the parameters: <span class="math inline">\(\eta=\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_2 x_2\ldots+\beta_n x_n\)</span></li>
<li>the link function is the logit: <span class="math inline">\(\eta=\text{logit}(\pi) = \log(\frac{\pi}{1-\pi})\)</span></li>
</ul>
<p>It follows that <span class="math inline">\(\pi = \frac{e^\eta}{1+e^\eta} = \frac{1}{1+e^{-\eta}}\)</span>, which is a sigmoid function in the explanatory variables. The equation <span class="math inline">\(\eta=0\)</span> defines a linear decision boundary or classification threshold.</p>
</section>
<section id="binary-logistic-regression-2" class="slide level2">
<h2>Binary Logistic Regression</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a></a><span class="sc">&gt;</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">eta =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="fl">0.2</span>)) <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">pi =</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>eta))) <span class="sc">%&gt;%</span> </span>
<span id="cb1-2"><a></a><span class="sc">+</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>eta, <span class="at">y=</span>pi)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">theme_bw</span>(<span class="at">base_size =</span> <span class="dv">38</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="BSMM_8740_lec_05_files/figure-html/unnamed-chunk-1-1.png" class="quarto-figure quarto-figure-center" width="672"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="font-size: smaller">
<p><span class="math display">\[\begin{align*}
\pi &amp; =\frac{1}{1+e^{-\eta}}\;\text{(logistic function)}\\
\log\left(\frac{\pi}{1-\pi}\right) &amp; =\log\left(\frac{\frac{1}{1+e^{-\eta}}}{1-\frac{1}{1+e^{-\eta}}}\right)\\
&amp; =\log\left(\frac{\frac{1}{1+e^{-\eta}}}{\frac{e^{-\eta}}{1+e^{-\eta}}}\right)=\log\left(e^{\eta}\right)=\eta
\end{align*}\]</span></p>
</div></div>
</section>
<section id="binary-logistic-regression-3" class="slide level2">
<h2>Binary Logistic Regression</h2>
<p>The term <span class="math inline">\(\frac{\pi}{1-\pi}\)</span> is called the the odds-ratio. By its definition:</p>
<p><span class="math display">\[
\frac{\pi}{1-\pi}=e^{\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_2 x_2\ldots+\beta_n x_n}
\]</span></p>
<p>So if <span class="math inline">\(x_1\)</span> changes by one unit (<span class="math inline">\(x_1\rightarrow x_1+1\)</span>), then the odds ratio changes by <span class="math inline">\(e^{\beta_1}\)</span>.</p>
</section>
<section id="binary-classifier-metrics" class="slide level2">
<h2>Binary Classifier metrics</h2>
<h3 id="confusion-matrix">Confusion matrix</h3>
<p>The confusion matrix is a 2x2 table summarizing the number of correct predictions of the model (a function of the decision boundary): <em>It is the foundation for understanding other evaluation metrics.</em></p>
<table class="caption-top">
<thead>
<tr class="header">
<th></th>
<th>predict 1</th>
<th>predict 0</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>data = 1</td>
<td>true positives (TP)</td>
<td>false negatives (FN)<sup>1</sup></td>
</tr>
<tr class="even">
<td>data = 0</td>
<td>false positives (FP)<sup>2</sup></td>
<td>true negatives (TN)</td>
</tr>
</tbody>
</table>
<aside><ol class="aside-footnotes"><li id="fn1"><p>Type II error</p></li><li id="fn2"><p>Type I error</p></li></ol></aside></section>
<section id="binary-classifier-metrics-1" class="slide level2">
<h2>Binary Classifier metrics</h2>
<h3 id="accuracy---the-simplest-metric">Accuracy - the simplest metric:</h3>
<p>Accuracy measures the percent of correct predictions:</p>
<p><span class="math display">\[
\begin{align*}
\frac{\text{TP}+\text{TN}}{\text{observation count}}
\end{align*}
\]</span></p>
</section>
<section id="binary-classifier-metrics-2" class="slide level2">
<h2>Binary Classifier metrics</h2>
<h3 id="accuracy">Accuracy:</h3>
<div class="panel-tabset">
<ul id="tabset-4" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-4-1">Usefulness</a></li><li><a href="#tabset-4-2">Limitations</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1">
<div style="font-size: x-large">
<p><strong>Accuracy</strong> is a useful metric in evaluating classification models under certain conditions:</p>
<ol type="1">
<li><p><strong>Balanced Datasets</strong>: When the classes in the dataset are roughly equal in number, accuracy can be a reliable indicator of model performance. For example, if you have a dataset where 50% of the samples are class A and 50% are class B, accuracy is a good measure of whether the model correctly predicts the classes.</p></li>
<li><p><strong>General Performance</strong>: For an overall sense of a model’s performance, accuracy provides a straightforward, easy-to-understand measure, giving the proportion of correct predictions out of all predictions made.</p></li>
</ol>
</div>
</div>
<div id="tabset-4-2">
<div style="font-size: x-large">
<p>Accuracy has several limitations, especially in the context of imbalanced datasets:</p>
</div>
<div class="panel-tabset">
<ul id="tabset-3" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-3-1">Imbalance</a></li><li><a href="#tabset-3-2">Importance</a></li><li><a href="#tabset-3-3">Behaviour</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1">
<div style="font-size: x-large">
<p><strong>Imbalanced Datasets</strong>:</p>
<ul>
<li><strong>False Sense of Performance</strong>: When one class significantly outnumbers the other, accuracy can be misleading: if 95% of the samples belong to class A and only 5% to class B, always predicting class A will have a high accuracy (95%) but may fail to correctly identify any instances of class B.</li>
</ul>
</div>
</div>
<div id="tabset-3-2">
<div style="font-size: x-large">
<p><strong>Ignoring Class Importance</strong>:</p>
<ul>
<li><strong>Precision and Recall</strong>: Accuracy does not take into account the importance of different classes or the costs of different types of errors (false positives and false negatives). In many business contexts, these costs are not equal, e.g., in medical diagnosis, a false -ve might be much more serious than a false +ve.</li>
</ul>
</div>
</div>
<div id="tabset-3-3">
<div style="font-size: x-large">
<p><strong>Lack of Insight into Model Behavior</strong>:</p>
<ul>
<li><strong>Detailed Performance</strong>: Accuracy give no insight into the types of errors the model is making. Precision and recall, on the other hand, offer more detailed information about the performance on individual classes - crucial for understanding and improving the model.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="binary-classifier-metrics-3" class="slide level2">
<h2>Binary Classifier metrics</h2>
<h3 id="accuracy-examples-of-limitation">Accuracy: examples of limitation</h3>
<div style="font-size: x-large">
<p>Fraud Detection:</p>
<ul>
<li>Scenario: Suppose you have a dataset with 1,000 transactions, of which 990 are legitimate and 10 are fraudulent. A model that always predicts “legitimate” will have an accuracy of 99% but fails to identify fraudulent transactions, making it useless for fraud detection purposes.</li>
</ul>
<p>Spam Detection:</p>
<ul>
<li>Scenario: Consider an email classification problem where 95% of emails are legitimate and 5% are spam. A model that always predicts “legitimate” will have high accuracy but will not catch any spam emails, which is the primary goal.</li>
</ul>
</div>
</section>
<section id="binary-classifier-metrics-4" class="slide level2">
<h2>Binary Classifier metrics</h2>
<h3 id="precision">Precision</h3>
<p>Precision measures the percent of positive predictions that are correct (true positives / all positives predicted):</p>
<p><span class="math display">\[
\frac{\text{TP}}{\text{TP}+\text{FP}}
\]</span></p>
</section>
<section id="binary-classifier-metrics-5" class="slide level2">
<h2>Binary Classifier metrics</h2>
<h4 id="recall-sensitivity">Recall / Sensitivity</h4>
<div style="font-size: large">
<p>Measures the success at predicting the first class (true positives predicted / actual positives):</p>
</div>
<p><span class="math display">\[
\frac{\text{TP}}{\text{TP}+\text{FN}}\qquad\text{(True Positive Rate - TPR)}
\]</span></p>
<h4 id="recall-specificity">Recall / Specificity</h4>
<div style="font-size: large">
<p>Measures the success at predicting the second class (true negatives predicted / actual negative):</p>
</div>
<p><span class="math display">\[
\frac{\text{TN}}{\text{TN}+\text{FP}}\qquad\text{(True Negative Rate - TNR)}
\]</span></p>
</section>
<section id="binary-classifier-metrics-6" class="slide level2">
<h2>Binary Classifier metrics</h2>
<p>Receiver Operating Characteristic (ROC) curve &amp; the Area Under the Curve (AUC)</p>
<div style="font-size: x-large">
<ul>
<li><p><strong>ROC Curve</strong>: Plot of the true positive rate (Recall) against the false positive rate (1 - Specificity) at various threshold settings.</p></li>
<li><p><strong>AUC</strong>: The area under the ROC curve, representing the probability that the model ranks a randomly chosen positive instance higher than a randomly chosen negative instance.</p>
<ul>
<li>AUC values range from 0.5 (no discrimination) to 1 (perfect discrimination).</li>
</ul></li>
</ul>
</div>
</section>
<section id="binary-classifier-metrics-7" class="slide level2">
<h2>Binary Classifier metrics</h2>
<h3 id="roc-curves">ROC Curves</h3>
<p>Consider plotting the TPR against the FPR (1-TNR) <u><strong>at different classification thresholds</strong></u>.</p>
<ul>
<li>the diagonal (TPR = 1-TNR) describes a process equivalent to tossing a fair coin (i.e.&nbsp;no predictive power)</li>
<li>our method should have a curve above the diagonal; which shape is better depends on the purpose of our classifier.</li>
</ul>
<p>So, how to compute?</p>
</section>
<section id="example-create-the-workflow" class="slide level2">
<h2>Example: create the workflow</h2>
<h4 id="workflow-to-model-credit-card-default">Workflow to model credit card default</h4>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a></a><span class="sc">&gt;</span> data <span class="ot">&lt;-</span> ISLR<span class="sc">::</span>Default <span class="sc">%&gt;%</span> tibble<span class="sc">::</span><span class="fu">as_tibble</span>()</span>
<span id="cb2-2"><a></a><span class="sc">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">8740</span>)</span>
<span id="cb2-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb2-4"><a></a><span class="er">&gt;</span> <span class="co"># split data</span></span>
<span id="cb2-5"><a></a><span class="er">&gt;</span> data_split <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(data)</span>
<span id="cb2-6"><a></a><span class="sc">&gt;</span> default_train <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(data_split)</span>
<span id="cb2-7"><a></a><span class="sc">&gt;</span> </span>
<span id="cb2-8"><a></a><span class="er">&gt;</span> <span class="co"># create a recipe</span></span>
<span id="cb2-9"><a></a><span class="er">&gt;</span> default_recipe <span class="ot">&lt;-</span> default_train <span class="sc">%&gt;%</span> </span>
<span id="cb2-10"><a></a><span class="sc">+</span>   recipes<span class="sc">::</span><span class="fu">recipe</span>(<span class="at">formula =</span> default <span class="sc">~</span> student <span class="sc">+</span> balance <span class="sc">+</span> income) <span class="sc">%&gt;%</span> </span>
<span id="cb2-11"><a></a><span class="sc">+</span>   recipes<span class="sc">::</span><span class="fu">step_dummy</span>(recipes<span class="sc">::</span><span class="fu">all_nominal_predictors</span>())</span>
<span id="cb2-12"><a></a><span class="sc">&gt;</span> </span>
<span id="cb2-13"><a></a><span class="er">&gt;</span> <span class="co"># create a linear regression model</span></span>
<span id="cb2-14"><a></a><span class="er">&gt;</span> default_model <span class="ot">&lt;-</span> parsnip<span class="sc">::</span><span class="fu">logistic_reg</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb2-15"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"glm"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb2-16"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span>
<span id="cb2-17"><a></a><span class="sc">&gt;</span> </span>
<span id="cb2-18"><a></a><span class="er">&gt;</span> <span class="co"># create a workflow</span></span>
<span id="cb2-19"><a></a><span class="er">&gt;</span> default_workflow <span class="ot">&lt;-</span> workflows<span class="sc">::</span><span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb2-20"><a></a><span class="sc">+</span>   workflows<span class="sc">::</span><span class="fu">add_recipe</span>(default_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb2-21"><a></a><span class="sc">+</span>   workflows<span class="sc">::</span><span class="fu">add_model</span>(default_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="example-fit-the-model-using-the-data" class="slide level2">
<h2>Example: fit the model using the data</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a></a><span class="sc">&gt;</span> <span class="co"># fit the model</span></span>
<span id="cb3-2"><a></a><span class="er">&gt;</span> lm_fit <span class="ot">&lt;-</span> </span>
<span id="cb3-3"><a></a><span class="sc">+</span>   default_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb3-4"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">fit</span>(default_train)</span>
<span id="cb3-5"><a></a><span class="sc">&gt;</span> </span>
<span id="cb3-6"><a></a><span class="er">&gt;</span> <span class="co"># augment the data with the predictions using the model fit</span></span>
<span id="cb3-7"><a></a><span class="er">&gt;</span> training_results <span class="ot">&lt;-</span> </span>
<span id="cb3-8"><a></a><span class="sc">+</span>   broom<span class="sc">::</span><span class="fu">augment</span>(lm_fit , default_train) </span>
<span id="cb3-9"><a></a><span class="sc">&gt;</span> </span>
<span id="cb3-10"><a></a><span class="er">&gt;</span> training_results <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">slice_head</span>(<span class="at">n=</span><span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 7
  .pred_class .pred_No .pred_Yes default student balance income
  &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;
1 No             0.998  0.00164  No      No         759. 45774.
2 No             1.00   0.000145 No      Yes        452. 19923.
3 No             0.770  0.230    Yes     No        1666. 30070.
4 No             1.00   0.000256 No      No         434. 57146.
5 No             1.00   0.000449 No      No         536. 32994.
6 No             0.973  0.0269   No      No        1252. 32721.</code></pre>
</div>
</div>
</section>
<section id="example-compute-the-auc" class="slide level2">
<h2>Example: compute the AUC</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a></a><span class="sc">&gt;</span> training_results <span class="sc">%&gt;%</span> </span>
<span id="cb5-2"><a></a><span class="sc">+</span>   yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(</span>
<span id="cb5-3"><a></a><span class="sc">+</span>     <span class="at">truth =</span> default</span>
<span id="cb5-4"><a></a><span class="sc">+</span>     , .pred_No</span>
<span id="cb5-5"><a></a><span class="sc">+</span>   ) <span class="sc">%&gt;%</span> </span>
<span id="cb5-6"><a></a><span class="sc">+</span>   <span class="fu">autoplot</span>() <span class="sc">+</span> </span>
<span id="cb5-7"><a></a><span class="sc">+</span>   <span class="fu">theme_bw</span>(<span class="at">base_size =</span> <span class="dv">38</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="BSMM_8740_lec_05_files/figure-html/unnamed-chunk-4-1.png" class="quarto-figure quarto-figure-center" width="672"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a></a><span class="sc">&gt;</span> training_results <span class="sc">%&gt;%</span> </span>
<span id="cb6-2"><a></a><span class="sc">+</span>    yardstick<span class="sc">::</span><span class="fu">roc_auc</span>(</span>
<span id="cb6-3"><a></a><span class="sc">+</span>      <span class="at">truth =</span> default</span>
<span id="cb6-4"><a></a><span class="sc">+</span>      , .pred_No</span>
<span id="cb6-5"><a></a><span class="sc">+</span>     )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 roc_auc binary         0.949</code></pre>
</div>
</div>
</div></div>
</section>
<section id="binary-classifier" class="slide level2">
<h2>Binary Classifier</h2>
<p><strong>Classification Threshold</strong></p>
<p><strong>Recall</strong>:</p>
<ul>
<li>In binary classification, the model predicts the probability of an instance belonging to the positive class. The classification threshold is the probability value above which an instance is classified as positive.</li>
<li>Commonly, this threshold is set at 0.5, meaning any instance with a predicted probability above 0.5 is classified as positive, and below 0.5 as negative.</li>
</ul>
</section>
<section id="binary-classifier-1" class="slide level2">
<h2>Binary Classifier</h2>
<p><strong>Classification Threshold Impact</strong></p>
<div style="font-size: large">
<p><strong>True Positives (TP) and False Positives (FP)</strong>:</p>
<ul>
<li><strong>Lower Threshold</strong>: A lower threshold increases the number of instances classified as positive, which increases both true positives and false positives.</li>
<li><strong>Higher Threshold</strong>: A higher threshold decreases the number of instances classified as positive, which decreases both true positives and false positives.</li>
</ul>
<p><strong>True Negatives (TN) and False Negatives (FN)</strong>:</p>
<ul>
<li><strong>Lower Threshold</strong>: A lower threshold decreases the number of instances classified as negative, which decreases both true negatives and increases false negatives.</li>
<li><strong>Higher Threshold</strong>: A higher threshold increases the number of instances classified as negative, which increases true negatives and decreases false negatives.</li>
</ul>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>Adjusting the threshold affects the trade-off between sensitivity (recall) and specificity. A lower threshold improves sensitivity but reduces specificity, and vice versa.</li>
</ul>
</div>
</section>
<section id="binary-classifier-2" class="slide level2">
<h2>Binary Classifier</h2>
<h3 id="roc-curve-and-auc">ROC Curve and AUC</h3>
<div style="font-size: x-large">
<ol type="1">
<li><strong>ROC Curve</strong>:
<ul>
<li>The ROC curve plots the true positive rate (TPR, or recall) against the false positive rate (FPR) at various threshold settings.</li>
<li>Each point on the ROC curve represents a TPR/FPR pair corresponding to a specific threshold.</li>
</ul></li>
<li><strong>AUC (Area Under the Curve)</strong>:
<ul>
<li>The AUC represents the model’s ability to discriminate between positive and negative classes across all threshold values.</li>
<li>A higher AUC indicates better overall performance.</li>
</ul></li>
</ol>
</div>
</section>
<section id="binary-classifier-optimal-threshold" class="slide level2">
<h2>Binary Classifier: optimal threshold</h2>
<div style="font-size: large">
<ol type="1">
<li><strong>Business Context</strong>:
<ul>
<li>The optimal threshold depends on the specific costs of false positives and false negatives in the business context. For example, in fraud prediction, the cost of missing a positive case (false negative) might be much higher than a false alarm (false positive).</li>
</ul></li>
<li><strong>Maximize Specific Metrics</strong>:
<ul>
<li>You can choose a threshold that maximizes a specific metric such as F1 score, which balances precision and recall.</li>
<li>Alternatively, you might want to maximize precision, recall, or minimize a cost function that accounts for both false positives and false negatives.</li>
</ul></li>
<li><strong>Youden’s Index</strong>:
<ul>
<li>One method to select an optimal threshold is to maximize Youden’s Index (<span class="math inline">\(J\)</span>), which is defined as: <span class="math inline">\(J=\mathrm{Sensitivity}+\mathrm{Specificity}-1\)</span></li>
<li>This index helps to find a threshold that maximizes the difference between true positive rate and false positive rate.</li>
</ul></li>
<li><strong>Cost-Benefit Analysis</strong>:
<ul>
<li>Perform a cost-benefit analysis by assigning costs to false positives and false negatives and choosing the threshold that minimizes the total expected cost.</li>
</ul></li>
</ol>
</div>
</section>
<section id="binary-classifier-threshold-examples" class="slide level2">
<h2>Binary Classifier: threshold examples</h2>
<div style="font-size: x-large">
<h3 id="example-scenario">Example Scenario</h3>
<p><strong>Fraud Detection</strong>: - In fraud detection, missing a fraudulent transaction (false negative) might be more costly than flagging a legitimate transaction as fraud (false positive). - You might choose a lower threshold to ensure higher sensitivity (recall), even if it means a higher false positive rate, thereby catching more fraudulent transactions.</p>
<h3 id="conclusion">Conclusion</h3>
<p>The choice of classification threshold in computing the ROC curve is crucial for balancing the trade-offs between sensitivity and specificity, and ultimately for optimizing the model’s performance in a way that aligns with business goals and context. Understanding and carefully selecting the appropriate threshold ensures that the model’s predictions are most useful and cost-effective for the specific application.</p>
</div>
</section></section>
<section>
<section id="other-classification-methods" class="title-slide slide level1 center">
<h1>Other Classification Methods</h1>

</section>
<section id="naive-bayes-classification" class="slide level2">
<h2>Naive Bayes Classification</h2>
<p>Bayes Rule - using the rules of conditional probability:</p>
<p><span class="math display">\[
\mathbb{P}(A,B) = \mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(B|A)\mathbb{P}(A)
\]</span> We can write:</p>
<p><span class="math display">\[
\mathbb{P}(B|A) = \frac{\mathbb{P}(A|B)\mathbb{P}(B)}{\mathbb{P}(A)}
\]</span></p>
</section>
<section id="naive-bayes-classification-1" class="slide level2">
<h2>Naive Bayes Classification</h2>
<p>This method starts with Bayes rule:</p>
<p>for <span class="math inline">\(K\)</span> classes and an observation <span class="math inline">\(x\)</span> consisting of <span class="math inline">\(N\)</span> features <span class="math inline">\(\{x_1,\ldots,x_N\}\)</span>, since <span class="math inline">\(\mathbb{P}\left[\left.C_{k}\right|x_{1},\ldots,x_{N}\right]\times\mathbb{P}\left[x_{1},\ldots,x_{N}\right]\)</span> is equal to <span class="math inline">\(\mathbb{P}\left[\left.x_{1},\ldots,x_{N}\right|C_{k}\right]\times\mathbb{P}\left[C_{k}\right]\)</span>, we can write</p>
<p><span class="math display">\[
\mathbb{P}\left[\left.C_{k}\right|x_{1},\ldots,x_{N}\right]=\frac{\mathbb{P}\left[\left.x_{1},\ldots,x_{N}\right|C_{k}\right]\times\mathbb{P}\left[C_{k}\right]}{\mathbb{P}\left[x_{1},\ldots,x_{N}\right]}
\]</span></p>
</section>
<section id="naive-bayes-classification-2" class="slide level2">
<h2>Naive Bayes Classification</h2>
<p>If we assume that the features are all independent we can write Bayes rule as</p>
<p><span class="math display">\[
\mathbb{P}\left[\left.C_{k}\right|x_{1},\ldots,x_{N}\right]=\frac{\mathbb{P}\left[C_{k}\right]\times\prod_{n=1}^{N}\mathbb{P}\left[\left.x_{n}\right|C_{k}\right]}{\prod_{n=1}^{N}\mathbb{P}\left[x_{n}\right]}
\]</span></p>
<p>and since the denominator is independent of <span class="math inline">\(C_{k}\)</span>, our classifier is</p>
<p><span class="math display">\[
C_{k}=\arg\max_{C_{k}}\mathbb{P}\left[C_{k}\right]\prod_{n=1}^{N}\mathbb{P}\left[\left.x_{n}\right|C_{k}\right]
\]</span></p>
</section>
<section id="naive-bayes-classification-3" class="slide level2">
<h2>Naive Bayes Classification</h2>
<p>So it remains to calculate the class probability <span class="math inline">\(\mathbb{P}\left[C_{k}\right]\)</span> and the conditional probabilities <span class="math inline">\(\mathbb{P}\left[\left.x_{n}\right|C_{k}\right]\)</span></p>
<p>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the conditional probabilities.</p>
</section>
<section id="naive-bayes-classification-4" class="slide level2">
<h2>Naive Bayes Classification</h2>
<p>If our features are all ordinal, then</p>
<ul>
<li><p>The class probabilities<sup>1</sup> are simply the frequency of observations that belong to each class divided by the total number of observations.</p></li>
<li><p>The conditional probabilities are the frequency of each feature value for a given class value divided by the frequency of measurements with that class value.</p></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn3"><p>i.e.&nbsp;empirical probabilities</p></li></ol></aside></section>
<section id="naive-bayes-classification-5" class="slide level2">
<h2>Naive Bayes Classification</h2>
<p>If any features are numeric, we can estimate conditional probabilities by assuming that the numeric features have a Gaussian distribution for each class</p>
</section>
<section id="naive-bayes-classification-6" class="slide level2">
<h2>Naive Bayes Classification</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a></a><span class="sc">&gt;</span> <span class="fu">library</span>(discrim)</span>
<span id="cb8-2"><a></a><span class="sc">&gt;</span> <span class="co"># create a naive bayes classifier</span></span>
<span id="cb8-3"><a></a><span class="er">&gt;</span> default_model_nb <span class="ot">&lt;-</span> parsnip<span class="sc">::</span><span class="fu">naive_Bayes</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb8-4"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"klaR"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb8-5"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span>
<span id="cb8-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb8-7"><a></a><span class="er">&gt;</span> <span class="co"># create a workflow</span></span>
<span id="cb8-8"><a></a><span class="er">&gt;</span> default_workflow_nb <span class="ot">&lt;-</span> workflows<span class="sc">::</span><span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb8-9"><a></a><span class="sc">+</span>   workflows<span class="sc">::</span><span class="fu">add_recipe</span>(default_recipe) <span class="sc">%&gt;%</span></span>
<span id="cb8-10"><a></a><span class="sc">+</span>   workflows<span class="sc">::</span><span class="fu">add_model</span>(default_model_nb)</span>
<span id="cb8-11"><a></a><span class="sc">&gt;</span> </span>
<span id="cb8-12"><a></a><span class="er">&gt;</span> <span class="co"># fit the model</span></span>
<span id="cb8-13"><a></a><span class="er">&gt;</span> lm_fit_nb <span class="ot">&lt;-</span> </span>
<span id="cb8-14"><a></a><span class="sc">+</span>   default_workflow_nb <span class="sc">%&gt;%</span> </span>
<span id="cb8-15"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">fit</span>(</span>
<span id="cb8-16"><a></a><span class="sc">+</span>     default_train</span>
<span id="cb8-17"><a></a><span class="sc">+</span>   , <span class="at">control =</span> </span>
<span id="cb8-18"><a></a><span class="sc">+</span>     workflows<span class="sc">::</span><span class="fu">control_workflow</span>(parsnip<span class="sc">::</span><span class="fu">control_parsnip</span>(<span class="at">verbosity =</span> <span class="dv">1</span>L))</span>
<span id="cb8-19"><a></a><span class="sc">+</span>   )</span>
<span id="cb8-20"><a></a><span class="sc">&gt;</span> </span>
<span id="cb8-21"><a></a><span class="er">&gt;</span> <span class="co"># augment the data with the predictions using the model fit</span></span>
<span id="cb8-22"><a></a><span class="er">&gt;</span> training_results_nb <span class="ot">&lt;-</span> </span>
<span id="cb8-23"><a></a><span class="sc">+</span>   broom<span class="sc">::</span><span class="fu">augment</span>(lm_fit_nb , default_train) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="naive-bayes-classification-7" class="slide level2">
<h2>Naive Bayes Classification</h2>
<div class="panel-tabset">
<ul id="tabset-5" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-5-1">AUC</a></li><li><a href="#tabset-5-2">ROC</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a></a><span class="sc">&gt;</span> training_results_nb <span class="sc">%&gt;%</span> </span>
<span id="cb9-2"><a></a><span class="sc">+</span>   yardstick<span class="sc">::</span><span class="fu">roc_auc</span>(.pred_No, <span class="at">truth =</span> default)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 roc_auc binary         0.942</code></pre>
</div>
</div>
</div>
<div id="tabset-5-2">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="BSMM_8740_lec_05_files/figure-html/unnamed-chunk-8-1.png" class="quarto-figure quarto-figure-center" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="naive-bayes-classification-uses" class="slide level2">
<h2>Naive Bayes Classification: uses</h2>
<div style="font-size: large">
<ol type="1">
<li><strong>Text Classification</strong>:
<ul>
<li><strong>Sentiment Analysis</strong>: It can classify text data into categories such as positive, negative, or neutral sentiment.</li>
</ul></li>
<li><strong>Document Categorization</strong>:
<ul>
<li>Useful in classifying news articles, blog posts, or any document into predefined categories based on content.</li>
</ul></li>
<li><strong>Medical Diagnosis</strong>:
<ul>
<li>Can be used to predict the likelihood of a disease based on patient symptoms and medical history, assuming independence between symptoms.</li>
</ul></li>
<li><strong>Recommender Systems</strong>:
<ul>
<li>Helps in predicting user preferences and recommending items such as movies, books, or products based on previous user behavior.</li>
</ul></li>
<li><strong>Real-time Prediction</strong>:
<ul>
<li>Due to its simplicity and speed, Naive Bayes is suitable for real-time prediction tasks where quick decisions are essential.</li>
</ul></li>
</ol>
</div>
</section>
<section id="naive-bayes-classification-abuses" class="slide level2">
<h2>Naive Bayes Classification: abuses</h2>
<div style="font-size: large">
<ol type="1">
<li><strong>Assumption of Feature Independence</strong>:
<ul>
<li><strong>Misuse</strong>: The model assumes that all features are independent given the class.</li>
<li><strong>Impact</strong>: Poor performance if features are highly correlated.</li>
</ul></li>
<li><strong>Imbalanced Data</strong>:
<ul>
<li><strong>Misuse</strong>: Naive Bayes can struggle with imbalanced class datasets.</li>
<li><strong>Impact</strong>: Bias towards the majority class, leads to high accuracy but poor minority recall.</li>
</ul></li>
<li><strong>Zero Probability Problem</strong>:
<ul>
<li><strong>Misuse</strong>: If a feature <span class="math inline">\(x_j\)</span> missing for class <span class="math inline">\(k\)</span>, then <span class="math inline">\(\mathbb{P}\left[\left.x_{j}\right|C_{k}\right]=0\)</span>, which can skew results.</li>
<li><strong>Solution</strong>: Use techniques like Laplace Smoothing to handle zero probabilities.</li>
</ul></li>
<li><strong>Overfitting on Small Datasets</strong>:
<ul>
<li><strong>Misuse</strong>: Naive Bayes may overfit if trained on a small dataset with noise or outliers.</li>
<li><strong>Impact</strong>: This can result in poor generalization to new data.</li>
</ul></li>
<li><strong>Ignoring Feature Scaling</strong>:
<ul>
<li><strong>Misuse</strong>: The model does not inherently handle features with different scales or units.</li>
<li><strong>Impact</strong>: Features with larger scales can disproportionately influence the model</li>
</ul></li>
</ol>
</div>
</section>
<section id="naive-bayes-best-practices" class="slide level2">
<h2>Naive Bayes Best Practices</h2>
<div style="font-size: large">
<ol type="1">
<li><strong>Feature Engineering</strong>:
<ul>
<li>Proper feature selection and engineering can help mitigate some of the independence assumption issues. For instance, combining related features can improve performance.</li>
</ul></li>
<li><strong>Handling Correlated Features</strong>:
<ul>
<li>While Naive Bayes assumes independence, it can still perform well with moderately correlated features. In cases of strong correlation, consider using other models.</li>
</ul></li>
<li><strong>Evaluation Metrics</strong>:
<ul>
<li>Use appropriate metrics such as precision, recall, and ROC-AUC, especially for imbalanced datasets, to get a comprehensive understanding of model performance.</li>
</ul></li>
<li><strong>Cross-Validation</strong>:
<ul>
<li>Employ cross-validation techniques to ensure that the model generalizes well to unseen data and to avoid overfitting.</li>
</ul></li>
<li><strong>Comparative Analysis</strong>:
<ul>
<li>Compare Naive Bayes with other classifiers (e.g., logistic regression, SVM, random forest) to ensure that it is the best choice for your specific problem.</li>
</ul></li>
</ol>
</div>
</section>
<section id="nearest-neighbour-classification" class="slide level2">
<h2>Nearest Neighbour Classification</h2>
<p>The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.</p>
<p>It is typically used as a classification algorithm, working off the assumption that similar class predictions can be made by predictors near one another.</p>
</section>
<section id="nearest-neighbour-classification-1" class="slide level2">
<h2>Nearest Neighbour Classification</h2>
<p>For classification problems, a class label is assigned on the basis of a majority vote—i.e.&nbsp;the label that is most frequently represented around a given data point is used.</p>
<p>Before a classification can be made, the distance between points must be defined. Euclidean distance is most commonly used.</p>
</section>
<section id="nearest-neighbour-classification-2" class="slide level2">
<h2>Nearest Neighbour Classification</h2>
<p>Note that the KNN algorithm is also part of a family of “lazy learning” models, meaning that it only stores a training dataset versus undergoing a training stage. This also means that all the computation occurs when a classification or prediction is being made.</p>
<p>The k value in the k-NN algorithm determines how many neighbors will be checked to determine the classification of a specific query point.</p>
</section>
<section id="knn-classification-distance-measures" class="slide level2">
<h2>KNN Classification: distance measures</h2>
<ul>
<li>Euclidean: <span class="math inline">\(\text{d}(x,y)=\sqrt{\sum_i(y_i- x_i)^2}\)</span></li>
<li>Manhattan: <span class="math inline">\(\text{d}(x,y)=\sum_{i}\left|y_{i}-x_{i}\right|\)</span></li>
<li>Minkowski: <span class="math inline">\(\text{d}(x,y;p)=\left(\sum_{i}\left|y_{i}-x_{i}\right|\right)^{1/p}\)</span></li>
</ul>
</section>
<section id="knn-classification-algorithm" class="slide level2">
<h2>KNN Classification: algorithm</h2>
<ol type="1">
<li>Choose the value of K, which is the number of nearest neighbors that will be used to make the prediction.</li>
<li>Calculate the distance between the observation you want to classify and all the observations in the training set.</li>
<li>Select the K nearest neighbors based on the distances calculated.</li>
<li>Assign the label of the majority class to the new data point.</li>
<li>Repeat steps 2 to 4 for all the data points in the test set.</li>
</ol>
</section>
<section id="knn-classification-model" class="slide level2">
<h2>KNN Classification: model</h2>
<p>Our classification workflow only differs by the model, e.g.:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a></a><span class="sc">&gt;</span> default_model_knn <span class="ot">&lt;-</span> parsnip<span class="sc">::</span><span class="fu">nearest_neighbor</span>(<span class="at">neighbors =</span> <span class="dv">4</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb11-2"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"kknn"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb11-3"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>k-NN regression</strong></p>
</div>
<div class="callout-content">
<p>To use k-NN for a regression problem, calculate the mean or median (or another aggregate measure) of the dependent variable among the k neighbors.</p>
</div>
</div>
</div>
</section>
<section id="knn-classification-metrics" class="slide level2">
<h2>KNN Classification: metrics</h2>
<p>In the context of k-Nearest Neighbors (kNN) classification, while the general evaluation metrics like accuracy, precision, recall, F1 score, and others are commonly used, there are no unique metrics that are exclusively specific to kNN. However, there are certain considerations and additional analyses that are particularly relevant when evaluating a kNN model.</p>
</section>
<section id="knn-classification-metrics-1" class="slide level2">
<h2>KNN Classification: metrics</h2>
<div style="font-size: smaller">
<ol type="1">
<li><p><strong>Choice of k (Number of Neighbors):</strong> The value of k affects the performance of a kNN model. Testing the model with various values of k and evaluating the performance using standard metrics (like accuracy, F1 score) can to select the best k.</p></li>
<li><p><strong>Feature Scaling Sensitivity:</strong> kNN is sensitive to the scale of the features because it relies on calculating distances. Evaluate the model’s performance before and after feature scaling (like Min-Max scaling or Z-score normalization)</p></li>
<li><p><strong>Curse of Dimensionality:</strong> kNN can perform poorly with high-dimensional data (many features). Evaluating the model’s performance in relation to the number of features (dimensionality) can be important. Dimensionality reduction techniques like PCA might be used with kNN.</p></li>
</ol>
</div>
</section>
<section id="knn-classification-uses" class="slide level2">
<h2>KNN Classification: uses</h2>
<div style="font-size: large">
<ol type="1">
<li><strong>Pattern Recognition</strong>:
<ul>
<li><strong>Handwriting Recognition</strong>: kNN can classify handwritten digits or letters using their features.</li>
<li><strong>Image Classification</strong>: Categorize by comparing pixels/features with known images.</li>
</ul></li>
<li><strong>Recommender Systems</strong>:
<ul>
<li><strong>Content-Based Filtering</strong>: kNN can recommend items using similarities between user preferences and the attributes of items.</li>
<li><strong>Collaborative Filtering</strong>: Can also recommend items based on preferences of similar users.</li>
</ul></li>
<li><strong>Medical Diagnosis</strong>:
<ul>
<li><strong>Disease Prediction</strong>: kNN can predict likelihood of diseases by comparing patient data to historical patient data with known diagnoses.</li>
</ul></li>
<li><strong>Anomaly Detection</strong>:
<ul>
<li><strong>Fraud Detection</strong>: kNN can identify unusual transactions by comparing to known legitimate and fraudulent transactions.</li>
</ul></li>
<li><strong>Customer Segmentation</strong>:
<ul>
<li><strong>Market Analysis</strong>: It can segment customers based on purchasing behavior, demographics, or other attributes.</li>
</ul></li>
</ol>
</div>
</section>
<section id="knn-classification-abuses" class="slide level2">
<h2>KNN Classification: abuses</h2>
<div style="font-size: large">
<ul>
<li><strong>High Dimensionality</strong>:
<ul>
<li><strong>Misuse</strong>: kNN can struggle with high-dimensional data as distances become less meaningful (curse of dimensionality).</li>
<li><strong>Impact</strong>: Performance degrades as irrelevant or noisy features overshadow important ones.</li>
<li><strong>Solution</strong>: Use dimensionality reduction (e.g., PCA, t-SNE) or feature selection before using kNN.</li>
</ul></li>
<li><strong>Large Datasets</strong>:
<ul>
<li><strong>Misuse</strong>: kNN stores all training data and calculates all distances during prediction, &amp; can be computationally expensive.</li>
<li><strong>Impact</strong>: It becomes slow and impractical for large datasets.</li>
<li><strong>Solution</strong>: Approximate kNN techniques or other algorithms suited for large datasets.</li>
</ul></li>
<li><strong>Imbalanced Datasets</strong>:
<ul>
<li><strong>Misuse</strong>: kNN can be biased towards the majority class in imbalanced datasets because the majority class neighbors dominate.</li>
<li><strong>Impact</strong>: Low recall for minority class, &amp; to poor performance in e.g.&nbsp;fraud detection or rare disease diagnosis.</li>
<li><strong>Solution</strong>: Use resampling, synthetic data generation (SMOTE), or adjusting the decision rule to account for class imbalance.</li>
</ul></li>
</ul>
</div>
</section>
<section id="knn-classification-abuses-1" class="slide level2">
<h2>KNN Classification: abuses</h2>
<div style="font-size: large">
<ul>
<li><strong>Choice of k and Distance Metric</strong>:
<ul>
<li><strong>Misuse</strong>: An inappropriate choice of ( k ) (too small or too large) or an unsuitable distance metric can lead to poor classification performance.</li>
<li><strong>Impact</strong>: A small ( k ) can make the model sensitive to noise (overfitting), while a large ( k ) can oversmooth the decision boundary (underfitting).</li>
<li><strong>Solution</strong>: Use cross-validation to choose the optimal ( k ) and experiment with different distance metrics (e.g., Euclidean, Manhattan).</li>
</ul></li>
<li><strong>Scalability</strong>:
<ul>
<li><strong>Misuse</strong>: without optimization, kNN does not scale well to datasets with a large number of features or samples.</li>
<li><strong>Impact</strong>: Slow predictions and high memory usage.</li>
<li><strong>Solution</strong>: Use k-d trees, ball trees, or locality-sensitive hashing to speed up nearest neighbor searches.</li>
</ul></li>
</ul>
</div>
</section>
<section id="knn-classification-best-practices" class="slide level2">
<h2>KNN Classification Best Practices</h2>
<div style="font-size: large">
<ol type="1">
<li><strong>Normalization and Scaling</strong>:
<ul>
<li><strong>Importance</strong>: Features should be on a similar scale for kNN to perform well, as distance calculations are sensitive to feature scales.</li>
<li><strong>Practice</strong>: Apply normalization (e.g., Min-Max scaling) or standardization (mean=0, variance=1) to the features.</li>
</ul></li>
<li><strong>Handling Missing Data</strong>:
<ul>
<li><strong>Importance</strong>: kNN cannot handle missing values directly.</li>
<li><strong>Practice</strong>: Impute missing values before applying kNN, using mean/mode imputation or kNN-based imputation.</li>
</ul></li>
<li><strong>Choosing k</strong>:
<ul>
<li><strong>Importance</strong>: The choice of <span class="math inline">\(k\)</span> can significantly affect model performance.</li>
<li><strong>Practice</strong>: Use cross-validation to determine the optimal value of <span class="math inline">\(k\)</span>.</li>
</ul></li>
<li><strong>Evaluating Model Performance</strong>:
<ul>
<li><strong>Importance</strong>: Use appropriate metrics to evaluate the model, especially with imbalanced data.</li>
<li><strong>Practice</strong>: Evaluate using precision, recall, and ROC-AUC, not just accuracy.</li>
</ul></li>
</ol>
</div>
</section>
<section id="support-vector-machine-classification" class="slide level2">
<h2>Support Vector Machine Classification</h2>
<p>The SVM assumes a training set of the form <span class="math inline">\((x_1,y_1),\ldots,(x_n,y_n)\)</span> where the <span class="math inline">\(y_i\)</span> are either <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>, indicating the class to which each <span class="math inline">\(x_i\)</span> belongs.</p>
<p>The SVM algorithm looks to find the <strong>maximum-margin hyperplane</strong> that divides the group of points <span class="math inline">\(x_i\)</span> for which <span class="math inline">\(y_1=-1\)</span> from the group for which <span class="math inline">\(Y_1=1\)</span>, such that the distance between the hyperplane and the nearest point <span class="math inline">\(x_i\)</span> from either group is maximized.</p>
</section>
<section id="svm-classification" class="slide level2">
<h2>SVM Classification</h2>
<h3 id="basic-concepts">Basic Concepts</h3>
<div style="font-size: smaller">
<ol type="1">
<li><strong>Separating Hyperplane:</strong> The core idea of SVM is to find a hyperplane (in two-dimensional space, this would be a line) that best separates the classes in the feature space.</li>
<li><strong>Support Vectors:</strong> Support vectors are the data points that are closest to the separating hyperplane. These points are critical in defining the position and orientation of the hyperplane.</li>
<li><strong>Margin:</strong> The algorithm aims to maximize the margin, which is the distance between the hyperplane and the nearest points from both classes. A larger margin is considered better as it may lead to lower generalization error of the classifier.</li>
</ol>
</div>
</section>
<section id="svm-classification-large-margin" class="slide level2">
<h2>SVM Classification: large margin</h2>
<p>Illustration of SVM large-margin principle</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="https://github.com/probml/pml-book/blob/main/book1-figures/Figure_17.12.png"><img data-src="images/lec-6/Murphy_Figure_17.12.png" class="quarto-figure quarto-figure-center" alt="from Murphy figure 17.12"></a></p>
</figure>
</div>
</section>
<section id="svm-classification-1" class="slide level2">
<h2>SVM Classification:</h2>
<div class="columns">
<div class="column" style="font-size: smaller">
<ul>
<li><p>Let our decision boundary be given by <span class="math inline">\(f\left(x\right)=w^{\top}x+w_{0}=0\)</span>, for a vector <span class="math inline">\(w\)</span> perpendicular to the boundary<sup>1</sup>.</p></li>
<li><p>We can express any point as <span class="math inline">\(x=x_{\bot}+r\frac{w}{\left\Vert w\right\Vert }\)</span></p></li>
<li><p>Note that <span class="math inline">\(f\left(x\right)=\left(w^{\top}x_{\bot}+w_{0}\right)+r\left\Vert w\right\Vert\)</span></p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lec-6/Murphy_Figure_17.13_A.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<aside><ol class="aside-footnotes"><li id="fn4"><p>any point <span class="math inline">\(x\)</span> on the red line satisfies <span class="math inline">\(w^\top x+w_0=0\)</span></p></li></ol></aside></section>
<section id="svm-classification-2" class="slide level2">
<h2>SVM Classification:</h2>
<div class="columns">
<div class="column" style="font-size: smaller">
<ul>
<li><p>Since <span class="math inline">\(f\left(x_{\bot}\right)=w^{\top}x_{\bot}+w_{0}=0\)</span>, we have <span class="math inline">\(f\left(x\right)=r\left\Vert w\right\Vert\)</span>.</p></li>
<li><p>We also require <span class="math inline">\(f\left(x_{n}\right)\tilde{y}_{n}&gt;0\)</span></p></li>
<li><p>To maximize the distance to the closest point, the objective is <span class="math inline">\(\max_{w,w_{0}}\min_{n}\left[\tilde{y}_{n}\left(w^{\top}x_{n}+w_{0}\right)\right]\)</span></p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lec-6/Murphy_Figure_17.13_A.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>
<section id="svm-classification-3" class="slide level2">
<h2>SVM Classification:</h2>
<p>It is common to scale the vector <span class="math inline">\(w\)</span> and the offset <span class="math inline">\(w_0\)</span> such that <span class="math inline">\(f_n\hat{y}_n=1\)</span> for the point nearest the decision boundary, such that <span class="math inline">\(f_n\hat{y}_n\ge1\)</span> for all <span class="math inline">\(n\)</span>.</p>
<p>In addition, since minimizing <span class="math inline">\(1/ \left\Vert w\right\Vert\)</span> is equivalent to minimizing <span class="math inline">\(\left\Vert w\right\Vert^2\)</span>, we can state the objective as</p>
<p><span class="math display">\[
\min_{w,w_{0}}\frac{1}{2}\left\Vert w\right\Vert ^{2}\quad\text{s.t.}\quad\tilde{y}_{n}\left(w^{\top}x_{n}+w_{0}\right)\ge 1, \forall n
\]</span></p>
</section>
<section id="svm-classification-4" class="slide level2">
<h2>SVM Classification:</h2>
<div class="columns">
<div class="column" style="font-size: smaller">
<p>If there is no solution to the objective we can add slack variables <span class="math inline">\(\xi_n\ge0\)</span> to replace the hard constraints that <span class="math inline">\(f_n\hat{y}_n\ge1\)</span> with the soft margin constraints that <span class="math inline">\(f_n\hat{y}_n\ge1-\xi_n\)</span>.</p>
<p>The new objective is</p>
<p><span class="math display">\[
\min_{w,w_{0},\xi}\frac{{1}}{2}\left\Vert w\right\Vert ^{2} + C\sum_n \xi_n \\
\text{s.t.}\xi_n\ge0, \quad\tilde{y}_{n}\left(w^{\top}x_{n}+w_{0}\right)\ge 1, \forall n
\]</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lec-6/Murphy_Figure_17.13_B.png" class="quarto-figure quarto-figure-center" alt="Murphy fig 17.13(b)" width="299"></p>
</figure>
</div>
</div></div>
</section>
<section id="svm-data-not-separable" class="slide level2">
<h2>SVM: data not separable</h2>
<div class="columns">
<div class="column" style="font-size: smaller">
<p>If the data is not separable:</p>
<ul>
<li><p>a transformation of data may make them separable</p></li>
<li><p>an embedding in a higher dimensional space might make them separable</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lec-6/Sebastian_Raschka.png" class="quarto-figure quarto-figure-center" width="600"></p>
</figure>
</div>
</div></div>
</section>
<section id="svm-classification-support-vectors" class="slide level2">
<h2>SVM Classification: Support Vectors</h2>
<ul>
<li><p>Support vectors are the data points that lie closest to the decision surface (or hyperplane)</p></li>
<li><p>They are the data points most difficult to classify</p></li>
<li><p>They have direct bearing on the optimum location of the decision surface</p></li>
<li><p>Support vectors are the elements of the training set that would change the position of the dividing hyperplane if</p>
<p>removed</p></li>
</ul>
</section>
<section id="svm-classification-example" class="slide level2">
<h2>SVM Classification: example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a></a><span class="sc">&gt;</span> <span class="co"># show_engines("svm_linear")</span></span>
<span id="cb12-2"><a></a><span class="er">&gt;</span> default_model_svm <span class="ot">&lt;-</span> parsnip<span class="sc">::</span><span class="fu">svm_linear</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb12-3"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"svm_linear"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb12-4"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="svm-classification-variants" class="slide level2">
<h2>SVM Classification: variants</h2>
<p>The <strong>linear SVM</strong> constructs a linear decision boundary (hyperplane) to separate classes in the feature space. It aims to find the hyperplane that maximizes the margin between the closest points (support vectors) of the classes. The decision function is <span class="math inline">\(f(x) = w \cdot x + b\)</span>, where <span class="math inline">\(w\)</span> is the weight vector and <span class="math inline">\(b\)</span> is the bias term.</p>
<p>There are similar SVM methods that are adapted to more complex boundaries.</p>
</section>
<section id="svm-classification-polynomial" class="slide level2">
<h2>SVM Classification: polynomial</h2>
<p><strong>Polynomial SVM</strong> uses a polynomial kernel to create a non-linear decision boundary. It transforms the input features into higher-dimensional space where a linear separation is possible.</p>
<p>The polynomial kernel is <span class="math inline">\(K(x, x') = (w \cdot x + b)^d\)</span>, where <span class="math inline">\(d\)</span> is the degree of the polynomial. This kernel is implemented though <code>parsnip::svm_poly</code> and engine <strong>kernlab</strong>.</p>
</section>
<section id="svm-classification-radial-basis" class="slide level2">
<h2>SVM Classification: radial basis</h2>
<p><strong>RBF SVM</strong> uses the Radial Basis Function (Gaussian) kernel to handle non-linear classification problems. It maps the input space into an infinite-dimensional space where a linear separation is possible.</p>
<p>The RBF kernel is <span class="math inline">\(K(x, x') = \exp\left(-\gamma \left\Vert x - x'\right\Vert^2\right)\)</span>, where <span class="math inline">\(\gamma\)</span> controls the width of the Gaussian function. This kernel is implemented though <code>parsnip::svm_rbf</code> and engine <strong>kernlab</strong>.</p>
</section>
<section id="svm-classification-comparisons" class="slide level2">
<h2>SVM Classification: Comparisons</h2>
<div style="font-size: large">
<table class="caption-top">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Linear SVM</th>
<th>Polynomial SVM</th>
<th>RBF SVM</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Kernel Function</strong></td>
<td>Linear</td>
<td>Polynomial</td>
<td>Radial Basis Function</td>
</tr>
<tr class="even">
<td><strong>Equation</strong></td>
<td><span class="math inline">\(w \cdot x + b\)</span></td>
<td><span class="math inline">\((w \cdot x + b)^d\)</span></td>
<td><span class="math inline">\(\exp\left(-\gamma \left\Vert x - x'\right\Vert^2\right)\)</span></td>
</tr>
<tr class="odd">
<td><strong>Complexity</strong></td>
<td>Low</td>
<td>Medium to High (depending on <span class="math inline">\(d\)</span>)</td>
<td>High</td>
</tr>
<tr class="even">
<td><strong>Interpretability</strong></td>
<td>High</td>
<td>Medium</td>
<td>Low</td>
</tr>
<tr class="odd">
<td><strong>Computational Cost</strong></td>
<td>Low</td>
<td>Medium to High (higher with increasing <span class="math inline">\(d\)</span>)</td>
<td>High</td>
</tr>
<tr class="even">
<td><strong>Flexibility</strong></td>
<td>Low</td>
<td>Medium to High</td>
<td>High</td>
</tr>
<tr class="odd">
<td><strong>Risk of Overfitting</strong></td>
<td>Low</td>
<td>Medium to High (higher with increasing <span class="math inline">\(d\)</span>)</td>
<td>Medium to High (depends on <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(C\)</span>)</td>
</tr>
<tr class="even">
<td><strong>Typical Use Cases</strong></td>
<td>Linearly separable, high-dimensional spaces (e.g., text)</td>
<td>Data with polynomial relationships</td>
<td>Highly non-linear data, complex patterns</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="svm-classification-best-practices" class="slide level2">
<h2>SVM Classification Best Practices</h2>
<div style="font-size: small">
<h3 id="data-preparation">1. <strong>Data Preparation</strong></h3>
<h4 id="a.-feature-scaling">a. <strong>Feature Scaling</strong></h4>
<ul>
<li><strong>Importance</strong>: SVM is sensitive to the scale of the features. Features with larger ranges can dominate the distance calculations, leading to suboptimal boundaries.</li>
<li><strong>Best Practice</strong>: Standardize or normalize the features so that they have similar scales. Common techniques include Min-Max scaling (to a [0, 1] range) and StandardScaler (to zero mean and unit variance).</li>
</ul>
<h4 id="b.-handling-missing-data">b. <strong>Handling Missing Data</strong></h4>
<ul>
<li><strong>Importance</strong>: SVM cannot handle missing values directly.</li>
<li><strong>Best Practice</strong>: Impute missing values using methods like mean/mode/median imputation or more sophisticated techniques such as k-NN imputation or using models to predict missing values.</li>
</ul>
<h3 id="choosing-the-kernel">2. <strong>Choosing the Kernel</strong></h3>
<h4 id="a.-linear-kernel">a. <strong>Linear Kernel</strong></h4>
<ul>
<li><strong>When to Use</strong>: If the data is approximately linearly separable or when the number of features is very large compared to the number of samples.</li>
<li><strong>Best Practice</strong>: Start with a linear kernel as a baseline, especially for high-dimensional data like text classification.</li>
</ul>
<h4 id="b.-non-linear-kernels-polynomial-rbf">b. <strong>Non-linear Kernels (Polynomial, RBF)</strong></h4>
<ul>
<li><strong>When to Use</strong>: If the data is not linearly separable. The RBF kernel is generally a good first choice for non-linear problems.</li>
<li><strong>Best Practice</strong>: Experiment with different kernels. Use cross-validation to compare performance and select the most appropriate kernel.</li>
</ul>
</div>
</section>
<section id="svm-classification-best-practices-1" class="slide level2">
<h2>SVM Classification Best Practices</h2>
<div style="font-size: small">
<h3 id="hyperparameter-tuning">3. <strong>Hyperparameter Tuning</strong></h3>
<h4 id="a.-regularization-parameter-cost-c">a. <strong>Regularization Parameter (cost</strong> <span class="math inline">\(C\)</span>)</h4>
<ul>
<li><strong>Importance</strong>: Controls the trade-off between achieving a low training error and a low testing error.</li>
<li><strong>Best Practice</strong>: Use cross-validation to find the optimal value of cost <span class="math inline">\(C\)</span>. Start with a wide range and then narrow down.</li>
</ul>
<h4 id="b.-kernel-specific-parameters">b. <strong>Kernel-specific Parameters</strong></h4>
<ul>
<li><strong>For RBF Kernel</strong>: Tune the <span class="math inline">\(\sigma\)</span> parameter, which defines the influence of individual data points.</li>
<li><strong>For Polynomial Kernel</strong>: Tune the degree <span class="math inline">\(d\)</span>, the margin coefficient <span class="math inline">\(r\)</span>, and the <code>scale_factor</code>.</li>
<li><strong>Best Practice</strong>: Use grid search or random search for hyperparameter tuning. Cross-validation is crucial to avoid overfitting and to find the best combination of parameters.</li>
</ul>
<h3 id="handling-imbalanced-data">4. <strong>Handling Imbalanced Data</strong></h3>
<h4 id="a.-class-weights">a. <strong>Class Weights</strong></h4>
<ul>
<li><strong>Importance</strong>: Imbalanced classes can bias the SVM towards the majority class.</li>
<li><strong>Best Practice</strong>: Adjust the class weights to give more importance to the minority class. Most SVM implementations allow setting the <code>class_weight</code> parameter to ‘balanced’ or manually specifying weights.</li>
</ul>
<h4 id="b.-resampling-techniques">b. <strong>Resampling Techniques</strong></h4>
<ul>
<li><strong>Importance</strong>: Can further help in dealing with imbalanced datasets.</li>
<li><strong>Best Practice</strong>: Use oversampling (e.g., <code>themis::step_smote()</code>) or undersampling techniques to balance the class distribution before training the model.</li>
</ul>
</div>
</section>
<section id="svm-classification-best-practices-2" class="slide level2">
<h2>SVM Classification Best Practices</h2>
<div style="font-size: small">
<h3 id="model-evaluation">5. <strong>Model Evaluation</strong></h3>
<h4 id="a.-performance-metrics">a. <strong>Performance Metrics</strong></h4>
<ul>
<li><strong>Importance</strong>: Accuracy alone may not be sufficient, especially for imbalanced datasets.</li>
<li><strong>Best Practice</strong>: Evaluate using metrics like precision, recall,and ROC-AUC to get a comprehensive understanding of model performance.</li>
</ul>
<h4 id="b.-cross-validation">b. <strong>Cross-validation</strong></h4>
<ul>
<li><strong>Importance</strong>: Ensures that the model generalizes well to unseen data.</li>
<li><strong>Best Practice</strong>: Use k-fold cross-validation to assess the model’s performance and robustness. This helps in reducing the variance in performance estimates.</li>
</ul>
<h3 id="implementation-considerations">6. <strong>Implementation Considerations</strong></h3>
<h4 id="a.-choosing-the-right-softwarelibrary">a. <strong>Choosing the Right Software/Library</strong></h4>
<ul>
<li><strong>Importance</strong>: Efficient and reliable implementation is crucial for performance.</li>
<li><strong>Best Practice</strong>: Use well-established libraries like those in parsnip.</li>
</ul>
<h4 id="b.-handling-large-datasets">b. <strong>Handling Large Datasets</strong></h4>
<ul>
<li><strong>Importance</strong>: SVM can be computationally intensive for large datasets.</li>
<li><strong>Best Practice</strong>: Use linear SVMs for very large datasets or use dimensionality reduction techniques (e.g., PCA) to reduce the feature space.</li>
</ul>
</div>
</section></section>
<section>
<section id="clustering-methods" class="title-slide slide level1 center">
<h1>Clustering Methods</h1>

</section>
<section id="clustering" class="slide level2">
<h2>Clustering</h2>
<p><em>Classification</em> and <em>clustering</em> serve different purposes in machine learning. <em>Classification</em> is a <u>supervised</u> learning technique used for predicting predefined labels, requiring labeled data and focusing on accuracy and interpretability.</p>
<p><em>Clustering</em>, on the other hand, is an <u>unsupervised</u> learning technique used for discovering natural groupings in data, requiring no labeled data and focusing on exploratory data analysis and pattern discovery. Understanding the strengths and limitations of each method is crucial for applying them effectively to solve real-world problems.</p>
</section>
<section id="clustering-1" class="slide level2">
<h2>Clustering</h2>
<p><strong><em>Cluster analysis</em></strong> refers to algorithms that group similar objects into groups called <em>clusters</em>. The endpoint of cluster analysis is a set of clusters<em>,</em> where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.</p>
<p>The purpose of cluster analysis is to help reveal patterns and structures within a dataset that may provide insights into underlying relationships and associations.</p>
</section>
<section id="clustering-applications" class="slide level2">
<h2>Clustering Applications</h2>
<div style="font-size: 70%">
<ol type="1">
<li><strong>Market Segmentation:</strong>&nbsp;Cluster analysis is often used in marketing to segment customers into groups based on their buying behavior, demographics, or other characteristics.</li>
<li><strong>Image Processing:</strong>&nbsp;In image processing, cluster analysis is used to group pixels with similar properties together, allowing for the identification of objects and patterns in images.</li>
<li><strong>Biology and Medicine:</strong>&nbsp;Cluster analysis is used in biology and medicine to identify genes associated with specific diseases or to group patients with similar clinical characteristics together.</li>
<li><strong>Social Network Analysis:</strong>&nbsp;In social network analysis, cluster analysis is used to group individuals with similar social connections and characteristics together, allowing for the identification of subgroups within a larger network.</li>
<li><strong>Anomaly Detection:</strong>&nbsp;Cluster analysis can be used to detect anomalies in data, such as fraudulent financial transactions, unusual patterns in network traffic, or outliers in medical data.</li>
</ol>
</div>
</section>
<section id="k-means-clustering" class="slide level2">
<h2>K-means Clustering</h2>
<p><em>k-means</em> is a method of <em>unsupervised</em> learning that produces a partitioning of observations into <em>k</em> unique clusters.</p>
<p>The goal of <em>k-means</em> is to minimize the sum of squared Euclidian distances between observations in a cluster and the <strong>centroid</strong>, or geometric mean, of that cluster.</p>
</section>
<section id="k-means-clustering-1" class="slide level2">
<h2>K-means Clustering</h2>
<p>In <em>k-means</em> clustering, observed variables (columns) are considered to be locations on axes in multidimensional space.</p>
<div style="font-size: x-large">
<p>The basic k-means algorithm has the following steps.</p>
<ol type="1">
<li>pick the number of clusters k</li>
<li>Choose <em>k</em> observations in the dataset. These locations in space are declared to be the <strong>initial centroids</strong>.</li>
<li>Assign each observation to the nearest <strong>centroid</strong>.</li>
<li>Compute the new <strong>centroids</strong> of each cluster (the mean of each measurement over all observations in the cluster).</li>
<li>Repeat steps 3 and 4 until the <strong>centroids</strong> do not change.</li>
</ol>
</div>
</section>
<section id="k-means-clustering-2" class="slide level2">
<h2>K-means Clustering</h2>
<p>There are three common methods for selecting initial centers:</p>
<ol type="1">
<li><strong>Random observations:</strong> Chosing random observations to act as our initial centers is the most commonly used approach, implemented in the <code>Forgy</code>, <code>Lloyd</code>, and <code>MacQueen</code> methods.</li>
<li><strong>Random partition:</strong> The observations are assigned to a cluster uniformly at random. The centroid of each cluster is computed, and these are used as the initial centers. This approach is implemented in the <code>Hartigan-Wong</code> method.</li>
<li><strong>k-means++:</strong> Beginning with one random set of the observations, further observations are sampled via probability-weighted sampling until <span class="math inline">\(k\)</span> clusters are formed. The centroids of these clusters are used as the initial centers.</li>
</ol>
</section>
<section id="k-means-clustering-3" class="slide level2">
<h2>K-means Clustering</h2>
<p>Because the initial conditions are based on random selection in both approaches, the k-means algorithm is not deterministic.</p>
<p>Running the clustering twice on the same data may not result in the same cluster assignments.</p>
</section>
<section id="k-means-example" class="slide level2">
<h2>K-means Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13" data-code-line-numbers="1-3|5-10|11-12|14-22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a></a><span class="sc">&gt;</span> <span class="co"># create recipe for 2-D clustering</span></span>
<span id="cb13-2"><a></a><span class="er">&gt;</span> cluster_recipe <span class="ot">&lt;-</span> data <span class="sc">|&gt;</span> </span>
<span id="cb13-3"><a></a><span class="sc">+</span>   recipes<span class="sc">::</span><span class="fu">recipe</span>(<span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> _)</span>
<span id="cb13-4"><a></a><span class="sc">&gt;</span> </span>
<span id="cb13-5"><a></a><span class="er">&gt;</span> <span class="co"># specify the workflows</span></span>
<span id="cb13-6"><a></a><span class="er">&gt;</span> all_workflows <span class="ot">&lt;-</span> </span>
<span id="cb13-7"><a></a><span class="sc">+</span>   workflowsets<span class="sc">::</span><span class="fu">workflow_set</span>(</span>
<span id="cb13-8"><a></a><span class="sc">+</span>     <span class="at">preproc =</span> <span class="fu">list</span>(<span class="at">base =</span> cluster_recipe),</span>
<span id="cb13-9"><a></a><span class="sc">+</span>     <span class="at">models =</span> <span class="fu">list</span>(tidyclust<span class="sc">::</span><span class="fu">k_means</span>( <span class="at">num_clusters =</span> parsnip<span class="sc">::</span><span class="fu">tune</span>() ) )</span>
<span id="cb13-10"><a></a><span class="sc">+</span>   )</span>
<span id="cb13-11"><a></a><span class="sc">&gt;</span> <span class="co"># create bootstrap samples</span></span>
<span id="cb13-12"><a></a><span class="er">&gt;</span> dat_resamples <span class="ot">&lt;-</span> data <span class="sc">|&gt;</span> rsample<span class="sc">::</span><span class="fu">bootstraps</span>(<span class="at">apparent =</span> <span class="cn">TRUE</span>)</span>
<span id="cb13-13"><a></a><span class="sc">&gt;</span> </span>
<span id="cb13-14"><a></a><span class="er">&gt;</span> tuned_results <span class="ot">&lt;-</span></span>
<span id="cb13-15"><a></a><span class="sc">+</span>    all_workflows <span class="sc">|&gt;</span> </span>
<span id="cb13-16"><a></a><span class="sc">+</span>    <span class="fu">workflow_map</span>(</span>
<span id="cb13-17"><a></a><span class="sc">+</span>       <span class="at">fn =</span> <span class="st">"tune_cluster"</span></span>
<span id="cb13-18"><a></a><span class="sc">+</span>       , <span class="at">resamples =</span> dat_resamples</span>
<span id="cb13-19"><a></a><span class="sc">+</span>       , <span class="at">grid =</span> dials<span class="sc">::</span><span class="fu">grid_regular</span>(dials<span class="sc">::</span><span class="fu">num_clusters</span>(), <span class="at">levels =</span> <span class="dv">10</span>)</span>
<span id="cb13-20"><a></a><span class="sc">+</span>       , <span class="at">metrics =</span> tidyclust<span class="sc">::</span><span class="fu">cluster_metric_set</span>(sse_within_total, sse_total, sse_ratio)</span>
<span id="cb13-21"><a></a><span class="sc">+</span>       , <span class="at">control =</span> tune<span class="sc">::</span><span class="fu">control_grid</span>(<span class="at">save_pred =</span> <span class="cn">TRUE</span>, <span class="at">extract =</span> identity)</span>
<span id="cb13-22"><a></a><span class="sc">+</span>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="k-means-example-1" class="slide level2">
<h2>K-means Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a></a><span class="sc">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">8740</span>)</span>
<span id="cb14-2"><a></a><span class="sc">&gt;</span> </span>
<span id="cb14-3"><a></a><span class="er">&gt;</span> centers <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(</span>
<span id="cb14-4"><a></a><span class="sc">+</span>   <span class="at">cluster =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>), </span>
<span id="cb14-5"><a></a><span class="sc">+</span>   <span class="at">num_points =</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">150</span>, <span class="dv">50</span>, <span class="dv">90</span>),  <span class="co"># number points in each cluster</span></span>
<span id="cb14-6"><a></a><span class="sc">+</span>   <span class="at">x1 =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">3</span>, <span class="sc">-</span><span class="dv">4</span>),              <span class="co"># x1 coordinate of cluster center</span></span>
<span id="cb14-7"><a></a><span class="sc">+</span>   <span class="at">x2 =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="fl">1.5</span>),               <span class="co"># x2 coordinate of cluster center</span></span>
<span id="cb14-8"><a></a><span class="sc">+</span> )</span>
<span id="cb14-9"><a></a><span class="sc">&gt;</span> </span>
<span id="cb14-10"><a></a><span class="er">&gt;</span> labelled_points <span class="ot">&lt;-</span> </span>
<span id="cb14-11"><a></a><span class="sc">+</span>   centers <span class="sc">|&gt;</span></span>
<span id="cb14-12"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb14-13"><a></a><span class="sc">+</span>     <span class="at">x1 =</span> purrr<span class="sc">::</span><span class="fu">map2</span>(num_points, x1, rnorm),</span>
<span id="cb14-14"><a></a><span class="sc">+</span>     <span class="at">x2 =</span> purrr<span class="sc">::</span><span class="fu">map2</span>(num_points, x2, rnorm)</span>
<span id="cb14-15"><a></a><span class="sc">+</span>   ) <span class="sc">|&gt;</span> </span>
<span id="cb14-16"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>num_points) <span class="sc">|&gt;</span> </span>
<span id="cb14-17"><a></a><span class="sc">+</span>   tidyr<span class="sc">::</span><span class="fu">unnest</span>(<span class="at">cols =</span> <span class="fu">c</span>(x1, x2))</span>
<span id="cb14-18"><a></a><span class="sc">&gt;</span> </span>
<span id="cb14-19"><a></a><span class="er">&gt;</span> p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(labelled_points, <span class="fu">aes</span>(x1, x2, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb14-20"><a></a><span class="sc">+</span>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.3</span>) <span class="sc">+</span> </span>
<span id="cb14-21"><a></a><span class="sc">+</span>   <span class="fu">geom_point</span>(<span class="at">data =</span> centers, <span class="at">size =</span> <span class="dv">10</span>, <span class="at">shape =</span> <span class="st">"o"</span>)</span>
<span id="cb14-22"><a></a><span class="sc">&gt;</span> p</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="BSMM_8740_lec_05_files/figure-html/unnamed-chunk-12-1.png" class="quarto-figure quarto-figure-center r-stretch" width="672"></section>
<section id="k-means-example-2" class="slide level2">
<h2>K-means Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15" data-code-line-numbers="1-3|4-5|6-9|10-13|14-15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a></a><span class="sc">&gt;</span> <span class="co"># create recipe</span></span>
<span id="cb15-2"><a></a><span class="er">&gt;</span> labelled_points_recipe <span class="ot">&lt;-</span> labelled_points <span class="sc">|&gt;</span> </span>
<span id="cb15-3"><a></a><span class="sc">+</span>   recipes<span class="sc">::</span><span class="fu">recipe</span>(<span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> _)</span>
<span id="cb15-4"><a></a><span class="sc">&gt;</span> <span class="co"># create model spec</span></span>
<span id="cb15-5"><a></a><span class="er">&gt;</span> kmeans_spec <span class="ot">&lt;-</span> tidyclust<span class="sc">::</span><span class="fu">k_means</span>( <span class="at">num_clusters =</span> <span class="dv">4</span> )</span>
<span id="cb15-6"><a></a><span class="sc">&gt;</span> <span class="co"># create workflow</span></span>
<span id="cb15-7"><a></a><span class="er">&gt;</span> wflow <span class="ot">&lt;-</span> workflows<span class="sc">::</span><span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb15-8"><a></a><span class="sc">+</span>   workflows<span class="sc">::</span><span class="fu">add_model</span>(kmeans_spec) <span class="sc">|&gt;</span></span>
<span id="cb15-9"><a></a><span class="sc">+</span>   workflows<span class="sc">::</span><span class="fu">add_recipe</span>(labelled_points_recipe)</span>
<span id="cb15-10"><a></a><span class="sc">&gt;</span> <span class="co"># fit workflow &amp; extract centroids</span></span>
<span id="cb15-11"><a></a><span class="er">&gt;</span> cluster_centers <span class="ot">&lt;-</span> wflow <span class="sc">|&gt;</span></span>
<span id="cb15-12"><a></a><span class="sc">+</span>   parsnip<span class="sc">::</span><span class="fu">fit</span>(labelled_points) <span class="sc">%&gt;%</span> tidyclust<span class="sc">::</span><span class="fu">extract_centroids</span>() <span class="sc">|&gt;</span> </span>
<span id="cb15-13"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">mutate</span>( <span class="at">cluster =</span> stringr<span class="sc">::</span><span class="fu">str_extract</span>(.cluster,<span class="st">"</span><span class="sc">\\</span><span class="st">d"</span>) )</span>
<span id="cb15-14"><a></a><span class="sc">&gt;</span> <span class="co"># plot</span></span>
<span id="cb15-15"><a></a><span class="er">&gt;</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">data =</span> cluster_centers, <span class="at">size =</span> <span class="dv">10</span>, <span class="at">shape =</span> <span class="st">"x"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="BSMM_8740_lec_05_files/figure-html/unnamed-chunk-13-1.png" class="quarto-figure quarto-figure-center r-stretch" width="672"><div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a></a><span class="sc">&gt;</span> <span class="co"># all_workflows &lt;- all_workflows %&gt;% </span></span>
<span id="cb16-2"><a></a><span class="er">&gt;</span> <span class="co">#   workflowsets::workflow_map(</span></span>
<span id="cb16-3"><a></a><span class="er">&gt;</span> <span class="co">#     resamples = train_resamples, grid = 5, verbose = TRUE</span></span>
<span id="cb16-4"><a></a><span class="er">&gt;</span> <span class="co">#   )</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a></a><span class="sc">&gt;</span> <span class="co"># all_workflows &lt;- </span></span>
<span id="cb17-2"><a></a><span class="er">&gt;</span> <span class="co">#   workflowsets::workflow_set(</span></span>
<span id="cb17-3"><a></a><span class="er">&gt;</span> <span class="co">#     preproc = list(base = labelled_points_recipe),</span></span>
<span id="cb17-4"><a></a><span class="er">&gt;</span> <span class="co">#     models = </span></span>
<span id="cb17-5"><a></a><span class="er">&gt;</span> <span class="co">#       3:20 %&gt;% purrr::map( ~tidyclust::k_means( num_clusters = .x ) )</span></span>
<span id="cb17-6"><a></a><span class="er">&gt;</span> <span class="co">#   )</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a></a><span class="sc">&gt;</span> all_workflows <span class="ot">&lt;-</span> </span>
<span id="cb18-2"><a></a><span class="sc">+</span>   workflowsets<span class="sc">::</span><span class="fu">workflow_set</span>(</span>
<span id="cb18-3"><a></a><span class="sc">+</span>     <span class="at">preproc =</span> <span class="fu">list</span>(<span class="at">base =</span> labelled_points_recipe),</span>
<span id="cb18-4"><a></a><span class="sc">+</span>     <span class="at">models =</span> <span class="fu">list</span>(tidyclust<span class="sc">::</span><span class="fu">k_means</span>( <span class="at">num_clusters =</span> parsnip<span class="sc">::</span><span class="fu">tune</span>() ) )</span>
<span id="cb18-5"><a></a><span class="sc">+</span>   )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="hierarchical-clustering" class="slide level2">
<h2>Hierarchical Clustering</h2>
<p><em>Hierarchical Clustering</em>, sometimes called <em>Agglomerative Clustering</em>, is a method of <em>unsupervised</em> learning that produces a <em>dendrogram</em>, which can be used to partition observations into clusters (see <a href="https://tidyclust.tidymodels.org/articles/hier_clust.html">tidyclust</a>)</p>
<p>For other clustering algorithms, see</p>
<div style="font-size: 80%">
<ul>
<li>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</li>
<li>Mean Shift Clustering</li>
<li>Spectral Clustering</li>
</ul>
</div>
</section>
<section id="clustering-general-considerations" class="slide level2">
<h2>Clustering: General Considerations</h2>
<ul>
<li><p><strong>Feature Scaling:</strong> Most clustering algorithms benefit from feature scaling.</p></li>
<li><p><strong>Choosing the Right Algorithm:</strong> Depends on the size, dimensionality of data, and the nature of the clusters.</p></li>
<li><p><strong>Evaluation:</strong> Since clustering is unsupervised, evaluating the results can be subjective and is often based on domain knowledge.</p></li>
</ul>
</section>
<section id="more" class="slide level2">
<h2>More</h2>
<ul>
<li>Read <a href="https://web.mit.edu/6.034/wwwbob/svm.pdf">An Idiot’s Guide to Support Vector Machines</a></li>
</ul>
</section>
<section id="recap" class="slide level2">
<h2>Recap</h2>
<ul>
<li><p>We have looked at several classification algorithms in the context of tidymodels workflows.</p></li>
<li><p>We also looked at clustering and several algorithms in the <code>tidyclust</code> package.</p></li>
</ul>


</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="images/logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://bsmm-8740-fall-2024.github.io/osb/">bsmm-8740-fall-2024.github.io/osb</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="../site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true,"numbers":true},
'chalkboard': {"buttons":true},
'multiplex': {"secret":null,"id":"1326423c599de105","url":"https://reveal-multiplex.glitch.me/"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 5.0e-2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/bsmm-8740-fall-2025\.github\.io\/osb\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>