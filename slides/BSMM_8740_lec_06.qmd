---
title: "Time series methods"
subtitle: "BSMM8740-2-R-2024F [WEEK - 7]"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2024.github.io/osb](https://bsmm-8740-fall-2024.github.io/osb/)"
logo: "images/logo.png"
# title-slide-attributes:
#   data-background-image: images/my-DRAFT.png
#   data-background-size: contain
#   data-background-opacity: "0.40"
format: 
  revealjs: 
    chalkboard: true
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    margin: 0.05
    html-math-method: mathjax
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

```{r opts, include = FALSE}
options(width = 95)
library(knitr)
opts_chunk$set(comment="", 
               digits = 3, 
               tidy = FALSE, 
               prompt = TRUE,
               fig.align = 'center')
require(magrittr)
require(ggplot2)
require(modeltime)
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

## Recap of last week:

-   Last week we introduced classification and clustering methods within the `Tidymodels` framework in R.
-   Today we look at methods for analysing time series, and we use the `timetk` and `modeltime` packages in conjunction with `Tidymodels` to create and evaluate time series models.

## This week:

-   Today we will explore time series - data where each observation includes a time measurement and the time measurements are ordered.

-   We'll look at how to manipulate our time values, create time-based features, plot our time series, and decompose time series into components.

-   Finally we will use our time series for forecasting, using regression, exponential smoothing and ARIMA[^1] models

[^1]: Auto Regressive Integrated Moving Average

# Time Series Methods

## Time series

A time series data is a data frame (tibble) with an ordered temporal measurement.

Why is this a separate area of study? Consider the simple linear regression model

$$
y_t=x_t^\top\beta + \epsilon_t;\;t=1,\ldots,R
$$

errors should not be serially correlated for OLS estimates:

-   $\mathbb{E}[\epsilon_t]=\mathbb{E}[\epsilon_t|\epsilon_{t-1},\epsilon_{t-1},\ldots]$, and
-   $\mathbb{E}[\epsilon_t\epsilon_{t-j}]=0,\forall j$

## Time series: characteristics

::: {style="font-size: 80%"}
-   Most economic and financial time series exhibit some form of serial correlation
    -   If economic output is large during the previous quarter then there is a good chance that it is going to be large in the current quarter
-   A change that arises in the current period may only affect other variables in the distant future
-   A particular shock may affect variables over successive quarters
    -   Hence, we need to start thinking about the dynamic structure of the system that we are investigating
:::

## Time series: dynamics

Whether time series data is used for forecasting or for testing various theories/hypotheses, we always need to identify the dynamic evolution of the variables, e.g.

$$
\begin{align*}
\text{(trend)}\qquad T_{t} & =1+0.05t\qquad\\
\text{(seasonality)}\qquad S_{t} & =1.5\cos(t\pi\times0.166)\\
\text{(noise)}\qquad I_{t} & =0.5I_{t-1}+\epsilon_{t}\quad\epsilon_t\sim\mathscr{N}(0,\sigma^2)
\end{align*}
$$

## Time series: generation

```{r}
#| echo: true
#| code-fold: true
set.seed(8740)

dat <- tibble::tibble(
  date = seq(as.Date('2015-04-7'),as.Date('2020-03-22'),'2 weeks')
) %>% 
  tibble::rowid_to_column("t") %>% 
  dplyr::mutate(
    trend = 1 + 0.05 * t
    , seasonality = 1.5 * cos(pi * t * 0.166)
    , noise = rnorm(length(t))
    , temp = dplyr::lag(noise)
  ) %>% 
  tidyr::replace_na(list(temp = 0)) %>% 
  dplyr::mutate(
    noise =
      purrr::map2_dbl(
        noise
        , temp
        , ~ .x + 0.5 * .y
      )
  ) %>% 
  dplyr::select(-temp)
```

## Time series: generation

::: {.panel-tabset style="font-size: large"}
## components

```{r}
#| echo: true
#| code-fold: true
dat %>% 
  tidyr::pivot_longer(-c(t, date)) %>% 
  ggplot(aes(x=date, y=value, color=name)) + geom_line(linewidth=1) + 
  labs(
    title = 'trend, seasonality, and noise'
    , subtitle = "deterministic: trend, seasonality | stochastic: noise"
    , color=NULL) + theme_minimal() + theme(plot.margin = margin(0,0,0,0, "cm"))
```

## combined

```{r}
#| echo: true
#| code-fold: true
dat %>% 
  dplyr::mutate(y = trend + seasonality + noise) %>% 
  ggplot(aes(x=date, y=y)) + geom_line(linewidth=1) +
  labs(
    title = 'trend + seasonality + noise'
    , subtitle = "deterministic: trend, seasonality | stochastic: noise") + theme_minimal() +
  theme(plot.margin = margin(0,0,0,0, "cm"))
```
:::

## Time series - processes

-   Time series is a collection of observations indexed by the date of each realisation

-   Using notation that starts at time, $t=1$, and using the end point, $t=T$

$$\{y_1,y_2,y_3,…,y_T\}$$

-   Time index can be of any frequency (e.g. daily, quarterly, etc.)

## Time series - processes

### Deterministic and Stochastic Processes

-   deterministic processes always produce the same output from a given starting point or state

-   stochastic processes have indeterminacy

    -   Usually described by some form of statistical distribution
    -   Examples include: white noise processes, random walks, Brownian motions, Markov chains, martingale difference sequences, etc.

## Time series - processes

### Stochastic Processes - White noise

::: {style="font-size: 80%"}
-   A white noise series is made of serially uncorrelated random variables with zero mean and finite variance
-   For example, errors may be characterised by a Gaussian white noise process, where such a variable has a normal distribution
-   Slightly stronger condition is that they are independent from one another

$$\epsilon_{t}\sim\text{{i.i.d.}}\mathscr{N}\left(0,\sigma_{\epsilon_{t}}^{2}\right)$$
:::

## Time series - processes

### Stochastic Processes - White noise

::: {style="font-size: 80%"}
implications:

-   $\mathbb{E}\left[\epsilon_{t}\right]=\mathbb{E}\left[\left.\epsilon_{t}\right|\epsilon_{t-1},\epsilon_{t-2},\ldots\right]=0$
-   $\mathbb{E}\left[\epsilon_{t}\epsilon_{t-j}\right]=\text{cov}\left(\epsilon_{t},\epsilon_{t-j}\right)=0$
-   $\text{var}\left(\epsilon_{t}\right)=\text{cov}\left(\epsilon_{t},\epsilon_{t}\right)=\sigma_{\epsilon_{t}}^{2}$
:::

## Time series - processes

### Stochastic Processes - Random walk

::: {style="font-size: 80%"}
-   Random walk would imply that the effect of a shock is permanent. $$y_t=y_{t-1}+\epsilon_t$$
-   Given the starting value $y_0$, and using recursive substitution, this process could be represented as $$y_t=y_0+\sum_{j=1}^t\epsilon_t$$
-   Since the effect of past shocks do not dissipate we say it has an infinite memory
:::

## Time series - processes

### Stochastic Processes - Random walk + drift

::: {style="font-size: x-large"}
-   Random walk plus a constant term. $$y_t=\mu+y_{t-1}+\epsilon_t$$
-   Given the starting value $y_0=0$, and using recursive substitution, this process could be represented as $$y_t=\mu t+\sum_{j=1}^t\epsilon_t$$
-   Shocks have permanent effects and are influenced by drift
:::

## Time series - processes

### Stochastic Processes - Random walks

::: {style="font-size: x-large"}
**Characteristics**:

1.  **Independence**:
    -   The steps in a random walk are independent of each other. The future position depends only on the current position and a random step.
2.  **Types**:
    -   **Simple Random Walk**: The step sizes are often $\pm 1$, with equal probability.
    -   **General Random Walk**: The step sizes can follow any distribution.
3.  **Applications**:
    -   Stock price movements, particle diffusion, and population genetics.
:::

## Time series - processes

### Stochastic Processes - Markov chains

::: {style="font-size: medium"}
**Characteristics**:

1.  **State Space**: - A Markov chain consists of a set of states and transition probabilities between these states.

2.  **Markov Property**:

    -   The probability of transitioning to the next state depends only on the current state: $\mathbb{P}(X_{t+1} = s' | X_t = s, X_{t-1}, ..., X_0) = \mathbb{P}(X_{t+1} = s' | X_t = s)$.

3.  **Transition Matrix**:

    -   The transitions are governed by a matrix of probabilities, where each entry $\mathbb{P}_{i,j}$ represents the probability of moving from state $i$ to state $j$.

4.  **Types**:

    -   **Discrete-Time Markov Chain**: The process is observed at discrete time intervals.
    -   **Continuous-Time Markov Chain**: The process evolves continuously over time.

5.  **Applications**:

    -   Weather modeling, queueing theory, board games (like Monopoly), and speech recognition.
:::

## Time series - processes

### Stochastic Processes - Markov chains

::: {style="font-size: x-large"}
In the discrete case, given a state transition matrix $A$ and an initial state $\pi_0$, then

$$
\begin{align*}
\pi_1 & = A\pi_0\\
\pi_2 & = A\pi_1 = A^2\pi_0\\
&\vdots\\
\pi_n & = A^n\pi_0
\end{align*}
$$

If the markov chain has a stationary distribution $\pi^{s}$, then $A\pi^{s}=\pi^{s}$ by definition and $(I-A)\pi^{s}=0$ determines $\pi^{s}$ up to a constant.
:::

## Time series - processes

### Autoregressive Processes

::: {style="font-size: x-large"}
-   In an AR(1) process the current value is a linear function of the previous value.

$$y_t=\phi_1 y_{t-1}+\epsilon_t$$ - Fixing the starting value at $y_0=0$, and with repeated substitution, this process could be represented as $$y_t=\sum_{j=1}^t\phi_1^{t-j}\epsilon_j$$ - The distribution of each error term is $\epsilon_t=\mathscr{N}(0,\sigma^2)$ with $\mathbb{E}[\epsilon_i\epsilon_j]=0,\,\forall i\ne j$, and we can generalize to several lags (i.e. an AR(p) model).
:::

## Time series - processes

### Moments of distribution

::: {style="font-size: 80%"}
-   The first moment of a stochastic process is the average of $y_t$ over all possible realisations $$\hat{y}=\mathbb{E}[y_t];\;\;t=1,\ldots,T$$
-   The second moment is defined as the variance $$\text{var}(y_t)=\mathbb{E}[y_t\times y_t]=\mathbb{E}[y_t- \mathbb{E}[y_t]^2];\;\;t=1,\ldots,T$$
-   The covariance, for $j$ $$\text{cov}(y_t,y_{t-j})=\mathbb{E}[y_t\times y_{t-j}]=\mathbb{E}[(y_t- \mathbb{E}[y_t])(y_{t-j}- \mathbb{E}[y_{t-j}])];\;\;t=1,\ldots,T$$
:::

## Time series - processes

### Conditional moments

::: {style="font-size: 80%"}
-   Conditional distribution is based on past realisations of a random variable
-   For the AR(1) model (where $\epsilon_t$ are iid Gausian and $|\phi|<0$) $$y_t=\phi y_{t-1}+\epsilon_t$$
-   The conditional moments are $$
    \begin{align*}
    \mathbb{E}\left[\left.y_{t}\right|y_{t-1}\right] & =\phi y_{t-1}\\
    \text{var}\left(\left.y_{t}\right|y_{t-1}\right) & =\mathbb{E}\left[\phi y_{t-1}+\epsilon_{t}-\phi y_{t-1}\right]^{2}=\mathbb{E}\left[\epsilon\right]^{2}=\sigma^{2}\\
    \text{cov}\left(\left.y_{t}\right|y_{t-1},\left.y_{t-j}\right|y_{t-j-1}\right) & =0;\;j>1
    \end{align*}
    $$
:::

## Time series - processes

### Conditional moments

::: {style="font-size: 80%"}
-   Conditioning on $y_{t-2}$ for $y_t$ $$
    \begin{align*}
    \mathbb{E}\left[\left.y_{t}\right|y_{t-2}\right] & =\phi^{2}y_{t-2}\\
    \text{var}\left(\left.y_{t}\right|y_{t-2}\right) & =\left(1+\phi^{2}\right)\sigma^{2}\\
    \text{cov}\left(\left.y_{t}\right|y_{t-2},\left.y_{t-j}\right|y_{t-j-2}\right) & =\phi\sigma^{2};\;j=1\\
     & =0;\;j>1
    \end{align*}
    $$
:::

## Time series - processes

### Unconditional moments

::: {style="font-size: 80%"}
-   For the AR(1) model (where $\epsilon_t$ are iid Gausian and $|\phi|<1$) $$y_t=\phi y_{t-1}+\epsilon_t$$
-   The unconditional moments are (assuming stationarity and $\phi<1$) $$
    \begin{align*}
    \mathbb{E}\left[y_{t}\right] & =0\\
    \text{var}\left(y_{t}\right) & =\text{var}\left(\phi y_{t-1}+\epsilon_t\right)\\
    & = \phi^2 \text{var}\left(y_{t-1}\right) + \text{var}\left(\epsilon_{t}\right)\\
    & = \frac{\sigma^2}{1-\phi^2}\\
    \text{cov}\left(y_{t},y_{t-k}\right) & =\phi^k\frac{\sigma^2}{1-\phi^2}
    \end{align*}
    $$
:::

## Time series - processes

### Stationarity

::: {style="font-size: 80%"}
-   A time series is strictly stationary if for any $\{j_1,j_2,\ldots,j_n\}$
-   the joint distribution of $\{y_t,y_{t+j_1},y_{t+j_2},\ldots,y_{t+j_n}\}$
-   depends only on the intervals separating the dates $\{j_1,j_2,\ldots,j_n\}$
-   and not on the date $t$ itself
:::

## Time series - processes

### Covariance stationary

::: {style="font-size: 80%"}
-   If neither the mean $\hat{y}$ nor the covariance $\text{cov}(y_t,y_{t-j})$ depend on $t$
-   the the process for $y_t$ is said to be covariance (weakly) stationary, where $\forall t,j$ $$
    \begin{align*}
    \mathbb{E}\left[y_{t}\right] & =\bar{y}\\
    \mathbb{E}\left[\left(y_{t}-\bar{y}\right)\left(y_{t-j}-\bar{y}\right)\right] & =\text{cov}\left(y_{t},y_{t-j}\right)
    \end{align*}
    $$
-   Note that the process $y_t=\alpha t+\epsilon_t$ would not be stationary, as the mean clearly depends on $t$
-   We saw that the unconditional moments of the AR(1) with $|ϕ|<1$ had a mean and covariance that did not depend on time
:::

## Time series - processes

### Autocorrelation function (ACF)

::: {style="font-size: 80%"}
-   For a stationary process we can plot the standardised covariance of the process over successive lags

-   The autocovariance function is denoted by $\gamma (j)\equiv\text{cov}(y_t,y_{t-j})$ for $t=1,\ldots,T$

-   Th autocovariance function is standardized by dividing each function by the variance, giving the ACF for successive values of $j$

    $$\rho(j)\equiv\frac{\gamma(j)}{\gamma(0)}$$

-   To display the results of the ACF we usually plot $ρ(j)$ against (non-negative) $j$ to illustrate the degree of persistence in a variable
:::

## Time series - processes

### Partial autocorrelation function (PACF)

::: {style="font-size: 80%"}
-   With an AR(1) process $y_t=\phi y_{t-1}+\epsilon_t$, the ACF would suggest $y_t$ and $y_{t-2}$ are correlated, even though $y_{t-2}$ does not appear in the model.

-   This is due to the pass through, where we noted that $y_t=\phi^2y_{t-2}$ when performing recursive substitution

-   PACF eliminates the effects of passthrough and puts the focus on the independent relationship between $y_t$ and $y_{t_2}$
:::

## Time series - processes

### Partial autocorrelation function (PACF)

```{r}
#| layout-ncol: 3
#| echo: true
#| code-fold: true
library(ggfortify)
# Parameters for the AR(1) process
phi = 0.8  # Autoregressive coefficient (should be less than 1 in absolute value)
n = 100    # Number of observations

# Simulate AR(1) process
set.seed(123)  # For reproducibility
epsilon = rnorm(n)  # White noise
X = rep(0, n)  # Initialize the series

# Generate the AR(1) series
for (t in 2:n) {
  X[t] = phi * X[t-1] + epsilon[t]
}

# Plot the AR(1) series
# plot(X, type = "l", main = "AR(1) Process", xlab = "Time", ylab = "Value")
tibble::tibble(y=X, x=1:length(X)) %>% ggplot(aes(x=x, y=y)) + geom_line() + 
  labs(title="AR(1) Process", x = "Time", y = "Value")

# Plot the autocorrelation function (ACF)
#stats::acf(X, main = "Autocorrelation of AR(1) Process")
autoplot(stats::acf(X, plot = FALSE)) + labs(title = "Autocorrelation of AR(1) Process")

# Plot the autocorrelation function (ACF)
#stats::pacf(X, main = "Partial Autocorrelation of AR(1) Process")
autoplot(stats::pacf(X, plot = FALSE)) + labs(title = "Partial Autocorrelation of AR(1) Process")

```

## 

## Time series - processes

### Autoregressive Processes

::: {style="font-size: x-large"}
-   The characteristic equation for an AR(p) process is derived from the autoregressive parameters $(\phi_1, \phi_2, \ldots, \phi_p)$.

-   The characteristic equation is: $1-\phi_1 z-\phi_2 z^2-\cdots-\phi_p z^p = 0$

-   A process ${y_t}$ is strictly stationary if for each $k$ and $t$, and $n$, the distribution of ${y_t,\ldots,y_{t+k}}$ is the same as the distribution of ${y_{t+n},\ldots,y_{t+k+n}}$

-   For an AR process to be stationary (its statistical properties do not change over time), the parameters $\phi_1, \phi_2, \ldots, \phi_p$ must satisfy certain conditions (typically related to the roots of the characteristic equation lying outside the unit circle).
:::

## Time series - processes

### Autoregressive Processes

::: {style="font-size: x-large"}
-   The AR(p) model is sometimes expressed in terms of the Lag operator $\mathrm{L}$, where $\mathrm{L}y_t=y_{t-1}$.

-   The lag operator can be raised to powers, e.g. $\mathrm{L}^2y_t=y_{t-2}$ and its powers can be combined into polynomials to form a new operator: $a(\mathrm{L})=a_0+a_1\mathrm{L}+a_2\mathrm{L}^2+\ldots+a_p\mathrm{L}^p$ such that $a(\mathrm{L})y_t=a_0y_t+a_1y_{t-1}+a_2y_{t-2}+\ldots+a_py_{t-p}$

-   Lag polymonials can be multiplied and multiplication commutes: $a(\mathrm{L})b(\mathrm{L})=b(\mathrm{L})a(\mathrm{L})$.

-   We define $(1-\rho\mathrm{L})^{-1}$ by $(1-\rho\mathrm{L})(1-\rho\mathrm{L})^{-1}\equiv1$.
:::

## Time series - processes

### Autoregressive Processes

::: {style="font-size: x-large"}
-   per the formula for the [geometric series](https://en.wikipedia.org/wiki/Geometric_series), if $|\rho|<1, then$

$$
(1-\rho\mathrm{L})^{-1} = \sum_{i=0}^\infty\rho^i\mathrm{L}^i
$$

-   EXERCISE: check that $(1-\rho\mathrm{L})(1-\rho\mathrm{L})^{-1}\equiv1$ holds on both sides of the equation above.

-   If the sum is to converge, then we need $|\rho|<1$
:::

## Time series - processes

### Autoregressive Processes

::: {style="font-size: x-large"}
For the AR(1) process, using the lag operator we can write:

$$
\begin{align*}
y_t & = \phi_1 y_{t-1}+\epsilon_t\\
(1-\phi L)y_t & =  \epsilon_t\\
y_t & = (1-\phi L)^{-1} \epsilon_t\\
  & = (\sum_{i=0}^\infty\phi^i\mathrm{L}^i)\epsilon_t\\
  & = \sum_{j=1}^t\phi_1^{t-j}\epsilon_j
\end{align*}
$$ - EXERCISE: check that the last equality holds.
:::

## Time series - processes

### Autoregressive Processes

AR(p) processes can be written as AR(1) vector processes, e.g. for AR(2)

$$
\left(\begin{array}{c}
y_{t}\\
y_{t-1}
\end{array}\right)=\left(\begin{array}{cc}
\phi_{1} & \phi_{2}\\
1 & 0
\end{array}\right)\left(\begin{array}{c}
y_{t-1}\\
y_{t-2}
\end{array}\right)+\left(\begin{array}{c}
\epsilon_{t}\\
0
\end{array}\right)
$$ where the matrix $A$ here is (i.e. AR(2)):

$$A=\left(\begin{array}{cc}
\phi_{1} & \phi_{2}\\
1 & 0
\end{array}\right)
$$

## Time series - processes

### Autoregressive Processes

Repeating the argument for AR(1) processes

$$
\begin{align*}
\vec{y}_t & = (1-A L)^{-1} \vec{\epsilon}_t\\
  & = (\sum_{i=0}^\infty A^i\mathrm{L}^i)\vec{\epsilon}_t\\
\end{align*}
$$

And we only converge if $A^i\rightarrow0 \;\mathrm{as}\;i\rightarrow\infty$, i.e. all eigenvalues are \< 1.

## Time series - processes

### Autoregressive Processes

-   In general if an $n\times n$ matrix $A$ has $n$ distinct eigenvalues, then all eigenvalues must have magnitude $<1$ for $A^i\rightarrow0 \;\mathrm{as}\;i\rightarrow\infty$.
-   In this case we have $A=X\Lambda X^{-1}$ where the columns of $x$ are the eigenvectors of $A$ and $\Lambda$ is a diagonal matrix with the eigenvalues on the diagonal.
-   $A^i=X\Lambda^i X^{-1}$ so $A^i\rightarrow0 \;\mathrm{as}\;i\rightarrow\infty$

## Time series - processes

### Computing determinants

::: {style="font-size: m"}
**Definitions**

1.  **Minor**:
    -   The minor $M_{ij}$ of an element $a_{ij}$ in an $n \times n$ matrix $A$ is the determinant of the $(n-1) \times (n-1)$ matrix that results from removing the $i$-th row and $j$-th column from $A$.
2.  **Cofactor**:
    -   The cofactor $C_{ij}$ of an element $a_{ij}$ is given by: $C_{ij} = (-1)^{i+j} M_{ij}$
    -   The sign factor $(-1)^{i+j}$ alternates according to the position of the element in the matrix.

**Cofactor Expansion**

The determinant of an $n \times n$ matrix $A$ can be computed by expanding along any row or column. The cofactor expansion along the $i$-th row is given by:

$$
\det(A) = \sum_{j=1}^n a_{ij} C_{ij}
$$ Similarly, the cofactor expansion along the $j$-th column is: $\det(A) = \sum_{i=1}^n a_{ij} C_{ij}$
:::

## Time series - processes

### Computing determinants

The structure of the AR(1) vector process makes it easy to compute the polynomial for the determinant in therms of the coefficients of the process.

The roots of the polynomial can be computed using `polyroot`, passing in a vector of polynomial coefficients in increasing order. The magnitudes of the eigenvalues can using the base function `Mod`.

``` r
polyroot(c(1,.2,3,.6)) |> Mod()
```

## Time series - processes

### Moving Average Processes

::: {style="font-size: 80%"}
-   In an MA(q) process the present value is a weighted sum on the current and previous errors. $$y_t= \epsilon_t +\theta_1\epsilon_{t-1};\;\text(MA(1))$$
-   MA(q) models describe processes where it takes a bit of time for the error (or "shock") to dissipate
-   This type of expression may be used to describe a wide variety of stationary time series processes
:::

## Time series - processes

### Moving Average Processes

::: {style="font-size: x-large"}
-   There is a duality of AR() and MA() processes.

-   We have already seen how AR processes can be expressed as MA() processes:

$$
\vec{y}_t = (1-A L)^{-1} \vec{\epsilon}_t = (\sum_{i=0}^\infty A^i\mathrm{L}^i)\vec{\epsilon}_t\\
$$

and MA() processes can be expressed as AR processes by a similar argument.
:::

## Time series - processes

### ARMA Processes

::: {style="font-size: 80%"}
-   A combination of an AR(1) and a MA(1) process is termed an ARMA(1,1) observation. $$y_t=\phi y_{t-1}+\epsilon_t+\theta\epsilon_{t-1}$$
-   AN ARMA(p,q) process takes the form $$y_t=\sum_{j=1}^p\phi_jy_{t-j}+\epsilon_t+\sum_{i=1}^q\theta_i\epsilon_{t-i}$$
-   This model was popularized by Box & Jenkins, who developed a methodology that may be used to identify the terms that should be included in the model
:::

## Time series - processes

### Long memory & fractional differencing

### 

::: {style="font-size: 80%"}
-   Most AR(p), MA(q) and ARMA(p,q) processes are termed short-memory process because the coefficients in the representation are dominated by exponential decay
-   Long-memory (or persistent) time series are considered intermediate compromises between the short-memory models and integrated nonstationary processes
:::

## Time series - processes

### ARIMA

::: {style="font-size: 75%"}
-   AutoRegressive Integrated Moving Average combines three key concepts: autoregression (AR), differencing (I - for Integrated), and moving average (MA).
-   **AR (AutoRegressive)**: captures the relationship between an observation and lagged observations.
-   **I (Integrated)**: Differencing is used to make the time series stationary.
-   **MA (Moving Average)**: captures the relationship between an observation and a residual error from a moving average model.
-   An ARIMA model is denoted as ARIMA(p, d, q), where: - p is the number of lag observations in the model (AR part), - d is the number of times that the raw observations are differenced (I part), - q is the size of the moving average window (MA part).
:::

## Time series - exponential smoothing

Exponential smoothing or exponential moving average (EMA) is a rule of thumb technique for smoothing time series data using the exponential window function.

For a raw data series $\{x_t\}$ the output of the exponential smoothing process is $\{y_t\}$, where, for $0\le\alpha\le1$ and $t>0$

$$
\begin{align*}
y_0 & = x_0 \\
y_t & = \alpha x_t + (1-\alpha)y_{t-1}
\end{align*}
$$

$\alpha$ is called the **smoothing factor**.

## Time series - exponential smoothing

The time constant $\tau$ is the amount of time for the smoothed response of a unit step function to reach $1-1/e\approx 63.2\%$ of the original signal.

$$
\begin{align*}
\alpha & = 1-e^{-\Delta T/\tau}\\
\tau & = -\frac{\Delta T}{ln(1-\alpha)}
\end{align*}
$$

where $\Delta T$ is the time interval.

## Time series - exponential smoothing

We can also include a trend term

$$
\begin{align*}
y_0 & = x_0 \\
b_0 & = x_1 - x_0\\
\end{align*}
$$

and for $t>0$, $0\le\alpha\le1$ and $0\le\beta\le1$

$$
\begin{align*}
y_t & = \alpha x_t + (1-\alpha)(y_{t-1} + b_{t-1})\\
b_t & = \beta(y_t-y_{t-1}) + (1-\beta)b_{t-1}
\end{align*}
$$

## Time series - exponential smoothing

Finally we can also include a seasonality term, with a cycle length of $K$ time intervals.

and for $t>0$, $0\le\alpha\le1$, $0\le\beta\le1$ and $0\le\gamma\le1$

$$
\begin{align*}
y_t & = \alpha \frac{x_t}{c_{t-L}} + (1-\alpha)(y_{t-1} + b_{t-1})\\
b_t & = \beta(y_t-y_{t-1}) + (1-\beta)b_{t-1}\\
c_t & = \gamma\frac{x_t}{y_t} + (1-\gamma)c_{t-L}
\end{align*}
$$

## Time series - ETS models

::: {style="font-size: 80%"}
ETS stands for Error-Trend-Seasonality, and the exponential smoothing model is clearly in this class, with each component additive. The more general taxonomy is:

-   **Error**: "Additive" (A), or "Multiplicative" (M);
-   **Trend**: "None" (N), "Additive" (A), "Additive damped" (Ad), "Multiplicative" (M), or "Multiplicative damped" (Md);
-   **Seasonality**: "None" (N), or "Additive" (A), or "Multiplicative" (M).

In this taxonomy, the exponential smoothing model is denoted as ETS(A,A,A).
:::

## Time series - ETS models

The additive ETS models are shown below, and more detailed discussion can be found [here](https://openforecast.org/adam/ETSConventional.html)

![](images/ETSTaxonomyAdditive-1.png){fig-align="center"}

# The Timetk package

## Time series - plotting

```{r}
#| echo: true
timetk::bike_sharing_daily %>% 
  dplyr::slice_head(n=5) %>% 
  dplyr::glimpse()
```

## Time series - plotting

The `timetk::plot_time_series()` function is a good way to to get a quick timeseries plot. From a tidy table we

-   select the time value and the columns we want to plot
-   pivot (longer) the columns we want to plot
-   plot

The `timetk::plot_time_series()` function has many options that can be changed.

## Time series - plotting

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "1|2|3|4-8"
timetk::bike_sharing_daily %>% 
  dplyr::select(dteday, casual, registered) %>% 
  tidyr::pivot_longer(-dteday) %>% 
  timetk::plot_time_series(
    .date_var = dteday
    , .value = value
    , .color_var = name
  )
```

## Time series - `timetk::`

### time downscaling

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "1|2|3-4|5|6-8"
timetk::bike_sharing_daily %>% 
  timetk::summarise_by_time(
    .date_var = dteday
    , .by = "week"
    , .week_start = 7
    , causal = sum(casual)
    , registered = mean(registered)
    , max_cnt = max(cnt)
  )
  
```

## Time series - `timetk::`

### time upscaling

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "1|2|3|4-8"
timetk::bike_sharing_daily %>% 
  dplyr::select(dteday, casual) %>% 
  timetk::pad_by_time(.date_var = dteday, .by = "hour") %>% 
  timetk::mutate_by_time(
    .date_var = dteday
    , .by = "day"
    , casual = sum(casual,na.rm=T)/24
  )
```

## Time series - `timetk::`

### time filtering

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "1|2-6"
timetk::bike_sharing_daily %>%
  timetk::filter_by_time(
    .date_var = dteday
    , .start_date="2012-01-15"
    , .end_date = "2012-07-01"
  ) %>% 
  timetk::plot_time_series(.date_var = dteday, casual)
```

## Time series - `timetk::`

### time offsets

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "1|2|3-7"
require(timetk, quietly = FALSE)
timetk::bike_sharing_daily %>%
  timetk::filter_by_time(
    .date_var = dteday
    , .start_date="2012-01-15"
    , .end_date = "2012-01-15" %+time% "12 weeks"
  ) %>% 
  timetk::plot_time_series(.date_var = dteday, casual)
```

## Time series - `timetk::`

### mutate by period

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "1|2|3-10"
timetk::bike_sharing_daily %>%
  dplyr::select(dteday, casual) %>% 
  timetk::mutate_by_time(
    .date_var = dteday
    , .by = "7 days"
    , casual_mean = mean(casual)
    , casual_median = median(casual)
    , casual_max = max(casual)
    , casual_min = min(casual)
  )
```

## Time series - `timetk::`

### summarize by period

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "1|2-8"
timetk::bike_sharing_daily %>%
  timetk::summarize_by_time(
    .date_var = dteday
    , .by = "7 days"
    , casual_mean = mean(casual)
    , registered_mean = mean(registered)
    , windspeed_max = max(windspeed)
  )
```

## Time series - `timetk::`

### create a timeseries

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "1|2-7|8"
tibble::tibble(
  date = 
    timetk::tk_make_timeseries(
      start_date = "2024"
      , length_out = 100
      , by = "month"
    )
  , values=1:100
)
```

## Time series - `timetk::`

### create a timeseries

```{r}
#| echo: true
#| code-fold: true
timetk::tk_make_holiday_sequence(
  start_date = "2024"
  , end_date = "2026"
  , calendar = "TSX"
) %>% 
  timetk::tk_get_holiday_signature(holiday_pattern = "Thanksgiving",locale_set = "CA", exchange = "TSX") %>% 
  dplyr::slice_head(n = 6) %>% 
  dplyr::glimpse()
```

## Time series - `timetk::`

### timeseries transformations

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "1|2-7|8"
#| layout-ncol: 2
# plot wind speed
timetk::bike_sharing_daily %>% 
  timetk::plot_time_series(dteday, windspeed, .title = "Time Series - Raw")

# plot transformed speed
timetk::bike_sharing_daily %>% 
  timetk::plot_time_series(
    dteday
    , timetk::box_cox_vec(windspeed, lambda="auto",  silent = T)
    , .title = "Time Series - Box Cox Tranformed")
```

## Time series - `timetk::`

### timeseries transformations

### See Also

::: {style="font-size: 30px"}
-   Lag Transformation: `lag_vec()`

-   Differencing Transformation: `diff_vec()`

-   Rolling Window Transformation: `slidify_vec()`

-   Loess Smoothing Transformation: `smooth_vec()`

-   Fourier Series: `fourier_vec()`

-   Missing Value Imputation for Time Series: `ts_impute_vec()`, `ts_clean_vec()`

Other common transformations to reduce variance: `log()`, `log1p()` and `sqrt()`
:::

## Time series - Feature engineering

```{r}
#| echo: true
#| code-fold: true
#| layout-ncol: 2
#| code-line-numbers: "3-5|6-8|12-16"
#| message: false
subscribers_tbl   <- readRDS("data/00_data/mailchimp_users.rds")

data_prepared_tbl <- subscribers_tbl %>% 
  timetk::summarize_by_time(optin_time, .by="day", optins=dplyr::n()) %>% 
  timetk::pad_by_time(.pad_value=0) %>% 
  # preprocessing
  dplyr::mutate(optins_trans=timetk::log_interval_vec(optins, limit_lower=0, offset=1)) %>% 
  dplyr::mutate(optins_trans=timetk::standardize_vec(optins_trans)) %>% 
  # fix missing vals at start
  timetk::filter_by_time(.start_date = "2018-07-03") %>% 
  # outliers clean
  dplyr::mutate(optins_trans_cleaned = timetk::ts_clean_vec(optins_trans, period=7)) %>% 
  dplyr::mutate(optins_trans=ifelse(optin_time %>% timetk::between_time("2018-11-18","2018-11-20")
                             , optins_trans_cleaned
                             , optins_trans
                             )) %>% 
  dplyr::select(-optins, -optins_trans_cleaned)
  
data_prepared_tbl     # show the dt

data_prepared_tbl %>% # plot the table
  tidyr::pivot_longer(contains("trans")) %>% 
  timetk::plot_time_series(optin_time,value,name) 
```

## Date Features

```{r}
#| echo: true
#| code-fold: true
data_prep_signature_tbl <- 
  data_prepared_tbl %>% 
  timetk::tk_augment_timeseries_signature(
    .date_var = optin_time
  ) 
```

```{r}
#| echo: false
data_prep_signature_tbl %>% 
  dplyr::slice_head(n=4) %>% 
  dplyr::glimpse()
```

## Trend

```{r}
#| echo: true
#| code-fold: true
data_prep_signature_tbl %>% 
  timetk::plot_time_series_regression(
    .date_var = optin_time
    , .formula = optins_trans ~ index.num
  )
```

## Non-linear trends

```{r}
#| echo: true
#| code-fold: true
#| layout-ncol: 2
data_prep_signature_tbl %>% 
  timetk::plot_time_series_regression(
    .date_var = optin_time
    , .formula = optins_trans ~ splines::bs(index.num, degree=3)
    , .show_summary = FALSE
    , .title = "B-spline, degree 3"
  )

data_prep_signature_tbl %>% 
  timetk::plot_time_series_regression(
    .date_var=optin_time
    , .formula=
      optins_trans ~ splines::ns(
        index.num
        , knots=quantile(index.num, probs=c(0.25, 0.5, 0.75)))
    , .show_summary = FALSE
    , .title = "Cubic spline, 3 knots"
  )

```

## Seasonality

```{r}
#| echo: true
#| code-fold: true
#| layout-ncol: 2
# Weekly Seasonality
data_prep_signature_tbl %>% 
  timetk::plot_time_series_regression(
    .date_var=optin_time
    , .formula=optins_trans ~ wday.lbl + splines::bs(index.num, degree=3)
    , .show_summary = FALSE
    , .title = "Weekday seasonality"
  )

# ** Monthly Seasonality
data_prep_signature_tbl %>% 
  timetk::plot_time_series_regression(
    .date_var=optin_time
    , .formula=optins_trans ~ month.lbl + splines::bs(index.num, degree=3)
    , .show_summary = FALSE
    , .title = "Monthly seasonality"
  )
```

## Seasonality

```{r}
#| echo: true
#| code-fold: true
# ** Together with Trend
model_formula_seasonality <- as.formula(
  optins_trans ~ wday.lbl + month.lbl +
    splines::ns(index.num
                , knots=quantile(index.num, probs=c(0.25, 0.5, 0.75))) + .
)
data_prep_signature_tbl %>% 
  timetk::plot_time_series_regression(
    .date_var=optin_time
    , .formula = model_formula_seasonality
    , .show_summary = FALSE
    , .title = "Day and Month seasonality + Cubic spline, 3 knots"
  )
```

## Fourier series

```{r}
#| echo: true
#| code-fold: true
data_prep_signature_tbl %>% 
  timetk::plot_acf_diagnostics(optin_time,optins_trans)
```

## Fourier series

```{r}
#| echo: true
#| code-fold: true
data_prep_fourier_tbl <- 
  data_prep_signature_tbl %>% 
  timetk::tk_augment_fourier(optin_time, .periods=c(7,14,30,90,365), .K=2)

data_prep_fourier_tbl %>% dplyr::slice_head(n=3) %>% dplyr::glimpse()
```

## Visualization

```{r}
#| echo: true
#| code-fold: true
# Model
model_formula_fourier <- 
  as.formula(
    optins_trans ~ . +
      splines::ns(index.num
                  , knots=quantile(index.num, probs=c(0.25, 0.5, 0.75)))
  )

# Visualize
data_prep_fourier_tbl %>% 
  timetk::filter_by_time(.start_date="2018-09-13") %>% 
  timetk::plot_time_series_regression(
    .date_var = optin_time
    , .formula = model_formula_fourier
    , .show_summary = FALSE
    , .title = "Fourier + Cubic spline, 3 knots"
  )
```

## Test-train splits

```{r}
#| echo: true
#| code-fold: true
#| message: false
dat <- subscribers_tbl %>% 
  timetk::summarize_by_time(optin_time, .by="day", optins=dplyr::n()) %>% 
  timetk::pad_by_time(.pad_value=0) %>% 
  timetk::filter_by_time(.start_date = "2018-12-15")

# Split Data 80/20
splits <- 
  timetk::time_series_split(
    data = dat
    , initial = "12 months"
    , assess = "1 months"
  )

splits %>%
  timetk::tk_time_series_cv_plan() %>%
  timetk::plot_time_series_cv_plan(.date_var = optin_time, .value = optins)
```

## Feature engineering w/ recipes

```{r}
#| echo: true
#| code-fold: true
#| message: false
time_rec <- dat %>% 
  recipes::recipe(optins ~ ., data = rsample::training(splits)) %>% 
  timetk::step_log_interval(optins, limit_lower = 0, offset = 1) %>% 
  recipes::step_normalize(recipes::all_outcomes()) %>% 
  timetk::step_timeseries_signature(optin_time) %>% 
  timetk::step_fourier(optin_time, period = c(7,14,30,90,365), K=2)

time_rec %>% recipes::prep(training = rsample::training(splits)) %>% 
  recipes::bake(new_data = NULL) %>% 
  timetk::plot_time_series_regression(
    .date_var = optin_time
    , .formula = optins ~ .
    , .show_summary = FALSE
  )
```

## Workflows

```{r}
#| echo: true
#| code-fold: false
#| message: false
model_spec_arima <- modeltime::arima_reg() %>%
    parsnip::set_engine("auto_arima")

recipe_spec_fourier <- 
  recipes::recipe(
    optins ~ optin_time
    , data = rsample::training(splits)
  ) %>%
    timetk::step_fourier(optin_time, period = c(7, 14, 30, 90), K = 1) 

workflow_fit_arima <- workflows::workflow() %>%
  workflows::add_recipe(recipe_spec_fourier) %>%
  workflows::add_model(model_spec_arima) %>%
  parsnip::fit(rsample::training(splits))


```

## Workflows

```{r}
#| echo: true
#| code-fold: false
#| message: false
model_spec_lm <- parsnip::linear_reg() %>%
  parsnip::set_engine("lm") 

recipe_spec_linear <- 
  recipes::recipe(
    optins ~ optin_time
    , data = rsample::training(splits)
  ) %>%
    timetk::step_fourier(optin_time, period = c(7, 14, 30, 90), K = 1) 

workflow_fit_linear <- workflows::workflow() %>%
  workflows::add_recipe(recipe_spec_linear) %>%
  workflows::add_model(model_spec_lm) %>%
  parsnip::fit(rsample::training(splits))
```

## Predict

```{r}
#| echo: true
#| code-fold: true
#| message: false
models_tbl <- 
  modeltime::modeltime_table(workflow_fit_arima, workflow_fit_linear)

calibration_tbl <- models_tbl %>%
  modeltime::modeltime_calibrate(new_data = rsample::testing(splits))

calibration_tbl %>%
  modeltime::modeltime_forecast(
    new_data    = rsample::testing(splits),
    actual_data = dat
  ) %>%
  modeltime::plot_modeltime_forecast(
    .legend_max_width = 25, # For mobile screens
    .interactive      = TRUE
  )
```

## Evaluate

```{r}
#| echo: true
#| code-fold: true
#| message: false
calibration_tbl %>%
  modeltime::modeltime_accuracy() %>%
  modeltime::table_modeltime_accuracy(
    .interactive = FALSE
  )
```

## Re-fit

```{r}
#| echo: true
#| code-fold: true
#| message: false
refit_tbl <- calibration_tbl %>%
  modeltime::modeltime_refit(data = dat)

refit_tbl %>%
  modeltime::modeltime_forecast(h = "3 months", actual_data = dat) %>%
  modeltime::plot_modeltime_forecast(
    .legend_max_width = 12, # For mobile screens
    .interactive      = TRUE
  )
```

## Recap

-   In this section we have worked with the `tidymodels` and `timetk` packages to build a workflow that facilitates building and evaluating multiple models.

-   Combined with the recipes package we now have a complete data modeling framework.
