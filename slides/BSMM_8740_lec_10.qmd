---
title: "Bayesian Methods"
subtitle: "BSMM8740-2-R-2024F [WEEK - 10]"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2024.github.io/osb](https://bsmm-8740-fall-2024.github.io/osb/)"
logo: "images/logo.png"
# title-slide-attributes:
#   data-background-image: images/my-DRAFT.png
#   data-background-size: contain
#   data-background-opacity: "0.40"
format: 
  revealjs: 
    chalkboard: true
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    margin: 0.05
    html-math-method: mathjax
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

{{< include 00-setup.qmd >}}

```{r}
#| message: false
#| warning: false
#| echo: false
# check if 'librarian' is installed and if not, install it
if (! "librarian" %in% rownames(installed.packages()) ){
  install.packages("librarian")
}
  
# load packages if not already loaded
librarian::shelf(
  tidyverse, broom, rsample, ggdag, causaldata, halfmoon, ggokabeito
  , magrittr, ggplot2, estimatr, Formula, gt, gtExtras,
  brms, tidybayesm, bayesplot, ggthemes, patchwork)

# set the default theme for plotting
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

## Recap of last week

-   Last week we introduced Markov Chain methods for integration and sampling from probability distributions.
-   We also built a basic understanding of the tools for sampling in Bayesian analysis.

## This week

-   We will explore Bayesian methods in more detail.
-   We will use one of the popular R packages for Bayesian analysis.

# Bayesian Regression

## Bayesian Regression

::: {style="font-size: x-large"}
From Bayes' Rule we have

$$
P(A \vert B) = \frac{P(B \vert A)P(A)}{P(B)} 
$$

If $B$ is interpreted as the data $\mathcal{D}$ and $A$ is chosen to be the set of parameters that you'd want to estimate, call this set $\theta$, then

$$
\underbrace{P(\theta \vert \mathcal{D})}_{\text{Posterior}} = 
    \frac{1}{\underbrace{P(\mathcal{D})}_{\text{Normalization}}}
\overbrace{P(\mathcal{D} \vert \theta)}^{\text{Likelihood}}\overbrace{P(\theta)}^{\text{Prior}}
$$
:::

## Bayesian Regression

::: {style="font-size: x-large"}
A statistical model $M$ is a model of a random process that could have generated our observable data. The observable data $\mathcal{D}$ contains both dependent variables $\mathcal{D}_\mathrm{DV}$ and independent variables $\mathcal{D}_\mathrm{IV}$ A model $M$ for data $\mathcal{D}$ fixes a likelihood function for $\mathcal{D}_\mathrm{DV}$. The likelihood function often has parameters, represented by a parameter vector $\theta$.

If $B$ is interpreted as the data $\mathcal{D}$ and $A$ is chosen to be the set of parameters that you'd want to estimate, call this set $\theta$, then

$$
\underbrace{P_M(\theta \vert \mathcal{D})}_{\text{Posterior}} = 
    \frac{1}{\underbrace{P(\mathcal{D},\theta)}_{\text{Normalization}}}
\overbrace{P_M(\mathcal{D}_\mathrm{DV} \vert\mathcal{D}_\mathrm{IV}, \theta)}^{\text{Likelihood}}\overbrace{P_M(\theta)}^{\text{Prior}}
$$
:::

## Bayesian Regression

#### Binomial Model

::: {style="font-size: x-large"}
The data we are interested in comes from a sequence of flips of a coin with bias $\theta_c\in [0,1]$.We have observed that $k$ of the $N$ flips turned out to be heads. We know $N$ and $k$, but we do not know $\theta_c$. We use the Binomial Model to infer the latent (= not directly observable) coin bias $\theta_c$

The coin’s bias $\theta_c$ is the only parameter of this model. The dependent variable is $k$. $N$ is another data observation (treated here as an independent variable.

The likelihood function for this model is the Binomial distribution:

$$
P_M(k \mid \theta_c, N) = \text{Binomial}(k, N, \theta_c) = \binom{N}{k}\theta_c^k(1-\theta_c)^{N-k}
$$ We use a Beta distribution for the prior of $\theta_c$. $P_M(\theta_c) = \text{Beta}(\theta_c, 1, 1)$
:::

## Bayesian Regression

#### Priors

::: {style="font-size: large"}
-   uninformative : $\theta_c\sim\mathrm{Beta}(1,1)$
-   weakly informative : $\theta_c\sim\mathrm{Beta}(5,2)$
-   strongly informative : $\theta_c\sim\mathrm{Beta}(50,20)$
-   point-valued : $\theta_c\sim\mathrm{Beta}(\alpha,\beta)$ with $\alpha,\beta\rightarrow\infty$ and $\alpha,\beta=52$
:::

![](images/ch-03-02-models-types-of-priors-1.png){fig-align="center" width="900"}

## Bayesian Regression

Predictions:

-   prior predictive distributions: $P_M(D_{\text{pred}})  = \sum_{\theta} P_M(\mathcal{D}_\mathrm{DV} \vert\mathcal{D}_\mathrm{IV}, \theta) \ P_M(\theta)$
-   posterior predictive distributions: $P_M(D_{\text{pred}})  = \sum_{\theta} P_M(\mathcal{D}_\mathrm{DV} \vert\mathcal{D}_\mathrm{IV}, \theta) \ P_M(\theta \vert \mathcal{D})$

## Maximum likelihood (MLE)

Recall that for a linear regression $y\sim \mathcal{N}(\beta x,\sigma^2)$ the likelihood of any one observation $y_i$ is (with $\theta$ representing the set of parameters)

$$
\pi\left(\left.y_{i}\right|x_{i},\beta,\sigma^{2}\right)=\pi\left(\left.y_{i}\right|x_{i},\theta\right)=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{(y_{i}-\beta x_{i})^{2}}{2\sigma^{2}}}
$$ and the log-likelihood of $N$ observations $\{y_i\}_{i=1}^N$ is

$$
\log\prod_{i=1}^{N}\pi\left(\left.y_{i}\right|x_{i},\theta\right) = \sum_{i=1}^{N}\log \pi\left(\left.y_{i}\right|x_{i},\theta\right)
$$

## Maximum likelihood (MLE)

The maximum likelihood estimate of $\beta$ is

$$
\hat{\theta}_{\text{MLE}}=\arg\max_{\theta} -\sum_{i=1}^{N}\log \pi\left(\left.y_{i}\right|x_{i},\theta\right)
$$

$$
\log\prod_{i=1}^{N}\pi\left(\left.y_{i}\right|x_{i},\theta\right) = \sum_{i=1}^{N}\log \pi\left(\left.y_{i}\right|x_{i},\theta\right)
$$

this is equivalent to minimizing the sum of the squared errors, and is also called the [**a priori**]{.underline} estimate.

## Bayesian model

The Bayesian model for linear regression is (to within a scaling constant)

$$
\begin{align*}
\pi_{\theta\vert\mathcal{D}}\left(\left.\theta\right|y_i,x_i\right)\sim\pi_{\mathcal{D}}\left(\left.y_i\right|x_i,\theta\right)\times\pi_\theta\left(\theta\right)
\end{align*}
$$

where the parameters are $\theta=\{\beta,\sigma^2\}$.

::: {style="font-size: 70%"}
In words: the joint probability of the parameters given the observed volume data is equal to (to within a scaling constant) the probability of the observed volume data given the parameters, times the prior probabilities of the parameters. In practice we refer to the probabilities as likelihoods, and use log-likelihoods to avoid numerical problems arising from the product of small probabilities.
:::

## Maximum a posteriori estimate (MAPE)

The maximum a posteriori estimate of the parameters is

$$
\begin{align*}
\hat{\theta}_{\text{MAP}} & =\arg\max_{\theta}\log\prod_{i=1}^{N}\pi_{\theta\vert\mathcal{D}}\left(\left. \theta \right|y_{i},x_{i}\right)\\
 & =\arg\max_{\theta}\sum_{i=1}^{N} \left(\log \pi\left(\left.y_{i}\right|x_{i},\theta\right)+\log\pi_\theta\left(\theta\right)\right)\\
 & =\arg\min_{\theta}-\sum_{i=1}^{N} \left(\log \pi\left(\left.y_{i}\right|x_{i},\theta\right)+\log\pi_\theta\left(\theta\right)\right)
\end{align*}
$$

## Maximum a posteriori estimate (MAPE)

::: {style="font-size: x-large"}
If $\theta$ is not uncertain/random, then $\pi(\theta)=1\rightarrow\log\pi(\theta)=0$ and the MAPE is equal to the MLE.

In linear regression we assume $\pi(\sigma^2) = 1$, so If $\theta$ is uncertain/random it remains to give a prior distribution to $\beta$ (as a vector of dimension $D$, in general). Assume $\beta=\mathscr{N}\left(0,\lambda^{-1}I\right)$ (with a single scale constant $\lambda$), then

$$
\pi_\theta(\beta) =  \frac{1}{\sqrt{(2\pi)^D \frac{1}{\lambda^D}}}exp(-\frac{1}{2}(\beta - 0)^\top (\frac{1}{\lambda} I)^{-1} (\beta - 0)) =
\frac{\lambda^{\frac{D}{2}}}{(2\pi)^{\frac{D}{2}}}exp(-\frac{\lambda}{2} \beta^\top \beta)
$$
:::

## Maximum a posteriori estimate (MAPE)

::: {style="font-size: x-large"}
With Gaussian $\pi(\beta)$ as in the last slide, and likelihood

$$
\pi(y_i \vert x_i, \beta) = \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{1}{2\sigma^2}(y_i- x_i^\top\beta)^2)
$$

we have, for linear regression

$$
\begin{align*}
\hat{\theta}_{\text{MAP}} & =\arg\min_{\theta}-\sum_{i=1}^{N}\left(\log\pi\left(\left.y_{i}\right|x_{i},\theta\right)+\log\pi_\theta\left(\theta\right)\right)\\
 & =\arg\min_{\theta}\left(\frac{1}{2\sigma^{2}}\sum_{i=1}^{N}(y_i-x_i^{T}\beta)^{2}+\frac{\lambda}{2}\beta^\top\beta\right)
\end{align*}
$$

which turns out to be a linear interpolation between the prior mean and the sample mean weighted by their respective covariances.
:::

## Maximum a posteriori estimate (MAPE)

In this MAPE for linear regression, with Gaussian priors, the posterior is also a Gaussian, since the product of Gaussian distributions is proportional to a Gaussian distribution, and the denominator in Bayes rule reflects the proportionality.

However, this a special case where the likelihood and prior distributions are [**conjugate**]{.underline}.

## Conjugate priors

If the posterior distribution $\pi_{\theta\vert\mathcal{D}}$ is in the same probability distribution family as the prior probability distribution $\pi_\theta$ (generally this means they are the same to within a normalizing constant), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function $\pi_{\mathcal{D}}$.

A conjugate prior is an algebraic convenience, giving a closed-form expression for the posterior.

## Conjugate priors

#### example 1

Consider a random variable which consists of the number of successes $s$ in $n$ Bernoulli trials with unknown probability of success $p\in[0,1]$. This random variable will follow the binomial distribution, with a probability mass function of the form

$$
\pi(s\vert p)={n \choose s}p^s(1-p)^{n-s}
$$

## Conjugate priors

#### example 1

The usual conjugate prior for the Bernoulli is the beta distribution with parameters ($\alpha, \beta$):

$$
\pi_\theta(p;\alpha,\beta) = \frac{p^{\alpha-1}(1-p)^{\beta-1}}{\mathrm{B}(\alpha,\beta)}
$$

where $\alpha$ and $\beta$ are chosen to reflect any existing belief or information ($\alpha = 1$ and $\beta = 1$ would give a uniform distribution) and $\mathrm{B}(\alpha,\beta)$ is the Beta function acting as a normalising constant.

## Conjugate priors

#### example 1

If we sample this random variable and get $s'$ successes and $f=n-s'$ failures, then we have

$$
\begin{align*}
\pi_{\theta\vert\mathcal{D}}(p=x) & \sim x^{s'}(1-x)^{n-s'}\times x^{\alpha-1}(1-x)^{\beta-1}\\
 & \sim x^{s'+\alpha+1}(1-x)^{(n-s')+\beta-1}\\
 & \sim\pi_{\theta\vert\mathcal{D}}(x;s'+\alpha+1,(n-s')+\beta-1)
\end{align*}
$$

And the posterior distribution $\pi_{\theta\vert\mathcal{D}}$ is in the same probability distribution family as the prior probability distribution $\pi_\theta$.

## Conjugate priors

#### example 2

::: {style="font-size: x-large"}
Suppose you've been asked to find the probability that you have exactly 5 outages at your website during any hour of the day. Your client has limited data, in fact they have just three data points $y=[3,4,1]$

If you assume that the data are generated by a Poisson distribution (which has a single parameter, the rate $\lambda$), then the maximum likelihood estimate of $\lambda$ is $\lambda=\frac{3+4+1}{3}\approx 2.67$, and you would estimate the probability as:

$$
\pi(n=5\vert\lambda\approx 2.67) = \frac{\lambda^n e^{-\lambda}}{n!}=\frac{2.67^5 e^{-2.67}}{5!}=0.078
$$
:::

## Conjugate priors

#### example 2

::: {style="font-size: x-large"}
We've assumed that the observed data $y$ is most likely to have been generated by a Poisson distribution with MLE for $\lambda= 2.67$.

But the data could also have come from another Poisson distribution, e.g., one with $\lambda =3$, or $\lambda =2$, etc. In fact, there is an infinite number of Poisson distributions that could have generated the observed data.

With relatively few data points, we should be quite uncertain about which exact Poisson distribution generated this data. Intuitively we should instead take a weighted average of the probability of $\pi(y\ge 0|\lambda )$ for each of those Poisson distributions, weighted by how likely they each are, given the data we've observed.

This is exactly what Bayes' Rule does.
:::

## Conjugate priors

#### example 2

::: {style="font-size: x-large"}
Luckily, the Poisson distribution has a conjugate, the Gamma distribution:

$$
\pi_\theta(x\lambda;\alpha,\beta)=\frac{x^{\alpha-1}e^{-\beta \lambda}\beta^\alpha}{\Gamma(\alpha)}
$$

and

$$
\begin{align*}
\pi\left(y\vert\lambda\right) & =\prod_{i=1}^{n}\frac{\lambda^{y_{i}}e^{-\lambda}}{y_{i}!}\\
 & =\lambda^{n\bar{y}}e^{-n\lambda}\prod_{i=1}^{n}\frac{1}{y_{i}!}
\end{align*}
$$

so $\pi\left(y\vert\lambda\right)\times \pi_\theta(x;\alpha,\beta)\sim \lambda^{n\bar{y}+\alpha-1}e^{-(n+\beta)\lambda}\sim\pi_\theta(\lambda;n\bar{y}+\alpha,(n+\beta))$
:::

## Conjugate priors

#### example 2

::::::: columns
:::: {.column width="60%"}
::: {style="font-size: large"}
Given our observations $\lambda=\frac{3+4+1}{3}\approx 2.67$, we might arbitrarily take the prior as a Gamma with $\alpha=9;\;\beta = 2$ so that the prior and posterior look like this:

```{r}
#| echo: true
#| code-fold: true
#| fig-height: 5
#| fig-width: 9
#| fig-aling: center
#| 
.shape <- 9 + 3*2.67  ; .rate <- 3+2

tibble::tibble(lambda = seq(0.04,15,0.02), plambda = dgamma(seq(0.04,15,0.02), shape=9, rate = 2), measure = "prior") |> 
  dplyr::bind_rows(
    tibble::tibble(lambda = seq(0.04,15,0.02), plambda = dgamma(seq(0.04,15,0.02), shape=.shape, rate = .rate), measure = "posterior")
  ) |> 
  ggplot(aes(x=lambda, y = plambda, color = measure)) + geom_line() + 
  labs(title = "Probability distributions for Lambda", subtitle = " prior and posterior predictive") +
  theme(legend.position = "right")
```
:::
::::

:::: {.column width="40%"}
::: {style="font-size: large"}
```{r}
#| echo: true
#| code-fold: true
.shape <- 9 + 3*2.67  ; .rate <- 3+2
ci <- qgamma(c(0.05,0.95), shape=.shape, rate = .rate) |> 
  purrr::map_dbl(function(x){round( x^5 * exp(-x)/factorial(5),digits=3)})
```

Given the posterior hyperparameters, we can finally compute the posterior predictive distribution and estimate the 90% confidence intervals for the probability as `{r} as.character(ci)`. This much more conservative estimate reflects the uncertainty in the model parameters, which the posterior predictive takes into account.
:::
::::
:::::::

## Conjugate priors

-   Conjugate priors offer ease of computation, efficiency in updates, and clear interpretation, making them suitable for simpler models or real-time applications.

-   However, they are often too restrictive for complex or non-standard models, where flexibility in capturing prior beliefs is crucial.

## Conjugate priors

::: {style="font-size: large"}
Limitations of Conjugate Priors

-   Restrictive Choice of Priors: Conjugate priors limit the choice of prior distributions to a specific family. This restricts flexibility, especially if real-world data suggests a prior belief outside the conjugate family, which may not accurately capture prior knowledge or uncertainty.
-   Lack of Flexibility with Complex Models: Conjugate priors are often insufficient for complex models, such as hierarchical or multi-level models, where dependencies between variables require more flexible priors. Non-conjugate priors, despite being more computationally intensive, can better accommodate the complexity of these models.
-   Potential for Over-Simplification: Choosing a conjugate prior for convenience can sometimes lead to oversimplification, especially if it does not match the true prior knowledge. This can introduce bias and reduce the model’s accuracy in reflecting genuine prior beliefs.
-   Less Suitable for Non-Standard Likelihoods: Conjugate priors work best with specific likelihood functions, and many real-world problems don’t have standard likelihoods that match conjugate prior forms. In these cases, using a conjugate prior may be infeasible, forcing the use of non-conjugate methods.
:::

## Generative (bayesian) modelling

-   Generative Bayesian modeling is an approach in Bayesian statistics where we create models that describe how data is generated, often by specifying probability distributions for both observed and latent (unobserved) variables.

-   This process involves defining a generative process — a step-by-step probabilistic framework that models how data could have arisen.

## Generative (bayesian) modelling

Note the similarity to DAGs:

Given a DAG, we next assign probability distributions to each node, relate the nodes through the parameters of the distributions and finally assign priors for any remaining/undetermined parameters, including parameters used to define the relationships between variables.

## Generative (bayesian) modelling

::: {style="font-size: x-large"}
**Key Components of Generative Bayesian Modeling**

-   Defining Priors: Start by assigning prior distributions to parameters, reflecting prior beliefs about these parameters before observing data. Priors incorporate domain knowledge and regularize the model.
-   Likelihood Function: Specify the likelihood, which represents the probability of observing the data given the parameters. It describes how data is assumed to be generated given specific values of the model parameters.
-   Posterior Inference: Using Bayes’ theorem, combine priors and the likelihood to calculate the posterior distribution of the parameters. This posterior reflects updated beliefs after seeing the data.
-   Latent Variables and Hierarchies: Generative models can include latent variables, which represent unobserved or hidden factors, and hierarchical structures, which model data with multiple levels of variation (e.g., nested or grouped data).
:::

## Generative (bayesian) modelling

::: {style="font-size: x-large"}
**Advantages of Generative Bayesian Models**

-   Interpretability: Generative models explicitly describe the data-generating process, making them interpretable and suitable for understanding complex systems.
-   Predictive Power: By learning the underlying structure of the data, generative models can predict unseen outcomes and infer missing or latent data.
-   Uncertainty Quantification: Bayesian models naturally quantify uncertainty in parameter estimates and predictions, enhancing decision-making with probabilistic insights.
:::

## Generative (bayesian) modelling

::: {style="font-size: x-large"}
**Limitations**

-   Computational Complexity: Generative Bayesian models, especially those with complex hierarchical structures or latent variables, can be computationally demanding, often requiring methods like MCMC.
-   Model Specification: The accuracy of generative Bayesian models heavily depends on correctly specifying the generative process, which can be challenging with limited domain knowledge or complex data.

**Applications**

Generative Bayesian modeling is widely used in areas requiring a deep understanding of data-generating processes, such as healthcare, natural language processing, finance, and other business applications. It enables tasks like anomaly detection, missing data imputation, and causal inference by modeling the probability structure of observed and unobserved variables.
:::

# BRMS

## Generative modelling with BRMS

`BRMS` (Bayesian Regression Models using Stan) is an R package for fitting complex Bayesian regression models using Stan, a powerful probabilistic programming language. BRMS provides a high-level, formula-based interface in R, making it easy to specify and fit Bayesian models.

`BRMS` is useful for performing complex Bayesian analyses in R without diving into raw Stan code.

## Generative modelling with BRMS

#### Basic model: manufacturing failures per $N$ units

:::::: {style="font-size: xx-large"}
::::: columns
::: {.column width="50%"}
We express the likelihood for our coin toss example as

$$y_{i} \sim \operatorname{Bernoulli}(\theta)$$

and our prior will be

$$\theta \sim \operatorname{Beta}(\alpha, \beta)$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-height: 9
dat <- readr::read_csv("data/z15N50.csv", show_col_types = FALSE)

dat |> 
  dplyr::mutate(y = y |> as.character()) |> 
  ggplot(aes(x = y)) +
  geom_bar() +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_minimal(base_size = 36)
```
:::
:::::
::::::

## Generative modelling with BRMS

#### Basic model: manufacturing failures per $N$ units

```{r}
#| echo: true
#| message: false
#| output: false
#| code-fold: true
#| code-summary: fit the model

fit8.1 <-
  brms::brm(data = dat, 
      family = brms::bernoulli(link = identity),
      formula = y ~ 1,
      brms::prior(beta(2, 2), class = Intercept, lb = 0, ub = 1),
      iter = 500 + 3334, warmup = 500, chains = 3,
      seed = 8,
      file = "fits/fit08.01")
```

```{r}
#| fig-align: center
#| code-fold: true
#| code-summary: plot the chains
plot(fit8.1)
```

## Generative modelling with BRMS

#### Basic model: manufacturing failures per $N$ units

:::: {style="font-size: xx-large"}
::: panel-tabset
## fit

```{r}
#| echo: true
print(fit8.1)
```

## posterior summary

```{r}
#| echo: true
brms::posterior_summary(fit8.1, robust = T)
```
:::
::::

## Generative modelling with BRMS

#### Basic model: manufacturing failures per $N$ units

::: panel-tabset
## draws

```{r}
#| message: false
#| echo: true
#| output: true
#| code-fold: true
#| code-summary: extract the draws
draws <- brms::as_draws_df(fit8.1) 
draws
```

## density

```{r}
#| echo: true
#| message: false
#| fig-height: 5
#| fig-width: 6
#| fig-align: center
#| code-fold: true
#| code-summary: plot the draws by chain
draws |> 
  dplyr::mutate(chain = .chain) |> 
  bayesplot::mcmc_dens_overlay(pars = vars(b_Intercept)) 
```

## ACF

```{r}
#| echo: true
#| message: false
#| fig-height: 5
#| fig-width: 10
#| fig-align: center
#| code-fold: true
#| code-summary: plot acf by chain
draws |> 
  dplyr::mutate(chain = .chain) |> 
  bayesplot::mcmc_acf(pars = vars(b_Intercept), lags = 35) +
  theme_minimal()
```
:::

## Generative modelling with BRMS

#### Basic model: manufacturing failures per $N$ units

```{r}
#| echo: false
dat <- readr::read_csv("data/z6N8z2N7.csv", show_col_types = FALSE) |> 
  dplyr::mutate(
    s = 
      dplyr::case_when(s == "Reginald" ~ "Windsor", TRUE ~ "London")
  )
```

```{r}
dat |> 
  dplyr::mutate(y = y |> as.character()) |> 
  ggplot(aes(x = y, fill = s)) +
  geom_bar(show.legend = F) +
  ggthemes::scale_fill_colorblind() +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_minimal() +
  facet_wrap(~ s)
```

## Generative modelling with BRMS

#### Basic model: manufacturing failures per $N$ units

```{r}
#| echo: true
#| message: false
#| output: false
#| code-fold: true
#| code-summary: fit the two-site model
fit8.2 <-
  brms::brm(data = dat, 
      family = brms::bernoulli(identity),
      y ~ 0 + s,
      brms::prior(beta(2, 2), class = b, lb = 0, ub = 1),
      iter = 2000, warmup = 500, cores = 4, chains = 4,
      seed = 8,
      file = "fits/fit08.02")
```

```{r}
#| fig-align: center
#| code-fold: true
#| code-summary: plot chains for both sites
plot(fit8.2, widths = c(2, 3))
```

## Generative modelling with BRMS

#### Basic model: manufacturing failures per $N$ units

:::: {style="font-size: x-large"}
::: panel-tabset
## fit

```{r}
#| echo: true
summary(fit8.2)
```

## pairs

```{r}
#| echo: true
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| code-fold: true
pairs(fit8.2,
      off_diag_args = list(size = 1/3, alpha = 1/3))
```

## draws

```{r}
#| fig-align: center
#| fig-height: 4
#| fig-width: 9
#| echo: true
#| code-fold: true
draws <- brms::as_draws_df(fit8.2)

draws <-
  draws |> 
  dplyr::rename(theta_Windsor = b_sWindsor, theta_London  = b_sLondon) |> 
  dplyr::mutate(`theta_Windsor - theta_London` = theta_Windsor - theta_London)

long_draws <-
  draws |> 
  dplyr::select(starts_with("theta")) |> 
  tidyr::pivot_longer(everything()) |> 
  dplyr::mutate(name = factor(name, levels = c("theta_Windsor", "theta_London", "theta_Windsor - theta_London"))) 
  
long_draws |> 
  ggplot(aes(x = value, y = 0, fill = name)) +
  tidybayes::stat_histinterval(point_interval = tidybayes::mode_hdi, .width = .95,
                    slab_color = "white", outline_bars = T,
                    normalize = "panels") +
  scale_fill_manual(values = ggthemes::colorblind_pal()(8)[2:4], breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_minimal() +
  facet_wrap(~ name, scales = "free")
```

## table

```{r}
#| echo: true
#| code-fold: true
long_draws |> 
  dplyr::group_by(name) |> 
  tidybayes::mode_hdi()
```
:::
::::

## Generative modelling with BRMS

#### Basic model: manufacturing failures per $N$ units

:::: {style="font-size: x-large"}
::: panel-tabset
## fit

```{r}
#| output: false
#| warning: false
#| echo: true
#| label: fit8.3
#| code-fold: true
#| code-summary: separate theta estimates

fit8.3 <-
  brms::brm(data = dat, 
      family = brms::bernoulli(identity),
      y ~ 0 + s,
      prior =
        c(brms::prior(beta(2, 2), class = b, coef = sWindsor),
          brms::prior(beta(2, 2), class = b, coef = sLondon),
          # this just sets the lower and upper bounds
          brms::prior(beta(2, 2), class = b, lb = 0, ub = 1)),
      iter = 2000, warmup = 500, cores = 4, chains = 4,
      sample_prior = "only",
      seed = 8,
      file = "fits/fit08.03")
```

## plots

```{r}
#| echo: true
#| code-fold: true
#| code-summary: separate theta estimates
#| fig-height: 4
#| fig-width: 9
#| fig-align: center
draws <- brms::as_draws_df(fit8.3) |> 
  dplyr::select(starts_with("b_"))

# dat |> 
#   dplyr::group_by(s) |> 
#   dplyr::summarise(z = sum(y), N = dplyr::n()) |> 
#   dplyr::mutate(`z/N` = z / N)

levels <- c("theta_Windsor", "theta_London", "theta_Windsor - theta_London")
d_line <-
  tibble::tibble(value = c(.75, .286, .75 - .286),
         name  =  factor(c("theta_Windsor", "theta_London", "theta_Windsor - theta_London"), 
                         levels = levels))

draws |> 
  dplyr::rename(theta_Windsor = b_sWindsor,
         theta_London  = b_sLondon) |> 
  dplyr::mutate("theta_Windsor - theta_London" = theta_Windsor - theta_London) |> 
  tidyr::pivot_longer(contains("theta")) |> 
  dplyr::mutate(name = factor(name, levels = levels)) |>
  
  ggplot(aes(x = value, y = 0)) +
  tidybayes::stat_histinterval(point_interval = tidybayes::mode_hdi, .width = .95,
                    fill = ggthemes::colorblind_pal()(8)[5], normalize = "panels") +
  geom_vline(data = d_line, 
             aes(xintercept = value), 
             linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = expression("The dashed vertical lines mark off "*italic(z[s])/italic(N[s]))) +
  cowplot::theme_cowplot() +
  facet_wrap(~ name, scales = "free")
```

## independence

```{r}
#| echo: true
#| code-fold: true
#| code-summary: separate theta estimates
#| fig-height: 4
#| fig-width: 6
#| fig-align: center
draws |> 
  dplyr::rename(theta_Windsor = b_sWindsor,
         theta_London  = b_sLondon) |> 
  
  ggplot(aes(x = theta_Windsor, y = theta_London)) +
  geom_point(alpha = 1/4, color = ggthemes::colorblind_pal()(8)[6]) +
  coord_equal() +
  cowplot::theme_minimal_grid()

```
:::
::::

## Generative modelling with BRMS

::: {style="font-size: large"}
Recall from the last chapter that our likelihood is the Bernoulli distribution,

$$y_i \sim \operatorname{Bernoulli}(\theta).$$

We'll use the beta density for our prior distribution for $\theta$,

$$\theta \sim \operatorname{Beta}(\alpha, \beta).$$

And we can re-express $\alpha$ and $\beta$ in terms of the mode $\omega$ and concentration $\kappa$, such that

$$\alpha = \omega(\kappa - 2) + 1 \;\;\; \textrm{and} \;\;\; \beta = (1 - \omega)(\kappa - 2) + 1.$$

As a consequence, we can re-express $\theta$ as

$$\theta \sim \operatorname{Beta}(\omega(\kappa - 2) + 1, (1 - \omega)(\kappa - 2) + 1).$$

The value of $\kappa$ governs how near $\theta$ is to $\omega$, with larger values of $\kappa$ generating values of $\theta$ more concentrated near $\omega$.
:::

## Generative modelling with BRMS

::: {style="font-size: large"}
Using $s$ for shape and $r$ for rate, Kruschke's Equations 9.7 and 9.8 are as follows:

$$
\begin{align*}
s & =\frac{\mu^{2}}{\sigma^{2}}\;\;\;\text{and}\;\;\;r=\frac{\mu}{\sigma^{2}}\;\;\;\text{for mean}\;\;\;\mu>0\\
s & =1+\omega r\;\;\;\text{where}\;\;\;r=\frac{\omega+\sqrt{\omega^{2}+4\sigma^{2}}}{2\sigma^{2}}\;\;\;\text{for mode}\;\;\;\omega>0
\end{align*}
$$

```{r}
#| echo: true
#| label: re-parameterization functions
#| code-fold: true
#| code-summary: re-parameterization functions
gamma_s_and_r_from_mean_sd <- function(mean, sd) {
  if (mean <= 0) stop("mean must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  shape <- mean^2 / sd^2
  rate  <- mean   / sd^2
  return(list(shape = shape, rate = rate))
}

gamma_s_and_r_from_mode_sd <- function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate  <- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape <- 1 + mode * rate
  return(list(shape = shape, rate = rate))
}
```
:::

## Generative modelling with BRMS

#### multiple sites

:::: {style="font-size: x-large"}
::: panel-tabset
## data

```{r}
#| echo: true
dat <- readr::read_csv("data/TherapeuticTouchData.csv", show_col_types = FALSE)
```

## by site

```{r}
#| warning: false
#| code-fold: true
#| fig-align: center
#| fig-height: 4

dat |> 
  dplyr::mutate(y = y |> as.character()) |> 
  
  ggplot(aes(y = y)) +
  geom_bar(aes(fill = after_stat(count))) +
  scale_fill_viridis_c(option = "A", end = .7, breaks = NULL) +
  scale_x_continuous(breaks = 0:4 * 2, expand = c(0, NA), limits = c(0, 9)) +
  cowplot::theme_minimal_vgrid() +
  cowplot::panel_border() +
  facet_wrap(~ s, ncol = 7)
```

## overall

```{r}
#| warning: false
#| code-fold: true
#| fig-align: center
#| fig-height: 4
a_purple <- viridis::viridis_pal(option = "A")(9)[4]
dat |> 
  dplyr::group_by(s) |> 
  dplyr::summarize(mean = mean(y)) |>
  
  ggplot(aes(x = mean)) +
  geom_histogram(color = "white", fill = a_purple,
                 linewidth = .2, binwidth = .1) +
  scale_x_continuous("Proportion Not Failing", limits = c(0, 1)) +
  scale_y_continuous("# Practitioners", expand = c(0, NA)) +
  cowplot::theme_minimal_hgrid()
```
:::
::::

## Generative modelling with BRMS

#### fit model

::::: {style="font-size: x-large"}
:::: panel-tabset
## fit

::: {style="font-size: large"}
```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| message: false
#| label: fit9.1
fit9.1 <-
  brms::brm(data = dat,
      family = brms::bernoulli(link = logit),
      y ~ 1 + (1 | s),
      prior = c(brms::prior(normal(0, 1.5), class = Intercept),
                brms::prior(normal(0, 1), class = sd)),
      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4,
      seed = 9,
      file = "fits/fit09.01")
print(fit9.1)
```
:::

## plot

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| message: false
#| fig-height: 4
#| fig-cap: population parameter estimates (log-odds)
plot(fit9.1, widths = c(2, 3))
```

## acf

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| message: false
#| fig-height: 4
#| fig-cap: acf by chain; population parameters
#| fig-cap-location: margin
draws <- brms::as_draws_df(fit9.1)
draws |> 
  dplyr::mutate(chain = .chain) |> 
  bayesplot::mcmc_acf(pars = vars(b_Intercept, sd_s__Intercept), lags = 10) +
  cowplot::theme_cowplot()
```

## neff

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| message: false
#| fig-height: 4
#| fig-cap: effective samples by variable
#| fig-cap-location: margin
bayesplot::neff_ratio(fit9.1) |> 
  bayesplot::mcmc_neff() +
  cowplot::theme_cowplot(font_size = 12)
```

## draws

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| message: false
#| fig-height: 4
#| fig-cap: selected parameters and contrasts on probability scale
#| fig-cap-location: top
draws_small <-
  draws |> 
  # convert the linear model parameters to the probability space with `inv_logit_scaled()`
  dplyr::mutate(`theta[1]`  = (b_Intercept + `r_s[S01,Intercept]`) |> brms::inv_logit_scaled(),
         `theta[14]` = (b_Intercept + `r_s[S14,Intercept]`) |> brms::inv_logit_scaled(),
         `theta[28]` = (b_Intercept + `r_s[S28,Intercept]`) |> brms::inv_logit_scaled()) |> 
  # make the difference distributions
  dplyr::mutate(`theta[1] - theta[14]`  = `theta[1]`  - `theta[14]`,
         `theta[1] - theta[28]`  = `theta[1]`  - `theta[28]`,
         `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) |> 
  dplyr::select(starts_with("theta"))

draws_small |> 
  tidyr::pivot_longer(everything()) |> 
  # this line is unnecessary, but will help order the plots 
  dplyr::mutate(name = factor(name, levels = c("theta[1]", "theta[14]", "theta[28]", 
                                        "theta[1] - theta[14]", "theta[1] - theta[28]", "theta[14] - theta[28]"))) |> 

  ggplot(aes(x = value, y = 0)) +
  tidybayes::stat_histinterval(point_interval = ggdist::mode_hdi, .width = .95,
                    fill = a_purple, breaks = 40, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  cowplot::theme_minimal_hgrid() +
  facet_wrap(~ name, scales = "free", ncol = 3)
```

## draws table

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| message: false
draws_small |> 
  tidyr::pivot_longer(everything()) |>
  dplyr::group_by(name) |> 
  tidybayes::mode_hdi(value) |> 
  gt::gt("name") |> 
  gt::fmt_number(columns=value:.width, decimals=3) |> 
  gt::tab_header(title = "Parameter values and contrasts (logit, aka log-odds)", subtitle = "sites 1,14,28") |> 
  gtExtras::gt_theme_espn()
```

## pairs

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| message: false
#| fig-height: 4
#| fig-width: 4
#| fig-align: center
#| fig-cap: pairs plot on probability scale
#| fig-cap-location: top
color_scheme_set("purple")
bayesplot::bayesplot_theme_set(theme_default() + cowplot::theme_minimal_grid())

stats::coef(fit9.1, summary = F)$s |> 
  brms::inv_logit_scaled() |> 
  data.frame() |> 
  rename(`theta[1]`  = S01.Intercept, 
         `theta[14]` = S14.Intercept, 
         `theta[28]` = S28.Intercept) |> 
  dplyr::select(`theta[1]`, `theta[14]`, `theta[28]`) |> 
  bayesplot::mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8)) 
```
::::
:::::

## Generative modelling with BRMS

#### Shrinkage in hierarchical models

::: {style="font-size: large"}
"In typical hierarchical models, the estimates of low-level parameters are pulled closer together than they would be if there were not a higher-level distribution. This pulling together is called shrinkage of the estimates"

Further,

"shrinkage is a rational implication of hierarchical model structure, and is (usually) desired by the analyst because the shrunken parameter estimates are less affected by random sampling noise than estimates derived without hierarchical structure. Intuitively, shrinkage occurs because the estimate of each low-level parameter is influenced from two sources: (1) the subset of data that are directly dependent on the low-level parameter, and (2) the higher-level parameters on which the low-level parameter depends. The higher- level parameters are affected by all the data, and therefore the estimate of a low-level parameter is affected indirectly by all the data, via their influence on the higher-level parameters."[^1]
:::

[^1]: Doing Bayesian Data Analysis, J.K. Kruschke (2015)

## Generative modelling with BRMS

#### Shrinkage in hierarchical models

```{r}
#| echo: true
#| warning: false
#| code-fold: true
#| code-summary: multilevel shrinkage
#| message: false
#| fig-align: center
dat |> 
  group_by(s) |> 
  summarise(p = mean(y)) |> 
  mutate(theta = coef(fit9.1)$s[, 1, "Intercept"] |> inv_logit_scaled()) |> 
  pivot_longer(-s) |> 
  # add a little jitter to reduce the overplotting
  mutate(value = value + runif(n = n(), min = -0.02, max = 0.02),
         name  = if_else(name == "p", "italic(z/N)", "theta")) |> 

  ggplot(aes(x = value, y = name, group = s)) +
  geom_point(color = alpha(a_purple, 1/2)) +
  geom_line(linewidth = 1/3, alpha = 1/3) +
  scale_x_continuous(breaks = 0:5 / 5, expand = c(0.01, 0.01), limits = 0:1) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  labs(title = "Multilevel shrinkage in model",
       x = "data proportion or theta value") +
  cowplot::theme_minimal_hgrid() +
  cowplot::panel_border() 
```

## Generative modelling with BRMS

#### speeding up model fits

::: {style="font-size: x-large"}
Full Bayesian inference is a computationally very demanding task and often we wish to run our models faster in shorter walltime (elapsed real time). With modern computers we nowadays have multiple processors available on a given machine such that the use of running the inference in parallel will shorten the overall walltime.

While between-chain parallelization is straightforward by merely launching multiple chains at the same time, the use of within-chain parallelization is more complicated in various ways. [This vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_threading.html) aims to introduce the user to within-chain parallelization with brms, since its efficient use depends on various aspects specific to the users model.
:::

## Generative modelling with BRMS

#### BRMS example

:::::: {style="font-size: large"}
::::: columns
::: {.column width="40%"}
```{r}
#| echo: true
#| code-fold: true
#| message: false

# load data
data("epilepsy", package = "brms")

epilepsy
```
:::

::: {.column width="60%"}
```{r}
#| echo: true
#| code-fold: true
#| message: false

output <- 
  capture.output(
    fit_epi_gaussian1 <- 
      brms::brm(
        count ~ 1 + Trt, data = epilepsy, silent = 2, seed = 8740, file = "fits/fit_epi_gaussian1")
  )
  
fit_epi_gaussian1
```
:::
:::::
::::::

## Generative modelling with BRMS

::: {style="font-size: x-large"}
At the top of the output, we see the formula and dataset. Moving to the actual posterior results, brms shows two blocks of parameters here, namely *Regression Coefficients* and *Further Distributional Parameters*. The latter refers to additional parameters of the likelihood that we did not specify with our regression, i.e. the residual standard deviation (sigma).

For each of the displayed parameters, we see seven summary statistics. The Estimate column is simply the empirical mean of the posterior draws, which is a sampling-based estimate of the analytical posterior mean.

The Est.Error columns denotes (the empirical estimate of) the posterior standard deviation, while the lower and upper bounds of the 95% credible interval are computed as the 2.5% and 97.5% quantile of the posterior samples, respectively. The three remaining columns, Rhat, Bulk_ESS, and Tail_ESS give us information about convergence and sampling eﬀiciency, as also shortly explained at the bottom of the summary output.
:::

## Generative modelling with BRMS

::: {style="font-size: x-large"}
Here we see, for each parameter separately, traces of the four MCMC chains in different colors with post-warmup iterations on the x-axis and parameter values on the y-axis. These trace plots are showing ideal convergence: All chains are **overlaying each other nicely**, are **stationary** (horizontal on average), and show **little autocorrelation**.
:::

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| fig-align: center
#| fig-height: 4
brms::mcmc_plot(fit_epi_gaussian1, type = "trace")
```

## Generative modelling with BRMS

::: {style="font-size: x-large"}
Having convinced us of convergence, at least graphically for now, we can move on to inspecting the posteriors. e.g., we can plot histograms of the posterior samples per parameter.
:::

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| fig-align: center
#| fig-height: 4
brms::mcmc_plot(fit_epi_gaussian1, type = "hist", bins=30)
```

## Generative modelling with BRMS

::: {style="font-size: x-large"}
In R, we can easily perform data transformations by first extracting the posterior draws and then applying the transformation per draws in a vectorized manner. e.g., using the functionality from the posterior:: package

```{r}
#| echo: true
#| code-fold: true
draws <- 
  posterior::as_draws_df(fit_epi_gaussian1) |> 
  posterior::mutate_variables(
    variance = sigma^2, mu_Trt = b_Intercept + b_Trt1
  )

draws
```
:::

## Generative modelling with BRMS

::: {style="font-size: x-large"}
With mu_Trt we computed the model-implied predictions of the mean for the treatment group. For the control group, it would just be mu_Ctrl = $\beta_0$ for this simple model.
:::

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
bayesplot::mcmc_hist(draws, c("variance", "mu_Trt"), bins = 30)
```

## Generative modelling with BRMS

::: {style="font-size: x-large"}
For more complex models, computing the predictions for different groups or, more generally, different predictor values manually becomes quite cumbersome. For this reason brms provides you with a convenient method to provide quick graphical summarizes of the model-implied predictions per predictor:
:::

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
ce <- brms::conditional_effects(fit_epi_gaussian1)
plot(ce)
```

## Generative modelling with BRMS

::: {style="font-size: x-large"}
The error bars in the last slide are representing 95% credible intervals by default, but we can change that value if we like via the `prob` argument. Comparing the conditional_effects plot with our manually computed posterior of mu_Trt, we see that they actually are the same.

To put the mean predictions into the context of the observed data, we can also show the data as points in the plot:
:::

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
plot(ce, points = TRUE)
```

## Generative modelling with BRMS

::: {style="font-size: large"}
What we do in `conditional_effects` by default, and have done above, is visualizing the expected value (mean parameter) of the likelihood distribution, conditional on certain predictor values. In brms, this is done via the `posterior_epred` (posterior expected predictions) method. For example, we can run the code below to create expected posterior predictions for both groups. The resulting object contains the posterior draws in the rows and the different conditions (here, treatment groups) in the columns.

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
newdata <- data.frame(Trt = c(0, 1))
pe <- brms::posterior_epred(fit_epi_gaussian1, newdata = newdata)
```

We also can summarize the draws, for example, via

```{r}
#| echo: true
brms::posterior_summary(pe)
```

In linear models, `posterior_epred` directly coincides with evaluating the linear predictor $\mu$ as exemplified above. What `posterior_epred` does not include is the residual uncertainty, which is represented by $\sigma$ in our linear models.
:::

## Generative modelling with BRMS

::: {style="font-size: x-large"}
Consider again the task of evaluating predictions for the treatment group. If we are only interested in (the posterior of) the likelihood’s mean parameter, we would compute $\mu^{(s)}_{\mathrm{Trt}}=\beta_0^{(s)} + \beta_1^{(s)}$.

In contrast, if we are interested in prediction of hypothetical new data points $y^{(s)}_{\mathrm{Trt}}$ from the treatment group (i.e., actual posterior predictions), we would sample

$$
y^{(s)}_{\mathrm{Trt}}\sim\mathscr{N}\left(\mu^{(s)}_{\mathrm{Trt}}, \sigma^{(s)} \right)
$$

This is exactly what happens behind the scenes when we execute the code below:
:::

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
options(brms.plot_points = TRUE)
brms::conditional_effects(fit_epi_gaussian1, method = "posterior_predict")
```

## Generative modelling with BRMS

::: {style="font-size: x-large"}
We could have also done this more manually via
:::

```{r}
#| echo: true
#| code-fold: true
newdata <- data.frame(Trt = c(0, 1))
pp <- brms::posterior_predict(fit_epi_gaussian1, newdata = newdata)

brms::posterior_summary(pp)
```

## Generative modelling with BRMS

::: {style="font-size: x-large"}
We are already aware that linear regression model is not ideal for the epilepsy data. But how bad is it? As quick graphical method, we can use posterior predictive (PP) checks, where we compare the observed outcome data with the model predicted outcome data, that is, with the posterior predictions. In brms, we can perform PP-checks via:
:::

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| message: false
brms::pp_check(fit_epi_gaussian1)
```

## Generative modelling with BRMS

::: {style="font-size: x-large"}
We see that the model predictions can neither account for the strong spike of observed outcomes close to zero nor for their right-skewness. Instead, the the model also predicts a lot of negative outcomes, which is impossible in reality because we are predicting counts of epileptic seizures.

In the plot, it looks as if the observed data also had few negative values (the dark blue density going below zero) but this is just an artifact of estimating a continuous density from counts. While this PP-check type is definitely not ideal to illustrate count outcome data, it still very clearly points to the shortcomings of our linear model.
:::

## Generative modelling with BRMS

::: {style="font-size: x-large"}
While the default PP-check was already eye-opening, there are lot of types that can further our understanding of model appropriateness. For example, an often very useful check is obtained by comparing the residuals = observed outcomes - model predictions with the observed outcomes, also known as residual plot. In pp_check this check type is called error_scatter_avg:
:::

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| message: false

brms::pp_check(fit_epi_gaussian1, type = "error_scatter_avg")
```

## Generative modelling with BRMS

::: {style="font-size: x-large"}
Here there is a strongly almost perfectly linear relationship indicating strong problems with the independence assumption of the errors.

Essentially, both PP-checks have told us that our initial model is a very bad for the data at hand.

If you don’t know which check types are available, you can simply pass an arbitrary non-supported type name to get a list of all currently supported types:
:::

```{r}
#| echo: true
#| eval: false
brms::pp_check(fit_epi_gaussian1, type = "help_me")
```

## Generative modelling with BRMS

::::::: {style="font-size: x-large"}
:::::: columns
:::: {.column width="50%"}
::: {style="font-size: large"}
```{r}
#| echo: true
#| code-fold: true
#| message: false

output <- 
  capture.output(
    fit_epi_student1 <- 
      brms::brm(
        count ~ Trt * Base,
        data = epilepsy,
        family = brms::student()
        , silent = 2, seed = 8740, file = "fits/fit_epi_student1"
      )
  )


summary(fit_epi_student1)
```
:::
::::

::: {.column width="50%"}
```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| message: false
#| fig-height: 10
brms::pp_check(fit_epi_student1) + xlim(-30, 30)
```
:::
::::::
:::::::

## Generative modelling with BRMS

#### Absolute predictive performance

:::::: {style="font-size: x-large"}
::::: columns
::: {.column width="50%"}
```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-height: 7.5
#| fig-cap: Default posterior predictive check for model fit_epi_skew_normal1

output <- 
  capture.output(
    fit_epi_gaussian2 <- brms::brm(count ~ Trt + Base, data = epilepsy, silent = 2, seed = 8740, file = "fits/fit_epi_gaussian2")
  )
# find a spot for this
output <- 
  capture.output(
    fit_epi_gaussian3 <- brms::brm(count ~ Trt * Base, data = epilepsy, silent = 2, seed = 8740, file = "fits/fit_epi_gaussian3")
  )
brms::pp_check(fit_epi_gaussian2) + theme_minimal(base_size = 18) + theme(legend.position = "side")
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-height: 7.5
#| fig-cap: Posterior predictive check comparing observed responses (y-axis) with the predictive residuals (x-axis) of model fit_epi_gaussian2
brms::pp_check(fit_epi_gaussian2, type = "error_scatter_avg") + theme_minimal(base_size = 18)
```
:::
:::::
::::::

## Generative modelling with BRMS

#### Measures of explained variance: $R^2$

::: {style="font-size: x-large"}
The standard $R^2$ measure is defined only for Gaussian models and is often referred to as the percentage of explained variance. Note that in the Gaussian context we compute draws from the posterior $R^2$

```{r}
#| echo: true
#| code-fold: false
#| message: false
# compute draws of the predictive errors based on posterior_epred
errors <- brms::predictive_error(fit_epi_gaussian2, method = "posterior_epred")
str(errors)
```

```{r}
#| echo: true
#| code-fold: false
#| message: false
# sum errors over observations
error_variation <- rowSums(errors^2)
str(error_variation)
```

```{r}
#| echo: true
#| code-fold: false
#| message: false
# compute R2_basic
overall_variation <- sum((epilepsy$count - mean(epilepsy$count))^2)
R2_basic_epi_gaussian2 <- 1 - error_variation / overall_variation
brms::posterior_summary(R2_basic_epi_gaussian2)
```
:::

## Generative modelling with BRMS

#### Measures of explained variance: $R^2$

::: {style="font-size: large"}
The $R^2_{\mathrm{basic}}$is a good starting point, but it doesn’t generalize to models that are more complicated than Gaussian linear models. In particular it doesn’t readily generalize to most other likelihood families.

We'll use a more general form of $R^2$ that we can apply to (almost) all brms models, regardless of what likelihood families they have. The measure is based on the ratio of explained variance and the sum of explained and error variance:

$$
R^2_{\mathrm{general}} = \frac{\mathrm{Var}(\hat{y})}{\mathrm{Var}(\hat{y})+$\mathrm{Var}(\hat{e})}
$$

Where $\mathrm{Var}(\hat{y})$) is the variance of the posterior predicted mean over observations (again `posterior_epred`) and $\mathrm{Var}(\hat{e})$)) is the variance of the model-implied errors over observations, where $\hat{e}=y_n-\hat{y}_n$.

```{r}
#| echo: true
brms::bayes_R2(fit_epi_gaussian2)
```
:::

## Generative modelling with BRMS

#### Measures of Squared Errors

:::::: {style="font-size: x-large"}
$\mathrm{RMSE}_{\mathrm{basic}}$ computes a mean square error of observations $n$ for each posterior draw $s$, and thus yields a posterior distribution over RMSE values

::::: columns
::: {.column width="50%"}
```{r}
#| echo: true
# compute draws of the predictive errors based on posterior_epred
errors_epi_gaussian3 <-
brms::predictive_error(fit_epi_gaussian3, method = "posterior_epred")
str(errors_epi_gaussian3)
```

```{r}
#| echo: true
# root mean of squared errors over observations
rmse_basic_epi_gaussian3 <- sqrt(rowMeans(errors_epi_gaussian3^2))
str(rmse_basic_epi_gaussian3)
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-height: 6
#| fig-cap: Posterior histogram of RMSE_basic for model fit_epi_gaussian3.
lattice::histogram(rmse_basic_epi_gaussian3) 
```
:::
:::::
::::::

## Generative modelling with BRMS

#### Measures of Squared Errors

:::::: {style="font-size: large"}
We can also exchange the use of $n$ and $s$ and compute a mean square error over draws $s$ for each observation $n$:

::::: columns
::: {.column width="50%"}
```{r}
#| echo: true
rmse_alt_epi_gaussian3 <- 
  sqrt(colMeans(errors_epi_gaussian3^2))
str(rmse_alt_epi_gaussian3)
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-height: 4
#| fig-cap: Posterior histogram of RMSE_alt for model fit_epi_gaussian3.
lattice::histogram(rmse_alt_epi_gaussian3) 
```
:::
:::::

In this case, we get a distribution of RMSE over observations, where each individual RMSE value would be computed over the posterior predictive distribution of a single observation. Both of the above RMSE measures are fully Bayesian as they take into account the uncertainty in the posterior distribution, but in different ways.
::::::

## Generative modelling with BRMS

#### Measures of Squared Errors

::: {style="font-size: x-large"}
Typically see only a point estimate $\hat{\bar{y}}_n$ being used to represent the model-implied predictions, instead of a (posterior) distribution over such predictions for each $n$. For example, for a Bayesian model this point estimate could simply be the posterior mean.

When using such a point prediction approach, our RMSE definition becomes:

```{r}
#| echo: true
# extract a point estimate of the predictions per observation
ppmean_epi_gaussian3 <- colMeans(brms::posterior_epred(fit_epi_gaussian3))
str(ppmean_epi_gaussian3)

# compute RMSE based on the responses and point predictions
(rmse_point_epi_gaussian3 <- sqrt(mean((epilepsy$count - ppmean_epi_gaussian3)^2)) )
```
:::

## Generative modelling with BRMS

#### 2.6 Relative predictive performance

::: {style="font-size: x-large"}
In general, it is more common to compare multiple models against each other and thus investigating their relative predictive performance.

```{r}
#| echo: true
errors_epi_student1 <-
  brms::predictive_error(fit_epi_student1, method = "posterior_epred")
rmse_alt_epi_student1 <- sqrt(colMeans(errors_epi_student1^2))
```

We can now even compute the pointwise (per-observation) difference in RMSE values:

```{r}
#| echo: true
rmse_alt_diff <- rmse_alt_epi_student1 - rmse_alt_epi_gaussian3
str(rmse_alt_diff)

se_mean <- function(x) {sd(x) / sqrt(length(x))}

se_rmse_alt_diff <- se_mean(rmse_alt_diff)
se_rmse_alt_diff
```
:::

## Generative modelling with BRMS

#### 2.6.1 Likelihood Density Scores

::: {style="font-size: x-large"}
We have looked at variations of $R^2$ and RMSE metrics. Next we use the log-likelihood of models as predictive metric more generally.

The log-likelihood plays a pivotal role not only in to derive the posterior in Bayesian statistics but also to obtain maximum likelihood estimates in a frequentist framework. Intuitively, the higher the likelihood of the data given the model’s parameters estimates (represented as either posterior draws or point estimates), the better the fit of the model to the data. Many important predictive metrics, Bayesian or otherwise, are based on log-likelihood scores.
:::

## Generative modelling with BRMS

#### 2.6.1 Likelihood Density Scores

::: {style="font-size: x-large"}
Since log is a strictly monotonic transformation, we are not changing anything fundamental by looking at log likelihoods instead of likelihoods. However, we are making the math much simpler by working with sums instead of products. In particular, this concerns computing gradients because the gradient of a sum is just the sum of the individual (pointwise) gradients. Much of the modern statistics and ML relies on this property.

`brms` comes with a dedicated `log_lik` method that does all the required math.

```{r}
#| echo: true
ll_epi_gaussian3 <- brms::log_lik(fit_epi_gaussian3)
str(ll_epi_gaussian3)
```
:::

## Generative modelling with BRMS

#### 2.6.1 Likelihood Density Scores

::: {style="font-size: large"}
The output of `log_lik` has the same structure as `posterior_predict` and friends, that is, it has as many columns as we have observations and as many rows as we posterior draws.

```{r}
#| fig-cap: Per-observation (pointwise) log-likelihoods of model fit_epi_gaussian3.
#| fig-height: 4
#| echo: true
#| code-fold: true
lattice::histogram(colMeans(ll_epi_gaussian3), nint=30, type = "density")
```
:::

## Generative modelling with BRMS

#### 2.6.1 Likelihood Density Scores

::: {style="font-size: large"}
Similar to the RMSE_alt metric earlier, we average over posterior draws per observation such that we obtain one log-likelihood value per observation.

The mean of the log-likelihood differences is slightly positive which points to a slightly better fit of the Student-t model.

```{r}
#| echo: true
#| code-fold: true
llm_epi_student1  <- colMeans( brms::log_lik(fit_epi_student1) )
llm_epi_gaussian3 <- colMeans( brms::log_lik(fit_epi_gaussian3) )

llm_epi_diff <- llm_epi_student1 - llm_epi_gaussian3
mean(llm_epi_diff)
```

```{r}
#| fig-cap: Pointwise log-likelihood values of model fit_epi_gaussian3 and fit_epi_student1 as well as their pointwise log-likelihood differences.
#| fig-align: center
#| fig-height: 4
llm_epi_gaussian3 <- colMeans(ll_epi_gaussian3)
lattice::histogram(~llm_epi_gaussian3 + llm_epi_student1 + llm_epi_diff, nint=50, type = "density")
```
:::

## Generative modelling with BRMS

#### 2.6.1 Likelihood Density Scores

::: {style="font-size: x-large"}
It is more typical to work with sums instead of means of log-likelihood values over observations, a quantity that we call LPD:

```{r}
#| echo: true
(lpd_epi_diff <- sum(llm_epi_diff))
```

The corresponding standard error is also not particular difficult to obtain:

```{r}
#| echo: true
se_sum <- function(x) {sd(x) * sqrt(length(x))}

(se_lpd_epi_diff <- se_sum(llm_epi_diff))
```
:::

## Generative modelling with BRMS

#### 2.7 Out-of-sample predictions

::: {style="font-size: x-large"}
```{r}
#| echo: true
#| code-fold: true
set.seed(8973)
splits <- epilepsy |> rsample::initial_split(prop = 0.8)
epilepsy_train <- rsample::training(splits)
epilepsy_test  <- rsample::testing(splits)
```

```{r}
#| echo: true
#| code-fold: true
output <- 
  capture.output(
    fit_student1_train <- 
      brms::brm(count ~ Trt * Base, data = epilepsy_train, family = brms::student(), silent = 2, seed = 8740, file = "fits/fit_student1_train")
  )
llm_epi_gaussian3_test <-  brms::log_lik(fit_student1_train, newdata = epilepsy_test)

output <- 
  capture.output(
    fit_gaussian3_train <- brms::brm(count ~ Trt * Base, data = epilepsy_train, silent = 2, seed = 8740, file = "fits/fit_gaussian3_train")
  )
llm_epi_student1_test <- colMeans( brms::log_lik(fit_gaussian3_train, newdata = epilepsy_test) )
```
:::

## Generative modelling with BRMS

#### 2.7 Out-of-sample predictions

::: {style="font-size: x-large"}
```{r}
#| eval: true
#| echo: true
#| code-fold: true
#| fig-height: 3
llm_epi_diff_test <- llm_epi_student1_test - llm_epi_gaussian3_test
lattice::histogram(llm_epi_diff_test |> matrix(), nint=50, type = "density")
```

```{r}
#| eval: true
#| echo: true
#| code-fold: true
(elpd_epi_diff_test <- sum(llm_epi_diff_test))
```

```{r}
#| echo: true
#| code-fold: true
(se_elpd_epi_diff_test <- se_sum(llm_epi_diff_test))
```
:::

## Generative modelling with BRMS

#### 2.7 Out-of-sample predictions

:::: {style="font-size: x-large"}
In leave-one-out cross-validation (LOO-CV), we perform $N$ training-test splits, where each time we are leaving out a single observations, fitting the model on the remaining $N-1$ observations before evaluating model fit on that single left-out observation.

::: panel-tabset
## loo

```{r}
#| echo: true
#| warning: false
#| message: false
loo_epi_gaussian3 <- brms::loo(fit_epi_gaussian3)
loo_epi_gaussian3
```

## loo1

```{r}
#| echo: true
#| warning: false
loo_epi_student1 <- brms::loo(fit_epi_student1)
loo_epi_student1
```

## loo2

```{r}
brms::loo_compare(loo_epi_gaussian3, loo_epi_student1)
```

## loo3
```{r}
brms::loo(fit_epi_gaussian3, fit_epi_student1)
```

:::

::::

## Generative modelling with BRMS

#### 2.7 Out-of-sample predictions

::: {style="font-size: x-large"}
In leave-one-out cross-validation (LOO-CV), we perform $N$ training-test splits, where each time we are leaving out a single observations, fitting the model on the remaining $N-1$ observations before evaluating model fit on that single left-out observation.
:::

## Generative modelling with BRMS

#### 2.8 Prior predictive performance

::: {style="font-size: x-large"}
We will now take a fundamentally different approach by evaluating prior predictive distribution, that is, by looking at what predictions a model implies before seeing any data. The starting point to investigating prior predictive performance is to perform graphical prior predictive checks.

```{r}
#| echo: true
#| code-fold: true
prior_epi_gaussian6 <-
  brms::prior(normal(6, 3), class = "Intercept") +
  brms::prior(normal(0, 5), class = "b", coef = "Trt1") +
  brms::prior(normal(0, 1), class = "b", coef = "Base") +
  brms::prior(normal(0, 1), class = "b", coef = "Trt1:Base") +
  brms::prior(normal(0, 15), class = "sigma")
prior_epi_gaussian6 
```
:::

## Generative modelling with BRMS

#### 2.8 Prior predictive performance

::: {style="font-size: x-large"}
Here we are “fitting” the model with the option sample_prior = "only", which ensures that Stan ignores the likelihood contribution to the posterior, such that the posterior directly resembles the prior.

```{r}
#| echo: true
#| code-fold: true
fit_prior_epi_gaussian6 <- 
  brms::brm(
    count ~ 1 + Trt * Base,
    data = epilepsy,
    prior = prior_epi_gaussian6,
    sample_prior = "only",
    file = "fits/fit_prior_epi_gaussian6"
  )
summary(fit_prior_epi_gaussian6)
```
:::

## Generative modelling with BRMS

#### 2.8 Prior predictive performance

::: {style="font-size: x-large"}
For all practical purposes, our “prior-only” brms model can be post-processed as any other brms model.
:::

```{r}
#| echo: true
#| warning: false
#| fig-align: center
brms::pp_check(fit_prior_epi_gaussian6, ndraws = 100) + xlim(-150, 150)
```

## Generative modelling with BRMS

#### 2.8 Marginal likelihood-based metrics

::: {style="font-size: large"}
To mathematically formalize the prior predictive performance of a model, consider the marginal likelihood that we find in the denominator of Bayes theorem (aka evidence): $$
p(y)=\int p(y\vert\theta)p(\theta)d\theta
$$ We can write this as the likelihood of the data given the model and we can do inference about the models based on the marginal likelihood. $$
p(y\vert M)=\int p(y\vert\theta,M)p(\theta\vert M)d\theta
$$ The absolute marginal likelihood values $p(y\vert M)$are very hard to interpret. We only know that higher is better.
:::

## Generative modelling with BRMS

#### 2.8 Marginal likelihood-based metrics

::: {style="font-size: x-large"}
The most common such comparative metric is the Bayes factor, defined as the ratio of two models’ marginal likelihoods:

$$
\mathrm{BF}_{1,2}=\frac{p(y\vert M_{1})}{p(y\vert M_{2})}
$$ If the Bayes factor is greater than 1, the data $y$ have a higher likelihood given model $M_1$ compared to $M_2$, and vice versa.
:::

## Generative modelling with BRMS

#### 2.8 Marginal likelihood-based metrics

::: {style="font-size: x-large"}
We can say "Given the data $y$, model $M_1$ is more likely than model $M_2$” if we use the posterior odds:

$$
\frac{p(M_{1}\vert y)}{p(M_{2}\vert y)}=\frac{p(y\vert M_{1})}{p(y\vert M_{2})}\frac{p(M_{1})}{p(M_{2})}=\mathrm{BF}_{1,2}\times\frac{p(M_{1})}{p(M_{2})}
$$ We often set the prior odds to 1 not actually because we really believe in models being equally likely a priori, but simply out of convenience; just as we often set wide or even completely flat priors on parameters.
:::

## Generative modelling with BRMS

#### 2.8 Marginal likelihood-based metrics

::: {style="font-size: x-large"}
Since the Bayes factor is based on marginal likelihoods, the computational challenges are substantial. Fortunately, there is one class of algorithms that enables reliable computation of (log) marginal likelihood on the basis of posterior draws. This class of algorithms is called bridge sampling.

Marginal likelihood estimation via bridge sampling usually requires several times more posterior draws than the estimation of posterior moments or quantiles (i.e., what we usually do with posterior draws).

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| warning: false
fit_epi_gaussian6 <- 
  brms::brm(
    count ~ 1 + Trt * Base, data = epilepsy,
    prior = prior_epi_gaussian6,
    save_pars = brms::save_pars(all = TRUE),
    iter = 5000, warmup = 1000,
    file = "fits/fit_epi_gaussian6"
  )

logml_epi_gaussian6 <- brms::bridge_sampler(fit_epi_gaussian6, silent = TRUE);
summary(logml_epi_gaussian6)
```
:::

## Elasticity estimation

Since elasticity is defined as the percentage change in volume ($\Delta V/V$) for a given percentage change in price ($\Delta p/p$), then with elasticity parameter $\beta$ we write:

$$
\begin{align*}
\frac{\Delta V}{V} & = \beta\times\frac{\Delta p}{p} \\
\frac{\partial V}{V} & = \beta\times\frac{\partial p}{p} \\
\partial\log(V) & = \beta\times\partial\log(p)
\end{align*}
$$ {#eq-elasticity}

## Elasticity estimation

This equation is the justification for the log-log regression model of elasticity, and this model has solution $V = Kp^\beta$, where $K$ is a constant.

As written, the value of $K$ is either the volume when $p=1$ which may or may not be useful, or it is the volume when $\beta=0$, which is uninteresting.

## Elasticity estimation

To make the interpretation of the constant $K$ more useful, the model can be written as

$$
\partial\log(V) = \beta\times\partial\log(p/p_{\text{baseline}});\qquad V = K\left(\frac{p}{p_{\text{baseline}}}\right)^{\beta}
$$

in which case the constant is interpreted as the volume when the price equals the baseline price; the elasticity parameter $\beta$ is unchanged.

## Elasticity estimation

If $V = Kp^\beta$ then $\log(V) = \log(K) + \beta\log(p)$, and $\partial\log(V)/\partial\log(p) = \beta$ as in the last line of equation (@eq-elasticity).

The equation $\log(V) = \log(K) + \beta\log(p)$ defines a linear relation between the log term and is sometimes estimated as a linear regression on the log terms.

## Elasticity estimation

In this version of the problem there are only two parameters, the constant $\log(K)$ (aka the intercept in the log-log plot of volume vs price plot) and the elasticity $\beta$, the slope of the log-log plot.

As in all linear regressions the variance of the error term is **assumed** constant and its mean is **assumed** zero.

## Likelihood Function

The key choice we need to make in the Bayesian model is the form of the likelihood function for the observed volumes given the parameters. This is a statistical model describing how the observed volume data is generated given the parameters.

Since the volume data is units sold per unit time (i.e. integers), we have several options for the likelihood function (e.g. Poisson, Negative Binomial, Binomial, mixture models of various sorts), but the Poisson model is the simplest.

## Likelihood Function

The Poisson model of the data has a single, positive, real-valued rate parameter $\lambda$ which represents the units sold per unit time (a rate), so we can choose:

$$
\begin{align*}
\lambda = \exp^{\log(K) + \beta\log(p)}\Rightarrow \log(\lambda) = \log(K) + \beta\log(p)
\end{align*}
$$ which gives us the log-log relationship of the model, with the crucial difference that we have additionally chosen a model for the data-generating process: a Poisson process with parameter $\lambda$.

## Likelihood Function

Note that a Poisson process is quite different than the Gaussian process, so we can't use a OLS model.

We need a glm model instead, e.g the regression should be modeled as

``` r
glm(volume ~ log(price), family = 'poisson')
```

## Economics

One challenge with standard regression models is that they don't admit assumptions outside the likelihoods.

In the case of elasticity models though, we have economic reasons to expect the estimated coefficient of price to be negative, i.e. that the demand curve slopes downwards.

So, how to incorporate this or any other assumptions about the data generating process (think DAGs again) when off-the-shelf packages aren't flexible enough?

## Stan

One popular option for developing flexible statistical models is the **Stan** language.

**Stan** is a high-level probabilistic programming language used for statistical modeling and Bayesian inference. It's designed to make it easier for researchers, data scientists, and statisticians to specify and estimate complex statistical models.

R has several interfaces to Stan, including [RStan](https://chat.openai.com/c/04f46742-c817-49ff-8b07-2497d363e0d5), [CmdStanR](https://mc-stan.org/cmdstanr/index.html), and [brms](https://mc-stan.org/users/interfaces/brms).

## Stan

In Stan, you declare your model using a domain-specific language. You specify the relationships between variables and define the likelihood and prior distributions.

Stan then samples from the posterior distributions of the model parameters.

## Stan

```{stan output.var='Y'}
#| echo: true
#| label: Stan model
#| code-fold: true
#| code-summary: "Poisson elasticity model implemented in the Stan language"
#| code-line-numbers: "1-14|16-19|21-28|30-39|41-48"
#| eval: false
data {
  /* Dimensions */
  int<lower=1> N; // rows

  /* log price vector (integer) */
  array[N] real P;
  
  /* demand vector (integer) */
  array[N] int<lower=0> Y;

  /* hyperparameters*/
  real<lower=0> s;       // scale parameter for intercept prior
  real<lower=0> e_scale; // scale parameter for elasticity prior
}

parameters {
  real <upper=0> elasticity;      // elasticities variable < 0 
  real intercept;                 // intercepts variable
}

transformed parameters {
  array[N] real log_lambda;       // log volume for likelihoods
  
  for (i in 1:N){
    log_lambda[i] = intercept + elasticity * P[i];
  }
  
}

model {
  /* Priors on the parameters */
  target += normal_lpdf(intercept  | 0, s);
  target += cauchy_lpdf(elasticity | 0, e_scale);

  /* Conditional log-likelihoods for each observed volume */
  for (i in 1 : N) {
    target += poisson_lpmf(Y[i] | exp(log_lambda[i]) );
  }
}

generated quantities {
  array[N] int<lower=-1> y_new;  // estimate volumes
  vector[N] log_lik;             // compute log-likelihood for this model
  for (i in 1 : N) {
      y_new[i]   = poisson_rng( exp(log_lambda[i]) );
      log_lik[i] = poisson_lpmf(Y[i] | exp(log_lambda[i]) );
  }
}
```

## Stan

::: {style="font-size: 65%"}
The Stan programme produces samples from the posterior distributions of the parameters (below). These can be used to produce posterior predictive samples for the volumes given the prices, for comparison with the observed data.
:::

![](images/ni_elasticity_samples.png){fig-align="center" width="800"}

## Draft

-   Bayesian nets \| DAGS

-   Use cases

-   Bayes without simulation - conjugate distros

## More

-   Read [Bayes Rules!](https://www.bayesrulesbook.com/)
-   Read [Think Bayes](http://allendowney.github.io/ThinkBayes2/index.html)
-   Read [Statistical Rethinking](https://github.com/rmcelreath/stat_rethinking_2022)

## Recap

-   We've had the smallest possible taste of statistical programming using Bayes theorem and sampling methods, in the context of adressing the limitations of off-the-shelf implementations of statistical methods and algorithms.
