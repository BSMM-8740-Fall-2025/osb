<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.39">

  <meta name="author" content="L.L. Odette">
  <title>BSMM-8740 - Fall 2024 – Regression methods</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-a8986e45384d4e2047f3d84b21c5a23c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Regression methods – BSMM-8740 - Fall 2024">
<meta property="og:description" content="BSMM8740-2-R-2023F [WEEK - 4]">
<meta property="og:site_name" content="BSMM-8740 - Fall 2024">
<meta name="twitter:title" content="Regression methods – BSMM-8740 - Fall 2024">
<meta name="twitter:description" content="BSMM8740-2-R-2023F [WEEK - 4]">
<meta name="twitter:image" content="https://bsmm-8740-fall-2024.github.io/osb/slides/images/twitter-card.png">
<meta name="twitter:creator" content="@lodette">
<meta name="twitter:card" content="summary_large_image">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Regression methods</h1>
  <p class="subtitle">BSMM8740-2-R-2023F [WEEK - 4]</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
L.L. Odette 
</div>
</div>
</div>

</section>
<section id="recap-of-last-lecture" class="slide level2">
<h2>Recap of last lecture</h2>
<ul>
<li><p>Last time we worked with the <code>recipes</code> package to develop workflows for pre-processing our data.</p></li>
<li><p>Today we look at regression methods we might apply to our data.</p></li>
</ul>
</section>
<section>
<section id="linear-regression" class="title-slide slide level1 center">
<h1>Linear regression</h1>

</section>
<section id="linear-regression-models" class="slide level2">
<h2>Linear regression models</h2>
<p>In the simple linear regression model, you have <span class="math inline">\(N\)</span> observations of the response variable <span class="math inline">\(Y\)</span> with a linear combination of <span class="math inline">\(D\)</span> predictor variables <span class="math inline">\(\mathbf{x}\)</span> where</p>
<p><span class="math display">\[
\pi\left(Y=y|\mathbf{x,\theta}\right)=\mathcal{N}\left(\left.y\right|\beta_{0}+\mathbf{\mathbf{\mathbf{\beta}}}'\mathbf{x},\sigma^{2}\right)
\]</span></p>
</section>
<section id="linear-regression-models-1" class="slide level2">
<h2>Linear regression models</h2>
<p>where <span class="math inline">\(\mathcal{N}\left(\left.y\right|\mu,\sigma^{2}\right)\)</span> is a Normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\theta=\left(\beta_{0},\mathbf{\mathbf{\mathbf{\beta}}},\sigma^{2}\right)\)</span> are the <em>parameters</em> of the model and the vector of parameters <span class="math inline">\(\beta_{1:D}\)</span> are the <em>weights</em> or <em>regression</em> coefficients.</p>
</section>
<section id="linear-regression-models-2" class="slide level2">
<h2>Linear regression models</h2>
<p>The mean function <span class="math inline">\(f(\mathbf{x})\equiv\mu\)</span> could be any linear function in <span class="math inline">\(\mathbf{x}=(x_1,\ldots,x_m)\)</span> in which case <span class="math inline">\(\beta_{0}+\mathbf{\mathbf{\mathbf{\beta}}}'\mathbf{x}\)</span> is a linear approximation around <span class="math inline">\(\beta_{0}\)</span> per the Taylor series for <span class="math inline">\(f(\mathbf{x})\)</span>. When <span class="math inline">\(D=1\)</span> the Taylor series is<sup>1</sup></p>
<p><span class="math display">\[
f(x)=f(\beta_{0})+f^{(1)}(\beta_{0})(x-\beta_{0})+\frac{1}{2}f^{(2)}(\beta_{0})(x-\beta_{0})^{2}+\ldots
\]</span></p>
<aside><ol class="aside-footnotes"><li id="fn1"><p>where <span class="math inline">\(\ldots = \sum_{n=3}^{\infty}\frac{1}{n!}f^{(n)}(\beta_{0})(x-\beta_{0})^{n}\)</span></p></li></ol></aside></section>
<section id="linear-regression-models-3" class="slide level2">
<h2>Linear regression models</h2>
<p>When <span class="math inline">\(D=2\)</span> the Taylor series is (writing <span class="math inline">\(f_{x}\equiv\frac{\partial f}{\partial x}\)</span>, <span class="math inline">\(f_{y}\equiv\frac{\partial f}{\partial y}\)</span>, <span class="math inline">\(f_{x,y}\equiv\frac{\partial^2 f}{\partial x,\partial x}\)</span> and so on):</p>
<p><span class="math display">\[
\begin{align*}
f(x,y) &amp; =f(\alpha_{0},\beta_{0})+f_{x}(\alpha_{0},\beta_{0})(x-\alpha_{0})+f_{y}(\alpha_{0},\beta_{0})(y-\beta_{0})\\
&amp; = + f_{x,x}(\alpha_{0},\beta_{0})(x-\alpha_{0})^{2}+f_{y,y}(\alpha_{0},\beta_{0})(y-\beta_{0})^{2}\\
&amp; = + f_{x,y}(\alpha_{0},\beta_{0})(x-\alpha_{0})(y-\beta_{0})+\ldots
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-4" class="slide level2">
<h2>Linear regression models</h2>
<p>To fit the 1D linear regression model to <span class="math inline">\(N\)</span> data samples, we minimize the negative log-likelihood on the training set.</p>
<p><span class="math display">\[
\begin{align*}
\text{NLL}\left(\beta,\sigma^{2}\right) &amp; =\sum_{n=1}^{N}\log\left[\left(\frac{1}{2\pi\sigma^{2}}\right)^{\frac{1}{2}}\exp\left(-\frac{1}{2\sigma^{2}}\left(y_{n}-\beta'x_{n}\right)^{2}\right)\right]\\
&amp; =-\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}\left(y_{n}-\hat{y}_{n}\right)^{2}-\frac{N}{2}\log\left(2\pi\sigma^{2}\right)
\end{align*}
\]</span></p>
<p>where the predicted response is <span class="math inline">\(\hat{y}\equiv\beta'x_{n}\)</span>.</p>
</section>
<section id="linear-regression-models-5" class="slide level2">
<h2>Linear regression models</h2>
<p>Focusing on just the weights, the minimum NLL is (up to a constant) the minimum of the residual sum of squares (RSS):</p>
<p><span class="math display">\[
\begin{align*}\text{RSS}\left(\beta\right) &amp; =\frac{1}{2}\sum_{n=1}^{N}\left(y_{n}-\beta'x_{n}\right)^{2}=\frac{1}{2}\left\Vert y_{n}-\beta'x_{n}\right\Vert ^{2}\\
&amp; =\frac{1}{2}\left(y_{n}-\beta'x_{n}\right)'\left(y_{n}-\beta'x_{n}\right)\\
\\
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-6" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols">Ordinary least squares (OLS)</h4>
<p>We can write our regression assumption as</p>
<p><span class="math display">\[
y_i=\beta_0+\beta_1 x_i + u_i
\]</span></p>
<p>where <span class="math inline">\(u_i\)</span> is a sample from <span class="math inline">\(\mathcal{N}\left(0,\sigma^{2}\right)\)</span> which in turn implies <span class="math inline">\(\mathbb{E}\left[u\right]=0;\;\mathbb{E}\left[\left.u\right|x\right]=0\)</span></p>
</section>
<section id="linear-regression-models-7" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-1">Ordinary least squares (OLS)</h4>
<p>It follows that</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left[y-\beta_{0}-\beta_{1}x\right] &amp; =0\\
\mathbb{E}\left[x\left(y-\beta_{0}-\beta_{1}x\right)\right] &amp; =0
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-8" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-2">Ordinary least squares (OLS)</h4>
<p>Writing the same thing for our samples (where <span class="math inline">\(\hat{\beta}_{0}, \hat{\beta}_{1}\)</span> are our estimates)</p>
<p><span class="math display">\[
\begin{align*}
\frac{1}{N}\sum_{i-1}^{N}y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i} &amp; =0\\
\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right) &amp; =0
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-9" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-3">Ordinary least squares (OLS)</h4>
<p>From the first equation</p>
<p><span class="math display">\[
\begin{align*}
\bar{y}-\hat{\beta}_{0}-\hat{\beta}_{1}\bar{x} &amp; =0\\
\bar{y}-\hat{\beta}_{1}\bar{x} &amp; =\hat{\beta}_{0}
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-10" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-4">Ordinary least squares (OLS)</h4>
<p>Substituting the expression for <span class="math inline">\(\hat{\beta}_{0}\)</span> in the independence equation</p>
<p><span class="math display">\[
\begin{align*}
\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(y_{i}-\left(\bar{y}-\hat{\beta}_{1}\bar{x}\right)-\hat{\beta}_{1}x_{i}\right) &amp; =0\\
\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(y_{i}-\bar{y}\right) &amp; =\hat{\beta}_{1}\frac{1}{N}\sum_{i-1}^{N}x_{i}\left(\bar{x}-x_{i}\right)\\
\frac{1}{N}\sum_{i-1}^{N}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right) &amp; =\hat{\beta}_{1}\frac{1}{N}\sum_{i-1}^{N}\left(\bar{x}-x_{i}\right)^2
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-11" class="slide level2">
<h2>Linear regression models</h2>
<h4 id="ordinary-least-squares-ols-5">Ordinary least squares (OLS)</h4>
<p>So as long as <span class="math inline">\(\sum_{i-1}^{N}\left(\bar{x}-x_{i}\right)^2\ne 0\)</span></p>
<p><span class="math display">\[
\begin{align*}
\hat{\beta}_{1} &amp; =\frac{\sum_{i-1}^{N}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i-1}^{N}\left(\bar{x}_{i}-x_{i}\right)^2}\\
&amp; =\frac{\text{sample covariance}(x_{i}y_{i})}{\text{sample variance}(x_{i})}
\end{align*}
\]</span></p>
</section>
<section id="linear-regression-models-12" class="slide level2">
<h2>Linear regression models</h2>
<p>Similarly, in the vector equation, the minimum of the RSS is solved by (assuming <span class="math inline">\(N&gt;D\)</span>):</p>
<p><span class="math display">\[
\hat{\mathbf{\beta}}_{OLS}=\left(X'X\right)^{-1}\left(X'Y\right) = \frac{\text{cov}(X,Y)}{\text{var}(X)}
\]</span></p>
<p>There are algorithmic issues though.</p>
</section>
<section id="linear-regression-algorithms" class="slide level2">
<h2>Linear regression algorithms</h2>
<p>Computing the inverse of <span class="math inline">\(X'X\)</span> directly, while theoretically possible, can be numerically unstable.</p>
<p>In R, the <span class="math inline">\(QR\)</span> decomposition is used to solve for <span class="math inline">\(\beta\)</span>. Let <span class="math inline">\(X=QR\)</span> where <span class="math inline">\(Q'Q=I\)</span> and write:</p>
<p><span class="math display">\[
\begin{align*}
(QR)\beta &amp; = y\\
Q'QR\beta &amp; = Q'y\\
\beta &amp; = R^{-1}(Q'y)
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(R\)</span> is upper triangular, the last equation can be solved by back-substitution.</p>
</section>
<section id="linear-regression-algorithms-1" class="slide level2">
<h2>Linear regression algorithms</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1" data-code-line-numbers="1|2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a></a><span class="sc">&gt;</span> A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>, <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>), <span class="at">nrow=</span><span class="dv">3</span>)</span>
<span id="cb1-2"><a></a><span class="sc">&gt;</span> QR <span class="ot">&lt;-</span> <span class="fu">qr</span>(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="panel-tabset">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1">Q</a></li><li><a href="#tabset-1-2">R</a></li><li><a href="#tabset-1-3">A</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a></a><span class="sc">&gt;</span> Q <span class="ot">&lt;-</span> <span class="fu">qr.Q</span>(QR); Q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]       [,2]          [,3]
[1,] -0.1825742 -0.4082483 -8.944272e-01
[2,] -0.3651484 -0.8164966  4.472136e-01
[3,] -0.9128709  0.4082483  2.593051e-16</code></pre>
</div>
</div>
</div>
<div id="tabset-1-2">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a></a><span class="sc">&gt;</span> R <span class="ot">&lt;-</span> <span class="fu">qr.R</span>(QR); R</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]      [,2]      [,3]
[1,] -5.477226 -7.302967 -4.381780
[2,]  0.000000 -1.632993 -2.449490
[3,]  0.000000  0.000000 -1.341641</code></pre>
</div>
</div>
</div>
<div id="tabset-1-3">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a></a><span class="sc">&gt;</span> Q <span class="sc">%*%</span> R</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    2    4    3
[3,]    5    6    3</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="linear-regression-algorithms-2" class="slide level2">
<h2>Linear regression algorithms</h2>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" data-code-line-numbers="1|2-3|5-6|8-9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a></a><span class="sc">&gt;</span> <span class="co"># A linear system of equations y = Ax</span></span>
<span id="cb8-2"><a></a><span class="er">&gt;</span> <span class="fu">cat</span>(<span class="st">"matrix A</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb8-3"><a></a><span class="sc">&gt;</span> A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>, .<span class="dv">5</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">4</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="at">nrow=</span><span class="dv">3</span>); A</span>
<span id="cb8-4"><a></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">"vector x</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb8-5"><a></a><span class="sc">&gt;</span> x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>); x</span>
<span id="cb8-6"><a></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">"vector y</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb8-7"><a></a><span class="sc">&gt;</span> y <span class="ot">&lt;-</span> A <span class="sc">%*%</span> x ; y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell quarto-layout-panel" data-layout-nrow="3" data-layout-align="center">
<div class="quarto-layout-row">
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>matrix A</code></pre>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>     [,1] [,2] [,3]
[1,]    3  2.0   -1
[2,]    2 -2.0    4
[3,]   -1  0.5   -1</code></pre>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>vector x</code></pre>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>[1]  1 -2 -2</code></pre>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>vector y</code></pre>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>     [,1]
[1,]    1
[2,]   -2
[3,]    0</code></pre>
</div>
</div>
</div>
</section>
<section id="linear-regression-algorithms-3" class="slide level2">
<h2>Linear regression algorithms</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15" data-code-line-numbers="1-4|6-7|9-10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a></a><span class="sc">&gt;</span> <span class="co"># Compute the QR decomposition of A</span></span>
<span id="cb15-2"><a></a><span class="er">&gt;</span> QR <span class="ot">&lt;-</span> <span class="fu">qr</span>(A)</span>
<span id="cb15-3"><a></a><span class="sc">&gt;</span> Q <span class="ot">&lt;-</span> <span class="fu">qr.Q</span>(QR)</span>
<span id="cb15-4"><a></a><span class="sc">&gt;</span> R <span class="ot">&lt;-</span> <span class="fu">qr.R</span>(QR)</span>
<span id="cb15-5"><a></a><span class="sc">&gt;</span> </span>
<span id="cb15-6"><a></a><span class="er">&gt;</span> <span class="co"># Compute b=Q'y</span></span>
<span id="cb15-7"><a></a><span class="er">&gt;</span> b <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(Q, y); b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]
[1,]  0.2672612
[2,]  2.1472519
[3,] -0.5638092</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17" data-code-line-numbers="1-4|6-7|9-10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a></a><span class="sc">&gt;</span> <span class="co"># Solve the upper triangular system Rx=b</span></span>
<span id="cb17-2"><a></a><span class="er">&gt;</span> <span class="fu">backsolve</span>(R, b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]
[1,]    1
[2,]   -2
[3,]   -2</code></pre>
</div>
</div>
</section>
<section id="linear-regression-models-13" class="slide level2">
<h2>Linear regression models</h2>
<p>Minimizing the NLL by minimizing the residual sum of squares (RSS) is the same as minimizing</p>
<ul>
<li>the <strong>mean squared error</strong> <span class="math inline">\(\text{MSE}\left(\beta\right) = \frac{1}{N}\text{RSS}\left(\beta\right)\)</span></li>
<li>the <strong>root mean squared error</strong> <span class="math inline">\(\text{RMSE}\left(\beta\right) = \sqrt{\text{MSE}\left(\beta\right)}\)</span></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The minimum NLL estimate is also the maximum likelihood estimate (MLE)</p>
</div>
</div>
</div>
</section>
<section id="empirical-risk-minimization" class="slide level2">
<h2>Empirical risk minimization</h2>
<p>The MLE can be generalized by replacing the NLL (<span class="math inline">\(\ell\left(y_{n},\theta;x_{n}\right)=-\log\pi\left(y_n|x_n,\theta\right)\)</span>) with any other loss function to get</p>
<p><span class="math display">\[
\mathcal{L}\left(\theta\right)=\frac{1}{N}\sum_{n=1}^{N}\ell\left(y_{n},\theta;x_{n}\right)
\]</span></p>
<p>This is known as the empirical risk minimization (ERM) - the expected loss taken with respect to the empirical distribution.</p>
</section>
<section id="collinearity" class="slide level2">
<h2>Collinearity</h2>
<p>One of the important assumptions of the classical linear regression models is that there is no exact collinearity among the regressors. While high correlation between regressors is a necessary indicator of the collinearity problem, a direct linear relationship beween regressors is sufficient.</p>
</section>
<section id="collinearity-1" class="slide level2">
<h2>Collinearity</h2>
<p>Data collection methods, constraints on the fitted regression model, model specification error, an overdefined model, may be some potential sources of multicollinearity. In other cases it is an artifact caused by creating new predictors from other predictors.</p>
</section>
<section id="collinearity-2" class="slide level2">
<h2>Collinearity</h2>
<p>The problem of collinearity has potentially serious effect on the regression estimates such as implausible coefficient signs, impossible inversion of matrix <span class="math inline">\(X'X\)</span> as it becomes near or exactly singular, large magnitude of coefficients in absolute value, large variance or standard errors with wider confidence intervals.</p>
</section>
<section id="ridge-regression" class="slide level2">
<h2>Ridge Regression</h2>
<p>Ridge regression is an example of a penalized regression model; in this case the magnitude of the weights are penalized by adding the <span class="math inline">\(\ell_2\)</span> norm of the weights to the loss function. In particular, the ridge regression weights are:</p>
<p><span class="math display">\[
\hat{\beta}_{\text{ridge}}=\arg\!\min\text{RSS}\left(\beta\right)+\lambda\left\Vert \beta\right\Vert _{2}^{2}
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the strength of the regularizer term.</p>
</section>
<section id="ridge-regression-1" class="slide level2">
<h2>Ridge Regression</h2>
<p>The solution is:</p>
<p><span class="math display">\[
\begin{align*}
\hat{\mathbf{\beta}}_{ridge} &amp; =\left(X'X-\lambda I_{D}\right)^{-1}\left(X'Y\right)\\
&amp; =\left(\sum_{n}x_{n}x'_{n}+\lambda I_{D}\right)^{-1}\left(\sum_{n}y_{n}x_{n}\right)
\end{align*}
\]</span></p>
</section>
<section id="ridge-regression-2" class="slide level2">
<h2>Ridge Regression</h2>
<p>As for un-penalized linear regression, using matrix inversion to solve for <span class="math inline">\(\hat{\mathbf{\beta}}_{ridge}\)</span> can be a bad idea. The QR transformation can be used here, however, ridge regression is often used when <span class="math inline">\(D&gt;N\)</span>, in which case the SVD transformation is faster.</p>
</section>
<section id="ridge-regression-example" class="slide level2">
<h2>Ridge Regression Example</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19" data-code-line-numbers="2|5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a></a><span class="sc">&gt;</span> <span class="co">#define response variable</span></span>
<span id="cb19-2"><a></a><span class="er">&gt;</span> y <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">pull</span>(hp)</span>
<span id="cb19-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb19-4"><a></a><span class="er">&gt;</span> <span class="co">#define matrix of predictor variables</span></span>
<span id="cb19-5"><a></a><span class="er">&gt;</span> x <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(mpg, wt, drat, qsec) <span class="sc">%&gt;%</span> <span class="fu">data.matrix</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ridge-regression-example-1" class="slide level2">
<h2>Ridge Regression Example</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20" data-code-line-numbers="1-2|4-5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a></a><span class="sc">&gt;</span> <span class="co"># fit ridge regression model</span></span>
<span id="cb20-2"><a></a><span class="er">&gt;</span> model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb20-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb20-4"><a></a><span class="er">&gt;</span> <span class="co"># get coefficients when lambda = 7.6</span></span>
<span id="cb20-5"><a></a><span class="er">&gt;</span> <span class="fu">coef</span>(model, <span class="at">s =</span> <span class="fl">7.6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>5 x 1 sparse Matrix of class "dgCMatrix"
                      s1
(Intercept) 477.91365858
mpg          -3.29697140
wt           20.31745927
drat         -0.09524492
qsec        -18.48934710</code></pre>
</div>
</div>
</section>
<section id="ridge-regression-example-2" class="slide level2">
<h2>Ridge Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>glmnet example</summary>
<div class="sourceCode cell-code" id="cb22" data-code-line-numbers="1-2|4-5|7-8|9|10|11|12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a></a><span class="sc">&gt;</span> <span class="co"># perform k-fold cross-validation to find optimal lambda value</span></span>
<span id="cb22-2"><a></a><span class="er">&gt;</span> cv_model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb22-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb22-4"><a></a><span class="er">&gt;</span> <span class="co"># find optimal lambda value that minimizes test MSE</span></span>
<span id="cb22-5"><a></a><span class="er">&gt;</span> best_lambda <span class="ot">&lt;-</span> cv_model<span class="sc">$</span>lambda.min</span>
<span id="cb22-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb22-7"><a></a><span class="er">&gt;</span> <span class="co"># produce plot of test MSE by lambda value</span></span>
<span id="cb22-8"><a></a><span class="er">&gt;</span> cv_model <span class="sc">%&gt;%</span> broom<span class="sc">::</span><span class="fu">tidy</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb22-9"><a></a><span class="sc">+</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>lambda, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb22-10"><a></a><span class="sc">+</span>   <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> conf.low, <span class="at">ymax =</span> conf.high), <span class="at">fill =</span> <span class="st">"#00ABFD"</span>, <span class="at">alpha=</span><span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb22-11"><a></a><span class="sc">+</span>   <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb22-12"><a></a><span class="sc">+</span>   <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>best_lambda) <span class="sc">+</span></span>
<span id="cb22-13"><a></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title=</span><span class="st">'Ridge Regression'</span></span>
<span id="cb22-14"><a></a><span class="sc">+</span>        , <span class="at">subtitle =</span> </span>
<span id="cb22-15"><a></a><span class="sc">+</span>          stringr<span class="sc">::</span><span class="fu">str_glue</span>(</span>
<span id="cb22-16"><a></a><span class="sc">+</span>            <span class="st">"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}"</span></span>
<span id="cb22-17"><a></a><span class="sc">+</span>          )</span>
<span id="cb22-18"><a></a><span class="sc">+</span>   ) <span class="sc">+</span></span>
<span id="cb22-19"><a></a><span class="sc">+</span>   ggplot2<span class="sc">::</span><span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="origBSMM_8740_lec_04_files/figure-revealjs/unnamed-chunk-9-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="ridge-regression-example-3" class="slide level2">
<h2>Ridge Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>glmnet coefficients</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a></a><span class="sc">&gt;</span> model<span class="sc">$</span>beta <span class="sc">%&gt;%</span> </span>
<span id="cb23-2"><a></a><span class="sc">+</span>   <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb23-3"><a></a><span class="sc">+</span>   <span class="fu">t</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb23-4"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb23-5"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">add_column</span>(<span class="at">lambda =</span> model<span class="sc">$</span>lambda, <span class="at">.before =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb23-6"><a></a><span class="sc">+</span>   tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(<span class="sc">-</span>lambda, <span class="at">names_to =</span> <span class="st">'parameter'</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb23-7"><a></a><span class="sc">+</span>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>lambda, <span class="at">y=</span>value, <span class="at">color=</span>parameter)) <span class="sc">+</span></span>
<span id="cb23-8"><a></a><span class="sc">+</span>   <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb23-9"><a></a><span class="sc">+</span>   <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">2000</span>) <span class="sc">+</span></span>
<span id="cb23-10"><a></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title=</span><span class="st">'Ridge Regression'</span></span>
<span id="cb23-11"><a></a><span class="sc">+</span>        , <span class="at">subtitle =</span> </span>
<span id="cb23-12"><a></a><span class="sc">+</span>          stringr<span class="sc">::</span><span class="fu">str_glue</span>(</span>
<span id="cb23-13"><a></a><span class="sc">+</span>            <span class="st">"Parameters as a function of lambda"</span></span>
<span id="cb23-14"><a></a><span class="sc">+</span>          )</span>
<span id="cb23-15"><a></a><span class="sc">+</span>   )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="origBSMM_8740_lec_04_files/figure-revealjs/unnamed-chunk-10-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="lasso-regression" class="slide level2">
<h2>Lasso Regression</h2>
<p>Lasso regression is another example of a penalized regression model; in this case both the magnitude of the weights and the number of parameters are penalized by using the <span class="math inline">\(\ell_1\)</span> norm of the weights to the loss function of the lasso regression. In particular, the lasso regression weights are:</p>
<p><span class="math display">\[
\hat{\beta}_{\text{lasso}}=\arg\!\min\text{RSS}\left(\beta\right)+\lambda\left\Vert \beta\right\Vert _{1}
\]</span></p>
</section>
<section id="lasso-regression-1" class="slide level2">
<h2>Lasso Regression</h2>
<p>The Lasso objective function is</p>
<p><span class="math display">\[
\mathcal{L}\left(\beta,\lambda\right)=\text{NLL}+\lambda\left\Vert \beta\right\Vert _{1}
\]</span></p>
</section>
<section id="lasso-regression-example" class="slide level2">
<h2>Lasso Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>lasso model</summary>
<div class="sourceCode cell-code" id="cb24" data-code-line-numbers="1-2|4-5|7-8|10-11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a></a><span class="sc">&gt;</span> <span class="co"># define response variable</span></span>
<span id="cb24-2"><a></a><span class="er">&gt;</span> y <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">pull</span>(hp)</span>
<span id="cb24-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb24-4"><a></a><span class="er">&gt;</span> <span class="co"># define matrix of predictor variables</span></span>
<span id="cb24-5"><a></a><span class="er">&gt;</span> x <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(mpg, wt, drat, qsec) <span class="sc">%&gt;%</span> <span class="fu">data.matrix</span>()</span>
<span id="cb24-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb24-7"><a></a><span class="er">&gt;</span> <span class="co"># fit ridge regression model</span></span>
<span id="cb24-8"><a></a><span class="er">&gt;</span> model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb24-9"><a></a><span class="sc">&gt;</span> </span>
<span id="cb24-10"><a></a><span class="er">&gt;</span> <span class="co"># get coefficients when lambda = 3.53</span></span>
<span id="cb24-11"><a></a><span class="er">&gt;</span> <span class="fu">coef</span>(model, <span class="at">s =</span> <span class="fl">3.53</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>5 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept) 480.761125
mpg          -3.036337
wt           20.222451
drat          .       
qsec        -18.944318</code></pre>
</div>
</div>
</section>
<section id="lasso-regression-example-1" class="slide level2">
<h2>Lasso Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>lasso example</summary>
<div class="sourceCode cell-code" id="cb26" data-code-line-numbers="2|5|8|9|10|11|12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a></a><span class="sc">&gt;</span> <span class="co">#perform k-fold cross-validation to find optimal lambda value</span></span>
<span id="cb26-2"><a></a><span class="er">&gt;</span> cv_model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb26-3"><a></a><span class="sc">&gt;</span> </span>
<span id="cb26-4"><a></a><span class="er">&gt;</span> <span class="co">#find optimal lambda value that minimizes test MSE</span></span>
<span id="cb26-5"><a></a><span class="er">&gt;</span> best_lambda <span class="ot">&lt;-</span> cv_model<span class="sc">$</span>lambda.min</span>
<span id="cb26-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb26-7"><a></a><span class="er">&gt;</span> <span class="co">#produce plot of test MSE by lambda value</span></span>
<span id="cb26-8"><a></a><span class="er">&gt;</span> cv_model <span class="sc">%&gt;%</span> broom<span class="sc">::</span><span class="fu">tidy</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb26-9"><a></a><span class="sc">+</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>lambda, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb26-10"><a></a><span class="sc">+</span>   <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> conf.low, <span class="at">ymax =</span> conf.high), <span class="at">fill =</span> <span class="st">"#00ABFD"</span>, <span class="at">alpha=</span><span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb26-11"><a></a><span class="sc">+</span>   <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb26-12"><a></a><span class="sc">+</span>   <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>best_lambda) <span class="sc">+</span></span>
<span id="cb26-13"><a></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title=</span><span class="st">'Lasso Regression'</span></span>
<span id="cb26-14"><a></a><span class="sc">+</span>        , <span class="at">subtitle =</span> </span>
<span id="cb26-15"><a></a><span class="sc">+</span>          stringr<span class="sc">::</span><span class="fu">str_glue</span>(</span>
<span id="cb26-16"><a></a><span class="sc">+</span>            <span class="st">"The best lambda value is {scales::number(best_lambda, accuracy=0.01)}"</span></span>
<span id="cb26-17"><a></a><span class="sc">+</span>          )</span>
<span id="cb26-18"><a></a><span class="sc">+</span>   ) <span class="sc">+</span></span>
<span id="cb26-19"><a></a><span class="sc">+</span>   <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="fu">exp</span>(<span class="dv">4</span>)) <span class="sc">+</span> ggplot2<span class="sc">::</span><span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="origBSMM_8740_lec_04_files/figure-revealjs/unnamed-chunk-12-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="lasso-regression-example-2" class="slide level2">
<h2>Lasso Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>lasso coefficients</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a></a><span class="sc">&gt;</span> model <span class="sc">%&gt;%</span></span>
<span id="cb27-2"><a></a><span class="sc">+</span>   broom<span class="sc">::</span><span class="fu">tidy</span>() <span class="sc">%&gt;%</span></span>
<span id="cb27-3"><a></a><span class="sc">+</span>   tidyr<span class="sc">::</span><span class="fu">pivot_wider</span>(<span class="at">names_from=</span>term, <span class="at">values_from=</span>estimate) <span class="sc">%&gt;%</span></span>
<span id="cb27-4"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(step,dev.ratio, <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb27-5"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">mutate_all</span>(dplyr<span class="sc">::</span>coalesce, <span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb27-6"><a></a><span class="sc">+</span>   tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(<span class="sc">-</span>lambda, <span class="at">names_to =</span> <span class="st">'parameter'</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb27-7"><a></a><span class="sc">+</span>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>lambda, <span class="at">y=</span>value, <span class="at">color=</span>parameter)) <span class="sc">+</span></span>
<span id="cb27-8"><a></a><span class="sc">+</span>   <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb27-9"><a></a><span class="sc">+</span>   <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">70</span>) <span class="sc">+</span></span>
<span id="cb27-10"><a></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title=</span><span class="st">'Ridge Regression'</span></span>
<span id="cb27-11"><a></a><span class="sc">+</span>        , <span class="at">subtitle =</span> </span>
<span id="cb27-12"><a></a><span class="sc">+</span>          stringr<span class="sc">::</span><span class="fu">str_glue</span>(</span>
<span id="cb27-13"><a></a><span class="sc">+</span>            <span class="st">"Parameters as a function of lambda"</span></span>
<span id="cb27-14"><a></a><span class="sc">+</span>          )</span>
<span id="cb27-15"><a></a><span class="sc">+</span>   )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="origBSMM_8740_lec_04_files/figure-revealjs/unnamed-chunk-13-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="elastic-net-regression" class="slide level2">
<h2>Elastic Net Regression</h2>
<p>Elastic Net regression is a hybrid of ridge and lasso regression.</p>
<p>The elastic net objective function is</p>
<p><span class="math display">\[
\mathcal{L}\left(\beta,\lambda,\alpha\right)=\text{NLL}+\lambda\left(\left(1-\alpha\right)\left\Vert \beta\right\Vert _{2}^{2}+\alpha\left\Vert \beta\right\Vert _{1}\right)
\]</span></p>
<p>so that <span class="math inline">\(\alpha=0\)</span> is ridge regression and <span class="math inline">\(\alpha=1\)</span> is lasso regression and <span class="math inline">\(\alpha\in\left(0,1\right)\)</span> is the general elastic net.</p>
</section>
<section id="elastic-net-regression-example" class="slide level2">
<h2>Elastic Net Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>elastic net example</summary>
<div class="sourceCode cell-code" id="cb28" data-code-line-numbers="5-9|10-13|15|16-17|18-29|33-35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a></a><span class="sc">&gt;</span> <span class="co"># set length of data and seed for reproducability</span></span>
<span id="cb28-2"><a></a><span class="er">&gt;</span> n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb28-3"><a></a><span class="sc">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">2467</span>)</span>
<span id="cb28-4"><a></a><span class="sc">&gt;</span> <span class="co"># create the dataset</span></span>
<span id="cb28-5"><a></a><span class="er">&gt;</span> dat <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(</span>
<span id="cb28-6"><a></a><span class="sc">+</span>   <span class="at">a =</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, n, <span class="at">replace =</span> T)<span class="sc">/</span><span class="dv">10</span></span>
<span id="cb28-7"><a></a><span class="sc">+</span>   , <span class="at">b =</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, n, <span class="at">replace =</span> T)<span class="sc">/</span><span class="dv">10</span></span>
<span id="cb28-8"><a></a><span class="sc">+</span>   , <span class="at">c =</span> <span class="fu">sort</span>(<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, n, <span class="at">replace =</span> T))</span>
<span id="cb28-9"><a></a><span class="sc">+</span> ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-10"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb28-11"><a></a><span class="sc">+</span>     <span class="at">z =</span> (a<span class="sc">*</span>b)<span class="sc">/</span><span class="dv">2</span> <span class="sc">+</span> c <span class="sc">+</span> <span class="fu">sample</span>(<span class="sc">-</span><span class="dv">10</span><span class="sc">:</span><span class="dv">10</span>, n, <span class="at">replace =</span> T)<span class="sc">/</span><span class="dv">10</span></span>
<span id="cb28-12"><a></a><span class="sc">+</span>     , <span class="at">.before =</span> <span class="dv">1</span></span>
<span id="cb28-13"><a></a><span class="sc">+</span>   )</span>
<span id="cb28-14"><a></a><span class="sc">&gt;</span> <span class="co"># cross validate to get the best alpha</span></span>
<span id="cb28-15"><a></a><span class="er">&gt;</span> alpha_dat <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>( <span class="at">alpha =</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="fl">0.01</span>) ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-16"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb28-17"><a></a><span class="sc">+</span>     <span class="at">mse =</span></span>
<span id="cb28-18"><a></a><span class="sc">+</span>       purrr<span class="sc">::</span><span class="fu">map_dbl</span>(</span>
<span id="cb28-19"><a></a><span class="sc">+</span>         alpha</span>
<span id="cb28-20"><a></a><span class="sc">+</span>         , (\(a){</span>
<span id="cb28-21"><a></a><span class="sc">+</span>           cvg <span class="ot">&lt;-</span> </span>
<span id="cb28-22"><a></a><span class="sc">+</span>            glmnet<span class="sc">::</span><span class="fu">cv.glmnet</span>(</span>
<span id="cb28-23"><a></a><span class="sc">+</span>              <span class="at">x =</span> dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>z) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb28-24"><a></a><span class="sc">+</span>              , <span class="at">y =</span> dat<span class="sc">$</span>z </span>
<span id="cb28-25"><a></a><span class="sc">+</span>              , <span class="at">family =</span> <span class="st">"gaussian"</span></span>
<span id="cb28-26"><a></a><span class="sc">+</span>              , <span class="at">gamma =</span> a</span>
<span id="cb28-27"><a></a><span class="sc">+</span>           )</span>
<span id="cb28-28"><a></a><span class="sc">+</span>           <span class="fu">min</span>(cvg<span class="sc">$</span>cvm)</span>
<span id="cb28-29"><a></a><span class="sc">+</span>         })</span>
<span id="cb28-30"><a></a><span class="sc">+</span>       )</span>
<span id="cb28-31"><a></a><span class="sc">+</span>   ) </span>
<span id="cb28-32"><a></a><span class="sc">&gt;</span> </span>
<span id="cb28-33"><a></a><span class="er">&gt;</span> best_alpha <span class="ot">&lt;-</span> alpha_dat <span class="sc">%&gt;%</span> </span>
<span id="cb28-34"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">filter</span>(mse <span class="sc">==</span> <span class="fu">min</span>(mse)) <span class="sc">%&gt;%</span> </span>
<span id="cb28-35"><a></a><span class="sc">+</span>   dplyr<span class="sc">::</span><span class="fu">pull</span>(alpha)</span>
<span id="cb28-36"><a></a><span class="sc">&gt;</span> </span>
<span id="cb28-37"><a></a><span class="er">&gt;</span> <span class="fu">cat</span>(<span class="st">"best alpha:"</span>, best_alpha)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>best alpha: 0.64</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>elastic net example, part 2</summary>
<div class="sourceCode cell-code" id="cb30" data-code-line-numbers="1-6|8-9|11-15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a></a><span class="sc">&gt;</span> elastic_cv <span class="ot">&lt;-</span> </span>
<span id="cb30-2"><a></a><span class="sc">+</span>   glmnet<span class="sc">::</span><span class="fu">cv.glmnet</span>(</span>
<span id="cb30-3"><a></a><span class="sc">+</span>     <span class="at">x =</span> dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>z) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb30-4"><a></a><span class="sc">+</span>     , <span class="at">y =</span> dat<span class="sc">$</span>z </span>
<span id="cb30-5"><a></a><span class="sc">+</span>     , <span class="at">family =</span> <span class="st">"gaussian"</span></span>
<span id="cb30-6"><a></a><span class="sc">+</span>     , <span class="at">gamma =</span> best_alpha)</span>
<span id="cb30-7"><a></a><span class="sc">&gt;</span> </span>
<span id="cb30-8"><a></a><span class="er">&gt;</span> best_lambda <span class="ot">&lt;-</span> elastic_cv<span class="sc">$</span>lambda.min</span>
<span id="cb30-9"><a></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">"best lambda:"</span>, best_lambda)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>best lambda: 0.01015384</code></pre>
</div>
<details class="code-fold">
<summary>elastic net example, part 2</summary>
<div class="sourceCode cell-code" id="cb32" data-code-line-numbers="1-6|8-9|11-15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a></a><span class="sc">&gt;</span> elastic_mod <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">glmnet</span>(</span>
<span id="cb32-2"><a></a><span class="sc">+</span>   <span class="at">x =</span> dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>z) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() </span>
<span id="cb32-3"><a></a><span class="sc">+</span>   , <span class="at">y =</span> dat<span class="sc">$</span>z </span>
<span id="cb32-4"><a></a><span class="sc">+</span>   , <span class="at">family =</span> <span class="st">"gaussian"</span></span>
<span id="cb32-5"><a></a><span class="sc">+</span>   , <span class="at">gamma =</span> best_alpha, <span class="at">lambda =</span> best_lambda)</span>
<span id="cb32-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb32-7"><a></a><span class="er">&gt;</span> elastic_mod <span class="sc">%&gt;%</span> broom<span class="sc">::</span><span class="fu">tidy</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 5
  term         step estimate lambda dev.ratio
  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)     1   -0.467 0.0102     0.963
2 a               1    0.221 0.0102     0.963
3 b               1    0.560 0.0102     0.963
4 c               1    1.03  0.0102     0.963</code></pre>
</div>
</div>
</section>
<section id="elastic-net-regression-example-1" class="slide level2">
<h2>Elastic Net Regression Example</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>elastic net example, part 3</summary>
<div class="sourceCode cell-code" id="cb34" data-code-line-numbers="1|3-5|7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a></a><span class="sc">&gt;</span> pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(elastic_mod, dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>z) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>())</span>
<span id="cb34-2"><a></a><span class="sc">&gt;</span> </span>
<span id="cb34-3"><a></a><span class="er">&gt;</span> rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>( (pred <span class="sc">-</span> dat<span class="sc">$</span>z)<span class="sc">^</span><span class="dv">2</span> ))</span>
<span id="cb34-4"><a></a><span class="sc">&gt;</span> R2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> (<span class="fu">sum</span>((dat<span class="sc">$</span>z <span class="sc">-</span> pred )<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">sum</span>((dat<span class="sc">$</span>z <span class="sc">-</span> <span class="fu">mean</span>(y))<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb34-5"><a></a><span class="sc">&gt;</span> mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((dat<span class="sc">$</span>z <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb34-6"><a></a><span class="sc">&gt;</span> </span>
<span id="cb34-7"><a></a><span class="er">&gt;</span> <span class="fu">cat</span>(<span class="st">" RMSE:"</span>, rmse, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, <span class="st">"R-squared:"</span>, R2, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, <span class="st">"MSE:"</span>, mse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> RMSE: 0.5817823 
 R-squared: 0.9999828 
 MSE: 0.3384707</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>elastic net example, part 4</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a></a><span class="sc">&gt;</span> dat <span class="sc">%&gt;%</span> </span>
<span id="cb36-2"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb36-3"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">add_column</span>(<span class="at">pred =</span> pred[,<span class="dv">1</span>]) <span class="sc">%&gt;%</span> </span>
<span id="cb36-4"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">rowid_to_column</span>(<span class="st">"ID"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb36-5"><a></a><span class="sc">+</span>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>ID, <span class="at">y=</span>z)) <span class="sc">+</span></span>
<span id="cb36-6"><a></a><span class="sc">+</span>   <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb36-7"><a></a><span class="sc">+</span>   <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>pred),<span class="at">color=</span><span class="st">'red'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="origBSMM_8740_lec_04_files/figure-revealjs/unnamed-chunk-17-1.png" class="quarto-figure quarto-figure-center r-stretch" width="960"></section>
<section id="generalized-linear-models" class="slide level2">
<h2>Generalized Linear Models</h2>
<p>A <strong>generalized linear model</strong> (<strong>GLM</strong>) is a flexible generalization of ordinary linear regression.</p>
<p>Ordinary linear regression predicts the expected value of the outcome variable, a random variable, as a linear combination of a set of observed values (<em>predictors</em>). In a generalized linear model (GLM), each outcome <span class="math inline">\(Y\)</span> is assumed to be generated from a particular distribution in an exponential family, The mean, <span class="math inline">\(\mu\)</span>, of the distribution depends on the independent variables, <span class="math inline">\(X\)</span>, through:</p>
<p><span class="math display">\[
\mathbb{E}\left[\left.Y\right|X\right]=\mu=\text{g}^{-1}\left(X\beta\right)
\]</span> where <span class="math inline">\(g\)</span> is called the <strong>link function</strong>.</p>
</section>
<section id="generalized-linear-models-1" class="slide level2">
<h2>Generalized Linear Models</h2>
<p>For example, if <span class="math inline">\(Y\)</span> is Poisson distributed, then</p>
<p><span class="math display">\[
\mathbb{P}\left[\left.Y=y\right|X,\lambda\right]=\frac{\lambda^{y}}{y!}e^{-\lambda}=e^{y\log\lambda-\lambda-\log y!}
\]</span></p>
<p>Where <span class="math inline">\(\lambda\)</span> is both the mean and the variance. In the glm the link function is <span class="math inline">\(\log\)</span> and</p>
<p><span class="math display">\[
\log\mathbb{E}\left[\left.Y\right|X\right] = \beta X=\log\lambda
\]</span></p>
</section></section>
<section>
<section id="tree-regression" class="title-slide slide level1 center">
<h1>Tree regression</h1>

</section>
<section id="regression-with-trees" class="slide level2">
<h2>Regression with trees</h2>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a></a><span class="sc">&gt;</span> dat <span class="ot">&lt;-</span> MASS<span class="sc">::</span>Boston</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>There are many methodologies for constructing regression trees but one of the oldest is known as the <strong>c</strong>lassification <strong>a</strong>nd <strong>r</strong>egression <strong>t</strong>ree (CART) approach.</p>
<p>Basic regression trees <em>partition</em> a data set into smaller subgroups and then fit a simple <em>constant</em> for each observation in the subgroup. The partitioning is achieved by successive binary partitions (aka&nbsp;<em>recursive partitioning</em>) based on the different predictors.</p>
</section>
<section id="regression-with-trees-1" class="slide level2">
<h2>Regression with trees</h2>
<p>As a simple example, consider a continuous response variable <span class="math inline">\(y\)</span> with two covariates <span class="math inline">\(x_1,x_2\)</span> and the support of <span class="math inline">\(x_1,x_2\)</span> partitioned into three regions. Then we write the tree regression model for <span class="math inline">\(y\)</span> as:</p>
<p><span class="math display">\[
\hat{y} = \hat{f}(x_1,x_2)=\sum_{i=1}^{3}c_1\times I_{(x_1,x_2)\in R_i}
\]</span> Tree algorithm differ in how they grow the regression tree, i.e.&nbsp;partition the space of the covariates.</p>
</section>
<section id="regression-with-trees-2" class="slide level2">
<h2>Regression with trees</h2>
<p>All partitioning of variables is done in a top-down, greedy fashion. This just means that a partition performed earlier in the tree will not change based on later partitions. In general the partitions are made to minimize following objective function (support initially partitioned into 2 regions, i.e.&nbsp;a binary tree):</p>
<p><span class="math display">\[
\text{SSE}=\left\{ \sum_{i\in R_{1}}\left(y_{i}-c_{i}\right)^{2}+\sum_{i\in R_{2}}\left(y_{i}-c_{i}\right)^{2}\right\}
\]</span></p>
</section>
<section id="regression-with-trees-3" class="slide level2">
<h2>Regression with trees</h2>
<p>Having found the best split, we repeat the splitting process on each of the two regions.</p>
<p>This process is continued until some stopping criterion is reached. What typically results is a very deep, complex tree that may produce good predictions on the training set, but is likely to overfit the data, particularly at the lower nodes.</p>
<p>By pruning these lower level nodes, we can introduce a little bit of bias in our model that help to stabilize predictions and will tend to generalize better to new, unseen data.</p>
</section>
<section id="regression-with-trees-4" class="slide level2">
<h2>Regression with trees</h2>
<p>As with penalized linear regression, we can us a complexity parameter <span class="math inline">\(\alpha\)</span> to penalize the number of terminal nodes of the tree (<span class="math inline">\(T\)</span>), like the lasso <span class="math inline">\(L_1\)</span> norm penalty, and find the smallest tree with lowest penalized error, i.e.&nbsp;the minimizing the following objective function:</p>
<p><span class="math display">\[
\text{SSE}+\alpha\left|T\right|
\]</span></p>
</section>
<section id="regression-with-trees-5" class="slide level2">
<h2>Regression with trees</h2>
<div class="columns">
<div class="column" style="font-size: 32px">
<p>Strengths</p>
<ul>
<li>They are very interpretable.</li>
<li>Making predictions is fast; just lookup constants in the tree.</li>
<li>Variables importance is easy; those variables that most reduce the SSE.</li>
<li>Tree models give a non-linear response; better if the true regression surface is not smooth.</li>
<li>There are fast, reliable algorithms to learn these trees.</li>
</ul>
</div><div class="column" style="font-size: 32px">
<p>Weaknesses</p>
<ul>
<li>Single regression trees have high variance, resulting in unstable predictions (an alternative subsample of training data can significantly change the terminal nodes).</li>
<li>Due to the high variance single regression trees have poor predictive accuracy.</li>
</ul>
</div></div>
</section>
<section id="regression-with-trees-bagging" class="slide level2">
<h2>Regression with trees (Bagging)</h2>
<p>As mentioned, single tree models suffer from high variance. Although pruning the tree helps reduce this variance, there are alternative methods that actually exploite the variability of single trees in a way that can significantly improve performance over and above that of single trees. <em><strong>B</strong>ootstrap</em> <em><strong>agg</strong>regat<strong>ing</strong></em> (<strong><em>bagging</em></strong>) is one such approach.</p>
<p>Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance.</p>
</section>
<section id="regression-with-trees-bagging-1" class="slide level2">
<h2>Regression with trees (Bagging)</h2>
<p>Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, improving predictive performance.</p>
</section>
<section id="regression-with-trees-bagging-2" class="slide level2">
<h2>Regression with trees (Bagging)</h2>
<p>Bagging follows three steps:</p>
<ul>
<li>Create <span class="math inline">\(m\)</span> <a href="http://uc-r.github.io/bootstrapping">bootstrap samples</a> from the training data. Bootstrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.</li>
<li>For each bootstrap sample train a single, unpruned regression tree.</li>
<li>Average individual predictions from each tree to create an overall average predicted value.</li>
</ul>
</section>
<section id="regression-with-trees-bagging-3" class="slide level2">
<h2>Regression with trees (Bagging)</h2>

<img data-src="https://uc-r.github.io/public/images/analytics/regression_trees/bagging3.png" class="r-stretch quarto-figure-center"><p class="caption">Fig: The bagging process.</p></section>
<section id="regression-with-a-random-forest" class="slide level2">
<h2>Regression with a random forest</h2>
<p>Bagging trees introduces a random component into the tree building process that reduces the variance of a single tree’s prediction and improves predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree.</p>
<p>So trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships. They are correlated.</p>
</section>
<section id="regression-with-a-random-forest-1" class="slide level2">
<h2>Regression with a random forest</h2>
<p>Tree correlation prevents bagging from optimally reducing the variance of the predictive values. Reducing variance further can be achieved by injecting more randomness into the tree-growing process. Random forests achieve this in two ways:</p>
<div style="font-size: smaller">
<ol type="1">
<li><strong>Bootstrap</strong>: similar to bagging - each tree is grown from a bootstrap resampled data set, which <em>somewhat</em> decorrelates them.</li>
<li><strong>Split-variable randomization</strong>: each time a split is made, the search for the split variable is limited to a random subset of <span class="math inline">\(m\)</span> of the <span class="math inline">\(p\)</span> variables.</li>
</ol>
</div>
</section>
<section id="regression-with-a-random-forest-2" class="slide level2">
<h2>Regression with a random forest</h2>
<p>For regression trees, typical default values used in split-value randomization are <span class="math inline">\(m=\frac{p}{3}\)</span> but this should be considered a tuning parameter.</p>
<p>When <span class="math inline">\(m=p\)</span>, the randomization amounts to using only step 1 and is the same as <em>bagging</em>.</p>
</section>
<section id="regression-with-a-random-forest-3" class="slide level2">
<h2>Regression with a random forest</h2>
<div class="columns">
<div class="column" style="font-size: 32px">
<p>Strengths</p>
<ul>
<li>Typically have very good performance</li>
<li>Remarkably good “out-of-the box” - very little tuning required</li>
<li>Built-in validation set - don’t need to sacrifice data for extra validation</li>
<li>No pre-processing required</li>
<li>Robust to outliers</li>
</ul>
</div><div class="column" style="font-size: 32px">
<p>Weaknesses</p>
<ul>
<li>Can become slow on large data sets</li>
<li>Although accurate, often cannot compete with advanced boosting algorithms</li>
<li>Less interpretable</li>
</ul>
</div></div>
</section>
<section id="regression-with-gradient-boosting" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions.</p>
</section>
<section id="regression-with-gradient-boosting-1" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Whereas <a href="http://uc-r.github.io/random_forests">random forests</a> build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms.</p>
</section>
<section id="regression-with-gradient-boosting-2" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>The main idea of boosting is to add new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.</p>

<img data-src="../images/boosted-trees-process.png" class="r-stretch quarto-figure-center"><p class="caption">Sequential ensemble approach.</p></section>
<section id="regression-with-gradient-boosting-3" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Boosting is a framework that iteratively improves <em>any</em> weak learning model. Many gradient boosting applications allow you to “plug in” various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner.</p>
</section>
<section id="regression-with-gradient-boosting-4" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>A weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. With regards to decision trees, shallow trees represent a weak learner. Commonly, trees with only 1-6 splits are used.</p>
</section>
<section id="regression-with-gradient-boosting-5" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Combining many weak models (versus strong ones) has a few benefits:</p>
<div style="font-size: smaller">
<ul>
<li>Speed: Constructing weak models is computationally cheap.</li>
<li>Accuracy improvement: Weak models allow the algorithm to <em>learn slowly</em>; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.</li>
<li>Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).</li>
</ul>
</div>
</section>
<section id="regression-with-gradient-boosting-6" class="slide level2">
<h2>Regression with gradient boosting</h2>
<p>Here is the algorithm for boosted regression trees with features <span class="math inline">\(x\)</span> and response <span class="math inline">\(y\)</span>:</p>
<div style="font-size: smaller">
<ol type="1">
<li>Fit a decision tree to the data: <span class="math inline">\(F_1(x)=y\)</span>,</li>
<li>We then fit the next decision tree to the residuals of the previous: <span class="math inline">\(h_1(x)=y−F_1(x)\)</span></li>
<li>Add this new tree to our algorithm: <span class="math inline">\(F_2(x)=F_1(x)+h_1(x)\)</span>,</li>
<li>Fit the next decision tree to the residuals of <span class="math inline">\(F_2: h_2(x)=y−F_2(x)\)</span>,</li>
<li>Add this new tree to our algorithm: <span class="math inline">\(F_3(x)=F_2(x)+h_1(x)\)</span>,</li>
<li>Continue this process until some mechanism (i.e.&nbsp;cross validation) tells us to stop.</li>
</ol>
</div>
</section>
<section id="xgboost-example" class="slide level2">
<h2>XGBoost Example</h2>
<p><strong>XGBoost</strong> is short for e<strong>X</strong>treme <strong>G</strong>radient <strong>Boost</strong>ing package.</p>
<p>While the <code>XGBoost</code> model often achieves higher accuracy than a single decision tree, it sacrifices the intrinsic interpretability of decision trees. For example, following the path that a decision tree takes to make its decision is trivial and self-explained, but following the paths of hundreds or thousands of trees is much harder.</p>
<p>We will work with <code>XGBoost</code> in today’s lab.</p>
</section>
<section id="regression-with-neural-nets" class="slide level2">
<h2>Regression with neural nets</h2>
<p>Architecture of a Neural Net (NN)</p>

<img data-src="../images/single_layer_nn.png" alt="credit deep learning.a" class="r-stretch quarto-figure-center"><p class="caption">Single layer NN architecture</p></section>
<section id="regression-with-neural-nets-1" class="slide level2">
<h2>Regression with neural nets</h2>

<img data-src="../images/activation_functions.png" alt="credits - analyticsindiamag" class="r-stretch quarto-figure-center"><p class="caption">Common Activation Functions</p></section>
<section id="regression-with-neural-nets-2" class="slide level2">
<h2>Regression with neural nets</h2>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38" data-code-line-numbers="1|3-4|6-11|13-17|19-25|27-28|30-39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a></a><span class="sc">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">500</span>)</span>
<span id="cb38-2"><a></a><span class="sc">&gt;</span>   </span>
<span id="cb38-3"><a></a><span class="er">&gt;</span> <span class="co"># Boston dataset from MASS</span></span>
<span id="cb38-4"><a></a><span class="er">&gt;</span> data <span class="ot">&lt;-</span> MASS<span class="sc">::</span>Boston</span>
<span id="cb38-5"><a></a><span class="sc">&gt;</span> </span>
<span id="cb38-6"><a></a><span class="er">&gt;</span> <span class="co"># Normalize the data</span></span>
<span id="cb38-7"><a></a><span class="er">&gt;</span> maxs <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">summarise_all</span>(max) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> <span class="fu">as.vector</span>()</span>
<span id="cb38-8"><a></a><span class="sc">&gt;</span> mins <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">summarise_all</span>(min) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> <span class="fu">as.vector</span>()</span>
<span id="cb38-9"><a></a><span class="sc">&gt;</span> data_scaled <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> </span>
<span id="cb38-10"><a></a><span class="sc">+</span>   <span class="fu">scale</span>(<span class="at">center =</span> mins, <span class="at">scale =</span> maxs <span class="sc">-</span> mins) <span class="sc">%&gt;%</span> </span>
<span id="cb38-11"><a></a><span class="sc">+</span>   tibble<span class="sc">::</span><span class="fu">as_tibble</span>()</span>
<span id="cb38-12"><a></a><span class="sc">&gt;</span>   </span>
<span id="cb38-13"><a></a><span class="er">&gt;</span> <span class="co"># Split the data into training and testing set</span></span>
<span id="cb38-14"><a></a><span class="er">&gt;</span> data_split <span class="ot">&lt;-</span> data_scaled <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(<span class="at">prop =</span> .<span class="dv">75</span>)</span>
<span id="cb38-15"><a></a><span class="sc">&gt;</span> <span class="co"># extracting training data and test data as two seperate dataframes</span></span>
<span id="cb38-16"><a></a><span class="er">&gt;</span> data_train <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(data_split)</span>
<span id="cb38-17"><a></a><span class="sc">&gt;</span> data_test  <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(data_split)</span>
<span id="cb38-18"><a></a><span class="sc">&gt;</span> </span>
<span id="cb38-19"><a></a><span class="er">&gt;</span> nn <span class="ot">&lt;-</span> data_train <span class="sc">%&gt;%</span> </span>
<span id="cb38-20"><a></a><span class="sc">+</span>   neuralnet<span class="sc">::</span><span class="fu">neuralnet</span>(</span>
<span id="cb38-21"><a></a><span class="sc">+</span>     medv <span class="sc">~</span> .</span>
<span id="cb38-22"><a></a><span class="sc">+</span>     , <span class="at">data =</span> .</span>
<span id="cb38-23"><a></a><span class="sc">+</span>     , <span class="at">hidden =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb38-24"><a></a><span class="sc">+</span>     , <span class="at">linear.output =</span> <span class="cn">TRUE</span></span>
<span id="cb38-25"><a></a><span class="sc">+</span>   )</span>
<span id="cb38-26"><a></a><span class="sc">&gt;</span>   </span>
<span id="cb38-27"><a></a><span class="er">&gt;</span> <span class="co"># Predict on test data</span></span>
<span id="cb38-28"><a></a><span class="er">&gt;</span> pr.nn <span class="ot">&lt;-</span> neuralnet<span class="sc">::</span><span class="fu">compute</span>( nn, data_test <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>medv) )</span>
<span id="cb38-29"><a></a><span class="sc">&gt;</span>   </span>
<span id="cb38-30"><a></a><span class="er">&gt;</span> <span class="co"># Compute mean squared error</span></span>
<span id="cb38-31"><a></a><span class="er">&gt;</span> pr.nn_ <span class="ot">&lt;-</span> </span>
<span id="cb38-32"><a></a><span class="sc">+</span>   pr.nn<span class="sc">$</span>net.result <span class="sc">*</span> </span>
<span id="cb38-33"><a></a><span class="sc">+</span>   (<span class="fu">max</span>(data<span class="sc">$</span>medv) <span class="sc">-</span> <span class="fu">min</span>(data<span class="sc">$</span>medv)) <span class="sc">+</span></span>
<span id="cb38-34"><a></a><span class="sc">+</span>   <span class="fu">min</span>(data<span class="sc">$</span>medv)</span>
<span id="cb38-35"><a></a><span class="sc">&gt;</span> test.r <span class="ot">&lt;-</span> </span>
<span id="cb38-36"><a></a><span class="sc">+</span>   data_test<span class="sc">$</span>medv <span class="sc">*</span> </span>
<span id="cb38-37"><a></a><span class="sc">+</span>   (<span class="fu">max</span>(data<span class="sc">$</span>medv) <span class="sc">-</span> <span class="fu">min</span>(data<span class="sc">$</span>medv)) <span class="sc">+</span> </span>
<span id="cb38-38"><a></a><span class="sc">+</span>   <span class="fu">min</span>(data<span class="sc">$</span>medv)</span>
<span id="cb38-39"><a></a><span class="sc">&gt;</span> MSE.nn <span class="ot">&lt;-</span> <span class="fu">sum</span>((test.r <span class="sc">-</span> pr.nn_)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">nrow</span>(data_test)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell quarto-layout-panel" data-layout-nrow="3" data-layout-align="center">

</div>
</section>
<section id="regression-with-neural-nets-3" class="slide level2">
<h2>Regression with neural nets</h2>
<div class="panel-tabset">
<ul id="tabset-2" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-2-1">NN</a></li><li><a href="#tabset-2-2">Regression</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/nn_plot.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
<div id="tabset-2-2">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="origBSMM_8740_lec_04_files/figure-revealjs/unnamed-chunk-20-1.png" class="quarto-figure quarto-figure-center" width="960"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap" class="slide level2">
<h2>Recap</h2>
<ul>
<li><p>Today we worked though a few regression methods that are useful for predicting a value given a set of covariates</p></li>
<li><p>Next week we will look at the <code>tidymodels</code> package which will give a way to develop a workflow for fitting and comparing our models</p></li>
</ul>
</section>
<section id="section" class="slide level2">
<h2></h2>


</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="images/logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://bsmm-8740-fall-2023.github.io/osb/">bsmm-8740-fall-2023.github.io/osb</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="../site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true,"numbers":true},
'multiplex': {"secret":null,"id":"263f2c54a906dcf5","url":"https://reveal-multiplex.glitch.me/"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/bsmm-8740-fall-2024\.github\.io\/osb\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>