---
title: "Creating and Preprocessing a Design Matrix with Recipes"
subtitle: "BSMM-8740 - Fall 2023"
author: "derived from Max Kuhn (RStudio)"
footer:  "[bsmm-8740-fall-2023.github.io/osb](https://bsmm-8740-fall-2023.github.io/osb/)"
logo: "images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
editor: visual
execute:
  freeze: auto
---

```{r opts, include = FALSE}
options(width = 90)
library(knitr)
opts_chunk$set(comment="", 
               digits = 3, 
               tidy = FALSE, 
               prompt = TRUE,
               fig.align = 'center')

# check if 'librarian' is installed and if not, install it
if (! "librarian" %in% rownames(installed.packages()) ){
  install.packages("librarian")
}
  
# load packages if not already loaded
librarian::shelf(
  magrittr, ggplot2, mlbench, caret, recipes, lattice, kernlab)

data("PimaIndiansDiabetes")
data("Sacramento")

theme_set(theme_bw() + theme(legend.position = "top"))
```

## R Model Formulas

A simple formula in a linear model to predict house prices:

```{r sac}
#| echo: true
#| message: false
#| results: false
mod1 <- stats::lm(
  log(price) ~ type + sqft
  , data = Sacramento
  , subset = beds > 2
  )
```

::: {style="font-size: 80%"}
The purpose of this code chunk:

1.  subset some of the data points (`subset`)
2.  create a design matrix for 2 predictor variable (but 3 model terms)
3.  log transform the outcome variable
4.  fit a linear regression model
:::

The first two steps create the *design matrix*.

## Summary: Model Formula Method

-   Model formulas are very expressive in that they can represent model terms easily
-   The formula/terms framework does some elegant functional programming
-   Functions can be embedded inline to do fairly complex things (on single variables) and these can be applied to new data sets.

## Summary: Model Formula Method

*However*, there are significant limitations to what this framework can do and, in some cases, it can be very inefficient.

This is mostly due of being written well before large scale modeling and machine learning were commonplace.

## Limitations of the Current System

-   Formulas are not very extensible especially with nested or sequential operations (e.g. `y ~ scale(center(knn_impute(x)))`).
-   When used in modeling functions, you cannot recycle the previous computations.
-   For wide data sets, the formula method can be very inefficient and consume a significant proportion of the total execution time.

## Limitations of the Current System

-   Multivariate outcomes are kludgy by requiring `cbind`
-   Formulas have a limited set of roles (next two slides)

A more in-depth discussion of these issues can be found in [this blog post](https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/).

## Variable Roles

Formulas have been re-implemented in different packages for a variety of different reasons:

```{r roles_1}
#| echo: true
#| message: false
#| results: false
#| eval: false
# ?lme4::lmer
# Subjects need to be in the data but are not part of the model
lme4::lmer(Reaction ~ Days + (Days | Subject), data = lme4::sleepstudy)

# BradleyTerry2
# We want to make the outcomes to be a function of a 
# competitor-specific function of reach 
BradleyTerry2::BTm(outcome = 1, player1 = winner, player2 = loser,
    formula = ~ SVL[..] + (1|..), 
    data = BradleyTerry2::flatlizards)

# modeltools::ModelEnvFormula (using the modeltools package for formulas)
# mob
data(PimaIndiansDiabetes, package = 'mlbench')
modeltools::ModelEnvFormula(diabetes ~ glucose | pregnant + mass +  age,
    data = PimaIndiansDiabetes)

```

## Variable Roles

::: {style="font-size: 80%"}
A general list of possible variables roles could be

-   outcomes
-   predictors
-   stratification
-   model performance data (e.g. loan amount to compute expected loss)
-   conditioning or faceting variables (e.g. [`lattice`](https://cran.r-project.org/package=lattice) or [`ggplot2`](https://cran.r-project.org/package=ggplot2))
-   random effects or hierarchical model ID variables
-   case weights (\*)
-   offsets (\*)
-   error terms (limited to `Error` in the `aov` function)(\*)

(\*) Can be handled in formulas but are hard-coded into functions.
:::

## Recipes

We can approach the design matrix and preprocessing steps by first specifying a **sequence of steps**.

1.  `price` is an outcome
2.  `type` and `sqft` are predictors
3.  log transform `price`
4.  convert `type` to dummy variables

## Recipes

A recipe is a specification of *intent*.

One issue with the formula method is that it couples the specification for your predictors along with the implementation.

Recipes, as you'll see, separates the *planning* from the *doing*.

Website: [`https://topepo.github.io/recipes`](https://topepo.github.io/recipes)

## Recipes

A *recipe* can be trained then applied to any data.

```{r rec_basic}
#| echo: true
#| message: false
## Create an initial recipe with only predictors and outcome
rec <- recipes::recipe(price ~ type + sqft, data = Sacramento)

rec <- rec %>% 
  recipes::step_log(price) %>%
  recipes::step_dummy(type)

rec_trained <- recipes::prep(rec, training = Sacramento, retain = TRUE)

design_mat  <- recipes::bake(rec_trained, new_data = Sacramento)
```

## Selecting Variables

In the last slide, we used `dplyr`-like syntax for selecting variables such as `step_dummy(type)`.

In some cases, the names of the predictors may not be known at the time when you construct a recipe (or model formula). For example:

-   dummy variable columns
-   PCA feature extraction when you keep components that capture $X$% of the variability.
-   discretized predictors with dynamic bins

## Selecting Variables

`dplyr` selectors can also be used on variables names, such as

```{r step_match}
#| echo: true
#| message: false
#| eval: false
rec %>% 
  recipes::step_spatialsign(
    matches("^PC[1-9]")
    , all_numeric()
    , -all_outcomes()
  )
```

Variables can be selected by name, role, data type, or any [combination of these](https://recipes.tidymodels.org/articles/Selecting_Variables.html).

## Extending

Need to add more preprocessing or other operations?

```{r rec_add}
#| echo: true
#| message: false
#| results: false
standardized <- rec_trained %>%
  recipes::step_center(recipes::all_numeric()) %>%
  recipes::step_scale(recipes::all_numeric()) %>%
  recipes::step_pca(recipes::all_numeric())
          
## Only estimate the new parts:
standardized <- recipes::prep(standardized)
```

If an initial step is computationally expensive, you don't have to redo those operations to add more.

## Available Steps

-   **Basic**: [logs](https://recipes.tidymodels.org/reference/step_log.html), [roots](https://recipes.tidymodels.org/reference/step_sqrt.html), [polynomials](https://recipes.tidymodels.org/reference/step_poly.html), [logits](https://recipes.tidymodels.org/reference/step_logit.html), [hyperbolics](https://recipes.tidymodels.org/reference/step_hyperbolic.html)
-   **Encodings**: [dummy variables](https://recipes.tidymodels.org/reference/step_dummy.html), ["other"](https://recipes.tidymodels.org/reference/step_other.html) factor level collapsing, [discretization](https://recipes.tidymodels.org/reference/discretize.html)
-   **Date Features**: Encodings for [day/doy/month](https://recipes.tidymodels.org/reference/step_date.html) etc, [holiday indicators](https://recipes.tidymodels.org/reference/step_holiday.html)
-   **Filters**: [correlation](https://recipes.tidymodels.org/reference/step_corr.html), [near-zero variables](https://recipes.tidymodels.org/reference/step_nzv.html), [linear dependencies](https://recipes.tidymodels.org/reference/step_lincomb.html)
-   **Imputation**: [*K*-nearest neighbors](https://recipes.tidymodels.org/reference/step_knnimpute.html), [bagged trees](https://recipes.tidymodels.org/reference/step_bagimpute.html), [mean](https://recipes.tidymodels.org/reference/step_meanimpute.html)/[mode](https://recipes.tidymodels.org/reference/step_modeimpute.html) imputation,

## Available Steps

-   **Normalization/Transformations**: [center](https://recipes.tidymodels.org/reference/step_center.html), [scale](https://recipes.tidymodels.org/reference/step_scale.html), [range](https://recipes.tidymodels.org/reference/step_range.html), [Box-Cox](https://recipes.tidymodels.org/reference/step_BoxCox.html), [Yeo-Johnson](https://recipes.tidymodels.org/reference/step_YeoJohnson.html)
-   **Dimension Reduction**: [PCA](https://recipes.tidymodels.org/reference/step_pca.html), [kernel PCA](https://recipes.tidymodels.org/reference/step_kpca.html), [ICA](https://recipes.tidymodels.org/reference/step_ica.html), [Isomap](https://recipes.tidymodels.org/reference/step_isomap.html), [data depth](https://recipes.tidymodels.org/reference/step_depth.html) features, [class distances](https://recipes.tidymodels.org/reference/step_classdist.html)
-   **Others**: [spline basis functions](https://recipes.tidymodels.org/reference/step_ns.html), [interactions](https://recipes.tidymodels.org/reference/step_interact.html), [spatial sign](https://recipes.tidymodels.org/reference/step_spatialsign.html)

More on the way (i.e. autoencoders, more imputation methods, etc.)

One of the [package vignettes](https://www.tidymodels.org/learn/develop/recipes/) shows how to write your own step functions.

## Extending

Recipes can also be created with different roles manually

```{r rec_man, eval = FALSE}
#| echo: true
#| message: false
rec <- 
  recipes::recipe(x  = Sacramento) %>%
  recipes::update_role(price, new_role = "outcome") %>%
  recipes::update_role(type, sqft, new_role = "predictor") %>%
  recipes::update_role(zip, new_role = "strata")
```

Also, the sequential nature of steps means that they don't have to be R operations and could call other compute engines (e.g. Weka, scikit-learn, Tensorflow, etc. )

## Extending

We can create wrappers to work with recipes too:

```{r lm}
#| echo: true
#| message: false
lin_reg.recipe <- function(rec, data, ...) {
  trained <- recipes::prep(rec, training = data)
  lm.fit(
    x = recipes::bake(trained, newdata = data, all_predictors())
    , y = recipes::bake(trained, newdata = data, all_outcomes())
    , ...
  )
}
```

## An Example

[Kuhn and Johnson](http://appliedpredictivemodeling.com) (2013) analyze a data set where thousands of cells are determined to be well-segmented (WS) or poorly segmented (PS) based on 58 image features. We would like to make predictions of the segmentation quality based on these features.

## An Example

```{r image_load}
#| echo: true
#| message: false
data(segmentationData, package = "caret")

seg_train <- segmentationData %>% 
  dplyr::filter(Case == "Train") %>% 
  dplyr::select(-Case, -Cell)

seg_test  <- segmentationData %>% 
  dplyr::filter(Case == "Test")  %>% 
  dplyr::select(-Case, -Cell)
```

## A Simple Recipe

```{r image_rec}
#| echo: true
#| message: false
rec <- recipes::recipe(Class  ~ ., data = seg_train)

basic <- rec %>%
  # Correct some predictors for skewness
  recipes::step_YeoJohnson(recipes::all_predictors()) %>%
  # Standardize the values
  recipes::step_center(recipes::all_predictors()) %>%
  recipes::step_scale(recipes::all_predictors())

# Estimate the transformation and standardization parameters 
basic <- 
  recipes::prep(
    basic
    , training = seg_train
    , verbose = FALSE
    , retain = TRUE
  )  
```

## Principal Component Analysis

```{r image_pca}
#| echo: true
#| message: false
pca <- basic %>% 
  recipes::step_pca(
    recipes::all_predictors()
    , threshold = .9
  )

summary(pca)
```

## Principal Component Analysis

```{r image_pca_train}
#| echo: true
#| message: false
pca %<>% recipes::prep() 

pca %>% summary()
```

## Principal Component Analysis

```{r}
#| echo: true
#| message: false
pca <-
  recipes::bake(
    pca
    , new_data = seg_test
    , everything()
  )
pca[1:4, 1:8]
```

## Principal Component Analysis

```{r image_pca_plot, fig.width = 5.5, fig.height = 5.6}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "PCA1"
ggplot(pca, aes(x = PC01, y = PC02, color = Class)) + 
  geom_point(alpha = .4)
```

## Principal Component Analysis

```{r image_pca_fig, fig.width = 5.5, fig.height = 5.6}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "PCA2"
rngs <- extendrange(c(pca$PC01, pca$PC02))
ggplot(pca, aes(x = PC01, y = PC02, color = Class)) + 
  geom_point(alpha = .4) + 
  xlim(rngs) + ylim(rngs) + 
  theme(legend.position = "top")
```

## Kernel Principal Component Analysis

```{r kpca}
#| echo: true
#| message: false
kern_pca <- basic %>% 
  recipes::step_kpca(
    recipes::all_predictors()
    , num_comp = 2
    , options = 
      list(
        kernel = "rbfdot"
        , kpar = list(sigma = 0.05)
      )
  )

kern_pca <- recipes::prep(kern_pca)

kern_pca <- recipes::bake(kern_pca, new_data = seg_test, everything())
```

## Kernel Principal Component Analysis

```{r image_kpca_fig, fig.width = 5.5, fig.height = 5.6}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "Kernel PCA"
rngs <- extendrange(c(kern_pca$kPC1, kern_pca$kPC2))
ggplot(kern_pca, aes(x = kPC1, y = kPC2, color = Class)) + 
  geom_point(alpha = .4) + 
  xlim(rngs) + ylim(rngs) + 
  theme(legend.position = "top")
```

## Distance to Each Class Centroid

```{r dists}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "Kernel PCA - distance to each centroid"
dist_to_classes <- basic %>% 
  recipes::step_classdist(recipes::all_predictors(), class = "Class") %>%
  # Take log of the new distance features
  recipes::step_log(starts_with("classdist"))

dist_to_classes <- recipes::prep(dist_to_classes, verbose = FALSE)

# All variables are retained plus an additional one for each class
dist_to_classes <- recipes::bake(dist_to_classes, new_data = seg_test, matches("[Cc]lass"))
dist_to_classes
```

## Distance to Each Class

```{r image_dists_fig, fig.width = 5.5, fig.height = 5.6}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "Kernel PCA - distance to each class"
rngs <- extendrange(c(dist_to_classes$classdist_PS, dist_to_classes$classdist_WS))
ggplot(dist_to_classes, aes(x = classdist_PS, y = classdist_WS, color = Class)) + 
  geom_point(alpha = .4) + 
  xlim(rngs) + ylim(rngs) + 
  theme(legend.position = "top") + 
  xlab("Distance to PS Centroid (log scale)") + 
  ylab("Distance to WS Centroid (log scale)")
```

## Next Steps

-   Get it on CRAN once `tidyselect` is on CRAN
-   Add more steps
-   `caret` methods for recipes (instead of using `preProcess`):

``` r
model1 <- train(recipe, data = data, method, ...)
```

as an alternative to

``` r
model2 <- train(x, y, method, preProcess, ...) # or
model3 <- train(y ~ x1 + x2, data = data, method, preProcess, ...)
```

------------------------------------------------------------------------

```{r, echo = FALSE}
sessionInfo()
```
